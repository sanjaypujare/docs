{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to DataTorrent RTS!\n\n\nDataTorrent RTS, built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, and data ingestion and distribution features.  \n\n\n\n  #docs-jumbotron {\n    margin-top: 40px;\n    font-size: 0;\n    background: rgba(0,0,0,0.05);\n    margin-bottom: 20px;\n    box-sizing: border-box;\n    padding: 0;\n    background-color: transparent;\n  }\n\n  .jumbotron {\n    display: -webkit-flex;\n    display: -ms-flexbox;\n    display: flex;\n  }\n\n  .jumbotron-section-space {\n    flex: .07;\n    background-color: transparent;\n  }\n\n  .jumbotron-section {\n    flex: 1;\n    padding-top: 20px;\n    width: 33.3%;\n    display: inline-block;\n    font-size: 18px;\n    vertical-align: top;\n    text-align: center;\n    color: #444 !important;\n    margin-bottom: 0;\n    padding-bottom: 20px;\n    border-radius: 4px;\n    box-shadow: inset 0 0 4px -1px rgba(0,0,0,0.3);\n    background-color: #e3e0eb;\n  }\n  .jumbotron-section:visited {\n    color: #444;\n  }\n  .jumbotron-section img {\n    height: 100px !important;\n    display: block;\n    margin: 10px auto 0;\n  }\n  .jumbotron-section h2 {\n    margin: 0;\n  }\n\n  .jumbotron-section:hover{\n    background-color: #bcb3ce;\n    cursor: pointer;\n  }\n  .jumbotron-section p {\n    padding: 0 1em;\n    margin: 0 auto;\n    max-width: 300px;\n    font-size: 80%;\n  }\n\n  @media all and (max-width: 1032px) and (min-width: 769px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n  @media all and (max-width: 569px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n\n\n\n\n\n  \n\n    \n\n    \nRun Demos\n\n    \nExperience the power of DataTorrent RTS quickly. Import, launch, manage and visualize applications in minutes.\n\n  \n\n\n  \n\n\n  \n\n    \n\n    \nDiscover RTS\n\n    \nLearn about the architecture and the rich feature set of the DataTorrent RTS platform and applications.\n\n  \n  \n\n  \n\n\n  \n\n    \n\n    \nCreate Apps\n\n    \nTutorials and code samples to rapidly create DataTorrent applications using Java or dtAssemble.", 
            "title": "DataTorrent RTS"
        }, 
        {
            "location": "/#welcome-to-datatorrent-rts", 
            "text": "DataTorrent RTS, built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, and data ingestion and distribution features.    \n  #docs-jumbotron {\n    margin-top: 40px;\n    font-size: 0;\n    background: rgba(0,0,0,0.05);\n    margin-bottom: 20px;\n    box-sizing: border-box;\n    padding: 0;\n    background-color: transparent;\n  }\n\n  .jumbotron {\n    display: -webkit-flex;\n    display: -ms-flexbox;\n    display: flex;\n  }\n\n  .jumbotron-section-space {\n    flex: .07;\n    background-color: transparent;\n  }\n\n  .jumbotron-section {\n    flex: 1;\n    padding-top: 20px;\n    width: 33.3%;\n    display: inline-block;\n    font-size: 18px;\n    vertical-align: top;\n    text-align: center;\n    color: #444 !important;\n    margin-bottom: 0;\n    padding-bottom: 20px;\n    border-radius: 4px;\n    box-shadow: inset 0 0 4px -1px rgba(0,0,0,0.3);\n    background-color: #e3e0eb;\n  }\n  .jumbotron-section:visited {\n    color: #444;\n  }\n  .jumbotron-section img {\n    height: 100px !important;\n    display: block;\n    margin: 10px auto 0;\n  }\n  .jumbotron-section h2 {\n    margin: 0;\n  }\n\n  .jumbotron-section:hover{\n    background-color: #bcb3ce;\n    cursor: pointer;\n  }\n  .jumbotron-section p {\n    padding: 0 1em;\n    margin: 0 auto;\n    max-width: 300px;\n    font-size: 80%;\n  }\n\n  @media all and (max-width: 1032px) and (min-width: 769px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n  @media all and (max-width: 569px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }", 
            "title": "Welcome to DataTorrent RTS!"
        }, 
        {
            "location": "/demo_videos/", 
            "text": "DataTorrent RTS Recorded Demos\n\n\nThe following videos provide an introduction to DataTorrent RTS. Application Builder shows how you can develop an application and the next 3 videos show how the platform provides built-in dimension computing, elastic scalability and fault tolerance. \n\n\nApplication Builder\n\n\n\n\n\nDimensional Computing\n\n\n\n\n\nElastic Scalability\n\n\n\n\n\nFault Tolerance", 
            "title": "Videos"
        }, 
        {
            "location": "/demo_videos/#datatorrent-rts-recorded-demos", 
            "text": "The following videos provide an introduction to DataTorrent RTS. Application Builder shows how you can develop an application and the next 3 videos show how the platform provides built-in dimension computing, elastic scalability and fault tolerance.", 
            "title": "DataTorrent RTS Recorded Demos"
        }, 
        {
            "location": "/demo_videos/#application-builder", 
            "text": "", 
            "title": "Application Builder"
        }, 
        {
            "location": "/demo_videos/#dimensional-computing", 
            "text": "", 
            "title": "Dimensional Computing"
        }, 
        {
            "location": "/demo_videos/#elastic-scalability", 
            "text": "", 
            "title": "Elastic Scalability"
        }, 
        {
            "location": "/demo_videos/#fault-tolerance", 
            "text": "", 
            "title": "Fault Tolerance"
        }, 
        {
            "location": "/demos/", 
            "text": "Running Demo Applications\n\n\nDataTorrent RTS includes a number of demo applications and they are available for import from the \nDevelopment \n App Packages\n section of the DataTorrent management console.\n\n\nImporting Demo Applications\n\n\n\n\nNavigate to \nDevelop \n App Packages \n Import\n section of the DataTorrent console.\n\n\nSelect one of the available packages, such as \nApache Apex Malhar Pi Demo\n and click \nImport\n button.\n\n\nImported packages and included applications will be listed under \nDevelop \n App Packages\n page.\n\n\n\n\nLaunching Demo Applications\n\n\nOnce imported, applications can be launched with a single click.  \nNote\n: Ensure Hadoop YARN and HDFS services are active and ready by checking for errors in the DataTorrent console before launching demo applications.\n\n\n\n\n\n\nNavigate to \nApp Packages\n under \nDevelop\n tab of the DataTorrent console, and select one of the imported demo packages.  In this example we will use \nPiDemo\n application package.\n\n\n\n\n\n\nFrom the list of available Applications, locate PiDemo and click the launch button.\n\n\n\n\n\n\n\n\nProceed with default options on launch confirmation screen by clicking the Launch button.\n\n\n\n\n\n\nOnce launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the \nMonitor\n section of the console and selecting the launched application.\n\n\n\n\n\n\n\n\nMore information about using DataTorrent console is available in \ndtManage Guide\n\n\nConfiguring Launch Parameters\n\n\nSome applications may require additional configuration changes prior to launching.  Configuration changes can be made on the launch confirmation screen or manually applied to \n~/.dt/dt-site.xml\n configuration file.  These typically include adding Twitter API keys for twitter demo, or changing performance settings for larger applications.  Guides for various demo applications can be found in the \nCreating Applications\n guide.\n\n\n\n\n\n\nNavigate to \nApp Packages\n under \nDevelop\n tab of the DataTorrent console.  In this example we will use \nApache Apex Malhar Twitter Demo\n application package.  Import this package from \nDevelop \n App Packages \n Import\n if it is not available.\n\n\n\n\n\n\nFrom the list of Applications, select TwitterDemo and press the corresponding launch button.\n\n\n\n\n\n\nRetrieve Twitter API access information by registering for \nTwitter Developer\n account, creating a new \nTwitter Application\n, and navigating to Keys and Access Tokens tab.  Twitter Demo application requires the following to be specified by the user:\n\n\ndt.operator.TweetSampler.accessToken\ndt.operator.TweetSampler.accessTokenSecret\ndt.operator.TweetSampler.consumerKey\ndt.operator.TweetSampler.consumerSecret\n\n\n\n\n\n\nSelect \nSpecify custom properties\n on the launch confirmation screen, click \nadd required properties\n button, and provide Twitter API access values.  Choose to save this configuration as \ntwitter.xml\n file and proceed to Launch the application.\n\n\n\n\n\n\n\n\nOnce launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the \nMonitor\n section of the console and selecting the launched application.\n\n\n\n\n\n\nView the top 10 tweeted hashtags in real time by generating and viewing the \ndashboard\n.\n\n\n\n\n\n\nStopping Applications\n\n\nApplications can be shut down or killed from the Monitor section of \ndtManage\n by selecting application from the list and clicking \nshutdown\n or \nkill\n buttons.", 
            "title": "Running Apps"
        }, 
        {
            "location": "/demos/#running-demo-applications", 
            "text": "DataTorrent RTS includes a number of demo applications and they are available for import from the  Development   App Packages  section of the DataTorrent management console.", 
            "title": "Running Demo Applications"
        }, 
        {
            "location": "/demos/#importing-demo-applications", 
            "text": "Navigate to  Develop   App Packages   Import  section of the DataTorrent console.  Select one of the available packages, such as  Apache Apex Malhar Pi Demo  and click  Import  button.  Imported packages and included applications will be listed under  Develop   App Packages  page.", 
            "title": "Importing Demo Applications"
        }, 
        {
            "location": "/demos/#launching-demo-applications", 
            "text": "Once imported, applications can be launched with a single click.   Note : Ensure Hadoop YARN and HDFS services are active and ready by checking for errors in the DataTorrent console before launching demo applications.    Navigate to  App Packages  under  Develop  tab of the DataTorrent console, and select one of the imported demo packages.  In this example we will use  PiDemo  application package.    From the list of available Applications, locate PiDemo and click the launch button.     Proceed with default options on launch confirmation screen by clicking the Launch button.    Once launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the  Monitor  section of the console and selecting the launched application.     More information about using DataTorrent console is available in  dtManage Guide", 
            "title": "Launching Demo Applications"
        }, 
        {
            "location": "/demos/#configuring-launch-parameters", 
            "text": "Some applications may require additional configuration changes prior to launching.  Configuration changes can be made on the launch confirmation screen or manually applied to  ~/.dt/dt-site.xml  configuration file.  These typically include adding Twitter API keys for twitter demo, or changing performance settings for larger applications.  Guides for various demo applications can be found in the  Creating Applications  guide.    Navigate to  App Packages  under  Develop  tab of the DataTorrent console.  In this example we will use  Apache Apex Malhar Twitter Demo  application package.  Import this package from  Develop   App Packages   Import  if it is not available.    From the list of Applications, select TwitterDemo and press the corresponding launch button.    Retrieve Twitter API access information by registering for  Twitter Developer  account, creating a new  Twitter Application , and navigating to Keys and Access Tokens tab.  Twitter Demo application requires the following to be specified by the user:  dt.operator.TweetSampler.accessToken\ndt.operator.TweetSampler.accessTokenSecret\ndt.operator.TweetSampler.consumerKey\ndt.operator.TweetSampler.consumerSecret    Select  Specify custom properties  on the launch confirmation screen, click  add required properties  button, and provide Twitter API access values.  Choose to save this configuration as  twitter.xml  file and proceed to Launch the application.     Once launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the  Monitor  section of the console and selecting the launched application.    View the top 10 tweeted hashtags in real time by generating and viewing the  dashboard .", 
            "title": "Configuring Launch Parameters"
        }, 
        {
            "location": "/demos/#stopping-applications", 
            "text": "Applications can be shut down or killed from the Monitor section of  dtManage  by selecting application from the list and clicking  shutdown  or  kill  buttons.", 
            "title": "Stopping Applications"
        }, 
        {
            "location": "/sandbox/", 
            "text": "DataTorrent RTS Sandbox\n\n\nWelcome to the DataTorrent Sandbox, an introduction to DataTorrent RTS, the industry\u2019s only unified stream and batch processing platform.  The Sandbox provides a quick and simple way to experience DataTorrent RTS without setting up and managing a complete Hadoop cluster.  The Sandbox contains pre-installed DataTorrent RTS Enterprise Edition along with all the Hadoop services required to launch and run the included demo applications.\n\n\nInstallation\n\n\nIf you have not already, sandbox can be downloaded by visiting \ndatatorrent.com/download\n.  To run the DataTorrent Sandbox, ensure you have downloaded and installed \nVirtualBox\n 4.3 or greater.\n\n\nAccessing Console\n\n\nWhen accessing DataTorrent console in the sandbox for the first time, you will be required to log in.  Use username \ndtadmin\n and password \ndtadmin\n.  Same credentials are also valid for sandbox system access.\n\n\n\n\n\n\n\n\nInside the DataTorrent RTS Sandbox console can be accessed by opening a browser and visiting \nhttp://localhost:9090/\n\n\nRunning Demo Applications\n\n\nOnce authenticated, you can continue to \nDemo Applications\n section to learn how to import, launch, and run demo applications.\n\n\nService Management \n\n\nThe DataTorrent Sandbox automatically launches Hadoop HDFS and YARN, dtGateway, and other required services during startup.  Depending on the host machine capabilities, these may take from several seconds to several minutes to start up.  Until Hadoop services are active and ready, it is normal to see error messages about availability of HDFS and YARN in the Issues section of the DataTorrent console.  Ensure there are no errors remaining in the console by allowing sufficient time for Hadoop services startup prior to launching applications.  \n\n\n\n\nNote\n: By default, this sandbox is designed to run with 6 GB of RAM.  Limited resources may cause delays during Hadoop services and applications startup.\n\n\n\n\nDataTorrent Sandbox automatically launches following services on startup.\n\n\n\n\nHadoop HDFS NameNode\n\n\nHadoop HDFS DataNode\n\n\nHadoop YARN ResourceManager\n\n\nHadoop YARN NodeManager\n\n\nDataTorrent Gateway\n\n\n\n\nFollowing service management actions are available:\n\n\n\n\nStart all Hadoop and DataTorrent services\n\n\nShut down all Hadoop and DataTorrent services\n\n\nRebuild HDFS (deletes all data!) and restart all services.\n\n\n\n\nTo manage the services following desktop launchers have been set up\n\n\n\n\n\n\nUbuntu Sandbox Edition\n - right-click on \nDataTorrent Services\n desktop launcher\n\n\n\n\n\n\n\n\nLubuntu Sandbox Edition\n - hover the mouse over Lubuntu launcher hidden in the bottom left corner of the sandbox screen, and navigate to DataTorrent menu\n\n\n\n\n\n\n\n\nSupport\n\n\nIf you experience issues while experimenting with the sandbox, or have any feedback and comments, please let us know, and we will be happy to help!  Contact us using one of the methods listed on \ndatatorrent.com/contact\n page.", 
            "title": "Sandbox"
        }, 
        {
            "location": "/sandbox/#datatorrent-rts-sandbox", 
            "text": "Welcome to the DataTorrent Sandbox, an introduction to DataTorrent RTS, the industry\u2019s only unified stream and batch processing platform.  The Sandbox provides a quick and simple way to experience DataTorrent RTS without setting up and managing a complete Hadoop cluster.  The Sandbox contains pre-installed DataTorrent RTS Enterprise Edition along with all the Hadoop services required to launch and run the included demo applications.", 
            "title": "DataTorrent RTS Sandbox"
        }, 
        {
            "location": "/sandbox/#installation", 
            "text": "If you have not already, sandbox can be downloaded by visiting  datatorrent.com/download .  To run the DataTorrent Sandbox, ensure you have downloaded and installed  VirtualBox  4.3 or greater.", 
            "title": "Installation"
        }, 
        {
            "location": "/sandbox/#accessing-console", 
            "text": "When accessing DataTorrent console in the sandbox for the first time, you will be required to log in.  Use username  dtadmin  and password  dtadmin .  Same credentials are also valid for sandbox system access.     Inside the DataTorrent RTS Sandbox console can be accessed by opening a browser and visiting  http://localhost:9090/", 
            "title": "Accessing Console"
        }, 
        {
            "location": "/sandbox/#running-demo-applications", 
            "text": "Once authenticated, you can continue to  Demo Applications  section to learn how to import, launch, and run demo applications.", 
            "title": "Running Demo Applications"
        }, 
        {
            "location": "/sandbox/#service-management", 
            "text": "The DataTorrent Sandbox automatically launches Hadoop HDFS and YARN, dtGateway, and other required services during startup.  Depending on the host machine capabilities, these may take from several seconds to several minutes to start up.  Until Hadoop services are active and ready, it is normal to see error messages about availability of HDFS and YARN in the Issues section of the DataTorrent console.  Ensure there are no errors remaining in the console by allowing sufficient time for Hadoop services startup prior to launching applications.     Note : By default, this sandbox is designed to run with 6 GB of RAM.  Limited resources may cause delays during Hadoop services and applications startup.   DataTorrent Sandbox automatically launches following services on startup.   Hadoop HDFS NameNode  Hadoop HDFS DataNode  Hadoop YARN ResourceManager  Hadoop YARN NodeManager  DataTorrent Gateway   Following service management actions are available:   Start all Hadoop and DataTorrent services  Shut down all Hadoop and DataTorrent services  Rebuild HDFS (deletes all data!) and restart all services.   To manage the services following desktop launchers have been set up    Ubuntu Sandbox Edition  - right-click on  DataTorrent Services  desktop launcher     Lubuntu Sandbox Edition  - hover the mouse over Lubuntu launcher hidden in the bottom left corner of the sandbox screen, and navigate to DataTorrent menu", 
            "title": "Service Management "
        }, 
        {
            "location": "/sandbox/#support", 
            "text": "If you experience issues while experimenting with the sandbox, or have any feedback and comments, please let us know, and we will be happy to help!  Contact us using one of the methods listed on  datatorrent.com/contact  page.", 
            "title": "Support"
        }, 
        {
            "location": "/create/", 
            "text": "Getting Started\n\n\nGet started quickly by following one of these tutorials, and create your first Apache Apex application today!\n\n\nTop N Words\n\n\nTop N Words\n is a complete guide to writing your first Apache Apex application using \nJava\n or \ndtAssemble\n\n\n\n\nSales Dimensions\n\n\nSales Dimensions\n is an introduction to assembling and visualizing sales analytics applicaiton with \ndtAssemble\n\n\n\n\n\n\nAdvanced Topics\n\n\n\n\nApplication Development\n - comprehensive guide to developing Apache Apex applications\n\n\nApplication Packaging\n - creating application packages, changing settings, and launching application packages\n\n\nOperator Development\n - creating new operators for Apache Apex applications\n\n\ndtGateway REST API\n - complete listing of all services offered by dtGateway", 
            "title": "Creating Applications"
        }, 
        {
            "location": "/create/#getting-started", 
            "text": "Get started quickly by following one of these tutorials, and create your first Apache Apex application today!", 
            "title": "Getting Started"
        }, 
        {
            "location": "/create/#top-n-words", 
            "text": "Top N Words  is a complete guide to writing your first Apache Apex application using  Java  or  dtAssemble", 
            "title": "Top N Words"
        }, 
        {
            "location": "/create/#sales-dimensions", 
            "text": "Sales Dimensions  is an introduction to assembling and visualizing sales analytics applicaiton with  dtAssemble", 
            "title": "Sales Dimensions"
        }, 
        {
            "location": "/create/#advanced-topics", 
            "text": "Application Development  - comprehensive guide to developing Apache Apex applications  Application Packaging  - creating application packages, changing settings, and launching application packages  Operator Development  - creating new operators for Apache Apex applications  dtGateway REST API  - complete listing of all services offered by dtGateway", 
            "title": "Advanced Topics"
        }, 
        {
            "location": "/tutorials/topnwords/", 
            "text": "Top N Words Application\n\n\nThe Top N words application is a tutorial on building a word counting application using:\n\n\n\n\nApache Apex platform\n\n\nApache Apex Malhar, an associated library of operators\n\n\nOther related tools\n\n\n\n\nNote: Before you begin, ensure that you have internet connectivity\nbecause, in order to complete this tutorial, you will need to download\nthe Apex and Malhar code.\n\n\nThe Top N words application monitors an input directory for new\nfiles. When the application detects a new file, it reads its lines,\nsplits them into words, and computes the word-frequency for that specific file,\nas well as across all files that are processed. The top N words (by\nfrequency) and their frequencies are output to a corresponding file in\nan output directory. Simultaneously, the word-frequency pairs are also\nupdated on \ndtDashboard\n, the browser-based dashboard of DataTorrent RTS.\n\n\nA simple word counting exercise was chosen because the goal of this tutorial is to focus on the use of:\n\n\n\n\nThe Apex platform\n\n\nThe operator library\n\n\nThe tools required for developing and deploying\n    applications on a cluster\n\n\ndtcli\n \n the command-line tool for managing\n    application packages and the constituent applications\n\n\ndtManage\n \n for monitoring the applications\n\n\ndtDashboard\n \n for visualizing the output\n\n\ndtAssemble\n \n for  visual application assembly\n\n\n\n\nIn the context of such an application, a number of questions arise:\n\n\n\n\nWhat operators do we need ?\n\n\nHow many are present in the Malhar library ?\n\n\nHow many need to be written from scratch ?\n\n\nHow are operators wired together ?\n\n\nHow do we monitor the running application ?\n\n\nHow do we display the output data in an aesthetically pleasing way ?\n\n\n\n\nThe answers to these and other questions are explored in the sections below.\n\n\nFor this tutorial, use the Data Torrent RTS Sandbox; it comes pre-installed\nwith Apache Hadoop and DataTorrent RTS 3.1.1 configured as a single-node\ncluster and includes a time-limited enterprise license. If you've already installed the RTS Enterprise Edition (evaluation or production license), you\ncan use that setup instead.", 
            "title": "Introduction"
        }, 
        {
            "location": "/tutorials/topnwords/#top-n-words-application", 
            "text": "The Top N words application is a tutorial on building a word counting application using:   Apache Apex platform  Apache Apex Malhar, an associated library of operators  Other related tools   Note: Before you begin, ensure that you have internet connectivity\nbecause, in order to complete this tutorial, you will need to download\nthe Apex and Malhar code.  The Top N words application monitors an input directory for new\nfiles. When the application detects a new file, it reads its lines,\nsplits them into words, and computes the word-frequency for that specific file,\nas well as across all files that are processed. The top N words (by\nfrequency) and their frequencies are output to a corresponding file in\nan output directory. Simultaneously, the word-frequency pairs are also\nupdated on  dtDashboard , the browser-based dashboard of DataTorrent RTS.  A simple word counting exercise was chosen because the goal of this tutorial is to focus on the use of:   The Apex platform  The operator library  The tools required for developing and deploying\n    applications on a cluster  dtcli    the command-line tool for managing\n    application packages and the constituent applications  dtManage    for monitoring the applications  dtDashboard    for visualizing the output  dtAssemble    for  visual application assembly   In the context of such an application, a number of questions arise:   What operators do we need ?  How many are present in the Malhar library ?  How many need to be written from scratch ?  How are operators wired together ?  How do we monitor the running application ?  How do we display the output data in an aesthetically pleasing way ?   The answers to these and other questions are explored in the sections below.  For this tutorial, use the Data Torrent RTS Sandbox; it comes pre-installed\nwith Apache Hadoop and DataTorrent RTS 3.1.1 configured as a single-node\ncluster and includes a time-limited enterprise license. If you've already installed the RTS Enterprise Edition (evaluation or production license), you\ncan use that setup instead.", 
            "title": "Top N Words Application"
        }, 
        {
            "location": "/tutorials/topnwords-c1/", 
            "text": "Setting up your development environment\n\n\nThis section describes how you can set up your development environment\nincluding starting the sandbox and downloading some sample input files for\ntesting the application.\n\n\nSample input files\n\n\nFor this tutorial, you need some sample text files to use as input application.\nBinary files such as PDF or DOCX files are not suitable since they contain a\nlot of meaningless strings that look like words (for example,  Wqgi ).\nSimilarly, files using markup languages such as XML or HTML files are also not\nsuitable since the tag names such as  div ,  td  and  p  dominate the word\ncounts. The RFC (Request for Comment) files that are used as de-facto\nspecifications for internet standards are good candidates since they contain\npure text; download a few of them as follows:\n\n\nOpen a terminal and run the following commands to create a directory named\n\ndata\n under your home directory and download 3 files there:\n\n\ncd; mkdir data; cd data  \nwget http://tools.ietf.org/rfc/rfc1945.txt  \nwget https://www.ietf.org/rfc/rfc2616.txt  \nwget https://tools.ietf.org/rfc/rfc4844.txt\n\n\n\nValidation for third-party applications\n\n\nIf you are using your own installation of DataTorrent RTS instead\nof Sandbox, make sure that you have Java JDK (version 1.7.0_79 or\nlater), Maven (version 3.0.5 or later), and Git (version 1.7.1 or later)\nby running the following commands:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\nExpected output\n\n\n\n\n\n\njava -version\n\n\njava version \n1.7.0_79\n\n\nJava(TM) SE Runtime Environment (build 1.7.0_79-b15) \n\n\nJava HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)\n\n\n\n\n\n\nmvn --version\n\n\nApache Maven 3.0.5 \n\n\nMaven home: /usr/share/maven \n\n\nJava version: 1.7.0_79, vendor: Oracle Corporation \n\n\nJava home: /home/\nuser\n/Software/java/jdk1.7.0_79/jre \n\n\nDefault locale: en_US, platform encoding: UTF-8 \n\n\nOS name: \nlinux\n, version: \n3.16.0-44-generic\n, arch: \namd64\n, family: \nunix\n \n\n\n\n\n\n\ngit --version\n\n\ngit version 1.7.1\n\n\n\n\n\n\n\n\n\n\n\nSet up the sandbox\n\n\nAt the time of writing, the sandbox corresponds to version 3.1.1 of DataTorrent\nRTS, which, as noted above, includes a complete, stand-alone, instance of the\nEnterprise Edition configured as a single-node Hadoop cluster.\n\n\nBefore you begin, ensure that you have Oracle VM VirtualBox version 4.3 or\nlater on your development machine. The sandbox is a virtual appliance bundled\nalong with Ubuntu 12.0.4 (or later) that needs to be imported into VirtualBox.\n\n\nThese steps describe how to download, import, and start the sandbox.\n\n\n\n\n\n\nDownload Sandbox:\n\n\n\n\nOpen \nhttps://www.datatorrent.com/download/\n in a web browser.\n\n\nUnder \nDataTorrent RTS Sandbox\n, click \nDOWNLOAD NOW\n button.\n\n\nOn the contact details form that appears, provide your name, email, and\n   your organization s name, and click \nSubmit\n.\n\n\nClick the link named \nclick here\n (scroll the page down if necessary to\n   see the link).\n\n\nOn the Sandbox downloads page that appears, click \nDownload\n under\n    \nRequirements\n.\n\n\n\n\n\n\n\n\nImport the sandBox into Oracle VirtualBox:\n\n\n\n\nOpen the VirtualBox Manager.\n\n\nIn the \nFile\n menu, choose \nImport Appliance\n.  \n\n\nOn the \nAppliance to import\n dialog box, type or select the full path to\n    the OVA template file that you downloaded and click \nNext\n.\n\n\nClick \nImport\n.\n\n\n\n\n\n\n\n\nAfter the import completes, click the \nStart\n button on the VirtualBox\n    Manager. If this is a first-time start, select the default operating\n    system and wait till the virtual machine initializes.\n\n\nAfter the virtual machine initializes, wait for a few minutes to allow all\nHadoop and Sandbox processes to start.\n\n\n\n\n\n\nLog on to Sandbox.\n\n\n\n\n\n\nIn the browser that appears displaying the Readme, click DataTorrent\n    Console. You can also click the \nDataTorrent Console\n button on the toolbar of your virtual machine.\n\n\n\n\n\n\nType the username and password (dtadmin/dtadmin), and click \nLogin\n.\n\n\n\n\n\n\n\n\n\n\n\n\nYou should see a welcome page with a diagram illustrating the components in the\nplatform stack; each block of the diagram is a clickable link for exploring\nthat component.\n\n\nValidate the Sandbox setup\n\n\nAfter setting up the sandbox, validate the installation:\n\n\n\n\n\n\nOn the top navigation bar, look for links named \nConfigure\n, \nDevelop\n,\n   \nMonitor\n, \nVisualize\n and \nLearn\n.\n\n\n\n\n\n\nIf Visualize is not present, click \nConfigure\n, and then \nLicense\n    Information\n.\n\n\n\n\n\n\nA page showing license details including the text: \nLicense Edition:\n  enterprise\n appears.\n\n\n\n\nIf the license details display   \ncommunity\n instead of \nenterprise\n, wait\n  for a few minutes, refresh the page, and check again.\n\n\nIf that does not work, navigate to the other tabs, and repeat these steps\n  again. The license edition should be \nenterprise\n and you should see the\n  \nVisualize\n link on the top navigation bar.\n\n\n\n\nNote: The enterprise license is present on the Hadoop HDFS file system. To\nretrieve details of this license, the HDFS servers need to be up and running,\nwhich can take a few minutes.", 
            "title": "Development Environment"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#setting-up-your-development-environment", 
            "text": "This section describes how you can set up your development environment\nincluding starting the sandbox and downloading some sample input files for\ntesting the application.", 
            "title": "Setting up your development environment"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#sample-input-files", 
            "text": "For this tutorial, you need some sample text files to use as input application.\nBinary files such as PDF or DOCX files are not suitable since they contain a\nlot of meaningless strings that look like words (for example,  Wqgi ).\nSimilarly, files using markup languages such as XML or HTML files are also not\nsuitable since the tag names such as  div ,  td  and  p  dominate the word\ncounts. The RFC (Request for Comment) files that are used as de-facto\nspecifications for internet standards are good candidates since they contain\npure text; download a few of them as follows:  Open a terminal and run the following commands to create a directory named data  under your home directory and download 3 files there:  cd; mkdir data; cd data  \nwget http://tools.ietf.org/rfc/rfc1945.txt  \nwget https://www.ietf.org/rfc/rfc2616.txt  \nwget https://tools.ietf.org/rfc/rfc4844.txt", 
            "title": "Sample input files"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#validation-for-third-party-applications", 
            "text": "If you are using your own installation of DataTorrent RTS instead\nof Sandbox, make sure that you have Java JDK (version 1.7.0_79 or\nlater), Maven (version 3.0.5 or later), and Git (version 1.7.1 or later)\nby running the following commands:         Command  Expected output    java -version  java version  1.7.0_79  Java(TM) SE Runtime Environment (build 1.7.0_79-b15)   Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)    mvn --version  Apache Maven 3.0.5   Maven home: /usr/share/maven   Java version: 1.7.0_79, vendor: Oracle Corporation   Java home: /home/ user /Software/java/jdk1.7.0_79/jre   Default locale: en_US, platform encoding: UTF-8   OS name:  linux , version:  3.16.0-44-generic , arch:  amd64 , family:  unix      git --version  git version 1.7.1", 
            "title": "Validation for third-party applications"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#set-up-the-sandbox", 
            "text": "At the time of writing, the sandbox corresponds to version 3.1.1 of DataTorrent\nRTS, which, as noted above, includes a complete, stand-alone, instance of the\nEnterprise Edition configured as a single-node Hadoop cluster.  Before you begin, ensure that you have Oracle VM VirtualBox version 4.3 or\nlater on your development machine. The sandbox is a virtual appliance bundled\nalong with Ubuntu 12.0.4 (or later) that needs to be imported into VirtualBox.  These steps describe how to download, import, and start the sandbox.    Download Sandbox:   Open  https://www.datatorrent.com/download/  in a web browser.  Under  DataTorrent RTS Sandbox , click  DOWNLOAD NOW  button.  On the contact details form that appears, provide your name, email, and\n   your organization s name, and click  Submit .  Click the link named  click here  (scroll the page down if necessary to\n   see the link).  On the Sandbox downloads page that appears, click  Download  under\n     Requirements .     Import the sandBox into Oracle VirtualBox:   Open the VirtualBox Manager.  In the  File  menu, choose  Import Appliance .    On the  Appliance to import  dialog box, type or select the full path to\n    the OVA template file that you downloaded and click  Next .  Click  Import .     After the import completes, click the  Start  button on the VirtualBox\n    Manager. If this is a first-time start, select the default operating\n    system and wait till the virtual machine initializes.  After the virtual machine initializes, wait for a few minutes to allow all\nHadoop and Sandbox processes to start.    Log on to Sandbox.    In the browser that appears displaying the Readme, click DataTorrent\n    Console. You can also click the  DataTorrent Console  button on the toolbar of your virtual machine.    Type the username and password (dtadmin/dtadmin), and click  Login .       You should see a welcome page with a diagram illustrating the components in the\nplatform stack; each block of the diagram is a clickable link for exploring\nthat component.", 
            "title": "Set up the sandbox"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#validate-the-sandbox-setup", 
            "text": "After setting up the sandbox, validate the installation:    On the top navigation bar, look for links named  Configure ,  Develop ,\n    Monitor ,  Visualize  and  Learn .    If Visualize is not present, click  Configure , and then  License\n    Information .    A page showing license details including the text:  License Edition:\n  enterprise  appears.   If the license details display    community  instead of  enterprise , wait\n  for a few minutes, refresh the page, and check again.  If that does not work, navigate to the other tabs, and repeat these steps\n  again. The license edition should be  enterprise  and you should see the\n   Visualize  link on the top navigation bar.   Note: The enterprise license is present on the Hadoop HDFS file system. To\nretrieve details of this license, the HDFS servers need to be up and running,\nwhich can take a few minutes.", 
            "title": "Validate the Sandbox setup"
        }, 
        {
            "location": "/tutorials/topnwords-c2/", 
            "text": "Building top N words using JAVA\n\n\nThis chapter describes the steps to build the application in Java using some\nsource files from the Malhar repository, suitably modified and customized to\nrun on the sandbox. We will use the \ndtManage\n GUI tool to launch the\napplication.\n\n\nNote\n: You can build the top N words application using an IDE such as NetBeans\nor maven on the command line. The following sections describe both ways.\n\n\nStep I: Clone the Apex Malhar repository\n\n\nClone the Malhar repository (we will use some of these source files in a later\nsection):\n\n\n\n\n\n\nOpen a terminal window and create a new directory where you want the code\n    to reside, for example: \ncd ~/src; mkdir dt; cd dt\n\n\n\n\n\n\nDownload the code for Malhar:\n\n\ngit clone https://github.com/apache/incubator-apex-malhar\n\n\n\nYou should now see a directory named \nincubator-apex-malhar\n.\n\n\n\n\n\n\nNavigate to the \nincubator-apex-malhar\n directory and switch to the\n    \ndevel-3\n branch:\n\n\ncd incubator-apex-malhar\ngit checkout devel-3\n\n\n\n\n\n\n\nStep II: Create the application project using NetBeans IDE\n\n\nThis section describes how to generate a new Apache Apex maven project using\nthe NetBeans IDE (downloadable from \nhttps://netbeans.org/downloads/\n). You can\nalso use other IDEs such as Eclipse if you wish. The next section describes an\nalternative method of doing the same thing from the command-line without using\nan IDE.\n\n\nGenerate a new Maven archetype project as follows:\n\n\n\n\nOpen NetBeans.\n\n\nClick File \n New Project.\n\n\n\n\nFrom the projects list, select \nProject from Archetype\n, and click \nNext\n.\n    \n\n\n\n\n\n\nOn the Maven Archetype window, type \napex\n in the \nSearch\n box, and\n     from the list of \nKnown Archetypes\n, select \napex-app-archetype\n.\n     \n\n\n\n\nMake sure that the values for the fields match the values shown in this\n     table:\n\n\n\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nField\n\n  \nValue\n\n  \n\n  \n\n  \nGroup ID\n\n  \ncom.datatorrent\n\n  \n\n  \n\n  \nArtifact ID\n\n  \napex-app-archetype\n\n  \n\n  \n\n  \nVersion\n\n  \n3.1.1\n\n  \n\n  \n\n  \nRepository\n\n  \nhttps://www.datatorrent.com/maven/content/repositories/releases/\n\n  \n\n  \n\n  \n\n\n\n\nClick Next.\n\n\n\n\nOn the \nName and Location\n window, do the following:\n\n\n\n\nEnter a name for this project in the \nProject Name\n box, for example,\n    \nTopNWordCount\n.\n\n\nEnter a location for this project in the \nProject Location\n box, for\n     example, \n/home/dtadmin/NetBeansProjects\n.\n\n\nEnter an ID in the \nGroup Id\n box, for example, \ncom.example\n.\n\n\nEnter a version for this project in the \nVersion\n box, for example,\n     \n1.0-SNAPSHOT\n.\n\n\nEnter the package name in the \nPackage\n box, for example,\n      \ncom.example.topnwordcount\n.\n\n\n\n\n\n\n\n\n\n\nClick Finish.\n\n\n\n\n\n\nThe project is generated at the specified location and should be visible in\nthe left panel with the name \nMy Apex Application\n. You can right-click the\nproject and choose \nRename\n to provide a more descriptive name such as\n\nTopNWordCount\n.\n\n\n\n\nStep II (Optional): Create the application project using the command line\n\n\nThe new maven project can be created using the command line (instead of an IDE)\nas follows:\n\n\n\n\n\n\nCopy this script to a simple text file named, for example, \nnewdt.sh\n.\n\n\n#!/bin/bash\n# script to create a new project\nmvn archetype:generate \\\n-DarchetypeRepository=https://www.datatorrent.com/maven/content/repositories/releases \\\n  -DarchetypeGroupId=com.datatorrent \\\n  -DarchetypeArtifactId=apex-app-archetype \\\n  -DarchetypeVersion=3.1.1 \\\n  -DgroupId=com.example \\\n  -Dpackage=com.example.topnwordcount \\\n  -DartifactId=topNwordcount \\\n  -Dversion=1.0-SNAPSHOT\n\n\n\n\n\n\n\nRun the file: \nbash newdt.sh\n\n\nNote\n: The command parameters might require minor adjustments as newer\nversions of Apache Apex are released. The parameters are displayed when you run this command.\n\n\n\n\n\n\nPress \nEnter\n when prompted with \nY : :\n. A new project directory named\n    \ntopNwordcount\n containing source files for a simple application should\n    appear.\n\n\n\n\n\n\nStep III: Copy application files to the new project\n\n\nWe assume you now have a new project created via one of the two methods\noutlined above. We will now copy over a few of the application files downloaded\nin Step I to the appropriate subdirectory of the new project.\n\n\n\n\nDelete files \nApplication.java\n and \nRandomNumberGenerator.java\n\n   under \nsrc/main/java/com/example/topnwordcount\n.\n\n\nDelete file \nApplicationTest.java\n file under\n   \nsrc/test/java/com/example/topnwordcount\n.\n\n\n\n\nCopy the following files from:\n\n\nincubator-apex-malhar/demos/wordcount/src/main/java/com/datatorrent/demos/wordcount/\n\n\n\nto\n\n\nsrc/main/java/com/example/topnwordcount\n\n\n\n\n\nApplicationWithQuerySupport.java\n\n\nFileWordCount.java\n\n\nLineReader.java\n\n\nWCPair.java\n\n\nWindowWordCount.java\n\n\nWordCountWriter.java\n\n\nWordReader.java\n\n\n\n\n\n\n\n\nCopy the file \nWordDataSchema.json\n from\n\n\nincubator-apex-malhar/demos/wordcount/src/main/resources/\n\n\n\nto\n\n\nsrc/main/resources/\n\n\n\nin the new project.\n\n\nNote\n: This file defines the format of data sent to the visualization widgets within \ndtDashboard\n.\n\n\n\n\n\n\nStep IV: Customize the application and operators\n\n\nWe will now customize the application so that we can build and run it in our\nsandbox environment.\n\n\nThe first customization involves the package name within the files which\ncurrently reflects the package from which they were copied. The relevant line\nwithin each file might look like this:\n\n\npackage com.datatorrent.demos.wordcount;\n\n\n\n\nChange this line to reflect the current location, for example:\n\n\npackage com.example.topnwordcount;\n\n\n\n\nNote: This change is easily accomplished in NetBeans by opening each file,\nclicking the red error icon in the left margin, and selecting the\n\nChange package declaration to com.example.topnwordcount\n entry from the\ndrop-down list. Do this for each Java file that you copied.\n\n\nCustomize the operators\n\n\nThe next customization involves the query operators: The new code has two\nquery operators that are embedded within other operators. Support for such\nembedding is absent in the 3.1.1 code, so we must:\n\n\n\n\nRemove the embedding calls\n\n\nAdd the operators directly to the DAG\n\n\nConnect these query operators to the rest of the DAG via suitable streams\n\n\n\n\nNote\n: All the operators used in this application are described in detail in the\nAppendix.\n\n\nTo make these changes, edit the file \nApplicationWithQuerySupport.java\n:\n\n\n\n\n\n\nRemove the lines containing calls to \nsetEmbeddableQueryInfoProvider()\n\n    and add these two lines in their place:\n\n\ndag.addOperator(\"QueryFile\",   wsQueryFile);  \ndag.addOperator(\"QueryGlobal\", wsQueryGlobal);\n\n\n\n\n\n\n\nAdd streams to connect the two query operators to the DAG by adding these\n    lines before the four existing \naddstream()\n calls:\n\n\ndag.addStream(\"QueryFileStream\", wsQueryFile.outputPort, snapshotServerFile.query);\ndag.addStream(\"QueryGlobalStream\", wsQueryGlobal.outputPort, snapshotServerGlobal.query);\n\n\n\n\n\n\n\nSave the file.\n\n\n\n\n\n\nNote\n: After you complete this procedure, the newly copied Java files\nshould not show the red error icon in the IDE.\n\n\nCustomize the application configuration\n\n\nLastly, you must add configure some properties of the application. These\nproperties accomplish the following aims:\n\n\n\n\nLimit the amount of memory used by most operators so that more memory can\n  be allocated for \nfileWordCount\n which maintains the frequency counts.\n\n\nSet the locality of a couple of streams to \nCONTAINER_LOCAL\n to further\n  reduce memory pressure (necessary on the memory-limited environment of the\n  sandbox).\n\n\nDefine the regular expression for matching the non-word string that\n  delimits words.\n\n\nDefine number of top (word, frequency) pairs we want output.\n\n\nDefine the path to the monitored input directory where input files are\n  dropped and the output directory (both HDFS) to which the per-file top N\n  (word, frequency) pairs are output.\n\n\nDefine the topics for sending queries and retrieving data for visualization.\n\n\n\n\nTo do this:\n\n\nOpen the src/main/resources/META-INF/properties.xml file, and replace its\ncontent with the following:\n\n\nconfiguration\n\n \nproperty\n\n   \nname\ndt.attr.MASTER_MEMORY_MB\n/name\n\n   \nvalue\n500\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.*.operator.*.attr.MEMORY_MB\n/name\n\n   \nvalue\n200\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.fileWordCount.attr.MEMORY_MB\n/name\n\n   \nvalue\n512\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.lineReader.directory\n/name\n\n   \nvalue\n/tmp/test/input-dir\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr\n/name\n\n   \nvalue\n[\\p{Punct}\\s]+\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wcWriter.filePath\n/name\n\n   \nvalue\n/tmp/test/output-dir\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.fileWordCount.topN\n/name\n\n   \nvalue\n10\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.stream.QueryFileStream.locality\n/name\n\n   \nvalue\nCONTAINER_LOCAL\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.stream.QueryGlobalStream.locality\n/name\n\n   \nvalue\nCONTAINER_LOCAL\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.QueryFile.topic\n/name\n\n   \nvalue\nTopNWordsQueryFile\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wsResultFile.topic\n/name\n\n   \nvalue\nTopNWordsQueryFileResult\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic\n/name\n\n   \nvalue\nTopNWordsQueryGlobal\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic\n/name\n\n   \nvalue\nTopNWordsQueryGlobalResult\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TwitterDemo.operator.wsResult.numRetries\n/name\n\n   \nvalue\n2147483647\n/value\n\n \n/property\n\n\n/configuration\n\n\n\n\n\nStep V: Build the top N words count application\n\n\nTo build the application from NetBeans\n\n\n\n\nOpen NetBeans IDE.\n\n\nRight-click the project, and click \nBuild\n.\n\n\n\n\nBuilding the application from the command line is equally simple:\n\n\ncd topNwordcount; mvn clean package -DskipTests\n\n\n\nIn either case, if the build is successful, it should have created the\napplication package file\n\ntopNwordcount/target/topNwordcount-1.0-SNAPSHOT.apa\n.\n\n\nStep VI: Upload the top N words application package\n\n\nTo upload the top N words application package\n\n\n\n\nLog on to the DataTorrent Console using the default username and password\n   (both are \ndtadmin\n).\n\n\nOn the top navigation bar, click \nDevelop\n.\n\n\nUnder \nApp Packages\n, click \nupload a package\n.\n\n\n\n\nNavigate to the location of the \ntopNwordcount-1.0-SNAPSHOT.apa\n\n   application package file is stored.\n\n\nWait till the package is successfully uploaded.\n\n\n\n\nStep VII: Launch the top N words application\n\n\nNote\n: Before launching the top N words application, shut down the IDE. If your\nIDE is running at the time of a launch, the sandbox might hang due to resource\nexhaustion.\n\n\n\n\nLog on to the DataTorrent Console (the default username and password are\n   both \ndtadmin\n).\n\n\nIn the top navigation bar, click \nDevelop\n.\n\n\nUnder \nApp Packages\n, locate the top N word count application, and click\n   \nLaunch Application\n.\n\n\n(Optional) To configure the application using a configuration file, select\n    \nUse a config file\n. To specify individual properties, select \nSpecify\n    custom properties\n.\n\n\nClick Launch.\n\n\n\n\nA message indicating success of the launch operation should appear along with\nthe application ID.\n\n\nNote\n: After a successful launch, monitor the top N words application following\ninstructions in the chapter \nMonitoring with dtManage\n.", 
            "title": "Building in Java"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#building-top-n-words-using-java", 
            "text": "This chapter describes the steps to build the application in Java using some\nsource files from the Malhar repository, suitably modified and customized to\nrun on the sandbox. We will use the  dtManage  GUI tool to launch the\napplication.  Note : You can build the top N words application using an IDE such as NetBeans\nor maven on the command line. The following sections describe both ways.", 
            "title": "Building top N words using JAVA"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-i-clone-the-apex-malhar-repository", 
            "text": "Clone the Malhar repository (we will use some of these source files in a later\nsection):    Open a terminal window and create a new directory where you want the code\n    to reside, for example:  cd ~/src; mkdir dt; cd dt    Download the code for Malhar:  git clone https://github.com/apache/incubator-apex-malhar  You should now see a directory named  incubator-apex-malhar .    Navigate to the  incubator-apex-malhar  directory and switch to the\n     devel-3  branch:  cd incubator-apex-malhar\ngit checkout devel-3", 
            "title": "Step I: Clone the Apex Malhar repository"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-ii-create-the-application-project-using-netbeans-ide", 
            "text": "This section describes how to generate a new Apache Apex maven project using\nthe NetBeans IDE (downloadable from  https://netbeans.org/downloads/ ). You can\nalso use other IDEs such as Eclipse if you wish. The next section describes an\nalternative method of doing the same thing from the command-line without using\nan IDE.  Generate a new Maven archetype project as follows:   Open NetBeans.  Click File   New Project.   From the projects list, select  Project from Archetype , and click  Next .\n        On the Maven Archetype window, type  apex  in the  Search  box, and\n     from the list of  Known Archetypes , select  apex-app-archetype .\n        Make sure that the values for the fields match the values shown in this\n     table:   \n   \n   \n   \n   \n   \n   \n   Field \n   Value \n   \n   \n   Group ID \n   com.datatorrent \n   \n   \n   Artifact ID \n   apex-app-archetype \n   \n   \n   Version \n   3.1.1 \n   \n   \n   Repository \n   https://www.datatorrent.com/maven/content/repositories/releases/ \n   \n   \n     Click Next.   On the  Name and Location  window, do the following:   Enter a name for this project in the  Project Name  box, for example,\n     TopNWordCount .  Enter a location for this project in the  Project Location  box, for\n     example,  /home/dtadmin/NetBeansProjects .  Enter an ID in the  Group Id  box, for example,  com.example .  Enter a version for this project in the  Version  box, for example,\n      1.0-SNAPSHOT .  Enter the package name in the  Package  box, for example,\n       com.example.topnwordcount .      Click Finish.    The project is generated at the specified location and should be visible in\nthe left panel with the name  My Apex Application . You can right-click the\nproject and choose  Rename  to provide a more descriptive name such as TopNWordCount .", 
            "title": "Step II: Create the application project using NetBeans IDE"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-ii-optional-create-the-application-project-using-the-command-line", 
            "text": "The new maven project can be created using the command line (instead of an IDE)\nas follows:    Copy this script to a simple text file named, for example,  newdt.sh .  #!/bin/bash\n# script to create a new project\nmvn archetype:generate \\\n-DarchetypeRepository=https://www.datatorrent.com/maven/content/repositories/releases \\\n  -DarchetypeGroupId=com.datatorrent \\\n  -DarchetypeArtifactId=apex-app-archetype \\\n  -DarchetypeVersion=3.1.1 \\\n  -DgroupId=com.example \\\n  -Dpackage=com.example.topnwordcount \\\n  -DartifactId=topNwordcount \\\n  -Dversion=1.0-SNAPSHOT    Run the file:  bash newdt.sh  Note : The command parameters might require minor adjustments as newer\nversions of Apache Apex are released. The parameters are displayed when you run this command.    Press  Enter  when prompted with  Y : : . A new project directory named\n     topNwordcount  containing source files for a simple application should\n    appear.", 
            "title": "Step II (Optional): Create the application project using the command line"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-iii-copy-application-files-to-the-new-project", 
            "text": "We assume you now have a new project created via one of the two methods\noutlined above. We will now copy over a few of the application files downloaded\nin Step I to the appropriate subdirectory of the new project.   Delete files  Application.java  and  RandomNumberGenerator.java \n   under  src/main/java/com/example/topnwordcount .  Delete file  ApplicationTest.java  file under\n    src/test/java/com/example/topnwordcount .   Copy the following files from:  incubator-apex-malhar/demos/wordcount/src/main/java/com/datatorrent/demos/wordcount/  to  src/main/java/com/example/topnwordcount   ApplicationWithQuerySupport.java  FileWordCount.java  LineReader.java  WCPair.java  WindowWordCount.java  WordCountWriter.java  WordReader.java     Copy the file  WordDataSchema.json  from  incubator-apex-malhar/demos/wordcount/src/main/resources/  to  src/main/resources/  in the new project.  Note : This file defines the format of data sent to the visualization widgets within  dtDashboard .", 
            "title": "Step III: Copy application files to the new project"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-iv-customize-the-application-and-operators", 
            "text": "We will now customize the application so that we can build and run it in our\nsandbox environment.  The first customization involves the package name within the files which\ncurrently reflects the package from which they were copied. The relevant line\nwithin each file might look like this:  package com.datatorrent.demos.wordcount;  Change this line to reflect the current location, for example:  package com.example.topnwordcount;  Note: This change is easily accomplished in NetBeans by opening each file,\nclicking the red error icon in the left margin, and selecting the Change package declaration to com.example.topnwordcount  entry from the\ndrop-down list. Do this for each Java file that you copied.  Customize the operators  The next customization involves the query operators: The new code has two\nquery operators that are embedded within other operators. Support for such\nembedding is absent in the 3.1.1 code, so we must:   Remove the embedding calls  Add the operators directly to the DAG  Connect these query operators to the rest of the DAG via suitable streams   Note : All the operators used in this application are described in detail in the\nAppendix.  To make these changes, edit the file  ApplicationWithQuerySupport.java :    Remove the lines containing calls to  setEmbeddableQueryInfoProvider() \n    and add these two lines in their place:  dag.addOperator(\"QueryFile\",   wsQueryFile);  \ndag.addOperator(\"QueryGlobal\", wsQueryGlobal);    Add streams to connect the two query operators to the DAG by adding these\n    lines before the four existing  addstream()  calls:  dag.addStream(\"QueryFileStream\", wsQueryFile.outputPort, snapshotServerFile.query);\ndag.addStream(\"QueryGlobalStream\", wsQueryGlobal.outputPort, snapshotServerGlobal.query);    Save the file.    Note : After you complete this procedure, the newly copied Java files\nshould not show the red error icon in the IDE.  Customize the application configuration  Lastly, you must add configure some properties of the application. These\nproperties accomplish the following aims:   Limit the amount of memory used by most operators so that more memory can\n  be allocated for  fileWordCount  which maintains the frequency counts.  Set the locality of a couple of streams to  CONTAINER_LOCAL  to further\n  reduce memory pressure (necessary on the memory-limited environment of the\n  sandbox).  Define the regular expression for matching the non-word string that\n  delimits words.  Define number of top (word, frequency) pairs we want output.  Define the path to the monitored input directory where input files are\n  dropped and the output directory (both HDFS) to which the per-file top N\n  (word, frequency) pairs are output.  Define the topics for sending queries and retrieving data for visualization.   To do this:  Open the src/main/resources/META-INF/properties.xml file, and replace its\ncontent with the following:  configuration \n  property \n    name dt.attr.MASTER_MEMORY_MB /name \n    value 500 /value \n  /property   property \n    name dt.application.*.operator.*.attr.MEMORY_MB /name \n    value 200 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.fileWordCount.attr.MEMORY_MB /name \n    value 512 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.lineReader.directory /name \n    value /tmp/test/input-dir /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr /name \n    value [\\p{Punct}\\s]+ /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wcWriter.filePath /name \n    value /tmp/test/output-dir /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.fileWordCount.topN /name \n    value 10 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.stream.QueryFileStream.locality /name \n    value CONTAINER_LOCAL /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.stream.QueryGlobalStream.locality /name \n    value CONTAINER_LOCAL /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.QueryFile.topic /name \n    value TopNWordsQueryFile /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wsResultFile.topic /name \n    value TopNWordsQueryFileResult /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.QueryGlobal.topic /name \n    value TopNWordsQueryGlobal /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic /name \n    value TopNWordsQueryGlobalResult /value \n  /property   property \n    name dt.application.TwitterDemo.operator.wsResult.numRetries /name \n    value 2147483647 /value \n  /property  /configuration", 
            "title": "Step IV: Customize the application and operators"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-v-build-the-top-n-words-count-application", 
            "text": "To build the application from NetBeans   Open NetBeans IDE.  Right-click the project, and click  Build .   Building the application from the command line is equally simple:  cd topNwordcount; mvn clean package -DskipTests  In either case, if the build is successful, it should have created the\napplication package file topNwordcount/target/topNwordcount-1.0-SNAPSHOT.apa .", 
            "title": "Step V: Build the top N words count application"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-vi-upload-the-top-n-words-application-package", 
            "text": "To upload the top N words application package   Log on to the DataTorrent Console using the default username and password\n   (both are  dtadmin ).  On the top navigation bar, click  Develop .  Under  App Packages , click  upload a package .   Navigate to the location of the  topNwordcount-1.0-SNAPSHOT.apa \n   application package file is stored.  Wait till the package is successfully uploaded.", 
            "title": "Step VI: Upload the top N words application package"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-vii-launch-the-top-n-words-application", 
            "text": "Note : Before launching the top N words application, shut down the IDE. If your\nIDE is running at the time of a launch, the sandbox might hang due to resource\nexhaustion.   Log on to the DataTorrent Console (the default username and password are\n   both  dtadmin ).  In the top navigation bar, click  Develop .  Under  App Packages , locate the top N word count application, and click\n    Launch Application .  (Optional) To configure the application using a configuration file, select\n     Use a config file . To specify individual properties, select  Specify\n    custom properties .  Click Launch.   A message indicating success of the launch operation should appear along with\nthe application ID.  Note : After a successful launch, monitor the top N words application following\ninstructions in the chapter  Monitoring with dtManage .", 
            "title": "Step VII: Launch the top N words application"
        }, 
        {
            "location": "/tutorials/topnwords-c3/", 
            "text": "Building top N words using dtAssemble\n\n\nYou can build top N words using dtAssemble \n the graphical drag-and-drop\napplication builder.\n\n\nNote\n: This tool is not available with the Community edition license.\n\n\nUsing the application builder, you can manually drag and drop participating operators on to the application canvas and connect them to build the DAG for\nthe top N words application. You can move the application canvas in any\ndirection using your mouse. You can also try the auto-layout and zoom-to-fit\nbuttons at the top-right of the canvas to create an initial arrangement.\n\n\nNote\n: You cannot undo the auto-layout or zoom-to-fit operations.\n\n\nPrerequisites\n\n\nTo use \ndtAssemble\n, you should have uploaded an application package that\ncontains all the operators you intend to use. The new application will reside\nin that package. For this exercise, we will use the package that you built and\nuploaded in the earlier chapter.\n\n\nTo ensure that the full complement of sandbox resources are available for\nrunning the current application, ensure that no other applications are running\nby clicking the \nMonitor\n tab, and under \nDataTorrent Applications\n, kill any\nrunning applications.\n\n\nStep I: Create top N words using dtAssemble\n\n\n\n\nLog on to the DataTorrent RTS console (default username and password are\n   both \ndtadmin\n).\n\n\nOn the DataTorrent RTS console, click \nDevelop\n \n \nApp Packages\n.\n\n\nMake sure that the top N words package that you built following the\n   instructions of the previous chapter is uploaded.\n\n\nClick \nTopNWordCount\n in the name column to see the application details.\n  \n\n\nClick \ncreate new application\n button.\n  \n\n\nType a name for your application, for example, \nTop N words\n, and click\n   \nCreate\n. The \nApplication Canvas\n should open.\n\n\n\n\nThe existing application is a JAVA application. Applications that you create\nusing dtAssemble are JSON applications. For each JSON application, there are\nthree additional buttons that allow editing, deleting, and cloning operations.\nThese operations are not supported for JAVA applications.\n\n\n\n\nStep II: Drag operators to the application canvas\n\n\n\n\nWait till the Application Canvas opens.\n  \n\n\nFrom the Operator Library list on the left, locate the desired\n   operators by either:\n\n\n\n\nExploring the categories.\n\n\n-or-\n\n\n\n\n\n\nUsing the \nquick find\n feature by typing the first few letters of the name of\n   the implementing class or related terms in the search box. For example, to\n   find the file reader, type \nfile\n  into the search box to see a list of\n   matching operators. For example, the first operator is \nLineReader\n.\n    \n\n\n\n\nDrag the operator onto the canvas.\n\n\nRepeat this process for all the operators described in Appendix entitled\n    \nOperators in Top N words application\n, and arrange them on the canvas.\n\n\nTo magnify or shrink the operators, use the buttons in the top-right corner.\n   You can also use the scroll wheel of your mouse to zoom in or out. The\n   entire canvas can also be moved in any direction with the mouse.\n\n\n\n\nStep III: Connect the operators\n\n\nObserve that each operator shows output ports in pink and input ports in blue.\nThe names of the operators and the corresponding JAVA classes are also shown.\nYou can change the operator name by clicking it, and then changing the name in\nthe Operator Name box in the Operator Inspector panel.\n\n\nConnect the operators as shown in the diagram below. Note the following points about\nthe \nFileWordCount\n operator which has the largest number of connections:\n\n\n\n\nThe control port is connected to the control port of \nLineReader\n.\n\n\nThe input port is connected to the output port of \nWindowWordCount\n.\n\n\nThe \nfileOutput\n port emits the final top N pairs to the corresponding\n  output file and so is connected to the input port of \nWordCountWriter\n.\n\n\nThe \noutputGlobal\n port emits the global top N pairs and so is connected to\n  the input port of \nAppDataSnapshotMapServer\n.\n\n\nThe \noutputPerFile\n port emits the top N pairs for the current file (while\n  the file is still being read) and so it is connected to \nConsoleOutput\n as\n  well as to \nAppDataSnapshotMapServer\n.\n\n\n\n\nNote\n: As you make changes, the top left corner displays \nAll\nchanges saved to HDFS\n. No explicit save step is needed.\n\n\nAfter you connect all the operators, the canvas looks like this.\nAlthough presented differently, it is the same as the logical DAG for\nthe Java application.\n\n\n\n\nStep IV: Configure the operator properties\n\n\nThe last step before running this application is to configure\nproperties of the operators.\n\n\n\n\nClick the first operator (\nLineReader\n) in the canvas to see the list of\n   configurable properties in the right panel.\n\n\n\n\nLocate the property labelled \nDirectory\n and enter the path to the input\n   directory: \n/tmp/test/input-dir\n:\n\n\n\n\n\n\n\n\nConfigure the properties of the remaining operators using this table for\n   reference. The table contains the same values that we set in the properties\n   file for the Java application.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\n\n\nProperty Name\n\n\nValue\n\n\n\n\n\n\n5\n\n\nFile Path\n\n\n/tmp/test/output-dir\n\n\n\n\n\n\n2\n\n\nNon Word Str\n\n\n[\\p{Punct}\\s]+\n\n\n\n\n\n\n9\n\n\nTopic\n\n\nTopNWordsQueryFile\n\n\n\n\n\n\n10\n\n\nTopic\n\n\nTopNWordsQueryGlobal\n\n\n\n\n\n\n11\n\n\nTopic\n\n\nTopNWordsQueryFileResult\n\n\n\n\n\n\n12\n\n\nTopic\n\n\nTopNWordsQueryGlobalResult\n\n\n\n\n\n\n7, 8\n\n\nSnapshot Schema JSON\n\n\n{ \nvalues\n: [{\nname\n: \nword\n, \ntype\n: \nstring\n},\n\n\n{\nname\n: \ncount\n, \ntype\n: \ninteger\n}] }\n\n\n\n\n\n\n4\n\n\nTop N\n\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\nClick Stream 8 and Stream 9, and change \nStream Locality\n from\n  \nAUTOMATIC\n to \nCONTAINER_LOCAL\n to match our earlier properties file.\n  After you perform this step, the line for the stream will change\n  from a solid line to a dotted line.\n\n\n\n\n\n\nClick the blank area of the canvas to see the\n   application attributes in the right panel. Scroll to \nMaster Memory Mb\n, and\n   change its value to 500.\n\n\n\n\n\n\nClick each operator, navigate to the \nMemory Mb\n attribute in the\n   \nAttributes\n section, and change the value to 200 except for \nOperator\n    4\n for which the value is 512.\n\n\n\n\n\n\nClick \nlaunch\n in the top-left corner. \nNote\n: Before launching the\n   application, shut down the IDE; if it is running at the time of a launch,\n   the sandbox might hang due to resource exhaustion.\n  \n\n\n\n\n\n\nOn the launch application dialog window, type a name for your application.\n  \n\n\n\n\n\n\n(Optional) To configure the application using a configuration file, select\n    \nUse a config file\n checkbox. To specify individual properties, select\n    \nSpecify custom properties\n checkbox.\n\n\n\n\nClick \nLaunch\n.\n\n\n\n\nA transient pop-up at the top-right indicating that the launch was successful\nshould appear.\n\n\n\nAfter a successful launch, monitor the application following\ninstructions in the Chapter entitled \nMonitoring with dtManage\n.", 
            "title": "Building with dtAssemble"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#building-top-n-words-using-dtassemble", 
            "text": "You can build top N words using dtAssemble   the graphical drag-and-drop\napplication builder.  Note : This tool is not available with the Community edition license.  Using the application builder, you can manually drag and drop participating operators on to the application canvas and connect them to build the DAG for\nthe top N words application. You can move the application canvas in any\ndirection using your mouse. You can also try the auto-layout and zoom-to-fit\nbuttons at the top-right of the canvas to create an initial arrangement.  Note : You cannot undo the auto-layout or zoom-to-fit operations.  Prerequisites  To use  dtAssemble , you should have uploaded an application package that\ncontains all the operators you intend to use. The new application will reside\nin that package. For this exercise, we will use the package that you built and\nuploaded in the earlier chapter.  To ensure that the full complement of sandbox resources are available for\nrunning the current application, ensure that no other applications are running\nby clicking the  Monitor  tab, and under  DataTorrent Applications , kill any\nrunning applications.", 
            "title": "Building top N words using dtAssemble"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-i-create-top-n-words-using-dtassemble", 
            "text": "Log on to the DataTorrent RTS console (default username and password are\n   both  dtadmin ).  On the DataTorrent RTS console, click  Develop     App Packages .  Make sure that the top N words package that you built following the\n   instructions of the previous chapter is uploaded.  Click  TopNWordCount  in the name column to see the application details.\n    Click  create new application  button.\n    Type a name for your application, for example,  Top N words , and click\n    Create . The  Application Canvas  should open.   The existing application is a JAVA application. Applications that you create\nusing dtAssemble are JSON applications. For each JSON application, there are\nthree additional buttons that allow editing, deleting, and cloning operations.\nThese operations are not supported for JAVA applications.", 
            "title": "Step I: Create top N words using dtAssemble"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-ii-drag-operators-to-the-application-canvas", 
            "text": "Wait till the Application Canvas opens.\n    From the Operator Library list on the left, locate the desired\n   operators by either:   Exploring the categories.  -or-    Using the  quick find  feature by typing the first few letters of the name of\n   the implementing class or related terms in the search box. For example, to\n   find the file reader, type  file   into the search box to see a list of\n   matching operators. For example, the first operator is  LineReader .\n       Drag the operator onto the canvas.  Repeat this process for all the operators described in Appendix entitled\n     Operators in Top N words application , and arrange them on the canvas.  To magnify or shrink the operators, use the buttons in the top-right corner.\n   You can also use the scroll wheel of your mouse to zoom in or out. The\n   entire canvas can also be moved in any direction with the mouse.", 
            "title": "Step II: Drag operators to the application canvas"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-iii-connect-the-operators", 
            "text": "Observe that each operator shows output ports in pink and input ports in blue.\nThe names of the operators and the corresponding JAVA classes are also shown.\nYou can change the operator name by clicking it, and then changing the name in\nthe Operator Name box in the Operator Inspector panel.  Connect the operators as shown in the diagram below. Note the following points about\nthe  FileWordCount  operator which has the largest number of connections:   The control port is connected to the control port of  LineReader .  The input port is connected to the output port of  WindowWordCount .  The  fileOutput  port emits the final top N pairs to the corresponding\n  output file and so is connected to the input port of  WordCountWriter .  The  outputGlobal  port emits the global top N pairs and so is connected to\n  the input port of  AppDataSnapshotMapServer .  The  outputPerFile  port emits the top N pairs for the current file (while\n  the file is still being read) and so it is connected to  ConsoleOutput  as\n  well as to  AppDataSnapshotMapServer .   Note : As you make changes, the top left corner displays  All\nchanges saved to HDFS . No explicit save step is needed.  After you connect all the operators, the canvas looks like this.\nAlthough presented differently, it is the same as the logical DAG for\nthe Java application.", 
            "title": "Step III: Connect the operators"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-iv-configure-the-operator-properties", 
            "text": "The last step before running this application is to configure\nproperties of the operators.   Click the first operator ( LineReader ) in the canvas to see the list of\n   configurable properties in the right panel.   Locate the property labelled  Directory  and enter the path to the input\n   directory:  /tmp/test/input-dir :     Configure the properties of the remaining operators using this table for\n   reference. The table contains the same values that we set in the properties\n   file for the Java application.          Operator  Property Name  Value    5  File Path  /tmp/test/output-dir    2  Non Word Str  [\\p{Punct}\\s]+    9  Topic  TopNWordsQueryFile    10  Topic  TopNWordsQueryGlobal    11  Topic  TopNWordsQueryFileResult    12  Topic  TopNWordsQueryGlobalResult    7, 8  Snapshot Schema JSON  {  values : [{ name :  word ,  type :  string },  { name :  count ,  type :  integer }] }    4  Top N  10       Click Stream 8 and Stream 9, and change  Stream Locality  from\n   AUTOMATIC  to  CONTAINER_LOCAL  to match our earlier properties file.\n  After you perform this step, the line for the stream will change\n  from a solid line to a dotted line.    Click the blank area of the canvas to see the\n   application attributes in the right panel. Scroll to  Master Memory Mb , and\n   change its value to 500.    Click each operator, navigate to the  Memory Mb  attribute in the\n    Attributes  section, and change the value to 200 except for  Operator\n    4  for which the value is 512.    Click  launch  in the top-left corner.  Note : Before launching the\n   application, shut down the IDE; if it is running at the time of a launch,\n   the sandbox might hang due to resource exhaustion.\n      On the launch application dialog window, type a name for your application.\n      (Optional) To configure the application using a configuration file, select\n     Use a config file  checkbox. To specify individual properties, select\n     Specify custom properties  checkbox.   Click  Launch .   A transient pop-up at the top-right indicating that the launch was successful\nshould appear.  After a successful launch, monitor the application following\ninstructions in the Chapter entitled  Monitoring with dtManage .", 
            "title": "Step IV: Configure the operator properties"
        }, 
        {
            "location": "/tutorials/topnwords-c4/", 
            "text": "Monitoring top N words using dtManage\n\n\ndtManage\n is an invaluable tool for monitoring the state of a running\napplication as well as for troubleshooting problems.\n\n\nMonitor the Application\n\n\nTo monitor the top N words application\n\n\n\n\nLog on to the Datatorrent Console (the default username and password\n   are both \ndtadmin\n).\n\n\nOn the top navigation bar, click \nMonitor\n.\n\n\nUnder \nDatatorrent Applications\n, check if the application started.\n\n\nWait till the state entry changes to \nRUNNING\n.\n\n\nClick \nTopNWordsWithQueries\n to see a page with four tabs: \nlogical\n,\n   \nphysical\n, \nphysical-dag-view\n, and \nmetric-view\n.\n\n\nUnder \nStramEvents\n, ensure that all the operators have started.\n    \n\n\n\n\nDAGs and widgets\n\n\nWhen monitoring an application, the logical view is selected by default, with\nthe following six panels, also called widgets: \nStram Events\n, \nApplication\nOverview\n, \nLogical DAG\n, \nLogical Operators\n, \nStreams\n, and \nMetrics Chart\n.\nThese panels can be resized, moved around, configured (using the gear wheel\nicon in the top-right corner), or removed (using the delete button in the\ntop-right corner).\n\n\nLogical view and associated widgets (panels)\n\n\nThis section describes the widgets that you see when you select the logical\ntab.\n\n\nStram Events\n\n\nAs shown in the screenshot above, this panel shows the lifecycle events of all\nthe operators. If one of the operators fails, a white button labelled \ndetails\n\nappears next to the event; click on it for additional details about the\nfailure.\n\n\nApplication Overview\n\n\nThis panel displays application properties such as state, number of operators,\nallocated memory, and the number of tuples processed. You can use the kill\nbutton to terminate the application. The \nvisualize\n button allows you to\ncreate one or more custom dashboards to visualize the application output.\n\n\n\nLogical DAG\n\n\nThe logical DAG illustrates operators and their interconnections. You can\ncustomize the logical DAG view by selecting operator properties that are\ndisplayed above and below each operator.\n\n\nTo customize these properties\n\n\n\n\nClick an operator for which you want to display additional details.\n\n\nTo display a detail on the top of this operator, click the Top list and\n   select a metric.\n\n\nTo display a detail at the bottom of this operator, click the Bottom list\n   and select a metric.\n\n\n\n\n\n\nLogical Operators\n\n\nThis panel displays a table with detailed information about each operator such\nas its name, the associated JAVA class, the number of tuples processed, and\nthe number of tuples emitted.\n\n\n\n\nStreams\n\n\nThis panel displays details of each stream such as the name, locality, source,\nand sinks.\n\n\n\n\nMetrics Chart\n\n\nThis panel displays the number tuples processed and the number of bytes\nprocessed by some internal components. Since this application has not processed\nany tuples so far (no input file was provided), the green and blue lines\ncoincide with the horizontal axis:\n\n\n\n\nPhysical view and associated widgets\n\n\nThe physical tab displays the \nApplication Overview\n and \nMetrics Chart\n\ndiscussed above along with additional panels: \nPhysical Operators\n and\n\nContainers\n. The \nPhysical Operators\n table shows one row per physical\noperator. When partitioning is enabled, some operators can be replicated to\nachieve better resource utilization and hence better throughput so a single\nlogical operator may correspond to multiple physical operators.\n\n\nPhysical Operators\n\n\n\n\nContainers\n\n\nFor each operator, a crucial piece of information is the process\n(the Java Virtual Machine) running that operator. It is also called a\ncontainer, and shown in a column with that name. Additional information about\nthe container (such as the host on which it is running) can be gleaned from the\nmatching row in the \nContainers\n table.\n\n\n\n\nIf the state of all the physical operators and containers is \nACTIVE\n\nand green   this is a healthy state. If the memory requirements for all the\noperators in the application exceeds the available memory in the cluster,\nyou'll see these status values changing continually from ACTIVE to PENDING.\nThis is an unhealthy state and, if it does not stabilize, your only option is\nto kill the application and reduce the memory needs or acquire more cluster\nresources.\n\n\nThe physical-dag-view\n\n\nThe \nphysical-dag-view\n tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their interconnections.\n\n\nThe metric-view\n\n\nThe metric-view tab displays only the \nMetrics Chart\n widget.\n\n\nView application logs\n\n\nWhen debugging applications, we are often faced with the task of examining\nlog files. This can be cumbersome, especially in a distributed environment\nwhere logs can be scattered across multiple machines. \ndtManage\n simplifies\nthis task by making all relevant logs accessible from the console.\n\n\nFor example, to examine logs for the \nFileWordCount\n operator, go to the physical\ntab of the application monitoring page and check the physical operator table to\nfind the corresponding container. An example value might be 000010.\n\n\nThe numeric values in the \ncontainer\n column are links that open a page\ncontaining a table of all physical operators running in that container.\nIn the \nContainer Overview\n panel, you should see a blue \nlogs\n dropdown\nbutton; click on it to see a menu containing three entries: \ndt.log\n, \nstderr\n,\nand \nstdout\n.\n\n\n\n\nAll messages output using \nlog4j\n classes will appear in \ndt.log\n\nwhereas messages written directly to the standard error or standard\noutput streams will appear in the other two entries. Choose the entry\nyou want to view.", 
            "title": "Monitoring with dtManage"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#monitoring-top-n-words-using-dtmanage", 
            "text": "dtManage  is an invaluable tool for monitoring the state of a running\napplication as well as for troubleshooting problems.", 
            "title": "Monitoring top N words using dtManage"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#monitor-the-application", 
            "text": "To monitor the top N words application   Log on to the Datatorrent Console (the default username and password\n   are both  dtadmin ).  On the top navigation bar, click  Monitor .  Under  Datatorrent Applications , check if the application started.  Wait till the state entry changes to  RUNNING .  Click  TopNWordsWithQueries  to see a page with four tabs:  logical ,\n    physical ,  physical-dag-view , and  metric-view .  Under  StramEvents , ensure that all the operators have started.", 
            "title": "Monitor the Application"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#dags-and-widgets", 
            "text": "When monitoring an application, the logical view is selected by default, with\nthe following six panels, also called widgets:  Stram Events ,  Application\nOverview ,  Logical DAG ,  Logical Operators ,  Streams , and  Metrics Chart .\nThese panels can be resized, moved around, configured (using the gear wheel\nicon in the top-right corner), or removed (using the delete button in the\ntop-right corner).  Logical view and associated widgets (panels)  This section describes the widgets that you see when you select the logical\ntab.  Stram Events  As shown in the screenshot above, this panel shows the lifecycle events of all\nthe operators. If one of the operators fails, a white button labelled  details \nappears next to the event; click on it for additional details about the\nfailure.  Application Overview  This panel displays application properties such as state, number of operators,\nallocated memory, and the number of tuples processed. You can use the kill\nbutton to terminate the application. The  visualize  button allows you to\ncreate one or more custom dashboards to visualize the application output.  Logical DAG  The logical DAG illustrates operators and their interconnections. You can\ncustomize the logical DAG view by selecting operator properties that are\ndisplayed above and below each operator.  To customize these properties   Click an operator for which you want to display additional details.  To display a detail on the top of this operator, click the Top list and\n   select a metric.  To display a detail at the bottom of this operator, click the Bottom list\n   and select a metric.    Logical Operators  This panel displays a table with detailed information about each operator such\nas its name, the associated JAVA class, the number of tuples processed, and\nthe number of tuples emitted.   Streams  This panel displays details of each stream such as the name, locality, source,\nand sinks.   Metrics Chart  This panel displays the number tuples processed and the number of bytes\nprocessed by some internal components. Since this application has not processed\nany tuples so far (no input file was provided), the green and blue lines\ncoincide with the horizontal axis:   Physical view and associated widgets  The physical tab displays the  Application Overview  and  Metrics Chart \ndiscussed above along with additional panels:  Physical Operators  and Containers . The  Physical Operators  table shows one row per physical\noperator. When partitioning is enabled, some operators can be replicated to\nachieve better resource utilization and hence better throughput so a single\nlogical operator may correspond to multiple physical operators.  Physical Operators   Containers  For each operator, a crucial piece of information is the process\n(the Java Virtual Machine) running that operator. It is also called a\ncontainer, and shown in a column with that name. Additional information about\nthe container (such as the host on which it is running) can be gleaned from the\nmatching row in the  Containers  table.   If the state of all the physical operators and containers is  ACTIVE \nand green   this is a healthy state. If the memory requirements for all the\noperators in the application exceeds the available memory in the cluster,\nyou'll see these status values changing continually from ACTIVE to PENDING.\nThis is an unhealthy state and, if it does not stabilize, your only option is\nto kill the application and reduce the memory needs or acquire more cluster\nresources.  The physical-dag-view  The  physical-dag-view  tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their interconnections.  The metric-view  The metric-view tab displays only the  Metrics Chart  widget.", 
            "title": "DAGs and widgets"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#view-application-logs", 
            "text": "When debugging applications, we are often faced with the task of examining\nlog files. This can be cumbersome, especially in a distributed environment\nwhere logs can be scattered across multiple machines.  dtManage  simplifies\nthis task by making all relevant logs accessible from the console.  For example, to examine logs for the  FileWordCount  operator, go to the physical\ntab of the application monitoring page and check the physical operator table to\nfind the corresponding container. An example value might be 000010.  The numeric values in the  container  column are links that open a page\ncontaining a table of all physical operators running in that container.\nIn the  Container Overview  panel, you should see a blue  logs  dropdown\nbutton; click on it to see a menu containing three entries:  dt.log ,  stderr ,\nand  stdout .   All messages output using  log4j  classes will appear in  dt.log \nwhereas messages written directly to the standard error or standard\noutput streams will appear in the other two entries. Choose the entry\nyou want to view.", 
            "title": "View application logs"
        }, 
        {
            "location": "/tutorials/topnwords-c5/", 
            "text": "Visualizing the application output using dtDashboard\n\n\nThis chapter covers how to add input files to the monitored input directory and\nvisualize the output.\n\n\nWhen adding files, it is important to add only one file at a time to the\nmonitored input directory; the application, as it stands, cannot handle\nsimultaneous addition of files at a time into the input directory. This\nissue is discussed in more detail in the Appendix entitled \nFurther Explorations\n.\n\n\nStep 1: Add files to the monitored directory\n\n\nTo add the files to the monitored input directory\n\n\n\n\nLog on to the Datatorrent Console (the default username and password are\n   both \ndtadmin\n).\n\n\nOn the top navigation bar, click \nMonitor\n.\n\n\nClick TopNWordsWithQueries to see a page with four tabs: \nlogical\n,\n   \nphysical\n, \nphysical-dag-view\n, and  \nmetric-view\n.\n\n\nClick the \nlogical\n tab and make sure that the DAG is visible.\n\n\nCreate the input and output directories in HDFS and drop a file into the\n   input directory by running the following commands:\nhdfs dfs -mkdir -p /tmp/test/input-dir\nhdfs dfs -mkdir -p /tmp/test/output-dir\nhdfs dfs -put ~/data/rfc4844.txt /tmp/test/input-dir\n\n\n\n\n\n\n\nYou should now see some numbers above and below some of the operators as the\nlines of the file are read and tuples start flowing through the DAG.\n\n\nYou can view the top 10 words and the frequencies for each input file by\nexamining the corresponding output file in the output directory, for example:\n\n\nhdfs dfs -cat /tmp/test/output-dir/rfc4844.txt\n\n\n\nFor operating on these input and output directories, you may find the following\nshell aliases and functions useful:\n\n\nin=/tmp/test/input-dir\nout=/tmp/test/output-dir\nalias ls-input=\"hdfs dfs -ls $in\"\nalias ls-output=\"hdfs dfs -ls $out\"\nalias clean-input=\"hdfs dfs -rm $in/\\*\"\nalias clean-output=\"hdfs dfs -rm $out/\\*\"\nfunction put-file ( ) {\n    hdfs dfs -put \"$1\" \"$in\"\n}\nfunction get-file ( ) {\n    hdfs dfs -get \"$out/$1\" \"$1\".out\n}\n\n\n\nPut them in a file called, say, \naliases\n and read them into your shell with:\n\nsource aliases\n.\n\n\nThereafter, you can list contents of the input and output directories with\n\nls-input\n and \nls-output\n, remove all files from them with \nclean-input\n and\n\nclean-output\n, drop an input file \nfoo.txt\n into the input directory with\n\nput-file foo.txt\n and finally, retrieve the corresponding output file with\n\nget-file foo.txt\n.\n\n\nNote\n: When you list files in the output directory, their sizes might show as\n0 but if you retrieve them with get-file or catenate them, the expected output\nwill be present.\n\n\nStep II: Visualize the results by generating dashboards\n\n\nTo generate dashboards\n\n\n\n\nPerform step I above.\n\n\nMake sure that the logical tab is selected and the \nApplication Overview\n\n  panel is visible.\n\n\n\n\nClick \nvisualize\n to see a dropdown containing previously created dashboards\n (if any), as well as the \ngenerate new dashboard\n entry.\n\n\n\n\n\n\nSelect the \ngenerate new dashboard\n entry.\n\n\nYou should now see panels with charts where one chart displays the data for\nthe current file and a second chart displays the cumulative global data\nacross all files processed so far.\n\n\n\n\n\n\n\nAdd more files, one at a time, to the input directory as described in\n  step I above.\n\n\n\n\nObserve the charts changing to reflect the new data.\n\n\n\n\nYou can create multiple dashboards in this manner for visualizing the output\nfrom different applications or from the same application in different ways.\n\n\nStep III: Add widgets\n\n\nTo derive more value out of application dashboards, you can add widgets to the\ndashboards. Widgets are charts in addition to the default charts that you can see on the dashboard. DataTorrent RTS Sandbox supports 5 widgets: \nbar chart\n,\n\npie chart\n, \nhorizontal bar chart\n, \ntable\n, and \nnote\n.\n\n\nTo add a widget\n\n\n\n\nGenerate a dashboard by following instructions of Step II above.\n\n\nClick the \nadd widget\n button below the name of the dashboard.\n\n\nIn the \nData Source\n list, select a data source for your widget.\n\n\n\n\nSelect a widget type under \nAvailable Widgets\n.\n\n\n\n\n\n\n\n\nClick \nadd widget\n.\n\n\n\n\n\n\nThe widget is added to your dashboard.\n\n\nStep IV: Configure a widget\n\n\nAfter you add a widget to your dashboard, you can configure it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.\n\n\nTo configure a widget\n\n\n\n\nTo change the size of the widget, click the border of the widget, and\n  resize it.\n\n\nTo move the widget around, click the widget, and drag it to the desired\n  position.\n\n\nTo change the title and other properties, click the \nedit\n button in the\n  top-right corner of the widget.\n    \n\n  You can now enter a new title in the \nTitle\n box or configure the rest of the\n  options in any suitable way.\n\n\nClick \nOK\n.\n\n\nTo remove a widget, click the delete button in the top-right corner.\n\n\n\n\nPerform additional tasks on dashboards\n\n\nAt any time, you can change the name and the description of a dashboard. You\ncan also delete dashboards.\n\n\nTo perform additional tasks\n\n\n\n\nEnsure that you generated a dashboard as described in Step II above and\n   select it.\n\n\nClick \nsettings\n button (next to buttons named \nadd widget\n,\n   \nauto generate\n, and \nsave settings\n), below the name of the dashboard to see the \nDashboard Settings\n dialog:\n    \n\n\nType a new name for the dashboard in the \nName of dashboard\n box.\n\n\nType a suitable description in the box below.\n\n\nMake sure that \nTopNWordsWithQueries\n is selected under \nChoose apps to\n    visualize\n.\n\n\nClick \nSave\n.\n\n\n\n\nDelete a dashboard\n\n\nYou can delete a dashboard at any time.\n\n\n\n\nLog on to the DataTorrent Console (default username and password are both\n  \ndtadmin\n)\n\n\nOn the top navigation bar, click \nVisualize\n.\n\n\n\n\nSelect a dashboard.\n\n\n\n\n\n\n\n\nClick delete.\n\n\n\n\n\n\n\n\nNote: The delete button becomes visible only if one or more rows are selected.", 
            "title": "Visualizing with dtDashboard"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#visualizing-the-application-output-using-dtdashboard", 
            "text": "This chapter covers how to add input files to the monitored input directory and\nvisualize the output.  When adding files, it is important to add only one file at a time to the\nmonitored input directory; the application, as it stands, cannot handle\nsimultaneous addition of files at a time into the input directory. This\nissue is discussed in more detail in the Appendix entitled  Further Explorations .", 
            "title": "Visualizing the application output using dtDashboard"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-1-add-files-to-the-monitored-directory", 
            "text": "To add the files to the monitored input directory   Log on to the Datatorrent Console (the default username and password are\n   both  dtadmin ).  On the top navigation bar, click  Monitor .  Click TopNWordsWithQueries to see a page with four tabs:  logical ,\n    physical ,  physical-dag-view , and   metric-view .  Click the  logical  tab and make sure that the DAG is visible.  Create the input and output directories in HDFS and drop a file into the\n   input directory by running the following commands: hdfs dfs -mkdir -p /tmp/test/input-dir\nhdfs dfs -mkdir -p /tmp/test/output-dir\nhdfs dfs -put ~/data/rfc4844.txt /tmp/test/input-dir    You should now see some numbers above and below some of the operators as the\nlines of the file are read and tuples start flowing through the DAG.  You can view the top 10 words and the frequencies for each input file by\nexamining the corresponding output file in the output directory, for example:  hdfs dfs -cat /tmp/test/output-dir/rfc4844.txt  For operating on these input and output directories, you may find the following\nshell aliases and functions useful:  in=/tmp/test/input-dir\nout=/tmp/test/output-dir\nalias ls-input=\"hdfs dfs -ls $in\"\nalias ls-output=\"hdfs dfs -ls $out\"\nalias clean-input=\"hdfs dfs -rm $in/\\*\"\nalias clean-output=\"hdfs dfs -rm $out/\\*\"\nfunction put-file ( ) {\n    hdfs dfs -put \"$1\" \"$in\"\n}\nfunction get-file ( ) {\n    hdfs dfs -get \"$out/$1\" \"$1\".out\n}  Put them in a file called, say,  aliases  and read them into your shell with: source aliases .  Thereafter, you can list contents of the input and output directories with ls-input  and  ls-output , remove all files from them with  clean-input  and clean-output , drop an input file  foo.txt  into the input directory with put-file foo.txt  and finally, retrieve the corresponding output file with get-file foo.txt .  Note : When you list files in the output directory, their sizes might show as\n0 but if you retrieve them with get-file or catenate them, the expected output\nwill be present.", 
            "title": "Step 1: Add files to the monitored directory"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-ii-visualize-the-results-by-generating-dashboards", 
            "text": "To generate dashboards   Perform step I above.  Make sure that the logical tab is selected and the  Application Overview \n  panel is visible.   Click  visualize  to see a dropdown containing previously created dashboards\n (if any), as well as the  generate new dashboard  entry.    Select the  generate new dashboard  entry.  You should now see panels with charts where one chart displays the data for\nthe current file and a second chart displays the cumulative global data\nacross all files processed so far.    Add more files, one at a time, to the input directory as described in\n  step I above.   Observe the charts changing to reflect the new data.   You can create multiple dashboards in this manner for visualizing the output\nfrom different applications or from the same application in different ways.", 
            "title": "Step II: Visualize the results by generating dashboards"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-iii-add-widgets", 
            "text": "To derive more value out of application dashboards, you can add widgets to the\ndashboards. Widgets are charts in addition to the default charts that you can see on the dashboard. DataTorrent RTS Sandbox supports 5 widgets:  bar chart , pie chart ,  horizontal bar chart ,  table , and  note .  To add a widget   Generate a dashboard by following instructions of Step II above.  Click the  add widget  button below the name of the dashboard.  In the  Data Source  list, select a data source for your widget.   Select a widget type under  Available Widgets .     Click  add widget .    The widget is added to your dashboard.", 
            "title": "Step III: Add widgets"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-iv-configure-a-widget", 
            "text": "After you add a widget to your dashboard, you can configure it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.  To configure a widget   To change the size of the widget, click the border of the widget, and\n  resize it.  To move the widget around, click the widget, and drag it to the desired\n  position.  To change the title and other properties, click the  edit  button in the\n  top-right corner of the widget.\n     \n  You can now enter a new title in the  Title  box or configure the rest of the\n  options in any suitable way.  Click  OK .  To remove a widget, click the delete button in the top-right corner.", 
            "title": "Step IV: Configure a widget"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#perform-additional-tasks-on-dashboards", 
            "text": "At any time, you can change the name and the description of a dashboard. You\ncan also delete dashboards.  To perform additional tasks   Ensure that you generated a dashboard as described in Step II above and\n   select it.  Click  settings  button (next to buttons named  add widget ,\n    auto generate , and  save settings ), below the name of the dashboard to see the  Dashboard Settings  dialog:\n      Type a new name for the dashboard in the  Name of dashboard  box.  Type a suitable description in the box below.  Make sure that  TopNWordsWithQueries  is selected under  Choose apps to\n    visualize .  Click  Save .", 
            "title": "Perform additional tasks on dashboards"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#delete-a-dashboard", 
            "text": "You can delete a dashboard at any time.   Log on to the DataTorrent Console (default username and password are both\n   dtadmin )  On the top navigation bar, click  Visualize .   Select a dashboard.     Click delete.     Note: The delete button becomes visible only if one or more rows are selected.", 
            "title": "Delete a dashboard"
        }, 
        {
            "location": "/tutorials/topnwords-c7/", 
            "text": "Top N Words (Advanced)\n\n\nThis section touches on some advanced features of the RTS platform in the context of the\n\nTop N Words\n application. Accordingly, readers are expected to be familiar with the material\nof the preceding sections.\n\n\n\n\n\nThe first topic we'd like to discuss is partitioning of operators to increase performance.\nHowever, partitioning increases the memory footprint of the application, so it is important\nto know how to allocate available memory to containers especially in a limited environment\nlike a sandbox. So we begin with a brief discussion of that topic.\n\n\nManaging Memory Allocation for Containers\n\n\nIn this chapter we describe how to monitor and manage the amount of memory allocated to the\ncontainers comprising the application. This is useful in an environment where the needs of\nthe application begins to equal or exceed the memory resources of the cluster.\n\n\nRecall the following facts from the earlier sections:\n\n\n\n\nA container (JVM process) can host multiple operators.\n\n\nThe memory requirements for an operator can be specified via a properties file.\n\n\n\n\nFor reference, here is the application DAG:\n\n\n\nIf we look at the information displayed in the physical tab of \ndtManage\n for the memory\nallocated to each container, we see something like this (the actual container id will most\nlikely be different each time the application is relaunched but the rest of the information\nshould be the same):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContainer Id\n\n\nAllocated Memory\n\n\nHosted Operators\n\n\n\n\n\n\n1\n\n\n1 GB\n\n\nNone (AppMaster)\n\n\n\n\n\n\n2\n\n\n768 MB\n\n\nsnapshotServerGlobal, QueryGlobal\n\n\n\n\n\n\n3\n\n\n768 MB\n\n\nsnapshotServerFile, QueryFile\n\n\n\n\n\n\n4\n\n\n128 MB\n\n\nwsResultGlobal\n\n\n\n\n\n\n5\n\n\n640 MB\n\n\nwindowWordCount\n\n\n\n\n\n\n6\n\n\n128 MB\n\n\nConsole\n\n\n\n\n\n\n7\n\n\n128 MB\n\n\nwsResultFile\n\n\n\n\n\n\n8\n\n\n640 MB\n\n\nwordReader\n\n\n\n\n\n\n9\n\n\n128 MB\n\n\nwcWriter\n\n\n\n\n\n\n10\n\n\n1.4 GB\n\n\nlineReader\n\n\n\n\n\n\n11\n\n\n1.9 GB\n\n\nfileWordCount\n\n\n\n\n\n\n\n\n\nIf we now look closely at column 2 (Allocated Memory) we notice some\nunexpected values, for example, the value for the App Master container should have\nbeen 300 MB since we had 300 as the value in the \nproperties.xml\n file for\n\ndt.attr.MASTER_MEMORY_MB\n. The discrepancy is due to the fact that the file\n\n.dt/dt-site.xml\n in the home directory of user \ndtadmin\n has a value of 1024 for\nthis key which overrides the application specified value.\n\n\nLooking now at container 2, we notice that it hosts 2 operators: \nsnapshotServerGlobal\n\nand \nQueryGlobal\n and each has a value of 128 MB specified in the application properties\nfile; so why is the value 768 MB ? Turns out that each output port of an operator\n\nconnected to another operator outside the container\n\nalso has an associated \nbuffer-server\n which buffers tuples exiting the output port\nto provide fault-tolerance. The buffer server is discussed in detail in\n\nApplication Development\n.\nThe space allocated to the buffer server is governed by properties of the form:\n\n\ndt.application.app-name.operator.op-name.port.port-name.attr.BUFFER_MEMORY_MB\n\n\n\nwhere \napp-name\n, \nop-name\n and \nport-name\n can be replaced by the appropriate\napplication, operator and port name respectively or by a wildcard (\n*\n). The\ndefault value is 512 MB. Of the two operators, only one (\nsnapshotServerGlobal\n)\nhas an output port connected externally so there\nis only one buffer-server involved which then explains the value of\n768 (= 512 + 128 + 128).\n\n\nThe values for \nwsResultGlobal\n, \nConsole\n, \nwsResultFile\n, \nwcWriter\n are, as\nexpected, 128 MB \n since no output ports are involved, there is no buffer-server.\nThe values for \nwindowWordCount\n and \nwordReader\n are also the expected values\nsince a single buffer-server is involved: 640 = 512 + 128. The value of 1.9 GB\nfor \nfileWordCount\n is obtained as follows: it has 3 buffer-servers since there\nare 3 output ports connected externally; our properties file setting requests\n300 MB for this operator which gives us a total of 3 * 512 + 300 = 1836 MB which\napproximates the value shown. The value for \nlineReader\n can be computed\nsimilarly.\n\n\nThe total amount of allocated space shown on the GUI is 6.5 GB. We can substantially\nreduce the memory footprint further by make a couple changes to attributes:\nCreate a new file named, say, \nlow-mem.xml\n at \nsrc/site/conf/\n with this content:\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\ndt.attr.MASTER_MEMORY_MB\n/name\n\n    \nvalue\n512\n/value\n\n  \n/property\n \nproperty\n\n    \nname\ndt.application.TopNWordsWithQueries.operator.*.port.*.attr.BUFFER_MEMORY_MB\n/name\n\n    \nvalue\n128\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\nWe will use this file at launch time.\n\n\nThe BUFFER_MEMORY_MB attribute changes the memory allocation per buffer server\nto 128 MB (from the default of 512 MB).\n\n\nThe MASTER_MEMORY_MB attribute change sets the memory allocated to the Application Master\nto 512 MB and is required for a rather obscure\nreason: The existing value of 300 MB in \nMETA-INF/properties.xml\n\nis actually overridden by the setting of 1024 MB for this parameter in\n\n~dtadmin/.dt/dt-site.xml\n; however, if we use a launch-time configuration file,\nvalues in it override those in \ndt-site.xml\n. A value of 300 is too\nsmall even for simple applications; in normal use and we rarely see a case, even in production,\nwhere a value larger than 1024 MB is needed, though it is possible if the number of\noperators is large. If the App Master runs out of memory,\nyou'll see messages like this in the corresponding log file (see \nDebugging\n\nsection below):\n\n\njava.lang.OutOfMemoryError: GC overhead limit exceeded.\n\n\n\nRebuild the application, upload the package and use this file at launch time:\n\n\n\nThe allocated memory shown in the \"Application Overview\" panel should now drop\nto around 3.1GB.\n\n\nDebugging\n\n\nOn the sandbox, various log files generated by YARN and Hadoop are located at\n\n/sfw/hadoop/shared/logs\n; the \nnodemanager\n directory has application specific\ndirectories with names like this: \napplication_1448033276100_0001\n within\nwhich there are container specific directories with names like\n\ncontainer_1448033276100_0001_01_000001\n. The App Master container has the \n000001\n\nsuffix and the corresponding directory will have these files:\n\n\nAppMaster.stderr  AppMaster.stdout  dt.log\n\n\n\nThe remaining container directories will have files:\n\n\ndt.log  stderr  stdout\n\n\n\nWhen problems occur, all these log files should be carefully examined. For example, the\n\ndt.log\n file contains the entire classpath used to launch each container; if an error\noccurs because a particular class is not found, you can check the classpath to ensure\nthat the appropriate jar file is included. It also shows the command line used to\nlaunch each container with lines like this:\n\n\n2015-12-20 14:31:43,896 INFO com.datatorrent.stram.LaunchContainerRunnable: Launching on node: localhost:8052 command: $JAVA_HOME/bin/java  -Xmx234881024  -Ddt.attr.APPLICATION_PATH=hdfs://localhost:9000/user/dtadmin/datatorrent/apps/application_1450648156272_0001 -Djava.io.tmpdir=$PWD/tmp -Ddt.cid=container_1450648156272_0001_01_000002 -Dhadoop.root.logger=INFO,RFA -Dhadoop.log.dir=\n com.datatorrent.stram.engine.StreamingContainer 1\n/stdout 2\n/stderr\n\n\nYou can provide your own \nlog4j\n configuration file called, say, \nlog4j.properties\n and place\nit in the directory \nsrc/main/resources\n as described in the\n\nconfiguration\n\npage. Alternatively, if you want to change the log level of a particular class or package\nfrom, say \nINFO\n to \nDEBUG\n while the application is running, you can click on the blue\n\nset logging level\n button in the \nApplication Overview\n panel of \ndtManage\n. It will then\ndisplay a dialog window where you can enter the name of the class or package and the desired\nlog level:\n\n\n\n\nNormally, the GUI can be used to navigate to the appropriate container page\nand log files examined from the \nlogs\n dropdown but sometimes using the commandline\nfrom a terminal window may be easier.\n\n\nPartitioning\n\n\nPartitioning is a mechanism to eliminate bottlenecks in your application and increase\nthroughput. If an operator is performing a resource intensive operation, it risks\nbecoming a bottleneck as the rate of incoming tuples increases. One way to cope\nis to replicate the operator as many times as necessary so that the load is\nevenly distributed across the replicas, thus eliminating the bottleneck. Of course,\nthis technique assumes that your cluster has adequate resources (CPU, memory and\nnetwork bandwidth) to support all the replicas.\n\n\nWithout partitioning, the DAG shown in the \nlogical\n and \nphysical-dag-view\n tabs\nwill be the same.\nHowever, once partitioning is triggered, the latter will show multiple copies of the\npartitioned operator, as well as a new operator immediately\ndownstream of all the copies, called a \nunifier\n. The job of the unifier is to join the\nresults emitted by all the copies, collate them in some application-specific way and\nemit the result just as it would have been emitted if no partitioning were involved.\nThe unifier can either be one that is custom-written for the needs of the application\nor a pass-through platform-generated one.\n\n\nFor our word counting example, we illustrate the technique by partitioning the \nwordReader\n\noperator into 2 copies. For operators that do not maintain state, partitioning does\nnot require additional code: We can simply set a couple of properties -- one to use\nthe \nStatelessPartitioner\n which is part of Malhar and one to specify the number\nof desired partitions. To do this, copy over the \nlow-mem.xml\n configuration file\nwe created above\nto a new file named \nsimple-partition.xml\n in the same directory and add this stanza to it:\n\n\nproperty\n\n  \nname\ndt.application.TopNWordsWithQueries.operator.wordReader.attr.PARTITIONER\n/name\n\n  \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:2\n/value\n\n\n/property\n\n\n\n\nWhen you build, upload and run the application using this configuration file, the\nphysical-dag-view tab should show the following DAG:\n\n\n\n\nNotice the two copies of \nwordReader\n and the generated unifier. The \nphysical\n tab will\nalso show the containers for these additional operators and their characteristics as well.\n\n\nA slight variation of the above theme occurs often in practice: We would like an entire\nlinear sequence of operators (i.e. a fragment of the DAG) replicated in the same way.\nIn our case, the sequence consists of two operators: \nwordReader\n and the next operator\n\nwindowWordCount\n. To accomplish this, again no additional code is required: We can simply\nadd this stanza to our properties file:\n\n\nproperty\n\n  \nname\ndt.application.TopNWordsWithQueries.operator.windowWordCount.inputport.input.attr.PARTITION_PARALLEL\n/name\n\n  \nvalue\ntrue\n/value\n\n\n/property\n\n\n\n\nIt enables the PARTITION_PARALLEL attribute on the \ninput port\n of the downstream operator,\nthus indicating to the platform that the downstream operator must be partitioned into just\nas many copies as the upstream operator so that they form parallel pipelines. Running the\napplication with this configuration file shows the following physical DAG:\n\n\n\n\nNotice that both operators have been replicated and the unifier added; we now have 15\noperators with a total of 4.8GB allocated to them.\n\n\nStreaming Windows and Application Windows\n\n\nOperators receive incoming tuples and emit outgoing tuples within a small temporal window\ncalled a \nstreaming window\n. Its boundaries are marked by calls to \nbeginWindow\n and\n\nendWindow\n within which the platform repeatedly invokes either \nemitTuples\n (for input\nadapters) or \nprocess\n on each input port for output adapters and generic operators.\nThese concepts are discussed in greater detail in the\n\nOperatorGuide\n.\n\n\nFor flexibility in operator and application development, the platform allows users to\nchange the size of the streaming window which is defined as a number of milliseconds.\nIt defaults to 500ms but can be changed by setting the\nvalue of an attribute named STREAMING_WINDOW_SIZE_MILLIS; for example, you can set it\nto 5s with:\n\n\nproperty\n\n  \nname\ndt.attr.STREAMING_WINDOW_SIZE_MILLIS\n/name\n\n  \nvalue\n5000\n/value\n\n\n/property\n\n\n\n\nThis is not a very common change but one reason for doing it might be if the stream is\nvery sparse, i.e. the number of incoming tuples in a 500ms window is very small; by\nincreasing the streaming window size, we can substantially reduce the platform bookkeeping\noverhead such as checkpointing.\n\n\nA second attribute is APPLICATION_WINDOW_COUNT; this is a per-operator attribute and is\na count of streaming windows that comprise a single application window. It\ncan be changed with an entry like this (where, as before, \napp-name\n and \nop-name\n should be\nreplaced by either wildcards or names of a specific application and/or operator):\n\n\nproperty\n\n  \nname\ndt.application.app-name.operator.op-name.attr.APPLICATION_WINDOW_COUNT\n/name\n\n  \nvalue\n5\n/value\n\n\n/property\n\n\n\n\nBy default this value is set to 1 meaning each application window consists of a single\nstreaming window. The the \nbeginWindow\n and \nendWindow\n are invoked once per application\nwindow. A typical reason for increasing this value is when you have an\noperator that is computing aggregates (such as sum, average, maximum, minimum) of one or\nmore fields of the incoming tuples: A larger application window may yield more\nmeaningful aggregates.", 
            "title": "Advanced Features"
        }, 
        {
            "location": "/tutorials/topnwords-c7/#top-n-words-advanced", 
            "text": "This section touches on some advanced features of the RTS platform in the context of the Top N Words  application. Accordingly, readers are expected to be familiar with the material\nof the preceding sections.   The first topic we'd like to discuss is partitioning of operators to increase performance.\nHowever, partitioning increases the memory footprint of the application, so it is important\nto know how to allocate available memory to containers especially in a limited environment\nlike a sandbox. So we begin with a brief discussion of that topic.", 
            "title": "Top N Words (Advanced)"
        }, 
        {
            "location": "/tutorials/topnwords-c7/#managing-memory-allocation-for-containers", 
            "text": "In this chapter we describe how to monitor and manage the amount of memory allocated to the\ncontainers comprising the application. This is useful in an environment where the needs of\nthe application begins to equal or exceed the memory resources of the cluster.  Recall the following facts from the earlier sections:   A container (JVM process) can host multiple operators.  The memory requirements for an operator can be specified via a properties file.   For reference, here is the application DAG:  If we look at the information displayed in the physical tab of  dtManage  for the memory\nallocated to each container, we see something like this (the actual container id will most\nlikely be different each time the application is relaunched but the rest of the information\nshould be the same):          Container Id  Allocated Memory  Hosted Operators    1  1 GB  None (AppMaster)    2  768 MB  snapshotServerGlobal, QueryGlobal    3  768 MB  snapshotServerFile, QueryFile    4  128 MB  wsResultGlobal    5  640 MB  windowWordCount    6  128 MB  Console    7  128 MB  wsResultFile    8  640 MB  wordReader    9  128 MB  wcWriter    10  1.4 GB  lineReader    11  1.9 GB  fileWordCount     If we now look closely at column 2 (Allocated Memory) we notice some\nunexpected values, for example, the value for the App Master container should have\nbeen 300 MB since we had 300 as the value in the  properties.xml  file for dt.attr.MASTER_MEMORY_MB . The discrepancy is due to the fact that the file .dt/dt-site.xml  in the home directory of user  dtadmin  has a value of 1024 for\nthis key which overrides the application specified value.  Looking now at container 2, we notice that it hosts 2 operators:  snapshotServerGlobal \nand  QueryGlobal  and each has a value of 128 MB specified in the application properties\nfile; so why is the value 768 MB ? Turns out that each output port of an operator connected to another operator outside the container \nalso has an associated  buffer-server  which buffers tuples exiting the output port\nto provide fault-tolerance. The buffer server is discussed in detail in Application Development .\nThe space allocated to the buffer server is governed by properties of the form:  dt.application.app-name.operator.op-name.port.port-name.attr.BUFFER_MEMORY_MB  where  app-name ,  op-name  and  port-name  can be replaced by the appropriate\napplication, operator and port name respectively or by a wildcard ( * ). The\ndefault value is 512 MB. Of the two operators, only one ( snapshotServerGlobal )\nhas an output port connected externally so there\nis only one buffer-server involved which then explains the value of\n768 (= 512 + 128 + 128).  The values for  wsResultGlobal ,  Console ,  wsResultFile ,  wcWriter  are, as\nexpected, 128 MB   since no output ports are involved, there is no buffer-server.\nThe values for  windowWordCount  and  wordReader  are also the expected values\nsince a single buffer-server is involved: 640 = 512 + 128. The value of 1.9 GB\nfor  fileWordCount  is obtained as follows: it has 3 buffer-servers since there\nare 3 output ports connected externally; our properties file setting requests\n300 MB for this operator which gives us a total of 3 * 512 + 300 = 1836 MB which\napproximates the value shown. The value for  lineReader  can be computed\nsimilarly.  The total amount of allocated space shown on the GUI is 6.5 GB. We can substantially\nreduce the memory footprint further by make a couple changes to attributes:\nCreate a new file named, say,  low-mem.xml  at  src/site/conf/  with this content:  configuration \n   property \n     name dt.attr.MASTER_MEMORY_MB /name \n     value 512 /value \n   /property   property \n     name dt.application.TopNWordsWithQueries.operator.*.port.*.attr.BUFFER_MEMORY_MB /name \n     value 128 /value \n   /property  /configuration   We will use this file at launch time.  The BUFFER_MEMORY_MB attribute changes the memory allocation per buffer server\nto 128 MB (from the default of 512 MB).  The MASTER_MEMORY_MB attribute change sets the memory allocated to the Application Master\nto 512 MB and is required for a rather obscure\nreason: The existing value of 300 MB in  META-INF/properties.xml \nis actually overridden by the setting of 1024 MB for this parameter in ~dtadmin/.dt/dt-site.xml ; however, if we use a launch-time configuration file,\nvalues in it override those in  dt-site.xml . A value of 300 is too\nsmall even for simple applications; in normal use and we rarely see a case, even in production,\nwhere a value larger than 1024 MB is needed, though it is possible if the number of\noperators is large. If the App Master runs out of memory,\nyou'll see messages like this in the corresponding log file (see  Debugging \nsection below):  java.lang.OutOfMemoryError: GC overhead limit exceeded.  Rebuild the application, upload the package and use this file at launch time:  The allocated memory shown in the \"Application Overview\" panel should now drop\nto around 3.1GB.", 
            "title": "Managing Memory Allocation for Containers"
        }, 
        {
            "location": "/tutorials/topnwords-c7/#debugging", 
            "text": "On the sandbox, various log files generated by YARN and Hadoop are located at /sfw/hadoop/shared/logs ; the  nodemanager  directory has application specific\ndirectories with names like this:  application_1448033276100_0001  within\nwhich there are container specific directories with names like container_1448033276100_0001_01_000001 . The App Master container has the  000001 \nsuffix and the corresponding directory will have these files:  AppMaster.stderr  AppMaster.stdout  dt.log  The remaining container directories will have files:  dt.log  stderr  stdout  When problems occur, all these log files should be carefully examined. For example, the dt.log  file contains the entire classpath used to launch each container; if an error\noccurs because a particular class is not found, you can check the classpath to ensure\nthat the appropriate jar file is included. It also shows the command line used to\nlaunch each container with lines like this:  2015-12-20 14:31:43,896 INFO com.datatorrent.stram.LaunchContainerRunnable: Launching on node: localhost:8052 command: $JAVA_HOME/bin/java  -Xmx234881024  -Ddt.attr.APPLICATION_PATH=hdfs://localhost:9000/user/dtadmin/datatorrent/apps/application_1450648156272_0001 -Djava.io.tmpdir=$PWD/tmp -Ddt.cid=container_1450648156272_0001_01_000002 -Dhadoop.root.logger=INFO,RFA -Dhadoop.log.dir=  com.datatorrent.stram.engine.StreamingContainer 1 /stdout 2 /stderr  You can provide your own  log4j  configuration file called, say,  log4j.properties  and place\nit in the directory  src/main/resources  as described in the configuration \npage. Alternatively, if you want to change the log level of a particular class or package\nfrom, say  INFO  to  DEBUG  while the application is running, you can click on the blue set logging level  button in the  Application Overview  panel of  dtManage . It will then\ndisplay a dialog window where you can enter the name of the class or package and the desired\nlog level:   Normally, the GUI can be used to navigate to the appropriate container page\nand log files examined from the  logs  dropdown but sometimes using the commandline\nfrom a terminal window may be easier.", 
            "title": "Debugging"
        }, 
        {
            "location": "/tutorials/topnwords-c7/#partitioning", 
            "text": "Partitioning is a mechanism to eliminate bottlenecks in your application and increase\nthroughput. If an operator is performing a resource intensive operation, it risks\nbecoming a bottleneck as the rate of incoming tuples increases. One way to cope\nis to replicate the operator as many times as necessary so that the load is\nevenly distributed across the replicas, thus eliminating the bottleneck. Of course,\nthis technique assumes that your cluster has adequate resources (CPU, memory and\nnetwork bandwidth) to support all the replicas.  Without partitioning, the DAG shown in the  logical  and  physical-dag-view  tabs\nwill be the same.\nHowever, once partitioning is triggered, the latter will show multiple copies of the\npartitioned operator, as well as a new operator immediately\ndownstream of all the copies, called a  unifier . The job of the unifier is to join the\nresults emitted by all the copies, collate them in some application-specific way and\nemit the result just as it would have been emitted if no partitioning were involved.\nThe unifier can either be one that is custom-written for the needs of the application\nor a pass-through platform-generated one.  For our word counting example, we illustrate the technique by partitioning the  wordReader \noperator into 2 copies. For operators that do not maintain state, partitioning does\nnot require additional code: We can simply set a couple of properties -- one to use\nthe  StatelessPartitioner  which is part of Malhar and one to specify the number\nof desired partitions. To do this, copy over the  low-mem.xml  configuration file\nwe created above\nto a new file named  simple-partition.xml  in the same directory and add this stanza to it:  property \n   name dt.application.TopNWordsWithQueries.operator.wordReader.attr.PARTITIONER /name \n   value com.datatorrent.common.partitioner.StatelessPartitioner:2 /value  /property   When you build, upload and run the application using this configuration file, the\nphysical-dag-view tab should show the following DAG:   Notice the two copies of  wordReader  and the generated unifier. The  physical  tab will\nalso show the containers for these additional operators and their characteristics as well.  A slight variation of the above theme occurs often in practice: We would like an entire\nlinear sequence of operators (i.e. a fragment of the DAG) replicated in the same way.\nIn our case, the sequence consists of two operators:  wordReader  and the next operator windowWordCount . To accomplish this, again no additional code is required: We can simply\nadd this stanza to our properties file:  property \n   name dt.application.TopNWordsWithQueries.operator.windowWordCount.inputport.input.attr.PARTITION_PARALLEL /name \n   value true /value  /property   It enables the PARTITION_PARALLEL attribute on the  input port  of the downstream operator,\nthus indicating to the platform that the downstream operator must be partitioned into just\nas many copies as the upstream operator so that they form parallel pipelines. Running the\napplication with this configuration file shows the following physical DAG:   Notice that both operators have been replicated and the unifier added; we now have 15\noperators with a total of 4.8GB allocated to them.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/tutorials/topnwords-c7/#streaming-windows-and-application-windows", 
            "text": "Operators receive incoming tuples and emit outgoing tuples within a small temporal window\ncalled a  streaming window . Its boundaries are marked by calls to  beginWindow  and endWindow  within which the platform repeatedly invokes either  emitTuples  (for input\nadapters) or  process  on each input port for output adapters and generic operators.\nThese concepts are discussed in greater detail in the OperatorGuide .  For flexibility in operator and application development, the platform allows users to\nchange the size of the streaming window which is defined as a number of milliseconds.\nIt defaults to 500ms but can be changed by setting the\nvalue of an attribute named STREAMING_WINDOW_SIZE_MILLIS; for example, you can set it\nto 5s with:  property \n   name dt.attr.STREAMING_WINDOW_SIZE_MILLIS /name \n   value 5000 /value  /property   This is not a very common change but one reason for doing it might be if the stream is\nvery sparse, i.e. the number of incoming tuples in a 500ms window is very small; by\nincreasing the streaming window size, we can substantially reduce the platform bookkeeping\noverhead such as checkpointing.  A second attribute is APPLICATION_WINDOW_COUNT; this is a per-operator attribute and is\na count of streaming windows that comprise a single application window. It\ncan be changed with an entry like this (where, as before,  app-name  and  op-name  should be\nreplaced by either wildcards or names of a specific application and/or operator):  property \n   name dt.application.app-name.operator.op-name.attr.APPLICATION_WINDOW_COUNT /name \n   value 5 /value  /property   By default this value is set to 1 meaning each application window consists of a single\nstreaming window. The the  beginWindow  and  endWindow  are invoked once per application\nwindow. A typical reason for increasing this value is when you have an\noperator that is computing aggregates (such as sum, average, maximum, minimum) of one or\nmore fields of the incoming tuples: A larger application window may yield more\nmeaningful aggregates.", 
            "title": "Streaming Windows and Application Windows"
        }, 
        {
            "location": "/tutorials/topnwords-c6/", 
            "text": "Appendix\n\n\nOperators in Top N words application\n\n\nThis section describes the operators used for building the top N\nwords application. The operators, the implementing classes and a brief description of their\nfunctionalities are described in this table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\n\n\nImplementing class\n\n\nDescription\n\n\n\n\n\n\nlineReader\n\n\nLineReader\n\n\nReads lines from input files.\n\n\n\n\n\n\nwordReader\n\n\nWordReader\n\n\nSplits a line into words.\n\n\n\n\n\n\nwindowWordCount\n\n\nWindowWordCount\n\n\nComputes word frequencies for a single window.\n\n\n\n\n\n\nfileWordCount\n\n\nFileWordCount\n\n\nMaintains per-file and global word frequencies.\n\n\n\n\n\n\nwcWriter\n\n\nWcWriter\n\n\nWrites top N words and their frequencies to output files.\n\n\n\n\n\n\nconsole\n\n\nConsoleOutputOperator\n\n\nWrites received tuples to console.\n\n\n\n\n\n\nsnapshotServerFile\n\n\nAppDataSnapshotServerMap\n\n\nCaches the last data set for the current file, and returns it in response to queries.\n\n\n\n\n\n\nsnapshotServerGlobal\n\n\nAppDataSnapshotServerMap\n\n\nCaches the last global data set, and returns it in response to queries.\n\n\n\n\n\n\nQueryFile\n\n\nPubSubWebSocketAppDataQuery\n\n\nReceives queries for per-file data.\n\n\n\n\n\n\nQueryGlobal\n\n\nPubSubWebSocketAppDataQuery\n\n\nReceives queries for global data.\n\n\n\n\n\n\nwsResultFile\n\n\nPubSubWebSocketAppDataResult\n\n\nReturns results for per-file queries.\n\n\n\n\n\n\nwsResultGlobal\n\n\nPubSubWebSocketAppDataResult\n\n\nReturns results for global queries.\n\n\n\n\n\n\n\n\n\nWe now describe the process of wiring these operators together in the\n\npopulateDAG()\n method of the main application class\n\nApplicationWithQuerySupport\n. First, the operators are created and added to\nthe DAG via the \naddOperator\n method:\n\n\nLineReader lineReader = dag.addOperator(\nlineReader\n,new LineReader());\n\n\n\n\nThe first argument is a string that names this instance of the\noperator; it is the same as the value in the first column of the above\ntable and also the node name in the Logical DAG.\n\n\nNext, we connect each output port of an operator with all the input ports that\nshould receive these tuples using the \naddStream\n function, for example:\n\n\ndag.addStream(\nlines\n, lineReader.output, wordReader.input);\n...\ndag.addStream(\nWordCountsFile\n, fileWordCount.outputPerFile, snapshotServerFile.input, console.input);\n\n\n\n\nNotice that the stream from \nfileWordCount.outputPerFile\n (which consists of\nthe top N words for the current file as the file is being read) goes to\n\nsnapshotServerFile.input\n (where it will be saved to respond to queries) and to\n\nconsole.input\n (which is used for debugging). Additional sinks can be provided\nin the same call as additional terminal arguments. You can examine the rest of\nthese calls and ensure that they match the names and connections of the\nLogical DAG.\n\n\nThis section provides detailed information about each operator.\n\n\nLineReader\n\n\nThis class extends \nAbstractFileInputOperator\nString\n to open a file, read its\nlines, and emit them as tuples. It has two output ports, one for the normal\noutput of tuples, and the other for the output of an EOF tuple indicating the\nend of the current input file. Ports should always be transient fields because\nthey should not be serialized and saved to the disk during checkpointing.\n\n\nThe base class keeps track of files already processed, files that\nshould be ignored, and files that failed part-way. Derived classes need to\noverride four methods: \nopenFile\n, \ncloseFile\n, \nreadEntity\n, and \nemit\n. Of\nthese, only the third is non-trivial: if a valid line is available, it is read\nand returned. Otherwise, the end of the file must have been reached. To\nindicate this, the file name is emitted on the control port where it\nwill be read by the \nFileWordCount\n operator.\n\n\nWordReader\n\n\nThis operator receives lines from \nLineReader\n on the input port and emits\nwords on the output port. It has a configurable property called \nnonWordStr\n\nalong with associated public getter and setter methods. Such properties can be\ncustomized in the appropriate properties file of the application. The values of\nthe properties are automatically injected into the operator at run-time. In\nthis scenario, this string is provided the value of the property\n\ndt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr\n.\nFor efficiency, this string is compiled into a pattern for repeated use.\nThe \nprocess\n method of the input port splits each input line into words using\nthis pattern as the separator, and emits non-empty words on the output port.\n\n\nWindowWordCount\n\n\nThis operator receives words and emits a list of word-frequency pairs for each\nwindow. It maintains a word-frequency map for the current window, updates this\nmap for each word received, emits the whole map (if non-empty) when\n\nendWindow\n is called, and clears the map in preparation for the next window.\nThis design pattern is appropriate because for normal text files, the number of\nwords received is far more than the size of the accumulated map. However, for\nsituations where data is emitted for each tuple, you should not wait till the\n\nendWindow\n call, but rather emit output tuples as each input tuple is\nprocessed.\n\n\nFileWordCount\n\n\nThis operator has two input ports, one for the per-window frequency maps it\ngets from the previous operator, and a control port to receive the file name\nwhen \nLineReader\n reaches the end of a file. When a file name is received on\nthe control port, it is saved and the final results for the file appear as\noutput at the next \nendWindow\n. The reason for waiting is subtle: there is no\nguarantee of the relative order in which tuples arrive at two input ports;\nadditional input tuples from the same window can arrive at the input port\neven after the EOF was received on the control port. Note however that we \ndo\n\nhave a guarantee that tuples on the input port will arrive in exactly the same\norder in which they were emitted on the output port between the bracketing\n\nbeginWindow\n and \nendWindow\n calls by the upstream operator.\n\n\nThis operator also has three output ports: the \noutputPerFile\n port for the top\nN pairs for the current file as it is being read; the \noutputGlobal\n port for\nthe global top N pairs, and the \nfileOutput\n port for the final top N pairs for\nthe current file computed after receiving the EOF control tuple. The output\nfrom the first is sent to the per-file snapshot server, the output from\nthe second is sent to the global snapshot server, and the output from the last\nis sent to the operator that writes results to the output file.\n\n\nFileWordCount\n also maintains two maps for per-file and global frequency\ncounts because they track frequencies of all words seen so far. These maps\ncan get very large as more and more files are processed.\n\n\nFileWordCount\n has a configurable property \ntopN\n for the number of top pairs we\nare interested in. This was configured in our properties file with a value of\n10 and the property name: \ndt.application.TopNWordsWithQueries.operator.fileWordCount.topN\n\n\nIn the \nendWindow\n call, both maps are passed to the \ngetTopNList\n function\nwhere they are flattened, sorted in descending order of frequency, stripped of\nall but the top N pairs, and returned for output. There are a couple of\nadditional fields used to cast the output into the somewhat peculiar form\nrequired by the snapshot server.\n\n\nWordCountWriter\n\n\nThis operator extends \nAbstractFileOutputOperator\nMap\nString,Object\n, and\nsimply writes the final top N pairs to the output file. As with \nLineReader\n,\nmost of the complexity of \nWordCountWriter\n is hidden in the base class. You must\nprovide implementations for 3 methods: \nendWindow\n, \ngetFileName\n, and\n\ngetBytesForTuple\n. The first method calls the base class method \nrequestFinalize\n.\nThe output file is written periodically to temporary files\nwith a synthetic file name that includes a timestamp. These files are removed\nand the actual desired file name is restored by this call. The \ngetFileName\n\nmethod retrieves the file name from the tuple, and the \ngetBytesForTuple\n\nmethod converts the list of pairs to a string in the desired format.\n\n\nConsoleOutputOperator\n\n\nThis is an output operator that is a part of the Malhar library. It simply\nwrites incoming tuples to the console and is useful when debugging.\n\n\nAppDataSnapshotServerMap\n\n\nThis operator is also part of the Malhar library and is used to store snapshots\nof data. These snapshots are used to respond to queries. For this application,\nwe use two snapshots \n  one for a per-file top N snapshot and one for a\nglobal snapshot.\n\n\nPubSubWebSocketAppDataQuery\n\n\nThis is an input operator that is a part of the Malhar library. It is used to\nsend queries to an operator via the Data Torrent Gateway, which can act as a\nmessage broker for limited amounts of data using a topic-based\npublish-subscribe model. The URL to connect is typically something like:\n\n\nws://gateway-host:port/pubsub\n\n\n\n\nwhere \ngateway-host\n and \nport\n should be replaced by appropriate values.\n\n\nA publisher sends a JSON message to the URL where the value of the \ndata\n key\nis the desired message content. The JSON might look like this:\n\n\n{\ntype\n:\npublish\n, \ntopic\n:\nfoobar\n, \ndata\n: ...}\n\n\n\n\nCorrespondingly, subscribers send messages like this to retrieve published\nmessage data:\n\n\n{\ntype\n:\nsubscribe\n, \ntopic\n:\nfoobar\n}\n\n\n\n\nTopic names need not be pre-registered anywhere but the same topic\nname (for example, \nfoobar\n in the example) must be used by both publisher and\nsubscriber. Additionally, if there are no subscribers when a message is\npublished, it is simply discarded.\n\n\nFor this tutorial, two query operators are used: one for per-file queries and\none for global queries. The topic names were configured in the properties file\nearlier with values \nTopNWordsQueryFile\n and \nTopNWordsQueryGlobal\n under the\nrespective names:\n\n\ndt.application.TopNWordsWithQueries.operator.QueryFile.topic\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic\n\n\n\n\nPubSubWebSocketAppDataResult\n\n\nAnalogous to the previous operator, this is an output operator used to publish\nquery results to a gateway topic. You must use two of these to match the query\noperators, and configure their topics in the properties file with values\n\nTopNWordsQueryFileResult\n and \nTopNWordsQueryGlobalResult\n corresponding to\nthe respective names:\n\n\ndt.application.TopNWordsWithQueries.operator.wsResultFile.topic\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic\n\n\n\n\nFurther Exploration\n\n\nIn this tutorial, the property values in the \nproperties.xml\n file were set to\nlimit the amount of memory allocated to each operator. You can try varying\nthese values and checking the impact of such an operation on the stability and\nperformance of the application. You can also explore the largest text\nfile that the application can handle.\n\n\nAnother aspect to explore is fixing the current limitation of\none-file-at-a-time processing; if multiple files are dropped into the\ninput directory simultaneously, the file reader can switch from one file to the\nnext in the same window. When the \nFileWordCount\n operator gets an EOF on the\ncontrol port, it waits for an \nendWindow\n call to emit word counts so those\ncounts will be incorrect if tuples from two different files arrive in the same\nwindow. Try fixing this issue.\n\n\nDataTorrent terminology\n\n\nOperators\n\n\nOperators are basic computation units that have properties and\nattributes, and are interconnected via streams to form an application.\nProperties customize the functional definition of the operator, while\nattributes customize the operational behavior. You can think of\noperators as classes for implementing the operator interface. They read\nfrom incoming streams of tuples and write to other streams.\n\n\nStreams\n\n\nA stream is a connector (edge) abstraction which is a fundamental building\nblock of DataTorrent RTS. A stream consists of tuples that flow from one input\nport to one or more output ports.\n\n\nPorts\n\n\nPorts are transient objects declared in the operator class and act connection\npoints for operators. Tuples flow in and out through ports. Input ports read\nfrom streams while output port write to streams.\n\n\nDirected Acyclic Graph (DAG)\n\n\nA DAG is a logical representation of real-time stream processing application.\nThe computational units within a DAG are called operators and the data-flow\nedges are called data streams.\n\n\nLogical Plan or DAG\n\n\nLogical Plan is the Data Object Model (DOM) that is created as operators and\nstreams are added to the DAG. It is identical to a DAG.\n\n\nPhysical Plan or DAG\n\n\nA physical plan is the physical representation of the logical plan of the\napplication, which depicts how applications run on physical containers and\nnodes of a DataTorrent cluster.\n\n\nData Tuples Processed\n\n\nThis is the number of data objects processed by real-time stream processing\napplications.\n\n\nData Tuples Emitted\n\n\nThis is the number of data objects emitted after real-time stream processing\napplications complete processing operations.\n\n\nStreaming Application Manager (STRAM)\n\n\nStreaming Application Manager (STRAM) is a YARN-native, lightweight controller\nprocess. It is the process that is activated first upon application launch to\norchestrate the streaming application.\n\n\nStreaming Window\n\n\nA streaming window is a duration during which a set of tuples are emitted. The\ncollection of these tuples constitutes a window data set, also called as an\natomic micro-batch.\n\n\nSliding Application Window\n\n\nSliding window is computation that requires \"n\" streaming windows. After each\nstreaming window, the nth window is dropped, and the new window is added to the\ncomputation.\n\n\nDemo Applications\n\n\nThe real-time stream processing applications which are packaged with the\nDataTorrent RTS binaries, are called demo applications. A Demo application can\nbe launched standalone, or on a Hadoop cluster.\n\n\nCommand-line Interface\n\n\nCommand line interface (CLI) is the access point for applications.\nThis is a wrapper around the web services layer, which makes the web\nservices user friendly.\n\n\nWeb services\n\n\nDataTorrent RTS platform provides a robust webservices layer called\nDT Gateway. Currently, Hadoop provides detailed web services for\nmap-reduce jobs. The DataTorrent RTS platform leverages the same\nframework to provide a web service interface for real-time streaming\napplications.", 
            "title": "Appendix"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#appendix", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#operators-in-top-n-words-application", 
            "text": "This section describes the operators used for building the top N\nwords application. The operators, the implementing classes and a brief description of their\nfunctionalities are described in this table.          Operator  Implementing class  Description    lineReader  LineReader  Reads lines from input files.    wordReader  WordReader  Splits a line into words.    windowWordCount  WindowWordCount  Computes word frequencies for a single window.    fileWordCount  FileWordCount  Maintains per-file and global word frequencies.    wcWriter  WcWriter  Writes top N words and their frequencies to output files.    console  ConsoleOutputOperator  Writes received tuples to console.    snapshotServerFile  AppDataSnapshotServerMap  Caches the last data set for the current file, and returns it in response to queries.    snapshotServerGlobal  AppDataSnapshotServerMap  Caches the last global data set, and returns it in response to queries.    QueryFile  PubSubWebSocketAppDataQuery  Receives queries for per-file data.    QueryGlobal  PubSubWebSocketAppDataQuery  Receives queries for global data.    wsResultFile  PubSubWebSocketAppDataResult  Returns results for per-file queries.    wsResultGlobal  PubSubWebSocketAppDataResult  Returns results for global queries.     We now describe the process of wiring these operators together in the populateDAG()  method of the main application class ApplicationWithQuerySupport . First, the operators are created and added to\nthe DAG via the  addOperator  method:  LineReader lineReader = dag.addOperator( lineReader ,new LineReader());  The first argument is a string that names this instance of the\noperator; it is the same as the value in the first column of the above\ntable and also the node name in the Logical DAG.  Next, we connect each output port of an operator with all the input ports that\nshould receive these tuples using the  addStream  function, for example:  dag.addStream( lines , lineReader.output, wordReader.input);\n...\ndag.addStream( WordCountsFile , fileWordCount.outputPerFile, snapshotServerFile.input, console.input);  Notice that the stream from  fileWordCount.outputPerFile  (which consists of\nthe top N words for the current file as the file is being read) goes to snapshotServerFile.input  (where it will be saved to respond to queries) and to console.input  (which is used for debugging). Additional sinks can be provided\nin the same call as additional terminal arguments. You can examine the rest of\nthese calls and ensure that they match the names and connections of the\nLogical DAG.  This section provides detailed information about each operator.  LineReader  This class extends  AbstractFileInputOperator String  to open a file, read its\nlines, and emit them as tuples. It has two output ports, one for the normal\noutput of tuples, and the other for the output of an EOF tuple indicating the\nend of the current input file. Ports should always be transient fields because\nthey should not be serialized and saved to the disk during checkpointing.  The base class keeps track of files already processed, files that\nshould be ignored, and files that failed part-way. Derived classes need to\noverride four methods:  openFile ,  closeFile ,  readEntity , and  emit . Of\nthese, only the third is non-trivial: if a valid line is available, it is read\nand returned. Otherwise, the end of the file must have been reached. To\nindicate this, the file name is emitted on the control port where it\nwill be read by the  FileWordCount  operator.  WordReader  This operator receives lines from  LineReader  on the input port and emits\nwords on the output port. It has a configurable property called  nonWordStr \nalong with associated public getter and setter methods. Such properties can be\ncustomized in the appropriate properties file of the application. The values of\nthe properties are automatically injected into the operator at run-time. In\nthis scenario, this string is provided the value of the property dt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr .\nFor efficiency, this string is compiled into a pattern for repeated use.\nThe  process  method of the input port splits each input line into words using\nthis pattern as the separator, and emits non-empty words on the output port.  WindowWordCount  This operator receives words and emits a list of word-frequency pairs for each\nwindow. It maintains a word-frequency map for the current window, updates this\nmap for each word received, emits the whole map (if non-empty) when endWindow  is called, and clears the map in preparation for the next window.\nThis design pattern is appropriate because for normal text files, the number of\nwords received is far more than the size of the accumulated map. However, for\nsituations where data is emitted for each tuple, you should not wait till the endWindow  call, but rather emit output tuples as each input tuple is\nprocessed.  FileWordCount  This operator has two input ports, one for the per-window frequency maps it\ngets from the previous operator, and a control port to receive the file name\nwhen  LineReader  reaches the end of a file. When a file name is received on\nthe control port, it is saved and the final results for the file appear as\noutput at the next  endWindow . The reason for waiting is subtle: there is no\nguarantee of the relative order in which tuples arrive at two input ports;\nadditional input tuples from the same window can arrive at the input port\neven after the EOF was received on the control port. Note however that we  do \nhave a guarantee that tuples on the input port will arrive in exactly the same\norder in which they were emitted on the output port between the bracketing beginWindow  and  endWindow  calls by the upstream operator.  This operator also has three output ports: the  outputPerFile  port for the top\nN pairs for the current file as it is being read; the  outputGlobal  port for\nthe global top N pairs, and the  fileOutput  port for the final top N pairs for\nthe current file computed after receiving the EOF control tuple. The output\nfrom the first is sent to the per-file snapshot server, the output from\nthe second is sent to the global snapshot server, and the output from the last\nis sent to the operator that writes results to the output file.  FileWordCount  also maintains two maps for per-file and global frequency\ncounts because they track frequencies of all words seen so far. These maps\ncan get very large as more and more files are processed.  FileWordCount  has a configurable property  topN  for the number of top pairs we\nare interested in. This was configured in our properties file with a value of\n10 and the property name:  dt.application.TopNWordsWithQueries.operator.fileWordCount.topN  In the  endWindow  call, both maps are passed to the  getTopNList  function\nwhere they are flattened, sorted in descending order of frequency, stripped of\nall but the top N pairs, and returned for output. There are a couple of\nadditional fields used to cast the output into the somewhat peculiar form\nrequired by the snapshot server.  WordCountWriter  This operator extends  AbstractFileOutputOperator Map String,Object , and\nsimply writes the final top N pairs to the output file. As with  LineReader ,\nmost of the complexity of  WordCountWriter  is hidden in the base class. You must\nprovide implementations for 3 methods:  endWindow ,  getFileName , and getBytesForTuple . The first method calls the base class method  requestFinalize .\nThe output file is written periodically to temporary files\nwith a synthetic file name that includes a timestamp. These files are removed\nand the actual desired file name is restored by this call. The  getFileName \nmethod retrieves the file name from the tuple, and the  getBytesForTuple \nmethod converts the list of pairs to a string in the desired format.  ConsoleOutputOperator  This is an output operator that is a part of the Malhar library. It simply\nwrites incoming tuples to the console and is useful when debugging.  AppDataSnapshotServerMap  This operator is also part of the Malhar library and is used to store snapshots\nof data. These snapshots are used to respond to queries. For this application,\nwe use two snapshots    one for a per-file top N snapshot and one for a\nglobal snapshot.  PubSubWebSocketAppDataQuery  This is an input operator that is a part of the Malhar library. It is used to\nsend queries to an operator via the Data Torrent Gateway, which can act as a\nmessage broker for limited amounts of data using a topic-based\npublish-subscribe model. The URL to connect is typically something like:  ws://gateway-host:port/pubsub  where  gateway-host  and  port  should be replaced by appropriate values.  A publisher sends a JSON message to the URL where the value of the  data  key\nis the desired message content. The JSON might look like this:  { type : publish ,  topic : foobar ,  data : ...}  Correspondingly, subscribers send messages like this to retrieve published\nmessage data:  { type : subscribe ,  topic : foobar }  Topic names need not be pre-registered anywhere but the same topic\nname (for example,  foobar  in the example) must be used by both publisher and\nsubscriber. Additionally, if there are no subscribers when a message is\npublished, it is simply discarded.  For this tutorial, two query operators are used: one for per-file queries and\none for global queries. The topic names were configured in the properties file\nearlier with values  TopNWordsQueryFile  and  TopNWordsQueryGlobal  under the\nrespective names:  dt.application.TopNWordsWithQueries.operator.QueryFile.topic\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic  PubSubWebSocketAppDataResult  Analogous to the previous operator, this is an output operator used to publish\nquery results to a gateway topic. You must use two of these to match the query\noperators, and configure their topics in the properties file with values TopNWordsQueryFileResult  and  TopNWordsQueryGlobalResult  corresponding to\nthe respective names:  dt.application.TopNWordsWithQueries.operator.wsResultFile.topic\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic", 
            "title": "Operators in Top N words application"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#further-exploration", 
            "text": "In this tutorial, the property values in the  properties.xml  file were set to\nlimit the amount of memory allocated to each operator. You can try varying\nthese values and checking the impact of such an operation on the stability and\nperformance of the application. You can also explore the largest text\nfile that the application can handle.  Another aspect to explore is fixing the current limitation of\none-file-at-a-time processing; if multiple files are dropped into the\ninput directory simultaneously, the file reader can switch from one file to the\nnext in the same window. When the  FileWordCount  operator gets an EOF on the\ncontrol port, it waits for an  endWindow  call to emit word counts so those\ncounts will be incorrect if tuples from two different files arrive in the same\nwindow. Try fixing this issue.", 
            "title": "Further Exploration"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#datatorrent-terminology", 
            "text": "Operators  Operators are basic computation units that have properties and\nattributes, and are interconnected via streams to form an application.\nProperties customize the functional definition of the operator, while\nattributes customize the operational behavior. You can think of\noperators as classes for implementing the operator interface. They read\nfrom incoming streams of tuples and write to other streams.  Streams  A stream is a connector (edge) abstraction which is a fundamental building\nblock of DataTorrent RTS. A stream consists of tuples that flow from one input\nport to one or more output ports.  Ports  Ports are transient objects declared in the operator class and act connection\npoints for operators. Tuples flow in and out through ports. Input ports read\nfrom streams while output port write to streams.  Directed Acyclic Graph (DAG)  A DAG is a logical representation of real-time stream processing application.\nThe computational units within a DAG are called operators and the data-flow\nedges are called data streams.  Logical Plan or DAG  Logical Plan is the Data Object Model (DOM) that is created as operators and\nstreams are added to the DAG. It is identical to a DAG.  Physical Plan or DAG  A physical plan is the physical representation of the logical plan of the\napplication, which depicts how applications run on physical containers and\nnodes of a DataTorrent cluster.  Data Tuples Processed  This is the number of data objects processed by real-time stream processing\napplications.  Data Tuples Emitted  This is the number of data objects emitted after real-time stream processing\napplications complete processing operations.  Streaming Application Manager (STRAM)  Streaming Application Manager (STRAM) is a YARN-native, lightweight controller\nprocess. It is the process that is activated first upon application launch to\norchestrate the streaming application.  Streaming Window  A streaming window is a duration during which a set of tuples are emitted. The\ncollection of these tuples constitutes a window data set, also called as an\natomic micro-batch.  Sliding Application Window  Sliding window is computation that requires \"n\" streaming windows. After each\nstreaming window, the nth window is dropped, and the new window is added to the\ncomputation.  Demo Applications  The real-time stream processing applications which are packaged with the\nDataTorrent RTS binaries, are called demo applications. A Demo application can\nbe launched standalone, or on a Hadoop cluster.  Command-line Interface  Command line interface (CLI) is the access point for applications.\nThis is a wrapper around the web services layer, which makes the web\nservices user friendly.  Web services  DataTorrent RTS platform provides a robust webservices layer called\nDT Gateway. Currently, Hadoop provides detailed web services for\nmap-reduce jobs. The DataTorrent RTS platform leverages the same\nframework to provide a web service interface for real-time streaming\napplications.", 
            "title": "DataTorrent terminology"
        }, 
        {
            "location": "/tutorials/salesdimensions/", 
            "text": "Building the Sales Dimension application in JAVA\n\n\nThe Sales Dimensions application demonstrates multiple\nfeatures of the DataTorrent RTS platform including the ability to:\n- transform data\n- analyze data\n- act, based on analysis, in real time\n- support scalable applications for high-volume, multi-dimensional computations\n  with very low latency using existing library operators.\n\n\nExample scenario\n\n\nA large national retailer with physical stores and online sales\nchannels is trying to gain better insights to improve decision making\nfor their business. By utilizing real-time sales data, they would like\nto detect and forecast customer demand across multiple product\ncategories, gauge pricing and promotional effectiveness across regions,\nand drive additional customer loyalty with real time cross purchase\npromotions.\n\n\nIn order to achieve these goals, they need to analyze large\nvolumes of transactions in real time by computing aggregations of sales\ndata across multiple dimensions, including retail channels, product\ncategories, and regions. This allows them to not only gain insights by\nvisualizing the data for any dimension, but also make decisions and take\nactions on the data in real time.\n\n\nThe application makes use of seven operators; along with the\nstreams connecting their ports, these operators are discussed in the\nsections that follow.\n\n\nThe application setup for this retailer requires:\n\n\n\n\nInput \n For receiving individual sales transactions\n\n\nTransform \n For converting incoming records into a consumable format\n\n\nEnrich \n For providing additional information for each record by\n    performing additional lookups\n\n\nCompute \n For performing aggregate computations on all possible\n    key field combinations\n\n\nStore \n For storing computed results for further\n    analysis and visualizations\n\n\nAnalyze, Alert \n Visualize \n For displaying graphs\n    for selected combinations, perform analysis, and take actions on\n    computed data in real time.\n\n\n\n\nStep I: Build the Sales Dimension application\n\n\nThis topic contains steps for creating a new maven project using\nan archetype, adding the source and data files to the project, and\nfinally, building and deploying the Sales Dimension application. The\napplication sources are available online. You can use them with\nappropriate modifications to save time.\n\n\nTo build an application\n\n\n\n\nOpen a file in a simple text editor.\n\n\n\n\nCopy the following script to this file.\n\n\n#!/bin/bash\nname=salesapp\nmvn archetype:generate \\\n-DarchetypeRepository=https://www.datatorrent.com/maven/content/repositories/releases \\\n -DarchetypeGroupId=com.datatorrent \\\n -DarchetypeArtifactId=apex-app-archetype \\\n -DarchetypeVersion=3.1.1 \\\n -DgroupId=com.example \\\n -Dpackage=com.example.$name \\\n -DartifactId=$name \\\n -Dversion=1.0-SNAPSHOT\n\n\n\n\n\n\n\nSave the file as, for example, \nnewapp.sh\n.\n\n\n\n\n\n\nRun the file using the \nbash\n command to generate a new Maven project:\n\n\nbash\n \nName of file\n.sh\n\n\nFor example, the command might look like this: \nbash newapp.sh\n\n\n\n\n\n\nAt the prompt, press \nEnter\n to create a new project under a new directory\n    named \nsalesapp\n.\n\n\n\n\n\n\nDelete the following generated JAVA files: \nApplication.java\n and\n    \nRandomNumberGenerator.java\n under \nsrc/main/java/com/example/salesapp\n\n    and \nApplicationTest.java\n under \nsrc/test/java/com/example/salesapp\n.\n\n\n\n\n\n\nCreate a new directory named, say \nsources\n, step into it, retrieve\n    the JAR file containing the source files using \nwget\n, and expand it:\n\n\nmkdir -p salesapp/sources; cd salesapp/sources\nrepo=\"https://www.datatorrent.com/maven/content/repositories/releases\"\nwget $repo/com/datatorrent/dimensions-demo/3.1.1/dimensions-demo-3.1.1-sources.jar\"\njar xvf dimensions-demo-3.1.1-sources.jar\n\n\n\n\n\n\n\nCopy the following files from the expanded JAR file at\n    \ncom/datatorrent/demos/dimensions/sales/generic\n to the main source directory\n    of the new project at \nsrc/main/java/com/example/salesapp\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnrichmentOperator.java\n\n\nJsonSalesGenerator.java\n\n\n\n\n\n\nJsonToMapConverter.java\n\n\nRandomWeightedMovableGenerator.java\n\n\n\n\n\n\nSalesDemo.java\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlso copy these text files from the expanded jar to\n    \nsrc/main/resources\n in the new project: \nsalesGenericDataSchema.json\n,\n    \nsalesGenericEventSchema.json\n, \nproducts.txt\n . The first two files\n    define the format of data for visualization queries and the last has\n    data used by the enrichment operator discussed below.\n\n\n\n\n\n\nChange the package location in each file to reflect\n    its current location by changing the line\n\n\npackage com.datatorrent.demos.dimensions.sales.generic;\n\n\n\nto\n\n\npackage com.example.salesapp;\n\n\n\n\n\n\n\nAdd a new file called \nInputGenerator.java\n to the same location\n    containing this block of\n    code:\n\n\npackage com.example.salesapp;\nimport com.datatorrent.api.InputOperator;\npublic interface InputGenerator\nT\n extends InputOperator {\n    public OutputPort\nT\n getOutputPort();\n}\n\n\n\n\n\n\n\nRemove these lines from \nJsonSalesGenerator.java\n (the first is\n    unused, while the second is now package local):\n\n\nimport com.datatorrent.demos.dimensions.ads.AdInfo;\nimport com.datatorrent.demos.dimensions.InputGenerator;\n\n\n\n\n\n\n\nAdd these lines to the dependencies section at the end of the \npom.xml\n\n    file:\n\n\ndependency\n\n    \ngroupId\ncom.datatorrent\n/groupId\n\n    \nartifactId\ndt-contrib\n/artifactId\n\n    \nversion\n${datatorrent.version}\n/version\n\n\n/dependency\n\n\ndependency\n\n    \ngroupId\ncom.datatorrent\n/groupId\n\n    \nartifactId\ndt-library\n/artifactId\n\n    \nversion\n${datatorrent.version}\n/version\n\n\ndependency\n\n\n\n\n\n\n\n\nBuild the project using the following command:\n\n\nmvn clean package -DskipTests\n\n\n\n\n\n\n\nAssuming the build is successful, you should see the package file named\n\nsalesApp-1.0-SNAPSHOT.jar\n under the target directory. The next step\nshows you how to use the \ndtManage\n GUI to upload the package and launch the\napplication from there.\n\n\nStep II: Upload the Sales Dimension application package\n\n\nTo upload the Sales Dimension application package\n\n\n\n\nLog on to the DataTorrent Console (the default username and password are\n    both \ndtadmin\n).\n\n\nOn the menu bar, click \nDevelop\n.\n\n\nUnder \nApp Packages\n, click on \nupload a package\n.\n    \n\n\nNavigate to the location of \nsalesApp-1.0-SNAPSHOT.apa\n and select it.\n\n\nWait till the package is successfully uploaded.\n\n\n\n\nStep III: Launch the Sales Dimension application\n\n\nNote\n: Before launching the Sales Dimension application, shut down the IDE. If\nyour IDE is running at the time of a launch, the sandbox might hang due to\nresource exhaustion.\n\n\n\n\nLog on to the DataTorrent Console (the default username and password\n   are both \ndtadmin\n).\n\n\nIn the menu bar, click \nDevelop\n.\n\n\nUnder \nApp Packages\n, locate the Sales Dimension application, and click\n   \nlaunch application\n.\n\n\n(Optional) To configure the application using a configuration file, select\n   \nUse a config file\n. To specify individual properties, select \nSpecify custom properties\n.\n\n\nClick Launch.\n\n\n\n\nIf the launch is successful a message indicating the success of the launch\noperation appears along with the application ID.\n\n\nOperator base classes and interfaces\n\n\nThis section briefly discusses operators (and ports) and the relevant interfaces;\nthe next section discusses the specific operators used in the application.\n\n\nOperators can have multiple input and output ports; they receive events on their input\nports and emit (potentially different) events on output ports. Thus, operators and ports\nare at the heart of all applications. The \nOperator\n interface extends the \nComponent\n\ninterface:\n\n\npublic interface Component \nCONTEXT extends Context\n {\n  public void setup(CONTEXT cntxt);\n  public void teardown();\n}\n\n\n\nThe \nOperator\n interface defines \nPort\n, \nInputPort\n, and \nOutputPort\n as inner interfaces with\n\nInputPort\n, and \nOutputPort\n extending \nPort\n.\n\n\npublic interface Operator extends Component\nContext.OperatorContext\n {\n\n  public static interface Port extends Component\nContext.PortContext\n {}\n\n  public static interface InputPort\nT extends Object\n extends Port {\n    public Sink\nT\n getSink();\n    public void setConnected(boolean bln);\n    public StreamCodec\nT\n getStreamCodec();\n  }\n\n  public static interface OutputPort\nT extends Object\n extends Port {\n    public void setSink(Sink\nObject\n sink);\n    public Unifier\nT\n getUnifier();\n  }\n\n  public void beginWindow(long l);\n  public void endWindow();\n}\n\n\n\nOperators typically extend the \nBaseOperator\n class which simply\ndefines empty methods for \nsetup\n, \nteardown\n, \nbeginWindow\n, and\n\nendWindow\n. Derived classes only need to define those functions for\nwhich they want to perform an action. For example the\n\nConsoleOutputOperator\n class, which is often used during testing and\ndebugging, does not override any of these methods.\n\n\nInput operators typically receive data from some external source such\nas a database, message broker, or a file system. They might also\ncreate synthetic data internally. They then transform this data into\none or more events and write these events on one or more output ports;\nthey have no input ports (this might seem paradoxical at first, but is\nconsistent with our usage of input ports that dictates that input\nports only be used to receive data from other operators, not from an\nexternal source).\n\n\nInput ports must implement the \nInputOperator\n interface.\n\n\npublic interface InputOperator extends Operator {\n  public void emitTuples();\n}\n\n\n\nThe \nemitTuples\n method will typically output one or more events on\nsome or all of the output ports defined in the operator. For example,\nthe simple application generated by the maven archetype command\ndiscussed earlier has an operator named \nRandomNumberGenerator\n,\nwhich is defined like this:\n\n\npublic class RandomNumberGenerator extends BaseOperator implements InputOperator {\n\n  public final transient DefaultOutputPort\nDouble\n out = new DefaultOutputPort\nDouble\n();\n\n  public void emitTuples()  {\n    if (count++ \n 100) {\n      out.emit(Math.random());\n    }\n  }\n}\n\n\n\nFinally, the \nDefaultInputPort\n and \nDefaultOutputPort\n classes are\nvery useful as base classes that can be extended when defining ports\nin operators.\n\n\npublic abstract class DefaultInputPort\nT\n implements InputPort\nT\n, Sink\nT\n {\n  private int count;\n\n  public Sink\nT\n getSink(){ return this; }\n\n  public void put(T tuple){\n    count++;\n    process(tuple);\n  }\n\n  public int getCount(boolean reset) {\n    try {\n      return count;\n    } finally {\n      if (reset) {\n        count = 0;\n      }\n    }\n  }\n\n  public abstract void process(T tuple);\n}\n\npublic class DefaultOutputPort\nT\n implements Operator.OutputPort\nT\n {\n  private transient Sink\nObject\n sink;\n\n  final public void setSink(Sink\nObject\n s) {\n    this.sink = s == null? Sink.BLACKHOLE: s;\n  }\n\n  public void emit(T tuple){\n    sink.put(tuple);\n  }\n}\n\n\n\nThe \nDefaultInputPort\n class automatically keeps track of the number\nof events emitted and also supports the notion of a sink if needed in\nspecial circumstances. The abstract \nprocess\n method needs to be\nimplemented by any concrete derived class; it will be invoked via the\n\nSink.put\n override.\n\n\nThe \nDefaultOutputPort\n class also supports a sink and forwards calls\nto \nemit\n to the sink. The \nsetSink\n method is called by the \nStrAM\n\nexecution engine to inject a suitable sink at deployment time.\n\n\nOutput operators are the opposite of input operators; they typically\nreceive data on one or more input ports from other operators and write\nthem to external sinks. They have no output ports. There is, however,\nno specific interface to implement or base class to extend for output\noperators, though they often end up extending \nBaseOperator\n for\nconvenience. For example, the \nConsoleOutputOperator\n mentioned earlier\nis defined like this:\n\n\npublic class ConsoleOutputOperator extends BaseOperator {\n  public final transient DefaultInputPort\nObject\n input = new DefaultInputPort\nObject\n() {\n    public void process(Object t) {\n      System.out.println(s); }\n    };\n}\n\n\n\nNotice that the implementation of the abstract method\n\nDefaultInputPort.process\n simply writes the argument object to the\nconsole (we have simplified the code in that function somewhat for the\npurposes of this discussion; the actual code also allows the message\nto be logged and also allows some control over the output format).\n\n\nOperators in the Sales Dimensions application\n\n\nThe application simulates an incoming stream of sales events by\ngenerating a synthetic stream of such events; these events are then\nconverted to Java objects, enriched by mapping numeric identifiers to\nmeaningful product names or categories. Aggregated data is then\ncomputed and stored for all possible combinations of dimensions such\nas channels, regions, product categories and customers. Finally, query\nsupport is added to enable visualization. Accordingly, a number of\noperators come into play and they are listed below. Within an\napplication, an operator can be instantiated multiple times; in order\nto distinguish these instances, an application-specific name is\nassociated with each instance (provided as the first argument of the\n\ndag.addoperator\n call). To facilitate easy cross-referencing with the\ncode, we use the actual Java class names in the list below along with\nthe instance name in parentheses.\n\n\nThis diagram represents the Sales Dimension DAG. The\nports on these operators are connected via streams.\n\n\n\nJsonSalesGenerator (InputGenerator)\n\n\nThis class (new operator) is an input operator that generates a single\nsales event defined by a class like this:\n\n\nclass SalesEvent {\n  /* dimension keys */\n  public long time;\n  public int productId;\n  public String customer;\n  public String channel;\n  public String region;\n  /* metrics */\n  public double sales;\n  public double discount;\n  public double tax;\n}\n\n\n\nJsonToMapConverter (Converter)\n\n\nThis operator uses some special utility classes (ObjectReader and\nObjectMapper) to transform JSON event data to Java maps for easy\nmanipulation in Java code; it is fairly simple:\n\n\npublic class JsonToMapConverter extends BaseOperator {\n\n...\n\n  public final transient DefaultInputPort\nbyte\\[\\]\n input = new DefaultInputPort\nbyte[]\n() {\n    public void process(byte\\[\\] message) {\n      Map\nString, Object\n tuple = reader.readValue(message);\n      outputMap.emit(tuple);\n    }\n  }\n\n  public final transient DefaultOutputPort\nMap\nString, Object\n outputMap\n     = new DefaultOutputPort\nMap\nString, Object\n();\n\n}\n\n\n\nEnrichmentOperator (Enrichment)\n\n\nThis operator performs category lookup based on incoming numeric\nproduct IDs and adds the corresponding category names to the output\nevents. The mapping is read from the text file \nproducts.txt\n that\nwe encountered earlier while building the application. It contains\ndata like this:\n\n\n{\"productId\":96,\"product\":\"Printers\"}\n{\"productId\":97,\"product\":\"Routers\"}\n{\"productId\":98,\"product\":\"Smart Phones\"}\n\n\n\nThe core functionality of this operator is in the \nprocess\n function of\nthe input port where it looks up the product identifier in the\nenrichment mapping and adds the result to the event before emitting it\nto the output port. The mapping file can be modified at runtime to add\nor remove productId to category mapping pairs, so there is also some\ncode to check the modification timestamp and re-read the file if necessary.\n\n\npublic class EnrichmentOperator extends BaseOperator {\n  ...\n  public transient DefaultOutputPort\nMap\nString, Object\n\n    outputPort = new DefaultOutputPort\nMap\nString, Object\n();\n\n  public transient DefaultInputPort\nMap\nString, Object\n\n    inputPort = new DefaultInputPort\nMap\nString, Object\n() {\n\n    public void process(Map\nString, Object\n tuple) {\n      ...\n    }\n  }\n}\n\n\n\nDimensionsComputationFlexibleSingleSchemaMap (DimensionsComputation)\n\n\nThis operator performs dimension computations on incoming data. Sales\nnumbers by all combinations of region, product category, customer, and\nsales channel should be computed and emitted.\n\n\nAppDataSingleDimensionStoreHDHT (Store)\n\n\nThis operator stores computed dimensional information on HDFS,\noptimized for fast retrieval so that it can respond to queries.\n\n\nPubSubWebSocketAppDataQuery (Query)\n\n\nThis is the dashboard connector for visualization queries.\nThis operator and the next are used respectively to send queries and\nretrieve results from the Data Torrent Gateway which can act like a\nmessage broker for limited amounts of data using a topic-based\npublish/subscribe model. The URL to connect to is typically something\nlike \nws://\ngateway-host\n:\nport\n/pubsub\n where\n\ngateway-host\n and \nport\n should be replaced by appropriate values.\n\n\nA publisher sends a JSON message that looks like this to the URL\nwhere the value of the \ndata\n key is the desired message content:\n\n\n{\"type\":\"publish\", \"topic\":\"foobar\", \"data\": ...}\n\n\n\nCorrespondingly, subscribers send messages like this\nto retrieve published message data:\n\n\n{\"type\":\"subscribe\", \"topic\":\"foobar\"}\n\n\n\nTopic names need not be pre-registered anywhere but obviously, the\nsame topic name (e.g. \nfoobar\n in the example above) must be used by both\npublisher and subscriber; additionally, if there are no subscribers when\na message is published, it is simply discarded.\n\n\nThis query operator is an input operator used to send queries from\nthe dashboard to the store via the gateway:\n\n\npublic class PubSubWebSocketAppDataQuery extends PubSubWebSocketInputOperator\nString\n\nimplements AppData.ConnectionInfoProvider {\n  ...\n  protected String convertMessage(String message) {\n    JSONObject jo = new JSONObject(message);\n    return jo.getString(\"data\");\n  }\n}\n\n\n\nThe important method here is \nconvertMessage\n to convert the input\nstring to a JSON object, get the value of the \ndata\n key from the object\nand return it. The base classes look like this:\n\n\npublic class PubSubWebSocketInputOperator\nT\n extends WebSocketInputOperator\nT\n {\n  ...\n}\n\n\n\nThis class simply converts a JSON event into Java maps via the\n\nconvertMessage\n method.\n\n\npublic class WebSocketInputOperator\nT\n extends\nSimpleSinglePortInputOperator\nT\n implements Runnable {\n  ...\n}\n\n\n\nThis code is intended to be run in an asynchronous thread to retrieve\nevents from an external source and emit them on the output port.\n\n\npublic abstract class SimpleSinglePortInputOperator\nT\n extends BaseOperator\nimplements InputOperator, Operator.ActivationListener\nOperatorContext\n {\n\n  final public transient BufferingOutputPort\nT\n outputPort;\n\n  final public void activate(OperatorContext ctx) {\n  }\n\n  public void emitTuples() {\n    outputPort.flush(Integer.MAX_VALUE);\n  }\n\n  public static class BufferingOutputPort\nT\n extends DefaultOutputPort\nT\n {\n    public void flush(int count) { ... }\n  }\n\n}\n\n\n\nThe class starts a separate thread which retrieves source events and\ninvokes the \nemit\n method of the output port; the output port buffers\nevents until the \nflush\n method is called at which point all buffered\nevents are emitted.\n\n\nPubSubWebSocketAppDataResult (QueryResult)\n\n\nThis is the dashboard connector for results of visualization queries\nand is the result counterpart of the previous input query operator:\n\n\npublic class PubSubWebSocketAppDataResult extends PubSubWebSocketOutputOperator\nString\n\nimplements AppData.ConnectionInfoProvider {\n  ...\n}\n\n\n\nThis class merely overrides the generic \nconvertMapToMessage\n method of the\nbase class to generate the required JSON publish message.\n\n\npublic class PubSubWebSocketOutputOperator\nT\n extends WebSocketOutputOperator\nT\n {\n  ...\n}\n\n\n\nThis class, similarly, doesn't do much \n the \nconvertMapToMessage\n\nmethod converts input data into a suitable JSON object for publishing to the\nregistered topic.\n\n\npublic class WebSocketOutputOperator\nT\n extends BaseOperator {\n  public final transient DefaultInputPort\nT\n input = new DefaultInputPort\nT\n() {\n    public void process(T t) {\n      ...\n\n      connection.sendTextMessage(convertMapToMessage(t));\n    }\n  }\n}\n\n\n\nThe key element in this class is the input port (the rest of the code\ndeals with establishing a connection and reconnecting if\nnecessary). As usual, the key method in the input port is \nprocess\n\nwhich converts the incoming event to a JSON message and sends it\nacross the connection.\n\n\nConnecting the operators\n\n\nNow that we've seen the operator details, we will look at how they are\nconnected in the application. An application must implement the\n\nStreamingApplication\n interface:\n\n\npublic class SalesDemo implements StreamingApplication {\n  ...\n  public void populateDAG(DAG dag, Configuration conf) {\n    JsonSalesGenerator input\n      = dag.addOperator(\"InputGenerator\", JsonSalesGenerator.class);\n    JsonToMapConverter converter\n      = dag.addOperator(\"Converter\", JsonToMapConverter.class);\n    EnrichmentOperator enrichmentOperator\n      = dag.addOperator(\"Enrichment\", EnrichmentOperator.class);\n    DimensionsComputationFlexibleSingleSchemaMap dimensions\n      = dag.addOperator(\"DimensionsComputation\", DimensionsComputationFlexibleSingleSchemaMap.class);\n    AppDataSingleSchemaDimensionStoreHDHT store\n      = dag.addOperator(\"Store\", AppDataSingleSchemaDimensionStoreHDHT.class);\n\n    Operator.OutputPort\nString\n queryPort;\n    Operator.InputPort\nString\n queryResultPort;\n\n    URI uri = URI.create(\"ws://\" + gatewayAddress + \"/pubsub\");\n\n    PubSubWebSocketAppDataQuery wsIn = new\n    PubSubWebSocketAppDataQuery();\n    wsIn.setUri(uri);\n    queryPort = wsIn.outputPort;\n    dag.addOperator(\"Query\", wsIn);\n    dag.addStream(\"Query\", queryPort, store.query).setLocality(Locality.CONTAINER_LOCAL);\n    PubSubWebSocketAppDataResult wsOut\n      = dag.addOperator(\"QueryResult\", new PubSubWebSocketAppDataResult());\n    wsOut.setUri(uri);\n\n    queryResultPort = wsOut.input;\n    dag.addStream(\"InputStream\", inputGenerator.getOutputPort(), converter.input);\n    dag.addStream(\"EnrichmentStream\", converter.outputMap, enrichmentOperator.inputPort);\n    dag.addStream(\"ConvertStream\", enrichmentOperator.outputPort, dimensions.input);\n    dag.addStream(\"DimensionalData\", dimensions.output, store.input);\n    dag.addStream(\"QueryResult\", store.queryResult, queryResultPort).setLocality(Locality.CONTAINER_LOCAL);\n  }\n}\n\n\n\nThe key method to implement in an application is \npopulateDAG\n; as shown\nabove, the first step is to create instances of all seven operators and\nadd them to the DAG (we have omitted some parts of the code that are\nrelated to advanced features or are not directly relevant to the\ncurrent discussion). Once the operators are added to the DAG, their\nports must be connected (as shown in the earlier diagram) using\nstreams. Recall that a stream is represented by the \nDAG.StreamMeta\n\ninterface and is created via \nDAG.addStream()\n. The first argument is\nthe name of the stream, the second is the output port and the third\nthe input port. These statements form the second part of the\n\npopulateDAG\n function.\n\n\nThese two simple steps (a) adding operators to the DAG; and (b)\nconnecting their ports with streams are all it takes to build most\napplications. Of course, additional steps may be needed to configure\nsuitable properties to achieve the desired performance levels but those\nare often easier.", 
            "title": "Building in Java"
        }, 
        {
            "location": "/tutorials/salesdimensions/#building-the-sales-dimension-application-in-java", 
            "text": "The Sales Dimensions application demonstrates multiple\nfeatures of the DataTorrent RTS platform including the ability to:\n- transform data\n- analyze data\n- act, based on analysis, in real time\n- support scalable applications for high-volume, multi-dimensional computations\n  with very low latency using existing library operators.", 
            "title": "Building the Sales Dimension application in JAVA"
        }, 
        {
            "location": "/tutorials/salesdimensions/#example-scenario", 
            "text": "A large national retailer with physical stores and online sales\nchannels is trying to gain better insights to improve decision making\nfor their business. By utilizing real-time sales data, they would like\nto detect and forecast customer demand across multiple product\ncategories, gauge pricing and promotional effectiveness across regions,\nand drive additional customer loyalty with real time cross purchase\npromotions.  In order to achieve these goals, they need to analyze large\nvolumes of transactions in real time by computing aggregations of sales\ndata across multiple dimensions, including retail channels, product\ncategories, and regions. This allows them to not only gain insights by\nvisualizing the data for any dimension, but also make decisions and take\nactions on the data in real time.  The application makes use of seven operators; along with the\nstreams connecting their ports, these operators are discussed in the\nsections that follow.  The application setup for this retailer requires:   Input   For receiving individual sales transactions  Transform   For converting incoming records into a consumable format  Enrich   For providing additional information for each record by\n    performing additional lookups  Compute   For performing aggregate computations on all possible\n    key field combinations  Store   For storing computed results for further\n    analysis and visualizations  Analyze, Alert   Visualize   For displaying graphs\n    for selected combinations, perform analysis, and take actions on\n    computed data in real time.", 
            "title": "Example scenario"
        }, 
        {
            "location": "/tutorials/salesdimensions/#step-i-build-the-sales-dimension-application", 
            "text": "This topic contains steps for creating a new maven project using\nan archetype, adding the source and data files to the project, and\nfinally, building and deploying the Sales Dimension application. The\napplication sources are available online. You can use them with\nappropriate modifications to save time.  To build an application   Open a file in a simple text editor.   Copy the following script to this file.  #!/bin/bash\nname=salesapp\nmvn archetype:generate \\\n-DarchetypeRepository=https://www.datatorrent.com/maven/content/repositories/releases \\\n -DarchetypeGroupId=com.datatorrent \\\n -DarchetypeArtifactId=apex-app-archetype \\\n -DarchetypeVersion=3.1.1 \\\n -DgroupId=com.example \\\n -Dpackage=com.example.$name \\\n -DartifactId=$name \\\n -Dversion=1.0-SNAPSHOT    Save the file as, for example,  newapp.sh .    Run the file using the  bash  command to generate a new Maven project:  bash   Name of file .sh  For example, the command might look like this:  bash newapp.sh    At the prompt, press  Enter  to create a new project under a new directory\n    named  salesapp .    Delete the following generated JAVA files:  Application.java  and\n     RandomNumberGenerator.java  under  src/main/java/com/example/salesapp \n    and  ApplicationTest.java  under  src/test/java/com/example/salesapp .    Create a new directory named, say  sources , step into it, retrieve\n    the JAR file containing the source files using  wget , and expand it:  mkdir -p salesapp/sources; cd salesapp/sources\nrepo=\"https://www.datatorrent.com/maven/content/repositories/releases\"\nwget $repo/com/datatorrent/dimensions-demo/3.1.1/dimensions-demo-3.1.1-sources.jar\"\njar xvf dimensions-demo-3.1.1-sources.jar    Copy the following files from the expanded JAR file at\n     com/datatorrent/demos/dimensions/sales/generic  to the main source directory\n    of the new project at  src/main/java/com/example/salesapp .         EnrichmentOperator.java  JsonSalesGenerator.java    JsonToMapConverter.java  RandomWeightedMovableGenerator.java    SalesDemo.java        Also copy these text files from the expanded jar to\n     src/main/resources  in the new project:  salesGenericDataSchema.json ,\n     salesGenericEventSchema.json ,  products.txt  . The first two files\n    define the format of data for visualization queries and the last has\n    data used by the enrichment operator discussed below.    Change the package location in each file to reflect\n    its current location by changing the line  package com.datatorrent.demos.dimensions.sales.generic;  to  package com.example.salesapp;    Add a new file called  InputGenerator.java  to the same location\n    containing this block of\n    code:  package com.example.salesapp;\nimport com.datatorrent.api.InputOperator;\npublic interface InputGenerator T  extends InputOperator {\n    public OutputPort T  getOutputPort();\n}    Remove these lines from  JsonSalesGenerator.java  (the first is\n    unused, while the second is now package local):  import com.datatorrent.demos.dimensions.ads.AdInfo;\nimport com.datatorrent.demos.dimensions.InputGenerator;    Add these lines to the dependencies section at the end of the  pom.xml \n    file:  dependency \n     groupId com.datatorrent /groupId \n     artifactId dt-contrib /artifactId \n     version ${datatorrent.version} /version  /dependency  dependency \n     groupId com.datatorrent /groupId \n     artifactId dt-library /artifactId \n     version ${datatorrent.version} /version  dependency     Build the project using the following command:  mvn clean package -DskipTests    Assuming the build is successful, you should see the package file named salesApp-1.0-SNAPSHOT.jar  under the target directory. The next step\nshows you how to use the  dtManage  GUI to upload the package and launch the\napplication from there.", 
            "title": "Step I: Build the Sales Dimension application"
        }, 
        {
            "location": "/tutorials/salesdimensions/#step-ii-upload-the-sales-dimension-application-package", 
            "text": "To upload the Sales Dimension application package   Log on to the DataTorrent Console (the default username and password are\n    both  dtadmin ).  On the menu bar, click  Develop .  Under  App Packages , click on  upload a package .\n      Navigate to the location of  salesApp-1.0-SNAPSHOT.apa  and select it.  Wait till the package is successfully uploaded.", 
            "title": "Step II: Upload the Sales Dimension application package"
        }, 
        {
            "location": "/tutorials/salesdimensions/#step-iii-launch-the-sales-dimension-application", 
            "text": "Note : Before launching the Sales Dimension application, shut down the IDE. If\nyour IDE is running at the time of a launch, the sandbox might hang due to\nresource exhaustion.   Log on to the DataTorrent Console (the default username and password\n   are both  dtadmin ).  In the menu bar, click  Develop .  Under  App Packages , locate the Sales Dimension application, and click\n    launch application .  (Optional) To configure the application using a configuration file, select\n    Use a config file . To specify individual properties, select  Specify custom properties .  Click Launch.   If the launch is successful a message indicating the success of the launch\noperation appears along with the application ID.", 
            "title": "Step III: Launch the Sales Dimension application"
        }, 
        {
            "location": "/tutorials/salesdimensions/#operator-base-classes-and-interfaces", 
            "text": "This section briefly discusses operators (and ports) and the relevant interfaces;\nthe next section discusses the specific operators used in the application.  Operators can have multiple input and output ports; they receive events on their input\nports and emit (potentially different) events on output ports. Thus, operators and ports\nare at the heart of all applications. The  Operator  interface extends the  Component \ninterface:  public interface Component  CONTEXT extends Context  {\n  public void setup(CONTEXT cntxt);\n  public void teardown();\n}  The  Operator  interface defines  Port ,  InputPort , and  OutputPort  as inner interfaces with InputPort , and  OutputPort  extending  Port .  public interface Operator extends Component Context.OperatorContext  {\n\n  public static interface Port extends Component Context.PortContext  {}\n\n  public static interface InputPort T extends Object  extends Port {\n    public Sink T  getSink();\n    public void setConnected(boolean bln);\n    public StreamCodec T  getStreamCodec();\n  }\n\n  public static interface OutputPort T extends Object  extends Port {\n    public void setSink(Sink Object  sink);\n    public Unifier T  getUnifier();\n  }\n\n  public void beginWindow(long l);\n  public void endWindow();\n}  Operators typically extend the  BaseOperator  class which simply\ndefines empty methods for  setup ,  teardown ,  beginWindow , and endWindow . Derived classes only need to define those functions for\nwhich they want to perform an action. For example the ConsoleOutputOperator  class, which is often used during testing and\ndebugging, does not override any of these methods.  Input operators typically receive data from some external source such\nas a database, message broker, or a file system. They might also\ncreate synthetic data internally. They then transform this data into\none or more events and write these events on one or more output ports;\nthey have no input ports (this might seem paradoxical at first, but is\nconsistent with our usage of input ports that dictates that input\nports only be used to receive data from other operators, not from an\nexternal source).  Input ports must implement the  InputOperator  interface.  public interface InputOperator extends Operator {\n  public void emitTuples();\n}  The  emitTuples  method will typically output one or more events on\nsome or all of the output ports defined in the operator. For example,\nthe simple application generated by the maven archetype command\ndiscussed earlier has an operator named  RandomNumberGenerator ,\nwhich is defined like this:  public class RandomNumberGenerator extends BaseOperator implements InputOperator {\n\n  public final transient DefaultOutputPort Double  out = new DefaultOutputPort Double ();\n\n  public void emitTuples()  {\n    if (count++   100) {\n      out.emit(Math.random());\n    }\n  }\n}  Finally, the  DefaultInputPort  and  DefaultOutputPort  classes are\nvery useful as base classes that can be extended when defining ports\nin operators.  public abstract class DefaultInputPort T  implements InputPort T , Sink T  {\n  private int count;\n\n  public Sink T  getSink(){ return this; }\n\n  public void put(T tuple){\n    count++;\n    process(tuple);\n  }\n\n  public int getCount(boolean reset) {\n    try {\n      return count;\n    } finally {\n      if (reset) {\n        count = 0;\n      }\n    }\n  }\n\n  public abstract void process(T tuple);\n}\n\npublic class DefaultOutputPort T  implements Operator.OutputPort T  {\n  private transient Sink Object  sink;\n\n  final public void setSink(Sink Object  s) {\n    this.sink = s == null? Sink.BLACKHOLE: s;\n  }\n\n  public void emit(T tuple){\n    sink.put(tuple);\n  }\n}  The  DefaultInputPort  class automatically keeps track of the number\nof events emitted and also supports the notion of a sink if needed in\nspecial circumstances. The abstract  process  method needs to be\nimplemented by any concrete derived class; it will be invoked via the Sink.put  override.  The  DefaultOutputPort  class also supports a sink and forwards calls\nto  emit  to the sink. The  setSink  method is called by the  StrAM \nexecution engine to inject a suitable sink at deployment time.  Output operators are the opposite of input operators; they typically\nreceive data on one or more input ports from other operators and write\nthem to external sinks. They have no output ports. There is, however,\nno specific interface to implement or base class to extend for output\noperators, though they often end up extending  BaseOperator  for\nconvenience. For example, the  ConsoleOutputOperator  mentioned earlier\nis defined like this:  public class ConsoleOutputOperator extends BaseOperator {\n  public final transient DefaultInputPort Object  input = new DefaultInputPort Object () {\n    public void process(Object t) {\n      System.out.println(s); }\n    };\n}  Notice that the implementation of the abstract method DefaultInputPort.process  simply writes the argument object to the\nconsole (we have simplified the code in that function somewhat for the\npurposes of this discussion; the actual code also allows the message\nto be logged and also allows some control over the output format).", 
            "title": "Operator base classes and interfaces"
        }, 
        {
            "location": "/tutorials/salesdimensions/#operators-in-the-sales-dimensions-application", 
            "text": "The application simulates an incoming stream of sales events by\ngenerating a synthetic stream of such events; these events are then\nconverted to Java objects, enriched by mapping numeric identifiers to\nmeaningful product names or categories. Aggregated data is then\ncomputed and stored for all possible combinations of dimensions such\nas channels, regions, product categories and customers. Finally, query\nsupport is added to enable visualization. Accordingly, a number of\noperators come into play and they are listed below. Within an\napplication, an operator can be instantiated multiple times; in order\nto distinguish these instances, an application-specific name is\nassociated with each instance (provided as the first argument of the dag.addoperator  call). To facilitate easy cross-referencing with the\ncode, we use the actual Java class names in the list below along with\nthe instance name in parentheses.  This diagram represents the Sales Dimension DAG. The\nports on these operators are connected via streams.  JsonSalesGenerator (InputGenerator)  This class (new operator) is an input operator that generates a single\nsales event defined by a class like this:  class SalesEvent {\n  /* dimension keys */\n  public long time;\n  public int productId;\n  public String customer;\n  public String channel;\n  public String region;\n  /* metrics */\n  public double sales;\n  public double discount;\n  public double tax;\n}  JsonToMapConverter (Converter)  This operator uses some special utility classes (ObjectReader and\nObjectMapper) to transform JSON event data to Java maps for easy\nmanipulation in Java code; it is fairly simple:  public class JsonToMapConverter extends BaseOperator {\n\n...\n\n  public final transient DefaultInputPort byte\\[\\]  input = new DefaultInputPort byte[] () {\n    public void process(byte\\[\\] message) {\n      Map String, Object  tuple = reader.readValue(message);\n      outputMap.emit(tuple);\n    }\n  }\n\n  public final transient DefaultOutputPort Map String, Object  outputMap\n     = new DefaultOutputPort Map String, Object ();\n\n}  EnrichmentOperator (Enrichment)  This operator performs category lookup based on incoming numeric\nproduct IDs and adds the corresponding category names to the output\nevents. The mapping is read from the text file  products.txt  that\nwe encountered earlier while building the application. It contains\ndata like this:  {\"productId\":96,\"product\":\"Printers\"}\n{\"productId\":97,\"product\":\"Routers\"}\n{\"productId\":98,\"product\":\"Smart Phones\"}  The core functionality of this operator is in the  process  function of\nthe input port where it looks up the product identifier in the\nenrichment mapping and adds the result to the event before emitting it\nto the output port. The mapping file can be modified at runtime to add\nor remove productId to category mapping pairs, so there is also some\ncode to check the modification timestamp and re-read the file if necessary.  public class EnrichmentOperator extends BaseOperator {\n  ...\n  public transient DefaultOutputPort Map String, Object \n    outputPort = new DefaultOutputPort Map String, Object ();\n\n  public transient DefaultInputPort Map String, Object \n    inputPort = new DefaultInputPort Map String, Object () {\n\n    public void process(Map String, Object  tuple) {\n      ...\n    }\n  }\n}  DimensionsComputationFlexibleSingleSchemaMap (DimensionsComputation)  This operator performs dimension computations on incoming data. Sales\nnumbers by all combinations of region, product category, customer, and\nsales channel should be computed and emitted.  AppDataSingleDimensionStoreHDHT (Store)  This operator stores computed dimensional information on HDFS,\noptimized for fast retrieval so that it can respond to queries.  PubSubWebSocketAppDataQuery (Query)  This is the dashboard connector for visualization queries.\nThis operator and the next are used respectively to send queries and\nretrieve results from the Data Torrent Gateway which can act like a\nmessage broker for limited amounts of data using a topic-based\npublish/subscribe model. The URL to connect to is typically something\nlike  ws:// gateway-host : port /pubsub  where gateway-host  and  port  should be replaced by appropriate values.  A publisher sends a JSON message that looks like this to the URL\nwhere the value of the  data  key is the desired message content:  {\"type\":\"publish\", \"topic\":\"foobar\", \"data\": ...}  Correspondingly, subscribers send messages like this\nto retrieve published message data:  {\"type\":\"subscribe\", \"topic\":\"foobar\"}  Topic names need not be pre-registered anywhere but obviously, the\nsame topic name (e.g.  foobar  in the example above) must be used by both\npublisher and subscriber; additionally, if there are no subscribers when\na message is published, it is simply discarded.  This query operator is an input operator used to send queries from\nthe dashboard to the store via the gateway:  public class PubSubWebSocketAppDataQuery extends PubSubWebSocketInputOperator String \nimplements AppData.ConnectionInfoProvider {\n  ...\n  protected String convertMessage(String message) {\n    JSONObject jo = new JSONObject(message);\n    return jo.getString(\"data\");\n  }\n}  The important method here is  convertMessage  to convert the input\nstring to a JSON object, get the value of the  data  key from the object\nand return it. The base classes look like this:  public class PubSubWebSocketInputOperator T  extends WebSocketInputOperator T  {\n  ...\n}  This class simply converts a JSON event into Java maps via the convertMessage  method.  public class WebSocketInputOperator T  extends\nSimpleSinglePortInputOperator T  implements Runnable {\n  ...\n}  This code is intended to be run in an asynchronous thread to retrieve\nevents from an external source and emit them on the output port.  public abstract class SimpleSinglePortInputOperator T  extends BaseOperator\nimplements InputOperator, Operator.ActivationListener OperatorContext  {\n\n  final public transient BufferingOutputPort T  outputPort;\n\n  final public void activate(OperatorContext ctx) {\n  }\n\n  public void emitTuples() {\n    outputPort.flush(Integer.MAX_VALUE);\n  }\n\n  public static class BufferingOutputPort T  extends DefaultOutputPort T  {\n    public void flush(int count) { ... }\n  }\n\n}  The class starts a separate thread which retrieves source events and\ninvokes the  emit  method of the output port; the output port buffers\nevents until the  flush  method is called at which point all buffered\nevents are emitted.  PubSubWebSocketAppDataResult (QueryResult)  This is the dashboard connector for results of visualization queries\nand is the result counterpart of the previous input query operator:  public class PubSubWebSocketAppDataResult extends PubSubWebSocketOutputOperator String \nimplements AppData.ConnectionInfoProvider {\n  ...\n}  This class merely overrides the generic  convertMapToMessage  method of the\nbase class to generate the required JSON publish message.  public class PubSubWebSocketOutputOperator T  extends WebSocketOutputOperator T  {\n  ...\n}  This class, similarly, doesn't do much   the  convertMapToMessage \nmethod converts input data into a suitable JSON object for publishing to the\nregistered topic.  public class WebSocketOutputOperator T  extends BaseOperator {\n  public final transient DefaultInputPort T  input = new DefaultInputPort T () {\n    public void process(T t) {\n      ...\n\n      connection.sendTextMessage(convertMapToMessage(t));\n    }\n  }\n}  The key element in this class is the input port (the rest of the code\ndeals with establishing a connection and reconnecting if\nnecessary). As usual, the key method in the input port is  process \nwhich converts the incoming event to a JSON message and sends it\nacross the connection.", 
            "title": "Operators in the Sales Dimensions application"
        }, 
        {
            "location": "/tutorials/salesdimensions/#connecting-the-operators", 
            "text": "Now that we've seen the operator details, we will look at how they are\nconnected in the application. An application must implement the StreamingApplication  interface:  public class SalesDemo implements StreamingApplication {\n  ...\n  public void populateDAG(DAG dag, Configuration conf) {\n    JsonSalesGenerator input\n      = dag.addOperator(\"InputGenerator\", JsonSalesGenerator.class);\n    JsonToMapConverter converter\n      = dag.addOperator(\"Converter\", JsonToMapConverter.class);\n    EnrichmentOperator enrichmentOperator\n      = dag.addOperator(\"Enrichment\", EnrichmentOperator.class);\n    DimensionsComputationFlexibleSingleSchemaMap dimensions\n      = dag.addOperator(\"DimensionsComputation\", DimensionsComputationFlexibleSingleSchemaMap.class);\n    AppDataSingleSchemaDimensionStoreHDHT store\n      = dag.addOperator(\"Store\", AppDataSingleSchemaDimensionStoreHDHT.class);\n\n    Operator.OutputPort String  queryPort;\n    Operator.InputPort String  queryResultPort;\n\n    URI uri = URI.create(\"ws://\" + gatewayAddress + \"/pubsub\");\n\n    PubSubWebSocketAppDataQuery wsIn = new\n    PubSubWebSocketAppDataQuery();\n    wsIn.setUri(uri);\n    queryPort = wsIn.outputPort;\n    dag.addOperator(\"Query\", wsIn);\n    dag.addStream(\"Query\", queryPort, store.query).setLocality(Locality.CONTAINER_LOCAL);\n    PubSubWebSocketAppDataResult wsOut\n      = dag.addOperator(\"QueryResult\", new PubSubWebSocketAppDataResult());\n    wsOut.setUri(uri);\n\n    queryResultPort = wsOut.input;\n    dag.addStream(\"InputStream\", inputGenerator.getOutputPort(), converter.input);\n    dag.addStream(\"EnrichmentStream\", converter.outputMap, enrichmentOperator.inputPort);\n    dag.addStream(\"ConvertStream\", enrichmentOperator.outputPort, dimensions.input);\n    dag.addStream(\"DimensionalData\", dimensions.output, store.input);\n    dag.addStream(\"QueryResult\", store.queryResult, queryResultPort).setLocality(Locality.CONTAINER_LOCAL);\n  }\n}  The key method to implement in an application is  populateDAG ; as shown\nabove, the first step is to create instances of all seven operators and\nadd them to the DAG (we have omitted some parts of the code that are\nrelated to advanced features or are not directly relevant to the\ncurrent discussion). Once the operators are added to the DAG, their\nports must be connected (as shown in the earlier diagram) using\nstreams. Recall that a stream is represented by the  DAG.StreamMeta \ninterface and is created via  DAG.addStream() . The first argument is\nthe name of the stream, the second is the output port and the third\nthe input port. These statements form the second part of the populateDAG  function.  These two simple steps (a) adding operators to the DAG; and (b)\nconnecting their ports with streams are all it takes to build most\napplications. Of course, additional steps may be needed to configure\nsuitable properties to achieve the desired performance levels but those\nare often easier.", 
            "title": "Connecting the operators"
        }, 
        {
            "location": "/tutorials/salesdimensions-c2/", 
            "text": "Building the Sales Dimensions application using dtAssemble\n\n\nThe DataTorrent RTS platform supports building new applications using \ndtAssemble\n, the Graphical\nApplication Builder which we will use for the Sales Dimensions application. \ndtAssemble\n\nis an easy and intuitive tool for constructing applications,\nwhile providing a great visualization of the logical operator connectivity and the\napplication data flow.\n\n\nNote\n: You can also find these instructions in the UI console. Click \nLearn\n in the menu\nbar, and then click the first link in the left panel: \nTransform, Analyze, Alert\n.\n\n\nStep 1: Open the Application Builder interface\n\n\n\n\nOn the DataTorrent RTS console, navigate to \nApp Packages\n.\n\n\nMake sure that the DataTorrent Dimensions Demos package is imported (if\n    not, use the Import Demos button to import it).\n\n\nClick the green \nCreate new application\n button, and name the application\n    Sales Dimensions. The Application Canvas window should open.\n    \n\n\n\n\nStep 2: Add and connect operators\n\n\n\n\n\n\nUnder \nOperator Library\n in the left panel, select the following\n    operators and drag them to the Application Canvas. Rename them to\n    the names given in parentheses.\n\n\n\n\nJSON Sales Event Generator (Input)\n \u2013 This operator generates\n   synthetic sales events and emits them as JSON string bytes.\n\n\nJSON to Map Parser (Parse)\n \u2013 This operator transforms JSON\n   data to Java maps for convenience in manipulating the sales data\n   in Java code.\n\n\nEnrichment (Enrich)\n \u2013 This operator performs category lookup based on\n   incoming product IDs, and adds the category ID to the output maps.\n\n\nDimension Computation Map (Compute)\n \u2013 This operator performs dimensions\n   computations, also known as cubing, on the incoming data. It\n   pre-computes the sales numbers by region, product category, customer,\n   and sales channel, and all combinations of the above. Having these\n   numbers available in advance, allows for viewing and taking action on\n   any of these combinations in real time.\n\n\nSimple App Data Dimensions Store (Store)\n \n This operator\n   stores the computed dimensional information on HDFS in an optimized manner.\n\n\nApp Data Pub Sub Query (Query)\n \n The dashboard connector for\n   visualization queries.\n\n\nApp Data Pub Sub Result (Result)\n \n The dashboard connector for\n   visualization data results.\n\n\n\n\n\n\n\n\nTo connect the operators, click the output port of each upstream operator,\n    and drag the connector to the input stream of the downstream operator as shown\n    in the diagram below:\n    \n\n\n\n\n\n\nStep 3: Customize application and operator settings\n\n\nCustomize the operators and streams as described in each item below; to do that,\nclick the individual operator or stream and use the \nOperator Inspector\n panel\non the right to edit the operator and stream settings as described in the item:\n\n\n\n\n\n\nCopy this Sales schema below into the \nEvent Schema JSON\n field of \nInput\n\n    operator, and the \nConfiguration Schema JSON\n of the \nCompute\n and \nStore\n\n    operators.\n\n\n{\n  \"keys\": [\n    {\"name\":\"channel\",\"type\":\"string\",\"enumValues\":[\"Mobile\",\"Online\",\"Store\"]},\n    {\"name\":\"region\",\"type\":\"string\",\n     \"enumValues\":[\"Atlanta\",\"Boston\",\"Chicago\",\"Cleveland\",\"Dallas\",\"Minneapolis\",\n                   \"New York\",\"Philadelphia\",\"San Francisco\",\"St. Louis\"]},\n    {\"name\":\"product\",\"type\":\"string\",\n     \"enumValues\":[\"Laptops\",\"Printers\",\"Routers\",\"Smart Phones\",\"Tablets\"]}],\n  \"timeBuckets\":[\"1m\", \"1h\", \"1d\"],\n  \"values\": [\n    {\"name\":\"sales\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n    {\"name\":\"discount\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n    {\"name\":\"tax\",\"type\":\"double\",\"aggregators\":[\"SUM\"]}],\n  \"dimensions\": [\n    {\"combination\":[]},\n    {\"combination\":[\"channel\"]},\n    {\"combination\":[\"region\"]},\n    {\"combination\":[\"product\"]},\n    {\"combination\":[\"channel\",\"region\"]},\n    {\"combination\":[\"channel\",\"product\"]},\n    {\"combination\":[\"region\",\"product\"]},\n    {\"combination\":[\"channel\",\"region\",\"product\"]}]\n}\n\n\n\n\n\n\n\nSet the \nTopic\n property for \nQuery\n and \nResult\n operators to\n    \nSalesDimensionsQuery\n and \nSalesDimensionsResult\n respectively.\n\n\n\n\nSelect the \nStore\n operator, and edit the \nFile Store\n property.\n    Set \nBase Path\n value to \nSalesDimensionsDemoStore\n. This sets the HDHT\n    storage path to write dimensions computation results to\n    \n/user/\nusername\n/SalesDimensionsDemoStore\n on HDFS.\n    \n\n\nClick the stream, and set the Stream Locality to CONTAINER_LOCAL\n    for all the streams between Input and Compute operators.\n\n\n\n\nNote\n: Changing stream locality controls which container operators\nget deployed to, and can lead to significant performance improvements\nfor an application. Once set, the connection will be represented by a\ndashed line to indicate the new locality setting.\n\n\nStep 4: Launch the application\n\n\nOnce the application is constructed, and validation checks are\nsatisfied, a launch button will become available at the top left of the\n\nApplication Canvas\n window. Clicking this button to open the application\nlaunch dialog box. You can use this dialog box to perform additional\nconfiguration of the application such as changing its name or modifying\nproperties.\n\n\nTo launch the Sales Dimension application\n\n\n\n\nClick the launch button at the top left of the application canvas screen.\n\n\nType a name for the application in the \nName this application\n box.\n\n\n(Optional) To configure the application using a configuration file, select\n    \nUse a config file\n checkbox.\n\n\n(Optional) To specify individual properties, select\n    \nSpecify custom properties\n checkbox.\n\n\nClick Launch.\n\n\n\n\n\n\nOnce the application is successfully launched, you can check its\nhealth and view some runtime statistics using the steps below.\nAdditional details are in the chapter entitled \nMonitoring the Sales\nDimensions Application with dtManage\n.\n\n\n\n\nGo to the Sales Dimensions application operations page under the \nMonitor\n tab.\n\n\nConfirm that the application is launched successfully by validating that\n    the state of the application under the \nApplication Overview\n section\n    is \nRunning\n.\n\n\nMake sure that all the operators are successfully started under the\n    \nStram Events\n section.\n\n\nNavigate to the \nPhysical view\n tab, observe the Input, Parse, Enrich, or\n    Compute operators, and ensure that they are deployed to a single container,\n    because of the stream locality setting of CONTAINER_LOCAL.\n\n\n\n\nNote\n: This is one of the many performance improvement techniques\navailable with the DataTorrent platform; in this case eliminating data\nserialization and networking stack overhead between groups of adjacent\noperators.", 
            "title": "Building with dtAssemble"
        }, 
        {
            "location": "/tutorials/salesdimensions-c2/#building-the-sales-dimensions-application-using-dtassemble", 
            "text": "The DataTorrent RTS platform supports building new applications using  dtAssemble , the Graphical\nApplication Builder which we will use for the Sales Dimensions application.  dtAssemble \nis an easy and intuitive tool for constructing applications,\nwhile providing a great visualization of the logical operator connectivity and the\napplication data flow.  Note : You can also find these instructions in the UI console. Click  Learn  in the menu\nbar, and then click the first link in the left panel:  Transform, Analyze, Alert .", 
            "title": "Building the Sales Dimensions application using dtAssemble"
        }, 
        {
            "location": "/tutorials/salesdimensions-c2/#step-1-open-the-application-builder-interface", 
            "text": "On the DataTorrent RTS console, navigate to  App Packages .  Make sure that the DataTorrent Dimensions Demos package is imported (if\n    not, use the Import Demos button to import it).  Click the green  Create new application  button, and name the application\n    Sales Dimensions. The Application Canvas window should open.", 
            "title": "Step 1: Open the Application Builder interface"
        }, 
        {
            "location": "/tutorials/salesdimensions-c2/#step-2-add-and-connect-operators", 
            "text": "Under  Operator Library  in the left panel, select the following\n    operators and drag them to the Application Canvas. Rename them to\n    the names given in parentheses.   JSON Sales Event Generator (Input)  \u2013 This operator generates\n   synthetic sales events and emits them as JSON string bytes.  JSON to Map Parser (Parse)  \u2013 This operator transforms JSON\n   data to Java maps for convenience in manipulating the sales data\n   in Java code.  Enrichment (Enrich)  \u2013 This operator performs category lookup based on\n   incoming product IDs, and adds the category ID to the output maps.  Dimension Computation Map (Compute)  \u2013 This operator performs dimensions\n   computations, also known as cubing, on the incoming data. It\n   pre-computes the sales numbers by region, product category, customer,\n   and sales channel, and all combinations of the above. Having these\n   numbers available in advance, allows for viewing and taking action on\n   any of these combinations in real time.  Simple App Data Dimensions Store (Store)    This operator\n   stores the computed dimensional information on HDFS in an optimized manner.  App Data Pub Sub Query (Query)    The dashboard connector for\n   visualization queries.  App Data Pub Sub Result (Result)    The dashboard connector for\n   visualization data results.     To connect the operators, click the output port of each upstream operator,\n    and drag the connector to the input stream of the downstream operator as shown\n    in the diagram below:", 
            "title": "Step 2: Add and connect operators"
        }, 
        {
            "location": "/tutorials/salesdimensions-c2/#step-3-customize-application-and-operator-settings", 
            "text": "Customize the operators and streams as described in each item below; to do that,\nclick the individual operator or stream and use the  Operator Inspector  panel\non the right to edit the operator and stream settings as described in the item:    Copy this Sales schema below into the  Event Schema JSON  field of  Input \n    operator, and the  Configuration Schema JSON  of the  Compute  and  Store \n    operators.  {\n  \"keys\": [\n    {\"name\":\"channel\",\"type\":\"string\",\"enumValues\":[\"Mobile\",\"Online\",\"Store\"]},\n    {\"name\":\"region\",\"type\":\"string\",\n     \"enumValues\":[\"Atlanta\",\"Boston\",\"Chicago\",\"Cleveland\",\"Dallas\",\"Minneapolis\",\n                   \"New York\",\"Philadelphia\",\"San Francisco\",\"St. Louis\"]},\n    {\"name\":\"product\",\"type\":\"string\",\n     \"enumValues\":[\"Laptops\",\"Printers\",\"Routers\",\"Smart Phones\",\"Tablets\"]}],\n  \"timeBuckets\":[\"1m\", \"1h\", \"1d\"],\n  \"values\": [\n    {\"name\":\"sales\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n    {\"name\":\"discount\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n    {\"name\":\"tax\",\"type\":\"double\",\"aggregators\":[\"SUM\"]}],\n  \"dimensions\": [\n    {\"combination\":[]},\n    {\"combination\":[\"channel\"]},\n    {\"combination\":[\"region\"]},\n    {\"combination\":[\"product\"]},\n    {\"combination\":[\"channel\",\"region\"]},\n    {\"combination\":[\"channel\",\"product\"]},\n    {\"combination\":[\"region\",\"product\"]},\n    {\"combination\":[\"channel\",\"region\",\"product\"]}]\n}    Set the  Topic  property for  Query  and  Result  operators to\n     SalesDimensionsQuery  and  SalesDimensionsResult  respectively.   Select the  Store  operator, and edit the  File Store  property.\n    Set  Base Path  value to  SalesDimensionsDemoStore . This sets the HDHT\n    storage path to write dimensions computation results to\n     /user/ username /SalesDimensionsDemoStore  on HDFS.\n      Click the stream, and set the Stream Locality to CONTAINER_LOCAL\n    for all the streams between Input and Compute operators.   Note : Changing stream locality controls which container operators\nget deployed to, and can lead to significant performance improvements\nfor an application. Once set, the connection will be represented by a\ndashed line to indicate the new locality setting.", 
            "title": "Step 3: Customize application and operator settings"
        }, 
        {
            "location": "/tutorials/salesdimensions-c2/#step-4-launch-the-application", 
            "text": "Once the application is constructed, and validation checks are\nsatisfied, a launch button will become available at the top left of the Application Canvas  window. Clicking this button to open the application\nlaunch dialog box. You can use this dialog box to perform additional\nconfiguration of the application such as changing its name or modifying\nproperties.  To launch the Sales Dimension application   Click the launch button at the top left of the application canvas screen.  Type a name for the application in the  Name this application  box.  (Optional) To configure the application using a configuration file, select\n     Use a config file  checkbox.  (Optional) To specify individual properties, select\n     Specify custom properties  checkbox.  Click Launch.    Once the application is successfully launched, you can check its\nhealth and view some runtime statistics using the steps below.\nAdditional details are in the chapter entitled  Monitoring the Sales\nDimensions Application with dtManage .   Go to the Sales Dimensions application operations page under the  Monitor  tab.  Confirm that the application is launched successfully by validating that\n    the state of the application under the  Application Overview  section\n    is  Running .  Make sure that all the operators are successfully started under the\n     Stram Events  section.  Navigate to the  Physical view  tab, observe the Input, Parse, Enrich, or\n    Compute operators, and ensure that they are deployed to a single container,\n    because of the stream locality setting of CONTAINER_LOCAL.   Note : This is one of the many performance improvement techniques\navailable with the DataTorrent platform; in this case eliminating data\nserialization and networking stack overhead between groups of adjacent\noperators.", 
            "title": "Step 4: Launch the application"
        }, 
        {
            "location": "/tutorials/salesdimensions-c3/", 
            "text": "Visualizing data from the Sales Dimension application using dtDashboard\n\n\nDataTorrent includes powerful data visualization tools, which\nallow you to visualize streaming data from multiple sources in real\ntime. For additional details see the tutorial entitled \ndtDashboard\n- Application Data Visualization\n at \nhttps://docs.datatorrent.com\n.\n\n\nAfter the application is started, a visualize button, available in\nthe Application Overview section, can be used to quickly generate a new\ndashboard for the Sales Dimensions application.\n\n\nGenerate dashboards\n\n\n\n\nIf you created dashboards already, the dashboards appear in the\ndropdown list. You can select one, or generate a new dashboard by\nselecting the generate new dashboard option from the dropdown list.\n\n\nAfter the dashboard is created, you can add additional widgets for\ndisplaying dimensions and combinations of the sales data. Here is an\nexample:\n\n\n\n\nAdding widgets\n\n\nTo derive more value out of application dashboards, you can add\nwidgets to the dashboards. Widgets are charts in addition to the default\ncharts that you can see on the dashboard. DataTorrent RTS supports five\nwidgets: \nbar chart\n, \npie chart\n, \nhorizontal bar chart\n, \ntable\n, and\n\nnote\n.\n\n\nTo add a widget\n\n\n\n\nClick the add widget button below the name of the dashboard, for example,\n    Sales Dimension.\n    \n\n\nIn the Data Source list, click a data source for your widget.\n\n\nSelect a widget type under \nAvailable Widgets\n.\n    \n\n\nClick \nadd widget\n button.\n\n\n\n\nThe widget is added to your dashboard.\n\n\nEdit a widget\n\n\nAfter you add a widget to your dashboard, you can update it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.\n\n\nTo edit a widget\n\n\n\n\n\n\nChange the size and position of the widget:\n    a. To change the size of the widget, click the\n       border of the widget, and resize it.\n    b. To move the widget around, click the widget, and\n       drag it to the desired location.\n\n\n\n\n\n\nEdit the widget:\n    a.  In the top-right corner of the widget, click \nedit\n.\n    b.  Type a new title in the \nTitle\n box.\n    c.  Use the remaining options to configure the widget.\n    d.  Click \nOK\n.\n    \n\n\n\n\n\n\nTo remove a widget, in the top-right corner, click the \ndelete\n button.", 
            "title": "Visualizing with dtDashboard"
        }, 
        {
            "location": "/tutorials/salesdimensions-c3/#visualizing-data-from-the-sales-dimension-application-using-dtdashboard", 
            "text": "DataTorrent includes powerful data visualization tools, which\nallow you to visualize streaming data from multiple sources in real\ntime. For additional details see the tutorial entitled  dtDashboard\n- Application Data Visualization  at  https://docs.datatorrent.com .  After the application is started, a visualize button, available in\nthe Application Overview section, can be used to quickly generate a new\ndashboard for the Sales Dimensions application.", 
            "title": "Visualizing data from the Sales Dimension application using dtDashboard"
        }, 
        {
            "location": "/tutorials/salesdimensions-c3/#generate-dashboards", 
            "text": "If you created dashboards already, the dashboards appear in the\ndropdown list. You can select one, or generate a new dashboard by\nselecting the generate new dashboard option from the dropdown list.  After the dashboard is created, you can add additional widgets for\ndisplaying dimensions and combinations of the sales data. Here is an\nexample:", 
            "title": "Generate dashboards"
        }, 
        {
            "location": "/tutorials/salesdimensions-c3/#adding-widgets", 
            "text": "To derive more value out of application dashboards, you can add\nwidgets to the dashboards. Widgets are charts in addition to the default\ncharts that you can see on the dashboard. DataTorrent RTS supports five\nwidgets:  bar chart ,  pie chart ,  horizontal bar chart ,  table , and note .  To add a widget   Click the add widget button below the name of the dashboard, for example,\n    Sales Dimension.\n      In the Data Source list, click a data source for your widget.  Select a widget type under  Available Widgets .\n      Click  add widget  button.   The widget is added to your dashboard.", 
            "title": "Adding widgets"
        }, 
        {
            "location": "/tutorials/salesdimensions-c3/#edit-a-widget", 
            "text": "After you add a widget to your dashboard, you can update it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.  To edit a widget    Change the size and position of the widget:\n    a. To change the size of the widget, click the\n       border of the widget, and resize it.\n    b. To move the widget around, click the widget, and\n       drag it to the desired location.    Edit the widget:\n    a.  In the top-right corner of the widget, click  edit .\n    b.  Type a new title in the  Title  box.\n    c.  Use the remaining options to configure the widget.\n    d.  Click  OK .\n        To remove a widget, in the top-right corner, click the  delete  button.", 
            "title": "Edit a widget"
        }, 
        {
            "location": "/tutorials/salesdimensions-c4/", 
            "text": "Monitoring the Sales Dimension application using dtManage\n\n\nRecall that after the application is built and validated, it can be\nlaunched from the \nApp Packages\n page as described in an earlier chapter;\napplications built with \ndtAssemble\n can also, optionally, be launched\nfrom the \nApplication Canvas\n page as described earlier. This section\ndescribes how you can monitor the running Sales Dimension application\nusing \ndtManage\n.\n\n\nThe Monitor menu option\n\n\nYou can monitor the Sales Dimension application by clicking\nMonitor on the menu bar. After you click \nMonitor\n, you can choose between\n4 tabs. Under each tab, you can see multiple widgets, which you can\nresize, move around, configure, or remove.\n\n\nlogical\n\n\nThis image of the logical tab shows 4 widgets; additional widgets can be\nadded by clicking the \n+\n button at the top-left corner and choosing\nfrom the resulting dropdown list.\n\n\n\n\n\n\n\n\nApplication Overview\n\n\nThis widget has the shutdown and kill buttons for shutting down or\nkilling an application. This widget also displays the state of the\napplication, the window IDs, the number of physical operators,\ncontainers, allocated memory, and statistics on the number of\nevents handled.\n\n\n\n\n\n\nStramEvents\n\n\nThis widget displays all the operators, containers, and nodes that\nare running. This widget also displays additional information,\nsuch as errors encountered and timestamps.\n\n\n\n\n\n\nLogical DAG\n\n\nThis widget displays operators and their\nconnections in the logical dag (as defined in the application)\nwithout partitions, that is, if an operator is partitioned to run\nmultiple copies to increase throughput, only one copy is displayed.\nThe Physical DAG (the physical-dag-view) shows the actual\nphysical operators. For each operator, you can choose to include\nadditional statistics.\n\n\nTo include additional details\n\n\n\n\nClick an operator for which you want to display additional details.\n\n\nTo display a detail on the top of this operator representation,\n    click the Top list, and select a metric.\n\n\nTo display a detail at the bottom of this operator representation,\n    click the Bottom list, and select a metric.\n\n\n\n\n\n\n\n\nLogical Operators\n\n\nThis widget displays a table of operators\nfor: the name, the Java class, status, and additional statistics for\nlatency and processed events.\n\n\n\n\n\n\nStreams\n (not shown in the image)\n\n\nThis operator displays a table with one row per stream showing:\nthe name, locality, source, and sinks.\n\n\n\n\n\n\nMetrics Chart\n (not shown in the image)\n\n\nThis widget displays moving averages of tuples processed and latencies.\n\n\n\n\n\n\nphysical\n\n\nThe physical tab displays, in addition to \nApplication Overview\n\nand \nMetrics Chart\n, 2 more widgets:\n\n\n\n\n\n\n\n\nPhysical Operators\n\n\nThis widget displays a table of physical operators for:\nname, status, host, container ID, and some additional statistics. The\ncontainer ID is a numeric value and a clickable link that takes you to a\npage showing additional details about that specific instance of the\noperator.\n\n\n\n\n\n\nContainers\n\n\nThis widget displays a table of containers (the Java Virtual\nMachine processes) and for each process: the ID, the process ID,\nhost, the number of hosted operators, and some additional memory\nstatistics.\n\n\n\n\n\n\nphysical-dag-view\n\n\nThe physical-dag-view tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their\ninterconnections:\n\n\n\n\nmetric-view\n\n\nThe metric-view tab displays only the \nMetrics Chart\n widget.\n\n\nMonitor Sales Dimension using the Monitor menu\n\n\nTo monitor the application\n\n\n\n\nClick \nMonitor\n on the menu bar to open the logical view of the DAG.\n    \n\n\nEnsure that the \nState\n is \nRunning\n, indicating that the application\n    is launched successfully.\n\n\nUnder \nStramEvents\n, ensure that the operators from within the\n    application have started.\n\n\nClick \nphysical\n tab to open the physical view.\n\n\n\n\nEnsure that the Input, Parse, Enrich, and\n    Compute operators are deployed to a single container.\n    \n\n\nNote: This is because we set the corresponding stream locality to\n\nCONTAINER_LOCAL\n earlier. This parameter is an example of performance\nimprovement technique, which eliminates data serialization and\nnetworking stack overhead between a group of adjacent operators.\n\n\n\n\n\n\nCreate additional tabs\n\n\nYou can create custom tabs in addition to logical, physical,\nphysical-dag-view, and metric-view. Under each tab, you can add\nwidgets, and customize these widgets according to your requirements.\nThis enables a deeper insight into how the Sales Dimension application\nworks. Each tab, default or otherwise, contains the \nApplication\nOverview\n widget.\n\n\nTo create additional tabs\n\n\n\n\nNext to the \nmetric-view\n tab, look for the plus sign (+) button.\n\n\nClick this button to create an additional tab.\n\n\nProvide a name for your tab.\n\n\nAdd widgets to your tab.", 
            "title": "Monitoring with dtManage"
        }, 
        {
            "location": "/tutorials/salesdimensions-c4/#monitoring-the-sales-dimension-application-using-dtmanage", 
            "text": "Recall that after the application is built and validated, it can be\nlaunched from the  App Packages  page as described in an earlier chapter;\napplications built with  dtAssemble  can also, optionally, be launched\nfrom the  Application Canvas  page as described earlier. This section\ndescribes how you can monitor the running Sales Dimension application\nusing  dtManage .", 
            "title": "Monitoring the Sales Dimension application using dtManage"
        }, 
        {
            "location": "/tutorials/salesdimensions-c4/#the-monitor-menu-option", 
            "text": "You can monitor the Sales Dimension application by clicking\nMonitor on the menu bar. After you click  Monitor , you can choose between\n4 tabs. Under each tab, you can see multiple widgets, which you can\nresize, move around, configure, or remove.  logical  This image of the logical tab shows 4 widgets; additional widgets can be\nadded by clicking the  +  button at the top-left corner and choosing\nfrom the resulting dropdown list.     Application Overview  This widget has the shutdown and kill buttons for shutting down or\nkilling an application. This widget also displays the state of the\napplication, the window IDs, the number of physical operators,\ncontainers, allocated memory, and statistics on the number of\nevents handled.    StramEvents  This widget displays all the operators, containers, and nodes that\nare running. This widget also displays additional information,\nsuch as errors encountered and timestamps.    Logical DAG  This widget displays operators and their\nconnections in the logical dag (as defined in the application)\nwithout partitions, that is, if an operator is partitioned to run\nmultiple copies to increase throughput, only one copy is displayed.\nThe Physical DAG (the physical-dag-view) shows the actual\nphysical operators. For each operator, you can choose to include\nadditional statistics.  To include additional details   Click an operator for which you want to display additional details.  To display a detail on the top of this operator representation,\n    click the Top list, and select a metric.  To display a detail at the bottom of this operator representation,\n    click the Bottom list, and select a metric.     Logical Operators  This widget displays a table of operators\nfor: the name, the Java class, status, and additional statistics for\nlatency and processed events.    Streams  (not shown in the image)  This operator displays a table with one row per stream showing:\nthe name, locality, source, and sinks.    Metrics Chart  (not shown in the image)  This widget displays moving averages of tuples processed and latencies.    physical  The physical tab displays, in addition to  Application Overview \nand  Metrics Chart , 2 more widgets:     Physical Operators  This widget displays a table of physical operators for:\nname, status, host, container ID, and some additional statistics. The\ncontainer ID is a numeric value and a clickable link that takes you to a\npage showing additional details about that specific instance of the\noperator.    Containers  This widget displays a table of containers (the Java Virtual\nMachine processes) and for each process: the ID, the process ID,\nhost, the number of hosted operators, and some additional memory\nstatistics.    physical-dag-view  The physical-dag-view tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their\ninterconnections:   metric-view  The metric-view tab displays only the  Metrics Chart  widget.", 
            "title": "The Monitor menu option"
        }, 
        {
            "location": "/tutorials/salesdimensions-c4/#monitor-sales-dimension-using-the-monitor-menu", 
            "text": "To monitor the application   Click  Monitor  on the menu bar to open the logical view of the DAG.\n      Ensure that the  State  is  Running , indicating that the application\n    is launched successfully.  Under  StramEvents , ensure that the operators from within the\n    application have started.  Click  physical  tab to open the physical view.   Ensure that the Input, Parse, Enrich, and\n    Compute operators are deployed to a single container.\n      Note: This is because we set the corresponding stream locality to CONTAINER_LOCAL  earlier. This parameter is an example of performance\nimprovement technique, which eliminates data serialization and\nnetworking stack overhead between a group of adjacent operators.", 
            "title": "Monitor Sales Dimension using the Monitor menu"
        }, 
        {
            "location": "/tutorials/salesdimensions-c4/#create-additional-tabs", 
            "text": "You can create custom tabs in addition to logical, physical,\nphysical-dag-view, and metric-view. Under each tab, you can add\nwidgets, and customize these widgets according to your requirements.\nThis enables a deeper insight into how the Sales Dimension application\nworks. Each tab, default or otherwise, contains the  Application\nOverview  widget.  To create additional tabs   Next to the  metric-view  tab, look for the plus sign (+) button.  Click this button to create an additional tab.  Provide a name for your tab.  Add widgets to your tab.", 
            "title": "Create additional tabs"
        }, 
        {
            "location": "/apex_development_setup/", 
            "text": "Apache Apex Development Environment Setup\n\n\nThis document discusses the steps needed for setting up a development environment for creating applications that run on the Apache Apex or the DataTorrent RTS streaming platform.\n\n\nMicrosoft Windows\n\n\nThere are a few tools that will be helpful when developing Apache Apex applications, some required and some optional:\n\n\n\n\n\n\ngit\n -- A revision control system (version 1.7.1 or later). There are multiple git clients available for Windows (\nhttp://git-scm.com/download/win\n for example), so download and install a client of your choice.\n\n\n\n\n\n\njava JDK\n (not JRE). Includes the Java Runtime Environment as well as the Java compiler and a variety of tools (version 1.7.0_79 or later). Can be downloaded from the Oracle website.\n\n\n\n\n\n\nmaven\n -- Apache Maven is a build system for Java projects (version 3.0.5 or later). It can be downloaded from \nhttps://maven.apache.org/download.cgi\n.\n\n\n\n\n\n\nVirtualBox\n -- Oracle VirtualBox is a virtual machine manager (version 4.3 or later) and can be downloaded from \nhttps://www.virtualbox.org/wiki/Downloads\n. It is needed to run the Data Torrent Sandbox.\n\n\n\n\n\n\nDataTorrent Sandbox\n -- The sandbox can be downloaded from \nhttps://www.datatorrent.com/download\n. It is useful for testing simple applications since it contains Apache Hadoop and Data Torrent RTS 3.1.1 pre-installed with a time-limited Enterprise License. If you already installed the RTS Enterprise Edition (evaluation or production license) on a cluster, you can use that setup for deployment and testing instead of the sandbox.\n\n\n\n\n\n\n(Optional) If you prefer to use an IDE (Integrated Development Environment) such as \nNetBeans\n, \nEclipse\n or \nIntelliJ\n, install that as well.\n\n\n\n\n\n\nAfter installing these tools, make sure that the directories containing the executable files are in your PATH environment; for example, for the JDK executables like \njava\n and \njavac\n, the directory might be something like \nC:\\\\Program Files\\\\Java\\\\jdk1.7.0\\_80\\\\bin\n; for \ngit\n it might be \nC:\\\\Program Files\\\\Git\\\\bin\n; and for maven it might be \nC:\\\\Users\\\\user\\\\Software\\\\apache-maven-3.3.3\\\\bin\n. Open a console window and enter the command:\n\n\necho %PATH%\n\n\n\nto see the value of the \nPATH\n variable and verify that the above directories are present. If not, you can change its value clicking on the button at \nControl Panel\n \n \nAdvanced System Settings\n \n \nAdvanced tab\n \n \nEnvironment Variables\n.\n\n\nNow run the following commands and ensure that the output is something similar to that shown in the table below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\nOutput\n\n\n\n\n\n\njavac -version\n\n\njavac 1.7.0_80\n\n\n\n\n\n\njava -version\n\n\njava version \n1.7.0_80\n\n\nJava(TM) SE Runtime Environment (build 1.7.0_80-b15)\n\n\nJava HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)\n\n\n\n\n\n\ngit --version\n\n\ngit version 2.6.1.windows.1\n\n\n\n\n\n\nmvn --version\n\n\nApache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T06:57:37-05:00)\n\n\nMaven home: C:\\Users\\ram\\Software\\apache-maven-3.3.3\\bin\\..\n\n\nJava version: 1.7.0_80, vendor: Oracle Corporation\n\n\nJava home: C:\\Program Files\\Java\\jdk1.7.0_80\\jre\n\n\nDefault locale: en_US, platform encoding: Cp1252\n\n\nOS name: \nwindows 8\n, version: \n6.2\n, arch: \namd64\n, family: \nwindows\n\n\n\n\n\n\n\n\n\nTo install the sandbox, first download it from \nhttps://www.datatorrent.com/download\n and import the downloaded file into VirtualBox. Once the import completes, you can select it and click the  Start button to start the sandbox.\n\n\nThe sandbox is configured with 6GB RAM; if your development machine has 16GB or more, you can increase the sandbox RAM to 8GB or more using the VirtualBox console. This will yield better performance and support larger applications. Additionally, you can change the network adapter from \nNAT\n to \nBridged Adapter\n; this will allow you to login to the sandbox from your host machine using an \nssh\n tool like \nPuTTY\n and also to transfer files to and from the host using \npscp\n on Windows. Of course all such configuration must be done when when the sandbox is not running.\n\n\nYou can choose to develop either directly on the sandbox or on your development machine. The advantage of the former is that most of the tools (e.g. \njdk\n, \ngit\n, \nmaven\n) are pre-installed and also the package files created by your project are directly available to the Data Torrent tools such as  \ndtManage\n and \ndtcli\n. The disadvantage is that the sandbox is a memory-limited environment so running a memory-hungry tool like a Java IDE on it may starve other applications of memory.\n\n\nYou can now use the maven archetype to create a basic Apache Apex project as follows: Put these lines in a Windows command file called, for example, \nnewapp.cmd\n and run it:\n\n\n@echo off\n@rem Script for creating a new application\nsetlocal\nmvn archetype:generate ^\n-DarchetypeRepository=https://www.datatorrent.com/maven/content/repositories/releases ^\n  -DarchetypeGroupId=com.datatorrent ^\n  -DarchetypeArtifactId=apex-app-archetype ^\n  -DarchetypeVersion=3.1.1 ^\n  -DgroupId=com.example ^\n  -Dpackage=com.example.myapexapp ^\n  -DartifactId=myapexapp ^\n  -Dversion=1.0-SNAPSHOT\nendlocal\n\n\n\nThe caret (^) at the end of some lines indicates that a continuation line follows. When you run this file, the properties will be displayed and you will be prompted with \nY: :\n; just press \nEnter\n to complete the project generation.\n\n\nThis command file also exists in the Data Torrent \nexamples\n repository which you can check out with:\n\n\ngit clone https://github.com/DataTorrent/examples\n\n\n\nYou will find the script under \nexamples\\tutorials\\topnwords\\scripts\\newapp.cmd\n.\n\n\nYou can also, if you prefer, use an IDE to generate the project as described in Section 3 of \nApplication Packages\n but use the archetype version 3.1.1 instead of 3.0.0.\n\n\nWhen the run completes successfully, you should see a new directory named \nmyapexapp\n containing a maven project for building a basic Apache Apex application. It includes 3 source files:\nApplication.java\n,  \nRandomNumberGenerator.java\n and \nApplicationTest.java\n. You can now build the application by stepping into the new directory and running the appropriate maven command:\n\n\ncd myapexapp\nmvn clean package -DskipTests\n\n\n\nThe build should create the application package file \nmyapexapp\\target\\myapexapp-1.0-SNAPSHOT.apa\n. This file can then be uploaded to the Data Torrent GUI tool on the sandbox (called \ndtManage\n) and launched  from there. It generates a stream of random numbers and prints them out, each prefixed by the string  \nhello world:\n.  If you built this package on the host, you can transfer it to the sandbox using the \npscp\n tool bundled with \nPuTTY\n mentioned earlier.\n\n\nIf you want to checkout the Apache Apex source repositories and build them, you can do so by running the script \nbuild-apex.cmd\n located in the same place in the examples repository described above. The source repositories contain more substantial demo applications and the associated source code. Alternatively, if you do not want to use the script, you can follow these simple manual steps:\n\n\n\n\n\n\nCheck out the source code repositories:\n\n\ngit clone https://github.com/apache/incubator-apex-core\ngit clone https://github.com/apache/incubator-apex-malhar\n\n\n\n\n\n\n\nSwitch to the appropriate release branch and build each repository:\n\n\npushd incubator-apex-core\ngit checkout release-3.1\nmvn clean install -DskipTests\npopd\npushd incubator-apex-malhar\ngit checkout release-3.1\nmvn clean install -DskipTests\npopd\n\n\n\n\n\n\n\nThe \ninstall\n argument to the \nmvn\n command installs resources from each project to your local maven repository (typically \n.m2/repository\n under your home directory), and \nnot\n to the system directories, so Administrator privileges are not required. The  \n-DskipTests\n argument skips running unit tests since they take a long time. If this is a first-time installation, it might take several minutes to complete because maven will download a number of associated plugins.\n\n\nAfter the build completes, you should see the demo application package files in the target directory under each demo subdirectory in \nincubator-apex-malhar\\demos\\\n.\n\n\nLinux\n\n\nMost of the instructions for Linux (and other Unix-like systems) are similar to those for Windows described above, so we will just note the differences.\n\n\nThe pre-requisites (such as \ngit\n, \nmaven\n, etc.) are the same as for Windows described above; please run the commands in the table and ensure that appropriate versions are present in your PATH environment variable (the command to display that variable is: \necho $PATH\n).\n\n\nThe maven archetype command is the same except that continuation lines use a backslash (\n\\\n) instead of caret (\n^\n); the script for it is available in the same location and is named \nnewapp\n (without the \n.cmd\n extension). The script to checkout and build the Apache Apex repositories is named \nbuild-apex\n.", 
            "title": "Apex Development Setup"
        }, 
        {
            "location": "/apex_development_setup/#apache-apex-development-environment-setup", 
            "text": "This document discusses the steps needed for setting up a development environment for creating applications that run on the Apache Apex or the DataTorrent RTS streaming platform.", 
            "title": "Apache Apex Development Environment Setup"
        }, 
        {
            "location": "/apex_development_setup/#microsoft-windows", 
            "text": "There are a few tools that will be helpful when developing Apache Apex applications, some required and some optional:    git  -- A revision control system (version 1.7.1 or later). There are multiple git clients available for Windows ( http://git-scm.com/download/win  for example), so download and install a client of your choice.    java JDK  (not JRE). Includes the Java Runtime Environment as well as the Java compiler and a variety of tools (version 1.7.0_79 or later). Can be downloaded from the Oracle website.    maven  -- Apache Maven is a build system for Java projects (version 3.0.5 or later). It can be downloaded from  https://maven.apache.org/download.cgi .    VirtualBox  -- Oracle VirtualBox is a virtual machine manager (version 4.3 or later) and can be downloaded from  https://www.virtualbox.org/wiki/Downloads . It is needed to run the Data Torrent Sandbox.    DataTorrent Sandbox  -- The sandbox can be downloaded from  https://www.datatorrent.com/download . It is useful for testing simple applications since it contains Apache Hadoop and Data Torrent RTS 3.1.1 pre-installed with a time-limited Enterprise License. If you already installed the RTS Enterprise Edition (evaluation or production license) on a cluster, you can use that setup for deployment and testing instead of the sandbox.    (Optional) If you prefer to use an IDE (Integrated Development Environment) such as  NetBeans ,  Eclipse  or  IntelliJ , install that as well.    After installing these tools, make sure that the directories containing the executable files are in your PATH environment; for example, for the JDK executables like  java  and  javac , the directory might be something like  C:\\\\Program Files\\\\Java\\\\jdk1.7.0\\_80\\\\bin ; for  git  it might be  C:\\\\Program Files\\\\Git\\\\bin ; and for maven it might be  C:\\\\Users\\\\user\\\\Software\\\\apache-maven-3.3.3\\\\bin . Open a console window and enter the command:  echo %PATH%  to see the value of the  PATH  variable and verify that the above directories are present. If not, you can change its value clicking on the button at  Control Panel     Advanced System Settings     Advanced tab     Environment Variables .  Now run the following commands and ensure that the output is something similar to that shown in the table below:         Command  Output    javac -version  javac 1.7.0_80    java -version  java version  1.7.0_80  Java(TM) SE Runtime Environment (build 1.7.0_80-b15)  Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)    git --version  git version 2.6.1.windows.1    mvn --version  Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T06:57:37-05:00)  Maven home: C:\\Users\\ram\\Software\\apache-maven-3.3.3\\bin\\..  Java version: 1.7.0_80, vendor: Oracle Corporation  Java home: C:\\Program Files\\Java\\jdk1.7.0_80\\jre  Default locale: en_US, platform encoding: Cp1252  OS name:  windows 8 , version:  6.2 , arch:  amd64 , family:  windows     To install the sandbox, first download it from  https://www.datatorrent.com/download  and import the downloaded file into VirtualBox. Once the import completes, you can select it and click the  Start button to start the sandbox.  The sandbox is configured with 6GB RAM; if your development machine has 16GB or more, you can increase the sandbox RAM to 8GB or more using the VirtualBox console. This will yield better performance and support larger applications. Additionally, you can change the network adapter from  NAT  to  Bridged Adapter ; this will allow you to login to the sandbox from your host machine using an  ssh  tool like  PuTTY  and also to transfer files to and from the host using  pscp  on Windows. Of course all such configuration must be done when when the sandbox is not running.  You can choose to develop either directly on the sandbox or on your development machine. The advantage of the former is that most of the tools (e.g.  jdk ,  git ,  maven ) are pre-installed and also the package files created by your project are directly available to the Data Torrent tools such as   dtManage  and  dtcli . The disadvantage is that the sandbox is a memory-limited environment so running a memory-hungry tool like a Java IDE on it may starve other applications of memory.  You can now use the maven archetype to create a basic Apache Apex project as follows: Put these lines in a Windows command file called, for example,  newapp.cmd  and run it:  @echo off\n@rem Script for creating a new application\nsetlocal\nmvn archetype:generate ^\n-DarchetypeRepository=https://www.datatorrent.com/maven/content/repositories/releases ^\n  -DarchetypeGroupId=com.datatorrent ^\n  -DarchetypeArtifactId=apex-app-archetype ^\n  -DarchetypeVersion=3.1.1 ^\n  -DgroupId=com.example ^\n  -Dpackage=com.example.myapexapp ^\n  -DartifactId=myapexapp ^\n  -Dversion=1.0-SNAPSHOT\nendlocal  The caret (^) at the end of some lines indicates that a continuation line follows. When you run this file, the properties will be displayed and you will be prompted with  Y: : ; just press  Enter  to complete the project generation.  This command file also exists in the Data Torrent  examples  repository which you can check out with:  git clone https://github.com/DataTorrent/examples  You will find the script under  examples\\tutorials\\topnwords\\scripts\\newapp.cmd .  You can also, if you prefer, use an IDE to generate the project as described in Section 3 of  Application Packages  but use the archetype version 3.1.1 instead of 3.0.0.  When the run completes successfully, you should see a new directory named  myapexapp  containing a maven project for building a basic Apache Apex application. It includes 3 source files: Application.java ,   RandomNumberGenerator.java  and  ApplicationTest.java . You can now build the application by stepping into the new directory and running the appropriate maven command:  cd myapexapp\nmvn clean package -DskipTests  The build should create the application package file  myapexapp\\target\\myapexapp-1.0-SNAPSHOT.apa . This file can then be uploaded to the Data Torrent GUI tool on the sandbox (called  dtManage ) and launched  from there. It generates a stream of random numbers and prints them out, each prefixed by the string   hello world: .  If you built this package on the host, you can transfer it to the sandbox using the  pscp  tool bundled with  PuTTY  mentioned earlier.  If you want to checkout the Apache Apex source repositories and build them, you can do so by running the script  build-apex.cmd  located in the same place in the examples repository described above. The source repositories contain more substantial demo applications and the associated source code. Alternatively, if you do not want to use the script, you can follow these simple manual steps:    Check out the source code repositories:  git clone https://github.com/apache/incubator-apex-core\ngit clone https://github.com/apache/incubator-apex-malhar    Switch to the appropriate release branch and build each repository:  pushd incubator-apex-core\ngit checkout release-3.1\nmvn clean install -DskipTests\npopd\npushd incubator-apex-malhar\ngit checkout release-3.1\nmvn clean install -DskipTests\npopd    The  install  argument to the  mvn  command installs resources from each project to your local maven repository (typically  .m2/repository  under your home directory), and  not  to the system directories, so Administrator privileges are not required. The   -DskipTests  argument skips running unit tests since they take a long time. If this is a first-time installation, it might take several minutes to complete because maven will download a number of associated plugins.  After the build completes, you should see the demo application package files in the target directory under each demo subdirectory in  incubator-apex-malhar\\demos\\ .", 
            "title": "Microsoft Windows"
        }, 
        {
            "location": "/apex_development_setup/#linux", 
            "text": "Most of the instructions for Linux (and other Unix-like systems) are similar to those for Windows described above, so we will just note the differences.  The pre-requisites (such as  git ,  maven , etc.) are the same as for Windows described above; please run the commands in the table and ensure that appropriate versions are present in your PATH environment variable (the command to display that variable is:  echo $PATH ).  The maven archetype command is the same except that continuation lines use a backslash ( \\ ) instead of caret ( ^ ); the script for it is available in the same location and is named  newapp  (without the  .cmd  extension). The script to checkout and build the Apache Apex repositories is named  build-apex .", 
            "title": "Linux"
        }, 
        {
            "location": "/application_development/", 
            "text": "Application Developer Guide\n\n\nReal-time big data processing is not only important but has become\ncritical for businesses which depend on accurate and timely analysis of\ntheir business data. A few businesses have yielded to very expensive\nsolutions like building an in-house, real-time analytics infrastructure\nsupported by an internal development team, or buying expensive\nproprietary software. A large number of businesses are dealing with the\nrequirement just by trying to make Hadoop do their batch jobs in smaller\niterations. Over the last few years, Hadoop has become ubiquitous in the\nbig data processing space, replacing expensive proprietary hardware and\nsoftware solutions for massive data processing with very cost-effective,\nfault-tolerant, open-sourced, and commodity-hardware-based solutions.\nWhile Hadoop has been a game changer for companies, it is primarily a\nbatch-oriented system, and does not yet have a viable option for\nreal-time data processing. \u00a0Most companies with real-time data\nprocessing end up having to build customized solutions in addition to\ntheir Hadoop infrastructure.\n\n\nThe DataTorrent platform is designed to process massive amounts of\nreal-time events natively in Hadoop. This can be event ingestion,\nprocessing, and aggregation for real-time data analytics, or can be\nreal-time business logic decisioning such as cell tower load balancing,\nreal-time ads bidding, or fraud detection. \u00a0The platform has the ability\nto repair itself in real-time (without data loss) if hardware fails, and\nadapt to changes in load by adding and removing computing resources\nautomatically.\n\n\nDataTorrent is a native Hadoop application. It runs as a YARN\n(Hadoop 2.x) application and leverages Hadoop as a distributed operating\nsystem. All the basic distributed operating system capabilities of\nHadoop like resource allocation (Resource Manager, distributed file system (HDFS),\nmulti-tenancy, security, fault-tolerance, scalability,\u00a0etc.\nare supported natively in all streaming applications. \u00a0Just as Hadoop\nfor map-reduce handles all the details of the application allowing you\nto only focus on writing the application (the mapper and reducer\nfunctions), the platform handles all the details of streaming execution,\nallowing you to only focus on your business logic. Using the platform\nremoves the need to maintain separate clusters for real-time\napplications.\n\n\nIn the platform, building a streaming application can be extremely\neasy and intuitive. \u00a0The application is represented as a Directed\nAcyclic Graph (DAG) of computation units called \nOperators\n interconnected\nby the data-flow edges called  \nStreams\n.\u00a0The operators process input\nstreams and produce output streams. A library of common operators is\nprovided to enable quick application development. \u00a0In case the desired\nprocessing is not available in the Operator Library, one can easily\nwrite a custom operator. We refer those interested in creating their own\noperators to the \nOperator Development Guide\n.\n\n\nRunning A Test Application\n\n\nThis chapter will help you with a quick start on running an\napplication. If you are starting with the platform for the first time,\nit would be informative to open an existing application and see it run.\nDo the following steps to run the PI demo, which computes the value of\nPI \u00a0in a simple\nmanner:\n\n\n\n\nOpen up platform files in your IDE (for example NetBeans, or Eclipse)\n\n\nOpen Demos project\n\n\nOpen Test Packages and run ApplicationTest.java\u00a0in pi\u00a0package\n\n\nSee the results in your system console\n\n\n\n\nCongratulations, you just ran your first real-time streaming demo\n:) This demo is very simple and has four operators. The first operator\nemits random integers between 0 to 30, 000. The second operator receives\nthese coefficients and emits a hashmap with x and y values each time it\nreceives two values. The third operator takes these values and computes\nx**2+y**2. The last operator counts how many computed values from\nthe previous operator were less than or equal to 30, 000**2. Assuming\nthis count is N, then PI is computed as N/number of values received.\nHere is the code snippet for the PI application. This code populates the\nDAG. Do not worry about what each line does, we will cover these\nconcepts later in this document.\n\n\n// Generates random numbers\nRandomEventGenerator rand = dag.addOperator(\nrand\n, new RandomEventGenerator());\nrand.setMinvalue(0);\nrand.setMaxvalue(30000);\n\n// Generates a round robin HashMap of \nx\n and \ny\n\nRoundRobinHashMap\nString,Object\n rrhm = dag.addOperator(\nrrhm\n, new RoundRobinHashMap\nString, Object\n());\nrrhm.setKeys(new String[] { \nx\n, \ny\n });\n\n// Calculates pi from x and y\nJavaScriptOperator calc = dag.addOperator(\npicalc\n, new Script());\ncalc.setPassThru(false);\ncalc.put(\ni\n,0);\ncalc.put(\ncount\n,0);\ncalc.addSetupScript(\nfunction pi() { if (x*x+y*y \n= \n+maxValue*maxValue+\n) { i++; } count++; return i / count * 4; }\n);\ncalc.setInvoke(\npi\n);\ndag.addStream(\nrand_rrhm\n, rand.integer_data, rrhm.data);\ndag.addStream(\nrrhm_calc\n, rrhm.map, calc.inBindings);\n\n// puts results on system console\nConsoleOutputOperator console = dag.addOperator(\nconsole\n, new ConsoleOutputOperator());\ndag.addStream(\nrand_console\n,calc.result, console.input);\n\n\n\n\nYou can review the other demos and see what they do. The examples\ngiven in the Demos project cover various features of the platform and we\nstrongly encourage you to read these to familiarize yourself with the\nplatform. In the remaining part of this document we will go through\ndetails needed for you to develop and run streaming applications in\nMalhar.\n\n\nTest Application: Yahoo! Finance Quotes\n\n\nThe PI\u00a0application was to\nget you started. It is a basic application and does not fully illustrate\nthe features of the platform. For the purpose of describing concepts, we\nwill consider the test application shown in Figure 1. The application\ndownloads tick data from  \nYahoo! Finance\n \u00a0and computes the\nfollowing for four tickers, namely \nIBM\n,\n\nGOOG\n, \nYHOO\n.\n\n\n\n\nQuote: Consisting of last trade price, last trade time, and\n    total volume for the day\n\n\nPer-minute chart data: Highest trade price, lowest trade\n    price, and volume during that minute\n\n\nSimple Moving Average: trade price over 5 minutes\n\n\n\n\nTotal volume must ensure that all trade volume for that day is\nadded, i.e. data loss would result in wrong results. Charting data needs\nall the trades in the same minute to go to the same slot, and then on it\nstarts afresh, so again data loss would result in wrong results. The\naggregation for charting data is done over 1 minute. Simple moving\naverage computes the average price over a 5 minute sliding window; it\ntoo would produce wrong results if there is data loss. Figure 1 shows\nthe application with no partitioning.\n\n\n\n\nThe operator StockTickerInput:\u00a0StockTickerInput\n\u00a0\nis\nthe input operator that reads live data from Yahoo! Finance once per\ninterval (user configurable in milliseconds), and emits the price, the\nincremental volume, and the last trade time of each stock symbol, thus\nemulating real ticks from the exchange. \u00a0We utilize the Yahoo! Finance\nCSV web service interface. \u00a0For example:\n\n\n$ GET 'http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO\nf=sl1vt1'\n\nIBM\n,203.966,1513041,\n1:43pm\n\n\nGOOG\n,762.68,1879741,\n1:43pm\n\n\nAAPL\n,444.3385,11738366,\n1:43pm\n\n\nYHOO\n,19.3681,14707163,\n1:43pm\n\n\n\n\n\nAmong all the operators in Figure 1, StockTickerInput is the only\noperator that requires extra code because it contains a custom mechanism\nto get the input data. \u00a0Other operators are used unchanged from the\nMalhar library.\n\n\nHere is the class implementation for StockTickInput:\n\n\npackage com.datatorrent.demos.yahoofinance;\n\nimport au.com.bytecode.opencsv.CSVReader;\nimport com.datatorrent.annotation.OutputPortFieldAnnotation;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DefaultOutputPort;\nimport com.datatorrent.api.InputOperator;\nimport com.datatorrent.lib.util.KeyValPair;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.util.*;\nimport org.apache.commons.httpclient.HttpClient;\nimport org.apache.commons.httpclient.HttpStatus;\nimport org.apache.commons.httpclient.cookie.CookiePolicy;\nimport org.apache.commons.httpclient.methods.GetMethod;\nimport org.apache.commons.httpclient.params.DefaultHttpParams;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * This operator sends price, volume and time into separate ports and calculates incremental volume.\n */\npublic class StockTickInput implements InputOperator\n{\n  private static final Logger logger = LoggerFactory.getLogger(StockTickInput.class);\n  /**\n   * Timeout interval for reading from server. 0 or negative indicates no timeout.\n   */\n  public int readIntervalMillis = 500;\n  /**\n   * The URL of the web service resource for the POST request.\n   */\n  private String url;\n  public String[] symbols;\n  private transient HttpClient client;\n  private transient GetMethod method;\n  private HashMap\nString, Long\n lastVolume = new HashMap\nString, Long\n();\n  private boolean outputEvenIfZeroVolume = false;\n  /**\n   * The output port to emit price.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, Double\n price = new DefaultOutputPort\nKeyValPair\nString, Double\n();\n  /**\n   * The output port to emit incremental volume.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, Long\n volume = new DefaultOutputPort\nKeyValPair\nString, Long\n();\n  /**\n   * The output port to emit last traded time.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, String\n time = new DefaultOutputPort\nKeyValPair\nString, String\n();\n\n  /**\n   * Prepare URL from symbols and parameters. URL will be something like: http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO\nf=sl1vt1\n   *\n   * @return the URL\n   */\n  private String prepareURL()\n  {\n    String str = \nhttp://download.finance.yahoo.com/d/quotes.csv?s=\n;\n    for (int i = 0; i \n symbols.length; i++) {\n      if (i != 0) {\n        str += \n,\n;\n      }\n      str += symbols[i];\n    }\n    str += \nf=sl1vt1\ne=.csv\n;\n    return str;\n  }\n\n  @Override\n  public void setup(OperatorContext context)\n  {\n    url = prepareURL();\n    client = new HttpClient();\n    method = new GetMethod(url);\n    DefaultHttpParams.getDefaultParams().setParameter(\nhttp.protocol.cookie-policy\n, CookiePolicy.BROWSER_COMPATIBILITY);\n  }\n\n  @Override\n  public void teardown()\n  {\n  }\n\n  @Override\n  public void emitTuples()\n  {\n\n    try {\n      int statusCode = client.executeMethod(method);\n      if (statusCode != HttpStatus.SC_OK) {\n        System.err.println(\nMethod failed: \n + method.getStatusLine());\n      }\n      else {\n        InputStream istream = method.getResponseBodyAsStream();\n        // Process response\n        InputStreamReader isr = new InputStreamReader(istream);\n        CSVReader reader = new CSVReader(isr);\n        List\nString[]\n myEntries = reader.readAll();\n        for (String[] stringArr: myEntries) {\n          ArrayList\nString\n tuple = new ArrayList\nString\n(Arrays.asList(stringArr));\n          if (tuple.size() != 4) {\n            return;\n          }\n          // input csv is \nSymbol\n,\nPrice\n,\nVolume\n,\nTime\n\n          String symbol = tuple.get(0);\n          double currentPrice = Double.valueOf(tuple.get(1));\n          long currentVolume = Long.valueOf(tuple.get(2));\n          String timeStamp = tuple.get(3);\n          long vol = currentVolume;\n          // Sends total volume in first tick, and incremental volume afterwards.\n          if (lastVolume.containsKey(symbol)) {\n            vol -= lastVolume.get(symbol);\n          }\n\n          if (vol \n 0 || outputEvenIfZeroVolume) {\n            price.emit(new KeyValPair\nString, Double\n(symbol, currentPrice));\n            volume.emit(new KeyValPair\nString, Long\n(symbol, vol));\n            time.emit(new KeyValPair\nString, String\n(symbol, timeStamp));\n            lastVolume.put(symbol, currentVolume);\n          }\n        }\n      }\n      Thread.sleep(readIntervalMillis);\n    }\n    catch (InterruptedException ex) {\n      logger.debug(ex.toString());\n    }\n    catch (IOException ex) {\n      logger.debug(ex.toString());\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n  }\n\n  @Override\n  public void endWindow()\n  {\n  }\n\n  public void setOutputEvenIfZeroVolume(boolean outputEvenIfZeroVolume)\n  {\n       this.outputEvenIfZeroVolume = outputEvenIfZeroVolume;\n  }\n\n}\n\n\n\n\nThe operator has three output ports that emit the price of the\nstock, the volume of the stock and the last trade time of the stock,\ndeclared as public member variables price, volume\u00a0and  time\u00a0of the class. \u00a0The tuple of the\nprice\u00a0output port is a key-value\npair with the stock symbol being the key, and the price being the value.\n\u00a0The tuple of the volume\u00a0output\nport is a key value pair with the stock symbol being the key, and the\nincremental volume being the value. \u00a0The tuple of the  time\u00a0output port is a key value pair with the\nstock symbol being the key, and the last trade time being the\nvalue.\n\n\nImportant: Since operators will be\nserialized, all input and output ports need to be declared transient\nbecause they are stateless and should not be serialized.\n\n\nThe method\u00a0setup(OperatorContext)\ncontains the code that is necessary for setting up the HTTP\nclient for querying Yahoo! Finance.\n\n\nMethod\u00a0emitTuples() contains\nthe code that reads from Yahoo! Finance, and emits the data to the\noutput ports of the operator. \u00a0emitTuples()\u00a0will be called one or more times\nwithin one application window as long as time is allowed within the\nwindow.\n\n\nNote that we want to emulate the tick input stream by having\nincremental volume data with Yahoo! Finance data. \u00a0We therefore subtract\nthe previous volume from the current volume to emulate incremental\nvolume for each tick.\n\n\nThe operator\nDailyVolume:\u00a0This operator\nreads from the input port, which contains the incremental volume tuples\nfrom StockTickInput, and\naggregates the data to provide the cumulative volume. \u00a0It uses the\nlibrary class  SumKeyVal\nK,V\n\u00a0provided in math\u00a0package. \u00a0In this case,\nSumKeyVal\nString,Long\n, where K is the stock symbol, V is the\naggregated volume, with cumulative\nset to true. (Otherwise if  cumulativewas set to false, SumKeyVal would\nprovide the sum for the application window.) \u00a0Malhar provides a number\nof built-in operators for simple operations like this so that\napplication developers do not have to write them. \u00a0More examples to\nfollow. This operator assumes that the application restarts before the\nmarket opens every day.\n\n\nThe operator Quote:\nThis operator has three input ports, which are price (from\nStockTickInput), daily_vol (from\nDaily Volume), and time (from\n StockTickInput). \u00a0This operator\njust consolidates the three data items and and emits the consolidated\ndata. \u00a0It utilizes the class ConsolidatorKeyVal\nK\n\u00a0from the\nstream\u00a0package.\n\n\nThe operator HighLow:\u00a0This operator reads from the input port,\nwhich contains the price tuples from StockTickInput, and provides the high and the\nlow price within the application window. \u00a0It utilizes the library class\n RangeKeyVal\nK,V\n\u00a0provided\nin the math\u00a0package. In this case,\nRangeKeyVal\nString,Double\n.\n\n\nThe operator MinuteVolume:\nThis operator reads from the input port, which contains the\nvolume tuples from StockTickInput,\nand aggregates the data to provide the sum of the volume within one\nminute. \u00a0Like the operator  DailyVolume, this operator also uses\nSumKeyVal\nString,Long\n, but\nwith cumulative set to false. \u00a0The\nApplication Window is set to one minute. We will explain how to set this\nlater.\n\n\nThe operator Chart:\nThis operator is very similar to the operator Quote, except that it takes inputs from\nHigh Low\u00a0and  Minute Vol\u00a0and outputs the consolidated tuples\nto the output port.\n\n\nThe operator PriceSMA:\nSMA stands for - Simple Moving Average. It reads from the\ninput port, which contains the price tuples from StockTickInput, and\nprovides the moving average price of the stock. \u00a0It utilizes\nSimpleMovingAverage\nString,Double\n, which is provided in the\n multiwindow\u00a0package.\nSimpleMovingAverage keeps track of the data of the previous N\napplication windows in a sliding manner. \u00a0For each end window event, it\nprovides the average of the data in those application windows.\n\n\nThe operator Console:\nThis operator just outputs the input tuples to the console\n(or stdout). \u00a0In this example, there are four console\u00a0operators, which connect to the output\nof  Quote, Chart, PriceSMA and VolumeSMA. \u00a0In\npractice, they should be replaced by operators that use the data to\nproduce visualization artifacts like charts.\n\n\nConnecting the operators together and constructing the\nDAG:\u00a0Now that we know the\noperators used, we will create the DAG, set the streaming window size,\ninstantiate the operators, and connect the operators together by adding\nstreams that connect the output ports with the input ports among those\noperators. \u00a0This code is in the file  YahooFinanceApplication.java. Refer to Figure 1\nagain for the graphical representation of the DAG. \u00a0The last method in\nthe code, namely getApplication(),\ndoes all that. \u00a0The rest of the methods are just for setting up the\noperators.\n\n\npackage com.datatorrent.demos.yahoofinance;\n\nimport com.datatorrent.api.ApplicationFactory;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DAG;\nimport com.datatorrent.api.Operator.InputPort;\nimport com.datatorrent.lib.io.ConsoleOutputOperator;\nimport com.datatorrent.lib.math.RangeKeyVal;\nimport com.datatorrent.lib.math.SumKeyVal;\nimport com.datatorrent.lib.multiwindow.SimpleMovingAverage;\nimport com.datatorrent.lib.stream.ConsolidatorKeyVal;\nimport com.datatorrent.lib.util.HighLow;\nimport org.apache.hadoop.conf.Configuration;\n\n/**\n * Yahoo! Finance application demo. \np\n\n *\n * Get Yahoo finance feed and calculate minute price range, minute volume, simple moving average of 5 minutes.\n */\npublic class Application implements StreamingApplication\n{\n  private int streamingWindowSizeMilliSeconds = 1000; // 1 second (default is 500ms)\n  private int appWindowCountMinute = 60;   // 1 minute\n  private int appWindowCountSMA = 5 * 60;  // 5 minute\n\n  /**\n   * Get actual Yahoo finance ticks of symbol, last price, total daily volume, and last traded price.\n   */\n  public StockTickInput getStockTickInputOperator(String name, DAG dag)\n  {\n    StockTickInput oper = dag.addOperator(name, StockTickInput.class);\n    oper.readIntervalMillis = 200;\n    return oper;\n  }\n\n  /**\n   * This sends total daily volume by adding volumes from each ticks.\n   */\n  public SumKeyVal\nString, Long\n getDailyVolumeOperator(String name, DAG dag)\n  {\n    SumKeyVal\nString, Long\n oper = dag.addOperator(name, new SumKeyVal\nString, Long\n());\n    oper.setType(Long.class);\n    oper.setCumulative(true);\n    return oper;\n  }\n\n  /**\n   * Get aggregated volume of 1 minute and send at the end window of 1 minute.\n   */\n  public SumKeyVal\nString, Long\n getMinuteVolumeOperator(String name, DAG dag, int appWindowCount)\n  {\n    SumKeyVal\nString, Long\n oper = dag.addOperator(name, new SumKeyVal\nString, Long\n());\n    oper.setType(Long.class);\n    oper.setEmitOnlyWhenChanged(true);\ndag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    return oper;\n  }\n\n  /**\n   * Get High-low range for 1 minute.\n   */\n  public RangeKeyVal\nString, Double\n getHighLowOperator(String name, DAG dag, int appWindowCount)\n  {\n    RangeKeyVal\nString, Double\n oper = dag.addOperator(name, new RangeKeyVal\nString, Double\n());\n    dag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Quote (Merge price, daily volume, time)\n   */\n  public ConsolidatorKeyVal\nString,Double,Long,String,?,?\n getQuoteOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n oper = dag.addOperator(name, new ConsolidatorKeyVal\nString,Double,Long,String,Object,Object\n());\n    return oper;\n  }\n\n  /**\n   * Chart (Merge minute volume and minute high-low)\n   */\n  public ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n getChartOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n oper = dag.addOperator(name, new ConsolidatorKeyVal\nString,HighLow,Long,Object,Object,Object\n());\n    return oper;\n  }\n\n  /**\n   * Get simple moving average of price.\n   */\n  public SimpleMovingAverage\nString, Double\n getPriceSimpleMovingAverageOperator(String name, DAG dag, int appWindowCount)\n  {\n    SimpleMovingAverage\nString, Double\n oper = dag.addOperator(name, new SimpleMovingAverage\nString, Double\n());\n    oper.setWindowSize(appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Get console for output.\n   */\n  public InputPort\nObject\n getConsole(String name, /*String nodeName,*/ DAG dag, String prefix)\n  {\n    ConsoleOutputOperator oper = dag.addOperator(name, ConsoleOutputOperator.class);\n    oper.setStringFormat(prefix + \n: %s\n);\n    return oper.input;\n  }\n\n  /**\n   * Create Yahoo Finance Application DAG.\n   */\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().put(DAG.STRAM_WINDOW_SIZE_MILLIS,streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator(\nStockTickInput\n, dag);\n    SumKeyVal\nString, Long\n dailyVolume = getDailyVolumeOperator(\nDailyVolume\n, dag);\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n quoteOperator = getQuoteOperator(\nQuote\n, dag);\n\n    RangeKeyVal\nString, Double\n highlow = getHighLowOperator(\nHighLow\n, dag, appWindowCountMinute);\n    SumKeyVal\nString, Long\n minuteVolume = getMinuteVolumeOperator(\nMinuteVolume\n, dag, appWindowCountMinute);\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n chartOperator = getChartOperator(\nChart\n, dag);\n\n    SimpleMovingAverage\nString, Double\n priceSMA = getPriceSimpleMovingAverageOperator(\nPriceSMA\n, dag, appWindowCountSMA);\n       DefaultPartitionCodec\nString, Double\n codec = new DefaultPartitionCodec\nString, Double\n();\n    dag.setInputPortAttribute(highlow.data, PortContext.STREAM_CODEC, codec);\n    dag.setInputPortAttribute(priceSMA.data, PortContext.STREAM_CODEC, codec);\n    dag.addStream(\nprice\n, tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream(\nvol\n, tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream(\ntime\n, tick.time, quoteOperator.in3);\n    dag.addStream(\ndaily_vol\n, dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream(\nquote_data\n, quoteOperator.out, getConsole(\nquoteConsole\n, dag, \nQUOTE\n));\n\n    dag.addStream(\nhigh_low\n, highlow.range, chartOperator.in1);\n    dag.addStream(\nvol_1min\n, minuteVolume.sum, chartOperator.in2);\n    dag.addStream(\nchart_data\n, chartOperator.out, getConsole(\nchartConsole\n, dag, \nCHART\n));\n\n    dag.addStream(\nsma_price\n, priceSMA.doubleSMA, getConsole(\npriceSMAConsole\n, dag, \nPrice SMA\n));\n\n    return dag;\n  }\n\n}\n\n\n\n\nNote that we also set a user-specific sliding window for SMA that\nkeeps track of the previous N data points. \u00a0Do not confuse this with the\nattribute APPLICATION_WINDOW_COUNT.\n\n\nIn the rest of this chapter we will run through the process of\nrunning this application. We assume that \u00a0you are familiar with details\nof your Hadoop infrastructure. For installation\ndetails please refer to the \nInstallation Guide\n.\n\n\nRunning a Test Application\n\n\nWe will now describe how to run the yahoo\nfinance application\u00a0described above in different modes\n(local mode, single node on Hadoop, and multi-nodes on Hadoop).\n\n\nThe platform runs streaming applications under the control of a\nlight-weight Streaming Application Manager (STRAM). Each application has\nits own instance of STRAM. STRAM launches the application and\ncontinually provides run time monitoring, analysis, and takes action\nsuch as load scaling or outage recovery as needed. \u00a0We will discuss\nSTRAM in more detail in the next chapter.\n\n\nThe instructions below assume that the platform was installed in a\ndirectory \nINSTALL_DIR\n and the command line interface (CLI) will\nbe used to launch the demo application. An application can be run in\n\nlocal mode\n\u00a0\n(in IDE or from command line) or on a  \nHadoop cluster\n \n.\n\n\nTo start the dtCli run\n\n\nINSTALL_DIR\n/bin/dtcli\n\n\n\nThe command line prompt appears.  To start the application in local mode (the actual version number in the file name may differ)\n\n\ndt\n launch -local \nINSTALL_DIR\n/yahoo-finance-demo-3.2.0-SNAPSHOT.apa\n\n\n\nTo terminate the application in local mode, enter Ctrl-C\n\n\nTu run the application on the Hadoop cluster (the actual version\nnumber in the file name may differ)\n\n\ndt\n launch \nINSTALL_DIR\n/yahoo-finance-demo-3.2.0-SNAPSHOT.apa\n\n\n\nTo stop the application running in Hadoop, terminate it in the dtCli:\n\n\ndt\n kill-app\n\n\n\nExecuting the application in either mode includes the following\nsteps. At a top level, STRAM (Streaming Application Manager) validates\nthe application (DAG), translates the logical plan to the physical plan\nand then launches the execution engine. The mode determines the\nresources needed and how how they are used.\n\n\nLocal Mode\n\n\nIn local mode, the application is run as a single-process\u00a0with multiple threads. Although a\nfew Hadoop classes are needed, there is no dependency on a Hadoop\ncluster or Hadoop services. The local file system is used in place of\nHDFS. This mode allows a quick run of an application in a single process\nsandbox, and hence is the most suitable to debug and analyze the\napplication logic. This mode is recommended for developing the\napplication and can be used for running applications within the IDE for\nfunctional testing purposes. Due to limited resources and lack \u00a0of\nscalability an application running in this single process mode is more\nlikely to encounter throughput bottlenecks. A distributed cluster is\nrecommended for benchmarking and production testing.\n\n\nHadoop Cluster\n\n\nIn this section we discuss various Hadoop cluster setups.\n\n\nSingle Node Cluster\n\n\nIn a single node Hadoop cluster all services are deployed on a\nsingle server (a developer can use his/her development machine as a\nsingle node cluster). The platform does not distinguish between a single\nor multi-node setup and behaves exactly the same in both cases.\n\n\nIn this mode, the resource manager, name node, data node, and node\nmanager occupy one process each. This is an example of running a\nstreaming application as a multi-process\u00a0application on the same server.\nWith prevalence of fast, multi-core systems, this mode is effective for\ndebugging, fine tuning, and generic analysis before submitting the job\nto a larger Hadoop cluster. In this mode, execution uses the Hadoop\nservices and hence is likely to identify issues that are related to the\nHadoop environment (such issues will not be uncovered in local mode).\nThe throughput will obviously not be as high as on a multi-node Hadoop\ncluster. Additionally, since each container (i.e. Java process) requires\na significant amount of memory, you will be able to run a much smaller\nnumber of containers than on a multi-node cluster.\n\n\nMulti-Node Cluster\n\n\nIn a multi-node Hadoop cluster all the services of Hadoop are\ntypically distributed across multiple nodes in a production or\nproduction-level test environment. Upon launch the application is\nsubmitted to the Hadoop cluster and executes as a  multi-processapplication on\u00a0multiple nodes.\n\n\nBefore you start deploying, testing and troubleshooting your\napplication on a cluster, you should ensure that Hadoop (version 2.2.0\nor later)\u00a0is properly installed and\nyou have basic skills for working with it.\n\n\n\n\nApache Apex Platform Overview\n\n\nStreaming Computational Model\n\n\nIn this chapter, we describe the the basics of the real-time streaming platform and its computational model.\n\n\nThe platform is designed to enable completely asynchronous real time computations\u00a0done in as unblocked a way as possible with\nminimal overhead .\n\n\nApplications running in the platform are represented by a Directed\nAcyclic Graph (DAG) made up of \u00a0operators and streams. All computations\nare done in memory on arrival of\nthe input data, with an option to save the output to disk (HDFS) in a\nnon-blocking way. The data that flows between operators consists of\natomic data elements. Each data element along with its type definition\n(henceforth called  schema) is\ncalled a tuple.\u00a0An application is a\ndesign of the flow of these tuples to and from\nthe appropriate compute units to enable the computation of the final\ndesired results.\u00a0A message queue (henceforth called\n\u00a0buffer server) manages tuples streaming\nbetween compute units in different processes.This server keeps track of\nall consumers, publishers, partitions, and enables replay. More\ninformation is given in later section.\n\n\nThe streaming application is monitored by a decision making entity\ncalled STRAM (streaming application\nmanager).\u00a0STRAM is designed to be a light weight\ncontroller that has minimal but sufficient interaction with the\napplication. This is done via periodic heartbeats. The\nSTRAM does the initial launch and periodically analyzes the system\nmetrics to decide if any run time action needs to be taken.\n\n\nA fundamental building block for the streaming platform\nis the concept of breaking up a stream into equal finite time slices\ncalled streaming windows. Each window contains the ordered\nset of tuples in that time slice. A typical duration of a window is 500\nms, but can be configured per application (the Yahoo! Finance\napplication configures this value in the  properties.xml\u00a0file to be 1000ms = 1s). Each\nwindow is preceded by a begin_window\u00a0event and is terminated by an\nend_window\u00a0event, and is assigned\na unique window ID. Even though the platform performs computations at\nthe tuple level, bookkeeping is done at the window boundary, making the\ncomputations within a window an atomic event in the platform. \u00a0We can\nthink of each window as an  atomic\nmicro-batch\u00a0of tuples, to be processed together as one\natomic operation (See Figure 2). \u00a0\n\n\nThis atomic batching allows the platform to avoid the very steep\nper tuple bookkeeping cost and instead has a manageable per batch\nbookkeeping cost. This translates to higher throughput, low recovery\ntime, and higher scalability. Later in this document we illustrate how\nthe atomic micro-batch concept allows more efficient optimization\nalgorithms.\n\n\nThe platform also has in-built support for\napplication windows.\u00a0 An application window is part of the\napplication specification, and can be a small or large multiple of the\nstreaming window. \u00a0An example from our Yahoo! Finance test application\nis the moving average, calculated over a sliding application window of 5\nminutes which equates to 300 (= 5 * 60) streaming windows.\n\n\nNote that these two window concepts are distinct. \u00a0A streaming\nwindow is an abstraction of many tuples into a higher atomic event for\neasier management. \u00a0An application window is a group of consecutive\nstreaming windows used for data aggregation (e.g. sum, average, maximum,\nminimum) on a per operator level.\n\n\n\n\nAlongside the platform,\u00a0a set of\npredefined, benchmarked standard library operator templates is provided\nfor ease of use and rapid development of application.\u00a0These\noperators are open sourced to Apache Software Foundation under the\nproject name \u201cMalhar\u201d as part of our efforts to foster community\ninnovation. These operators can be used in a DAG as is, while others\nhave  \nproperties\n\n\n\u00a0\nthat can be set to specify the\ndesired computation. Those interested in details, should refer to\n\nApex Malhar Operator Library\n\n.\n\n\nThe platform is a Hadoop YARN native\napplication. It runs in a Hadoop cluster just like any\nother YARN application (MapReduce etc.) and is designed to seamlessly\nintegrate with rest of Hadoop technology stack. It leverages Hadoop as\nmuch as possible and relies on it as its distributed operating system.\nHadoop dependencies include resource management, compute/memory/network\nallocation, HDFS, security, fault tolerance, monitoring, metrics,\nmulti-tenancy, logging etc. Hadoop classes/concepts are reused as much\nas possible.  The aim is to enable enterprises\nto leverage their existing Hadoop infrastructure for real time streaming\napplications. The platform is designed to scale with big\ndata applications and scale with Hadoop.\n\n\nA streaming application is an asynchronous execution of\ncomputations across distributed nodes. All computations are done in\nparallel on a distributed cluster. The computation model is designed to\ndo as many parallel computations as possible in a non-blocking fashion.\nThe task of monitoring of the entire application is done on (streaming)\nwindow boundaries with a streaming window as an atomic entity. A window\ncompletion is a quantum of work done. There is no assumption that an\noperator can be interrupted at precisely a particular tuple or window.\n\n\nAn operator itself also\ncannot assume or predict the exact time a tuple that it emitted would\nget consumed by downstream operators. The operator processes the tuples\nit gets and simply emits new tuples based on its business logic. The\nonly guarantee it has is that the upstream operators are processing\neither the current or some later window, and the downstream operator is\nprocessing either the current or some earlier window. The completion of\na window (i.e. propagation of the  end_window\u00a0event through an operator) in any\noperator guarantees that all upstream operators have finished processing\nthis window. Thus, the end_window\u00a0event is blocking on an operator\nwith multiple outputs, and is a synchronization point in the DAG. The\n begin_window\u00a0event does not have\nany such restriction, a single begin_window\u00a0event from any upstream operator\ntriggers the operator to start processing tuples.\n\n\nStreaming Application Manager (STRAM)\n\n\nStreaming Application Manager (STRAM) is the Hadoop YARN native\napplication master. STRAM is the first process that is activated upon\napplication launch and orchestrates the streaming application on the\nplatform. STRAM is a lightweight controller process. The\nresponsibilities of STRAM include\n\n\n\n\n\n\nRunning the Application\n\n\n\n\nRead the\u00a0logical plan\u00a0of the application (DAG) submitted by the client\n\n\nValidate the logical plan\n\n\nTranslate the logical plan into a physical plan, where certain operators may  be partitioned (i.e. replicated) to multiple operators for  handling load.\n\n\nRequest resources (Hadoop containers) from Resource Manager,\n    per physical plan\n\n\nBased on acquired resources and application attributes, create\n    an execution plan\u00a0by partitioning the DAG into fragments,\n    each assigned to different containers.\n\n\nExecutes the application by deploying each fragment to\n    its container. Containers then start stream processing and run\n    autonomously, processing one streaming window after another. Each\n    container is represented as an instance of the  StreamingContainer\u00a0class, which updates\n    STRAM via the heartbeat protocol and processes directions received\n    from STRAM.\n\n\n\n\n\n\n\n\nContinually monitoring the application via heartbeats from each StreamingContainer\n\n\n\n\nCollecting Application System Statistics and Logs\n\n\nLogging all application-wide decisions taken\n\n\nProviding system data on the state of the application via a  Web Service.\n\n\n\n\nSupporting \nFault Tolerance\n\n\na.  Detecting a node outage\nb.  Requesting a replacement resource from the Resource Manager\n    and scheduling state restoration for the streaming operators\nc.  Saving state to Zookeeper\n\n\n\n\n\n\nSupporting \nDynamic\n    Partitioning\n:\n\u00a0Periodically\n    evaluating the SLA and modifying the physical plan if required\n    (logical plan does not change).\n\n\n\n\nEnabling \nSecurity\n:\n\u00a0Distributing\n    security tokens for distributed components of the execution engine\n    and securing web service requests.\n\n\nEnabling \nDynamic  modification\n\u00a0\nof\n    DAG: In the future, we intend to allow for user initiated\n    modification of the logical plan to allow for changes to the\n    processing logic and functionality.\n\n\n\n\nAn example of the Yahoo! Finance Quote application scheduled on a\ncluster of 5 Hadoop containers (processes) is shown in Figure 3.\n\n\n\n\nAn example for the translation from a logical plan to a physical\nplan and an execution plan for a subset of the application is shown in\nFigure 4.\n\n\n\n\nHadoop Components\n\n\nIn this section we cover some aspects of Hadoop that your\nstreaming application interacts with. This section is not meant to\neducate the reader on Hadoop, but just get the reader acquainted with\nthe terms. We strongly advise readers to learn Hadoop from other\nsources.\n\n\nA streaming application runs as a native Hadoop 2.2 application.\nHadoop 2.2 does not differentiate between a map-reduce job and other\napplications, and hence as far as Hadoop is concerned, the streaming\napplication is just another job. This means that your application\nleverages all the bells and whistles Hadoop provides and is fully\nsupported within Hadoop technology stack. The platform is responsible\nfor properly integrating itself with the relevant components of Hadoop\nthat exist today and those that may emerge in the future\n\n\nAll investments that leverage multi-tenancy (for example quotas\nand queues), security (for example kerberos), data flow integration (for\nexample copying data in-out of HDFS), monitoring, metrics collections,\netc. will require no changes when streaming applications run on\nHadoop.\n\n\nYARN\n\n\nYARN\nis\nthe core library of Hadoop 2.2 that is tasked with resource management\nand works as a distributed application framework. In this section we\nwill walk through Yarn's components. In Hadoop 2.2, the old jobTracker\nhas been replaced by a combination of ResourceManager (RM) and\nApplicationMaster (AM).\n\n\nResource Manager (RM)\n\n\nResourceManager\n(RM)\nmanages all the distributed resources. It allocates and arbitrates all\nthe slots and the resources (cpu, memory, network) of these slots. It\nworks with per-node NodeManagers (NMs) and per-application\nApplicationMasters (AMs). Currently memory usage is monitored by RM; in\nupcoming releases it will have CPU as well as network management. RM is\nshared by map-reduce and streaming applications. Running streaming\napplications requires no changes in the RM.\n\n\nApplication Master (AM)\n\n\nThe AM is the watchdog or monitoring process for your application\nand has the responsibility of negotiating resources with RM and\ninteracting with NodeManagers to get the allocated containers started.\nThe AM is the starting point of your application and is considered user\ncode (not system Hadoop code). The AM itself runs in one container. All\nresource management within the application are managed by the AM. This\nis a critical feature for Hadoop 2.2 where tasks done by jobTracker in\nHadoop 1.0 have been distributed allowing Hadoop 2.2 to scale much\nbeyond Hadoop 1.0. STRAM is a native YARN ApplicationManager.\n\n\nNode Managers (NM)\n\n\nThere is one \nNodeManager\n(NM)\nper node in the cluster. All the containers (i.e. processes) on that\nnode are monitored by the NM. It takes instructions from RM and manages\nresources of that node as per RM instructions. NMs interactions are same\nfor map-reduce and for streaming applications. Running streaming\napplications requires no changes in the NM.\n\n\nRPC Protocol\n\n\nCommunication among RM, AM, and NM is done via the Hadoop RPC\nprotocol. Streaming applications use the same protocol to send their\ndata. No changes are needed in RPC support provided by Hadoop to enable\ncommunication done by components of your application.\n\n\nHDFS\n\n\nHadoop includes a highly fault tolerant, high throughput\ndistributed file system (\nHDFS\n).\nIt runs on commodity hardware, and your streaming application will, by\ndefault, use it. There is no difference between files created by a\nstreaming application and those created by map-reduce.\n\n\nDeveloping An Application\n\n\nIn this chapter we describe the methodology to develop an\napplication using the Realtime Streaming Platform. The platform was\ndesigned to make it easy to build and launch sophisticated streaming\napplications with the developer having to deal only with the\napplication/business logic. The platform deals with details of where to\nrun what operators on which servers and how to correctly route streams\nof data among them.\n\n\nDevelopment Process\n\n\nWhile the platform does not mandate a specific methodology or set\nof development tools, we have recommendations to maximize productivity\nfor the different phases of application development.\n\n\nDesign\n\n\n\n\nIdentify common, reusable operators. Use a library\n    if possible.\n\n\nIdentify scalability and performance requirements before\n    designing the DAG.\n\n\nLeverage attributes that the platform supports for scalability\n    and performance.\n\n\nUse operators that are benchmarked and tested so that later\n    surprises are minimized. If you have glue code, create appropriate\n    unit tests for it.\n\n\nUse THREAD_LOCAL locality for high throughput streams. If all\n    the operators on that stream cannot fit in one container,\n    try\u00a0NODE_LOCAL\u00a0locality. Both THREAD_LOCAL and\n    NODE_LOCAL streams avoid the Network Interface Card (NIC)\n    completly. The former uses intra-process communication to also avoid\n    serialization-deserialization overhead.\n\n\nThe overall throughput and latencies are are not necessarily\n    correlated to the number of operators in a simple way -- the\n    relationship is more nuanced. A lot depends on how much work\n    individual operators are doing, how many are able to operate in\n    parallel, and how much data is flowing through the arcs of the DAG.\n    It is, at times, better to break a computation down into its\n    constituent simple parts and then stitch them together via streams\n    to better utilize the compute resources of the cluster. Decide on a\n    per application basis the fine line between complexity of each\n    operator vs too many streams. Doing multiple computations in one\n    operator does save network I/O, while operators that are too complex\n    are hard to maintain.\n\n\nDo not use operators that depend on the order of two streams\n    as far as possible. In such cases behavior is not idempotent.\n\n\nPersist key information to HDFS if possible; it may be useful\n    for debugging later.\n\n\nDecide on an appropriate fault tolerance mechanism. If some\n    data loss is acceptable, use the at-most-once mechanism as it has\n    fastest recovery.\n\n\n\n\nCreating New Project\n\n\nPlease refer to the \nApex Application Packages\n\u00a0for\nthe basic steps for creating a new project.\n\n\nWriting the application code\n\n\nPreferably use an IDE (Eclipse, Netbeans etc.) that allows you to\nmanage dependencies and assists with the Java coding. Specific benefits\ninclude ease of managing operator library jar files, individual operator\nclasses, ports and properties. It will also highlight and assist to\nrectify issues such as type mismatches when adding streams while\ntyping.\n\n\nTesting\n\n\nWrite test cases with JUnit or similar test framework so that code\nis tested as it is written. For such testing, the DAG can run in local\nmode within the IDE. Doing this may involve writing mock input or output\noperators for the integration points with external systems. For example,\ninstead of reading from a live data stream, the application in test mode\ncan read from and write to files. This can be done with a single\napplication DAG by instrumenting a test mode using settings in the\nconfiguration that is passed to the application factory\ninterface.\n\n\nGood test coverage will not only eliminate basic validation errors\nsuch as missing port connections or property constraint violations, but\nalso validate the correct processing of the data. The same tests can be\nre-run whenever the application or its dependencies change (operator\nlibraries, version of the platform etc.)\n\n\nRunning an application\n\n\nThe platform provides a commandline tool called dtcli\u00a0for managing applications (launching,\nkilling, viewing, etc.). This tool was already discussed above briefly\nin the section entitled Running the Test Application. It will introspect\nthe jar file specified with the launch command for applications (classes\nthat implement ApplicationFactory) or property files that define\napplications. It will also deploy the dependency jar files from the\napplication package to the cluster.\n\n\nDtcli can run the application in local mode (i.e. outside a\ncluster). It is recommended to first run the application in local mode\nin the development environment before launching on the Hadoop cluster.\nThis way some of the external system integration and correct\nfunctionality of the application can be verified in an easier to debug\nenvironment before testing distributed mode.\n\n\nFor more details on CLI please refer to the \ndtCli Guide\n.\n\n\nApplication API\n\n\nThis section introduces the API to write a streaming application.\nThe work involves connecting operators via streams to form the logical\nDAG. The steps are\n\n\n\n\n\n\nInstantiate an application (DAG)\n\n\n\n\n\n\n(Optional) Set Attributes\n\n\n\n\nAssign application name\n\n\nSet any other attributes as per application requirements\n\n\n\n\n\n\n\n\nCreate/re-use and instantiate operators\n\n\n\n\nAssign operator name that is unique within the  application\n\n\nDeclare schema upfront for each operator (and thereby its  \nports\n)\n\n\n(Optional) Set \nproperties\n\u00a0\n and \nattributes\n\u00a0\n on the dag as per specification\n\n\nConnect ports of operators via streams\n\n\nEach stream connects one output port of an operator to one or  more input ports of other operators.\n\n\n(Optional) Set attributes on the streams\n\n\n\n\n\n\n\n\n\n\n\n\nTest the application.\n\n\n\n\n\n\nThere are two methods to create an application, namely Java, and\nProperties file. Java API is for applications being developed by humans,\nand properties file (Hadoop like) is more suited for DAGs generated by\ntools.\n\n\nJava API\n\n\nThe Java API is the most common way to create a streaming\napplication. It is meant for application developers who prefer to\nleverage the features of Java, and the ease of use and enhanced\nproductivity provided by IDEs like NetBeans or Eclipse. Using Java to\nspecify the application provides extra validation abilities of Java\ncompiler, such as compile time checks for type safety at the time of\nwriting the code. Later in this chapter you can read more about\nvalidation support in the platform.\n\n\nThe developer specifies the streaming application by implementing\nthe ApplicationFactory interface, which is how platform tools (CLI etc.)\nrecognize and instantiate applications. Here we show how to create a\nYahoo! Finance application that streams the last trade price of a ticker\nand computes the high and low price in every 1 min window. Run above\n test application\u00a0to execute the\nDAG in local mode within the IDE.\n\n\nLet us revisit how the Yahoo! Finance test application constructs the DAG:\n\n\npublic class Application implements StreamingApplication\n{\n\n  ...\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().attr(DAG.STRAM_WINDOW_SIZE_MILLIS).set(streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator(\nStockTickInput\n, dag);\n    SumKeyVal\nString, Long\n dailyVolume = getDailyVolumeOperator(\nDailyVolume\n, dag);\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n quoteOperator = getQuoteOperator(\nQuote\n, dag);\n\n    RangeKeyVal\nString, Double\n highlow = getHighLowOperator(\nHighLow\n, dag, appWindowCountMinute);\n    SumKeyVal\nString, Long\n minuteVolume = getMinuteVolumeOperator(\nMinuteVolume\n, dag, appWindowCountMinute);\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n chartOperator = getChartOperator(\nChart\n, dag);\n\n    SimpleMovingAverage\nString, Double\n priceSMA = getPriceSimpleMovingAverageOperator(\nPriceSMA\n, dag, appWindowCountSMA);\n\n    dag.addStream(\nprice\n, tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream(\nvol\n, tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream(\ntime\n, tick.time, quoteOperator.in3);\n    dag.addStream(\ndaily_vol\n, dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream(\nquote_data\n, quoteOperator.out, getConsole(\nquoteConsole\n, dag, \nQUOTE\n));\n\n    dag.addStream(\nhigh_low\n, highlow.range, chartOperator.in1);\n    dag.addStream(\nvol_1min\n, minuteVolume.sum, chartOperator.in2);\n    dag.addStream(\nchart_data\n, chartOperator.out, getConsole(\nchartConsole\n, dag, \nCHART\n));\n\n    dag.addStream(\nsma_price\n, priceSMA.doubleSMA, getConsole(\npriceSMAConsole\n, dag, \nPrice SMA\n));\n\n    return dag;\n  }\n}\n\n\n\n\nProperty File API\n\n\nThe platform also supports specification of a DAG via a property\nfile. The aim here to make it easy for tools to create and run an\napplication. This method of specification does not have the Java\ncompiler support of compile time check, but since these applications\nwould be created by software, they should be correct by construction.\nThe syntax is derived from Hadoop properties and should be easy for\nfolks who are used to creating software that integrated with\nHadoop.\n\n\nCreate an application (DAG): myApplication.properties\n\n\n# input operator that reads from a file\ndt.operator.inputOp.classname=com.acme.SampleInputOperator\ndt.operator.inputOp.fileName=somefile.txt\n\n# output operator that writes to the console\ndt.operator.outputOp.classname=com.acme.ConsoleOutputOperator\n\n# stream connecting both operators\ndt.stream.inputStream.source=inputOp.outputPort\ndt.stream.inputStream.sinks=outputOp.inputPort\n\n\n\n\nAbove snippet is intended to convey the basic idea of specifying\nthe DAG without using Java. Operators would come from a predefined\nlibrary and referenced in the specification by class name and port names\n(obtained from the library providers documentation or runtime\nintrospection by tools). For those interested in details, see later\nsections and refer to the  Operation and\nInstallation Guide\u00a0mentioned above.\n\n\nAttributes\n\n\nAttributes impact the runtime behavior of the application. They do\nnot impact the functionality. An example of an attribute is application\nname. Setting it changes the application name. Another example is\nstreaming window size. Setting it changes the streaming window size from\nthe default value to the specified value. Users cannot add new\nattributes, they can only choose from the ones that come packaged and\npre-supported by the platform. Details of attributes are covered in the\n Operation and Installation\nGuide.\n\n\nOperators\n\n\nOperators\u00a0are basic compute units.\nOperators process each incoming tuple and emit zero or more tuples on\noutput ports as per the business logic. The data flow, connectivity,\nfault tolerance (node outage), etc. is taken care of by the platform. As\nan operator developer, all that is needed is to figure out what to do\nwith the incoming tuple and when (and which output port) to send out a\nparticular output tuple. Correctly designed operators will most likely\nget reused. Operator design needs care and foresight. For details, refer\nto the  \nOperator Developer Guide\n. As an application developer you need to connect operators\nin a way that it implements your business logic. You may also require\noperator customization for functionality and use attributes for\nperformance/scalability etc.\n\n\nAll operators process tuples asynchronously in a distributed\ncluster. An operator cannot assume or predict the exact time a tuple\nthat it emitted will get consumed by a downstream operator. An operator\nalso cannot predict the exact time when a tuple arrives from an upstream\noperator. The only guarantee is that the upstream operators are\nprocessing the current or a future window, i.e. the windowId of upstream\noperator is equals or exceeds its own windowId. Conversely the windowId\nof a downstream operator is less than or equals its own windowId. The\nend of a window operation, i.e. the API call to endWindow on an operator\nrequires that all upstream operators have finished processing this\nwindow. This means that completion of processing a window propagates in\na blocking fashion through an operator. Later sections provides more\ndetails on streams and data flow of tuples.\n\n\nEach operator has a unique name within the DAG as provided by the\nuser. This is the name of the operator in the logical plan. The name of\nthe operator in the physical plan is an integer assigned to it by STRAM.\nThese integers are use the sequence from 1 to N, where N is total number\nof physically unique operators in the DAG. \u00a0Following the same rule,\neach partitioned instance of a logical operator has its own integer as\nan id. This id along with the Hadoop container name uniquely identifies\nthe operator in the execution plan of the DAG. The logical names and the\nphysical names are required for web service support. Operators can be\naccessed via both names. These same names are used while interacting\nwith  dtcli\u00a0to access an operator.\nIdeally these names should be self-descriptive. For example in Figure 1,\nthe node named \u201cDaily volume\u201d has a physical identifier of 2.\n\n\nOperator Interface\n\n\nOperator interface in a DAG consists of \nports\n,\n\u00a0\nproperties\n,\n\u00a0and\n \nattributes\n\n\n.\n\u00a0Operators interact with other\ncomponents of the DAG via ports. Functional behavior of the operators\ncan be customized via parameters. Run time performance and physical\ninstantiation is controlled by attributes. Ports and parameters are\nfields (variables) of the Operator class/object, while attributes are\nmeta information that is attached to the operator object via an\nAttributeMap. An operator must have at least one port. Properties are\noptional. Attributes are provided by the platform and always have a\ndefault value that enables normal functioning of operators.\n\n\nPorts\n\n\nPorts are connection points by which an operator receives and\nemits tuples. These should be transient objects instantiated in the\noperator object, that implement particular interfaces. Ports should be\ntransient as they contain no state. They have a pre-defined schema and\ncan only be connected to other ports with the same schema. An input port\nneeds to implement the interface  Operator.InputPort\u00a0and\ninterface Sink. A default\nimplementation of these is provided by the abstract class DefaultInputPort. An output port needs to\nimplement the interface  Operator.OutputPort. A default implementation\nof this is provided by the concrete class DefaultOutputPort. These two are a quick way to\nimplement the above interfaces, but operator developers have the option\nof providing their own implementations.\n\n\nHere are examples of an input and an output port from the operator\nSum.\n\n\n@InputPortFieldAnnotation(name = \ndata\n)\npublic final transient DefaultInputPort\nV\n data = new DefaultInputPort\nV\n() {\n  @Override\n  public void process(V tuple)\n  {\n    ...\n  }\n}\n@OutputPortFieldAnnotation(optional=true)\npublic final transient DefaultOutputPort\nV\n sum = new DefaultOutputPort\nV\n(){ \u2026 };\n\n\n\n\nThe process call is in the Sink interface. An emit on an output\nport is done via emit(tuple) call. For the above example it would be\nsum.emit(t), where the type of t is the generic parameter V.\n\n\nThere is no limit on how many ports an operator can have. However\nany operator must have at least one port. An operator with only one port\nis called an Input Adapter if it has no input port and an Output Adapter\nif it has no output port. These are special operators needed to get/read\ndata from outside system/source into the application, or push/write data\ninto an outside system/sink. These could be in Hadoop or outside of\nHadoop. These two operators are in essence gateways for the streaming\napplication to communicate with systems outside the application.\n\n\nPort connectivity can be validated during compile time by adding\nPortFieldAnnotations shown above. By default all ports have to be\nconnected, to allow a port to go unconnected, you need to add\n\u201coptional=true\u201d to the annotation.\n\n\nAttributes can be specified for ports that affect the runtime\nbehavior. An example of an attribute is parallel partition that specifes\na parallel computation flow per partition. It is described in detail in\nthe \nParallel\nPartitions\n\u00a0\nsection.\nAnother example is queue capacity that specifies the buffer size for the\nport. Details of attributes are covered in  Operation and Installation Guide.\n\n\nProperties\n\n\nProperties are the abstractions by which functional behavior of an\noperator can be customized. They should be non-transient objects\ninstantiated in the operator object. They need to be non-transient since\nthey are part of the operator state and re-construction of the operator\nobject from its checkpointed state must restore the operator to the\ndesired state. Properties are optional, i.e. an operator may or may not\nhave properties; they are part of user code and their values are not\ninterpreted by the platform in any way.\n\n\nAll non-serializable objects should be declared transient.\nExamples include sockets, session information, etc. These objects should\nbe initialized during setup call, which is called every time the\noperator is initialized.\n\n\nAttributes\n\n\nAttributes are values assigned to the operators that impact\nrun-time. This includes things like the number of partitions, at most\nonce or at least once or exactly once recovery modes, etc. Attributes do\nnot impact functionality of the operator. Users can change certain\nattributes in runtime. Users cannot add attributes to operators; they\nare pre-defined by the platform. They are interpreted by the platform\nand thus cannot be defined in user created code (like properties).\nDetails of attributes are covered in  \nConfiguration Guide\n.\n\n\nOperator State\n\n\nThe state of an operator is defined as the data that it transfers\nfrom one window to a future window. Since the computing model of the\nplatform is to treat windows like micro-batches, the operator state can\nbe \ncheckpointed\n\u00a0\nevery\nNth window, or every T units of time, where T is significantly greater\nthan the streaming window. \u00a0When an operator is checkpointed, the entire\nobject is written to HDFS. \u00a0The larger the amount of state in an\noperator, the longer it takes to recover from a failure. A stateless\noperator can recover much quicker than a stateful one. The needed\nwindows are preserved by the upstream buffer server and are used to\nrecompute the lost windows, and also rebuild the buffer server in the\ncurrent container.\n\n\nThe distinction between Stateless and Stateful is based solely on\nthe need to transfer data in the operator from one window to the next.\nThe state of an operator is independent of the number of ports.\n\n\nStateless\n\n\nA Stateless operator is defined as one where no data is needed to\nbe kept at the end of every window. This means that all the computations\nof a window can be derived from all the tuples the operator receives\nwithin that window. This guarantees that the output of any window can be\nreconstructed by simply replaying the tuples that arrived in that\nwindow. Stateless operators are more efficient in terms of fault\ntolerance, and cost to achieve SLA.\n\n\nStateful\n\n\nA Stateful operator is defined as one where data is needed to be\nstored at the end of a window for computations occurring in later\nwindow; a common example is the computation of a sum of values in the\ninput tuples.\n\n\nOperator API\n\n\nThe Operator API consists of methods that operator developers may\nneed to override. In this section we will discuss the Operator APIs from\nthe point of view of an application developer. Knowledge of how an\noperator works internally is critical for writing an application. Those\ninterested in the details should refer to  Malhar Operator Developer Guide.\n\n\nThe APIs are available in three modes, namely Single Streaming\nWindow, Sliding Application Window, and Aggregate Application Window.\nThese are not mutually exclusive, i.e. an operator can use single\nstreaming window as well as sliding application window. A physical\ninstance of an operator is always processing tuples from a single\nwindow. The processing of tuples is guaranteed to be sequential, no\nmatter which input port the tuples arrive on.\n\n\nIn the later part of this section we will evaluate three common\nuses of streaming windows by applications. They have different\ncharacteristics and implications on optimization and recovery mechanisms\n(i.e. algorithm used to recover a node after outage) as discussed later\nin the section.\n\n\nStreaming Window\n\n\nStreaming window is atomic micro-batch computation period. The API\nmethods relating to a streaming window are as follows\n\n\npublic void process(\ntuple_type\n tuple) // Called on the input port on which the tuple arrives\npublic void beginWindow(long windowId) // Called at the start of the window as soon as the first begin_window tuple arrives\npublic void endWindow() // Called at the end of the window after end_window tuples arrive on all input ports\npublic void setup(OperatorContext context) // Called once during initialization of the operator\npublic void teardown() // Called once when the operator is being shutdown\n\n\n\n\nA tuple can be emitted in any of the three streaming run-time\ncalls, namely beginWindow, process, and endWindow but not in setup or\nteardown.\n\n\nAggregate Application Window\n\n\nAn operator with an aggregate window is stateful within the\napplication window timeframe and possibly stateless at the end of that\napplication window. An size of an aggregate application window is an\noperator attribute and is defined as a multiple of the streaming window\nsize. The platform recognizes this attribute and optimizes the operator.\nThe beginWindow, and endWindow calls are not invoked for those streaming\nwindows that do not align with the application window. For example in\ncase of streaming window of 0.5 second and application window of 5\nminute, an application window spans 600 streaming windows (5*60*2 =\n600). At the start of the sequence of these 600 atomic streaming\nwindows, a beginWindow gets invoked, and at the end of these 600\nstreaming windows an endWindow gets invoked. All the intermediate\nstreaming windows do not invoke beginWindow or endWindow. Bookkeeping,\nnode recovery, stats, UI, etc. continue to work off streaming windows.\nFor example if operators are being checkpointed say on an average every\n30th window, then the above application window would have about 20\ncheckpoints.\n\n\nSliding Application Window\n\n\nA sliding window is computations that requires previous N\nstreaming windows. After each streaming window the Nth past window is\ndropped and the new window is added to the computation. An operator with\nsliding window is a stateful operator at end of any window. The sliding\nwindow period is an attribute and is a multiple of streaming window. The\nplatform recognizes this attribute and leverages it during bookkeeping.\nA sliding aggregate window with tolerance to data loss does not have a\nvery high bookkeeping cost. The cost of all three recovery mechanisms,\n at most once\u00a0(data loss tolerant),\nat least once\u00a0(data loss\nintolerant), and exactly once\u00a0(data\nloss intolerant and no extra computations) is same as recovery\nmechanisms based on streaming window. STRAM is not able to leverage this\noperator for any extra optimization.\n\n\nSingle vs Multi-Input Operator\n\n\nA single-input operator by definition has a single upstream\noperator, since there can only be one writing port for a stream. \u00a0If an\noperator has a single upstream operator, then the beginWindow on the\nupstream also blocks the beginWindow of the single-input operator. For\nan operator to start processing any window at least one upstream\noperator has to start processing that window. A multi-input operator\nreads from more than one upstream ports. Such an operator would start\nprocessing as soon as the first begin_window event arrives. However the\nwindow would not close (i.e. invoke endWindow) till all ports receive\nend_window events for that windowId. Thus the end of a window is a\nblocking event. As we saw earlier, a multi-input operator is also the\npoint in the DAG where windows of all upstream operators are\nsynchronized. The windows (atomic micro-batches) from a faster (or just\nahead in processing) upstream operators are queued up till the slower\nupstream operator catches up. STRAM monitors such bottlenecks and takes\ncorrective actions. The platform ensures minimal delay, i.e processing\nstarts as long as at least one upstream operator has started\nprocessing.\n\n\nRecovery Mechanisms\n\n\nApplication developers can set any of the recovery mechanisms\nbelow to deal with node outage. In general, the cost of recovery depends\non the state of the operator, while data integrity is dependant on the\napplication. The mechanisms are per window as the platform treats\nwindows as atomic compute units. Three recovery mechanisms are\nsupported, namely\n\n\n\n\nAt-least-once: All atomic batches are processed at least once.\n    No data loss occurs.\n\n\nAt-most-once: All atomic batches are processed at most once.\n    Data loss is possible; this is the most efficient setting.\n\n\nExactly-once: All atomic batches are processed exactly once.\n    No data loss occurs; this is the least efficient setting since\n    additional work is needed to ensure proper semantics.\n\n\n\n\nAt-least-once is the default. During a recovery event, the\noperator connects to the upstream buffer server and asks for windows to\nbe replayed. At-least-once and exactly-once mechanisms start from its\ncheckpointed state. At-most-once starts from the next begin-window\nevent.\n\n\nRecovery mechanisms can be specified per Operator while writing\nthe application as shown below.\n\n\nOperator o = dag.addOperator(\u201coperator\u201d, \u2026);\ndag.setAttribute(o,  OperatorContext.PROCESSING_MODE,  ProcessingMode.AT_MOST_ONCE);\n\n\n\n\nAlso note that once an operator is attributed to AT_MOST_ONCE,\nall the operators downstream to it have to be AT_MOST_ONCE. The client\nwill give appropriate warnings or errors if that\u2019s not the case.\n\n\nDetails are explained in the chapter on Fault Tolerance\nbelow\n.\n\n\nStreams\n\n\nA stream\u00a0is a connector\n(edge) abstraction, and is a fundamental building block of the platform.\nA stream consists of tuples that flow from one port (called the\noutput\u00a0port) to one or more ports\non other operators (called  input\u00a0ports) another -- so note a potentially\nconfusing aspect of this terminology: tuples enter a stream through its\noutput port and leave via one or more input ports. A stream has the\nfollowing characteristics\n\n\n\n\nTuples are always delivered in the same order in which they\n    were emitted.\n\n\nConsists of a sequence of windows one after another. Each\n    window being a collection of in-order tuples.\n\n\nA stream that connects two containers passes through a\n    buffer server.\n\n\nAll streams can be persisted (by default in HDFS).\n\n\nExactly one output port writes to the stream.\n\n\nCan be read by one or more input ports.\n\n\nConnects operators within an application, not outside\n    an application.\n\n\nHas an unique name within an application.\n\n\nHas attributes which act as hints to STRAM.\n\n\n\n\nStreams have four modes, namely in-line, in-node, in-rack,\n    and other. Modes may be overruled (for example due to lack\n    of containers). They are defined as follows:\n\n\n\n\nTHREAD_LOCAL: In the same thread, uses thread\n    stack (intra-thread). This mode can only be used for a downstream\n    operator which has only one input port connected; also called\n    in-line.\n\n\nCONTAINER_LOCAL: In the same container (intra-process); also\n    called in-container.\n\n\nNODE_LOCAL: In the same Hadoop node (inter processes, skips\n    NIC); also called in-node.\n\n\nRACK_LOCAL: On nodes in the same rack; also called\n    in-rack.\n\n\nunspecified: No guarantee. Could be anywhere within the\n    cluster\n\n\n\n\n\n\n\n\nAn example of a stream declaration is given below\n\n\nDAG dag = new DAG();\n \u2026\ndag.addStream(\nviews\n, viewAggregate.sum, cost.data).setLocality(CONTAINER_LOCAL); // A container local  stream\ndag.addStream(\u201cclicks\u201d, clickAggregate.sum, rev.data); // An example of unspecified locality\n\n\n\n\nThe platform guarantees in-order delivery of tuples in a stream.\nSTRAM views each stream as collection of ordered windows. Since no tuple\ncan exist outside a window, a replay of a stream consists of replay of a\nset of windows. When multiple input ports read the same stream, the\nexecution plan of a stream ensures that each input port is logically not\nblocked by the reading of another input port. The schema of a stream is\nsame as the schema of the tuple.\n\n\nIn a stream all tuples emitted by an operator in a window belong\nto that window. A replay of this window would consists of an in-order\nreplay of all the tuples. Thus the tuple order within a stream is\nguaranteed. However since an operator may receive multiple streams (for\nexample an operator with two input ports), the order of arrival of two\ntuples belonging to different streams is not guaranteed. In general in\nan asynchronous distributed architecture this is expected. Thus the\noperator (specially one with multiple input ports) should not depend on\nthe tuple order from two streams. One way to cope with this\nindeterminate order, if necessary, is to wait to get all the tuples of a\nwindow and emit results in endWindow call. All operator templates\nprovided as part of  \nstandard operator template\nlibrary\n \n\u00a0\nfollow\nthese principles.\n\n\nA logical stream gets partitioned into physical streams each\nconnecting the partition to the upstream operator. If two different\nattributes are needed on the same stream, it should be split using\nStreamDuplicator\u00a0operator.\n\n\nModes of the streams are critical for performance. An in-line\nstream is the most optimal as it simply delivers the tuple as-is without\nserialization-deserialization. Streams should be marked\ncontainer_local, specially in case where there is a large tuple volume\nbetween two operators which then on drops significantly. Since the\nsetLocality call merely provides a hint, STRAM may ignore it. An In-node\nstream is not as efficient as an in-line one, but it is clearly better\nthan going off-node since it still avoids the potential bottleneck of\nthe network card.\n\n\nTHREAD_LOCAL and CONTAINER_LOCAL streams do not use a buffer\nserver as this stream is in a single process. The other two do.\n\n\nValidating an Application\n\n\nThe platform provides various ways of validating the application\nspecification and data input. An understanding of these checks is very\nimportant for an application developer since it affects productivity.\nValidation of an application is done in three phases, namely\n\n\n\n\nCompile Time: Caught during application development, and is\n    most cost effective. These checks are mainly done on declarative\n    objects and leverages the Java compiler. An example is checking that\n    the schemas specified on all ports of a stream are\n    mutually compatible.\n\n\nInitialization Time: When the application is being\n    initialized, before submitting to Hadoop. These checks are related\n    to configuration/context of an application, and are done by the\n    logical DAG builder implementation. An example is the checking that\n    all non-optional ports are connected to other ports.\n\n\nRun Time: Validations done when the application is running.\n    This is the costliest of all checks. These are checks that can only\n    be done at runtime as they involve data. For example divide by 0\n    check as part of business logic.\n\n\n\n\nCompile Time\n\n\nCompile time validations apply when an application is specified in\nJava code and include all checks that can be done by Java compiler in\nthe development environment (including IDEs like NetBeans or Eclipse).\nExamples include\n\n\n\n\nSchema Validation: The tuples on ports are POJO (plain old\n    java objects) and compiler checks to ensure that all the ports on a\n    stream have the same schema.\n\n\nStream Check: Single Output Port and at least one Input port\n    per stream. A stream can only have one output port writer. This is\n    part of the addStream api. This\n    check ensures that developers only connect one output port to\n    a stream. The same signature also ensures that there is at least one\n    input port for a stream\n\n\nNaming: Compile time checks ensures that applications\n    components operators, streams are named\n\n\n\n\nInitialization/Instantiation Time\n\n\nInitialization time validations include various checks that are\ndone post compile, and before the application starts running in a\ncluster (or local mode). These are mainly configuration/contextual in\nnature. These checks are as critical to proper functionality of the\napplication as the compile time validations.\n\n\nExamples include\n\n\n\n\n\n\nJavaBeans Validation\n:\n    Examples include\n\n\n\n\n@Max(): Value must be less than or equal to the number\n\n\n@Min(): Value must be greater than or equal to the\n    number\n\n\n@NotNull: The value of the field or property must not be\n    null\n\n\n@Pattern(regexp = \u201c....\u201d): Value must match the regular\n    expression\n\n\nInput port connectivity: By default, every non-optional input\n    port must be connected. A port can be declared optional by using an\n    annotation: \u00a0 \u00a0 @InputPortFieldAnnotation(name = \"...\", optional\n    = true)\n\n\nOutput Port Connectivity: Similar. The annotation here is: \u00a0 \u00a0\n    @OutputPortFieldAnnotation(name = \"...\", optional = true)\n\n\n\n\n\n\n\n\nUnique names in application scope: Operators, streams, must have\n    unique names.\n\n\n\n\nCycles in the dag: DAG cannot have a cycle.\n\n\nUnique names in operator scope: Ports, properties, annotations\n    must have unique names.\n\n\nOne stream per port: A port can connect to only one stream.\n    This check applies to input as well as output ports even though an\n    output port can technically write to two streams. If you must have\n    two streams originating from a single output port, use \u00a0a\u00a0streamDuplicator operator.\n\n\nApplication Window Period: Has to be an integral multiple the\n    streaming window period.\n\n\n\n\nRun Time\n\n\nRun time checks are those that are done when the application is\nrunning. The real-time streaming platform provides rich run time error\nhandling mechanisms. The checks are exclusively done by the application\nbusiness logic, but the platform allows applications to count and audit\nthese. Some of these features are in the process of development (backend\nand UI) and this section will be updated as they are developed. Upon\ncompletion examples will be added to  \ndemos\n \nt\no\nillustrate these.\n\n\nError ports are output ports with error annotations. Since they\nare normal ports, they can be monitored and tuples counted, persisted\nand counts shown in the UI.\n\n\n\n\nMulti-Tenancy and Security\n\n\nHadoop is a multi-tenant distributed operating system. Security is\nan intrinsic element of multi-tenancy as without it a cluster cannot be\nreasonably be shared among enterprise applications. Streaming\napplications follow all multi-tenancy security models used in Hadoop as\nthey are native Hadoop applications. For details refer to the\n\nOperation and Installation\nGuide\n\n.\n\n\nSecurity\n\n\nThe platform includes Kerberos support. Both access points, namely\nSTRAM and Bufferserver are secure. STRAM passes the token over to\nStreamingContainer, which then gives it to the Bufferserver. The most\nimportant aspect for an application developer is to note that STRAM is\nthe single point of access to ensure security measures are taken by all\ncomponents of the platform.\n\n\nResource Limits\n\n\nHadoop enforces quotas on resources. This includes hard-disk (name\nspace and total disk quota) as well as priority queues for schedulers.\nThe platform uses Hadoop resource limits to manage a streaming\napplication. In addition network I/O quotas can be enforced. An operator\ncan be dynamically partitioned if it reaches its resource limits; these\nlimits may be expressed in terms of throughput, latency, or just\naggregate resource utilization of a container.\n\n\n\n\nScalability and Partitioning\n\n\nScalability is a foundational element of this platform and is a\nbuilding block for an eco-system where big-data meets real-time.\nEnterprises need to continually meet SLA as data grows. Without the\nability to scale as load grows, or new applications with higher loads\ncome to fruition, enterprise grade SLA cannot be met. A big issue with\nthe streaming application space is that, it is not just about high load,\nbut also the fluctuations in it. There is no way to guarantee future\nload requirements and there is a big difference between high and low\nload within a day for the same feed. Traditional streaming platforms\nsolve these two cases by simply throwing more hardware at the\nproblem.\n\n\nDaily spikes are managed by ensuring enough hardware for peak\nload, which then idles during low load, and future needs are handled by\na very costly re-architecture, or investing heavily in building a\nscalable distributed operating system. Another salient and often\noverlooked cost is the need to manage SLA -- let\u2019s call it  buffer capacity. Since this means computing the\npeak load within required time, that translates to allocating enough\nresources over and above peak load as daily peaks fluctuate. For example\nan average peak load of 100 resource units (cpu and/or memory and/or\nnetwork) may mean allocating about 200 resource units to be safe. A\ndistributed cluster that cannot dynamically scale up and down, in effect\npays buffer capacity per application. Another big aspect of streaming\napplications is that the load is not just ingestion rate, more often\nthan not, the internal operators produce lot more events than the\ningestion rate. For example a dimensional data (with, say  d\u00a0dimensions) computation needs 2*d -1\u00a0computations per ingested event. A lot\nof applications have over 10 dimensions, i.e over 1000 computations per\nincoming event and these need to be distributed across the cluster,\nthereby causing an explosion in the throughput (events/sec) that needs\nto be managed.\n\n\nThe platform is designed to handle such cases at a very low cost.\nThe platform scales linearly with Hadoop. If applications need more\nresources, the enterprise can simply add more commodity nodes to Hadoop\nwithout any downtime, and the Hadoop native platform will take care of\nthe rest. If some nodes go bad, these can be removed without downtime.\nThe daily peaks and valleys in the load are managed by the platform by\ndynamically scaling at the peak and then giving the resources back to\nHadoop during low load. This means that a properly designed Hadoop\ncluster does several things for enterprises: (a) reduces the cost of\nhardware due to use of commodity hardware (b) shares buffer capacity\nacross all applications as peaks of all applications may not align and\n(c) raises the average CPU usage on a 24x7 basis. As a general design\nthis is similar to scale that a map-reduce application can deliver. In\nthe following sections of this chapter we will see how this is\ndone.\n\n\nPartitioning\n\n\nIf all tuples sent through the stream(s) that are connected to the\ninput port(s) of an operator in the DAG are received by a single\nphysical instance of that operator, that operator can become a\nperformance bottleneck. This leads to scalability issues when\nthroughput, memory, or CPU needs exceed the processing capacity of that\nsingle instance.\n\n\nTo address the problem, the platform offers the capability to\npartition the inflow of data so that it is divided across multiple\nphysical instances of a logical operator in the DAG. There are two\nfunctional ways to partition\n\n\n\n\nLoad balance: Incoming load is simply partitioned\n    into stream(s) that go to separate instances of physical operators\n    and scalability is achieved via adding more physical operators. Each\n    tuple is sent to physical operator (partition) based on a\n    round-robin or other similar algorithm. This scheme scales linearly.\n    A lot of key based computations can load balance in the platform due\n    to the ability to insert  Unifiers. For many computations, the\n    endWindow and Unifier setup is similar to the combiner and reducer\n    mechanism in a Map-Reduce computation.\n\n\nSticky Key: The key assertion is that distribution of tuples\n    are sticky, i.e the data with\n    same key will always be processed by the same physical operator, no\n    matter how many times it is sent through the stream. This stickiness\n    will continue even if the number of partitions grows dynamically and\n    can eventually be leveraged for advanced features like\n    bucket testing. How this is accomplished and what is required to\n    develop compliant operators will be explained below.\n\n\n\n\nWe plan to add more partitioning mechanisms proactively to the\nplatform over time as needed by emerging usage patterns. The aim is to\nallow enterprises to be able to focus on their business logic, and\nsignificantly reduce the cost of operability. As an enabling technology\nfor managing high loads, this platform provides enterprises with a\nsignificant innovative edge. Scalability and Partitioning is a\nfoundational building block for this platform.\n\n\nSticky Partition vs Round Robin\n\n\nAs noted above, partitioning via sticky key is data aware but\nround-robin partitioning is not. An example for non-sticky load\nbalancing would be round robin distribution over multiple instances,\nwhere for example a tuple stream of  A, A,\nA with 3 physical operator\ninstances would result in processing of a single A by each of the instances, In contrast, sticky\npartitioning means that exactly one instance of the operators will\nprocess all of the  Atuples if they\nfall into the same bucket, while B\nmay be processed by another operator. Data aware mapping of\ntuples to partitions (similar to distributed hash table) is accomplished\nvia Stream Codecs. In later sections we would show how these two\napproaches can be used in combination.\n\n\nStream Codec\n\n\nThe platform does not make assumptions about the tuple\ntype, it could be any Java object. The operator developer knows what\ntuple type an input port expects and is capable of processing. Each\ninput port has a stream codec \u00a0associated thatdefines how data is serialized when transmitted over a socket\nstream; it also defines another\nfunction that computes the partition hash key for the tuple. The engine\nuses that key to determine which physical instance(s) \u00a0(for a\npartitioned operator) receive that \u00a0tuple. For this to work, consistent hashing is required.\nThe default codec uses the Java Object#hashCode function, which is\nsufficient for basic types such as Integer, String etc. It will also\nwork with custom tuple classes as long as they implement hashCode\nappropriately. Reliance on hashCode may not work when generic containers\nare used that do not hash the actual data, such as standard collection\nclasses (HashMap etc.), in which case a custom stream codec must be\nassigned to the input port.\n\n\nStatic Partitioning\n\n\nDAG designers can specify at design time how they would like\ncertain operators to be partitioned. STRAM then instantiates the DAG\nwith the physical plan which adheres to the partitioning scheme defined\nby the design. This plan is the initial partition of the application. In\nother words, Static Partitioning is used to tell STRAM to compute the\nphysical DAG from a logical DAG once, without taking into consideration\nruntime states or loads of various operators.\n\n\nDynamic Partitioning\n\n\nIn streaming applications the load changes during the day, thus\ncreating situations where the number of partitioned operator instances\nneeds to adjust dynamically. The load can be measured in terms of\nprocessing within the DAG based on throughput, or latency, or\nconsiderations in external system components (time based etc.) that the\nplatform may not be aware of. Whatever the trigger, the resource\nrequirement for the current processing needs to be adjusted at run-time.\nThe platform may detect that operator instances are over or under\nutilized and may need to dynamically adjust the number of instances on\nthe fly. More instances of a logical operator may be required (partition\nsplit) or underutilized operator instances may need decommissioning\n(partition merge). We refer to either of the changes as dynamic\npartitioning. The default partitioning scheme supports split and merge\nof partitions, but without state transfer. The contract of the\nPartitioner\u00a0interface allows the operator\ndeveloper to implement split/merge and the associated state transfer, if\nnecessary.\n\n\nSince partitioning is a key scalability measure, our goal is to\nmake it as simple as possible without removing the flexibility needed\nfor sophisticated applications. Basic partitioning can be enabled at\ncompile time through the DAG specification. A slightly involved\npartitioning involves writing custom codecs to calculate data aware\npartitioning scheme. More complex partitioning cases may require users\nto provide a custom implementation of Partitioner, which gives the\ndeveloper full control over state transfer between multiple instances of\nthe partitioned operator.\n\n\nDefault Partitioning\n\n\nThe platform provides a default partitioning implementation that\ncan be enabled without implementing Partitioner\u00a0(or writing any other extra Java\ncode), which is designed to support simple sticky partitioning out of\nthe box for operators with logic agnostic to the partitioning scheme\nthat can be enabled by means of DAG construction alone.\n\n\nTypically an operator that can work with the default partitioning\nscheme would have a single input port. If there are multiple input\nports, only one port will be partitioned (the port first connected in\nthe DAG). The number of partitions will be calculated based on the\ninitial partition count - set as attribute on the operator in the DAG\n(if the attribute is not present, partitioning is off). Each partition\nwill handle tuples based on matching the lower bits of the hash code.\nFor example, if the tuple type was Integer and 2 partitions requested,\nall even numbers would go to one operator instance and all odd numbers\nto the other.\n\n\nDefault Dynamic Partitioning\n\n\nTriggering partition load evaluation and repartitioning action\nitself are separate concerns. Triggers are not specified further here,\nwe are planning to support it in a customizable fashion that, for\nexample, allows latency or SLA based implementations. Triggers calculate\na load indicator (signed number) that tells the framework that a given\npartition is either underutilized, operating normally within the\nexpected thresholds or overloaded and becoming a bottleneck. The\nindicator is then presented to the partitioning logic (default or custom\nimplementation of Partitioner) to provide the opportunity to make any\nneeded adjustments.\n\n\nThe default partitioning logic divides the key space\naccording to the lower bits of the hash codes that are generated by the\nstream codec, by assigning each partitioned operator instance via a bit\nmask and the respective value. For example, the operator may have\ninitially two partitions,  0and 1, each\nwith a bit mask of 1.\nIn the case where load evaluation flags partition\n0  as over utilized\n(most data tuples processed yield a hash code with lowest bit cleared),\napartition split\u00a0occurs, resulting in 00\nand  10with mask 11. Operator instance 0 will be replaced with 2 new instances and partition\n1  remains unchanged,\nresulting in three active partitions. The same process could repeat if\nmost tuples fall into the01 partition, leading to a split into 001  and101\nwith mask 111, etc.\n\n\nShould load decrease in two sibling partitions, a\npartition merge\u00a0could\nreverse the split, reducing the mask length and replacing two operators\nwith one. Should only one of two sibling partitions be underutilized,\n it cannot be\u00a0merged.\nInstead, the platform can attempt to deploy the affected operator\ninstance along with other operator instances for resource sharing\namongst underutilized partitions (not implemented yet). Keeping separate\noperator instances allows\u00a0us  to\npin load increases directly to the affected instance with a single\nspecific partition key, which would not be the case had we assigned a\nshared instance to handle multiple keys.\n\n\nNxM Partitions\n\n\nWhen two consecutive logical operators are partitioned a special\noptimization is done. Technically the output of the first operator\nshould be unified and streamed to the next logical node. But that can\ncreate a network bottleneck. The platform optimizes this by partitioning\nthe output stream of each partition of the first operator as per the\npartitions needed by the next operator. For example if the first\noperator has N partitions and the second operator has M partitions then\neach of the N partitions would send out M streams. The first of each of\nthese M streams would be unified and routed to the first of the M\npartitions, and so on. Such an optimization allows for higher\nscalability and eliminates a network bottleneck (one unifier in between\nthe two operators) by having M unifiers. This also enables the\napplication to perform within the resource limits enforced by YARN.\nSTRAM has a much better understanding and estimation of unifier resource\nneeds and is thus able to optimize for resource constraints.\n\n\nFigure 5 shows a case where we have a 3x2 partition; the single\nintermediate unifier between operator 1\u00a0and 2\u00a0is\noptimized away. The partition computation for operator  2\u00a0is executed on outbound streams of each\npartitions of operator 1. Each\npartition of operator 2\u00a0has its own\nCONTAINER_LOCAL unifier. In such a situation, the in-bound network\ntuple flow is split between containers for  2a\u00a0and 2b\u00a0each of which take half the traffic. STRAM\ndoes this by default since it always has better performance.\n\n\n\n\nParallel\n\n\nIn cases where all the downstream operators use the same\npartitioning scheme and the DAG is network bound an optimization called\nparallel partition\u00a0is very\neffective. In such a scenario all the downstream operators are also\npartitioned to create computation flow per partition. This optimization\nis extremely efficient for network bound streams, In some cases this\noptimization would also apply for CPU or RAM bounded\napplications.\n\n\nIn Figure 6a, operator 1\u00a0is\npartitioned into 1a\u00a0and\n1b. Both the downstream operators\n2\u00a0and  3\u00a0follow the same partition scheme as\n1, however the network I/O between\n1\u00a0and 2, and between 2\u00a0and  3\u00a0is\nhigh. Then users can decide to optimize using parallel partitions. This\nallows STRAM to completely skip the insertion of intermediate Unifier\noperators between 1 and 2 as well as between 2 and 3; a single unifier\njust before operator  4, is\nadequate by which time tuple flow volume is low.\n\n\nSince operator 4 has sufficient resources to manage the combined\noutput of multiple instances of operator 3, it need not be partitioned. A further\noptimization can be done by declaring operators  1, 2, and\n3\u00a0as THREAD_LOCAL (intra-thread)\nor CONTAINER_LOCAL (intra-process) or NODE_LOCAL (intra-node).\nParallel partition is not used by default, users have to specify it\nexplicitly via an attribute of the input port (reader) of the stream as\nshown below.\n\n\n\n\nThe following code shows an example of creating a parallel partition.\n\n\ndag.addStream(\nDenormalizedUserId\n, idAssigner.userid, uniqUserCount.data);\ndag.setInputPortAttribute(uniqUserCount.data, PortContext.PARTITION_PARALLEL, partitionParallel);\n\n\n\n\nParallel partitions can be used with other partitions, for example\na parallel partition could be sticky key or load balanced.\n\n\nParallel Partitions with Streams Modes\n\n\nParallel partitions can be further optimized if the parallel\npartitions are combined with streams being in-line or in-node or in-rack\nmode. This is very powerful feature and should be used if operators have\nvery high throughput within them and the outbound merge does an\naggregation. For example in Figure 6b, if operator 3 significantly\nreduces the throughput, which usually is a reason to do parallel\npartition, then making the streams in-line or in-node within nodes\n1-\n2 and 2-\n3 significantly impacts the performance.\n\n\nCONTAINER_LOCAL stream has high bandwidth, and can manage to\nconsume massive tuple count without taxing the NIC and networking stack.\nThe downside is that all operators (1,2,3) in this case need to be able\nto fit within the resource limits of CPU and memory enforced on a Hadoop\ncontainer. A way around this is to request RM to provide a big\ncontainer. On a highly used Hadoop grid, getting a bigger container may\nbe a problem, and operational complexities of managing a Hadoop cluster\nwith different container sizes may be higher. If THREAD_LOCAL or\nCONTAINER_LOCAL streams are needed to get the throughput, increasing\nthe partition count should be considered. In future STRAM may take this\ndecision automatically. Unless there is a very bad skew and sticky key\npartitioning is in use, the approach to partition till each container\nhas enough resources works well.\n\n\nA NODE_LOCAL stream has lower bandwidth compared to a\nCONTAINER_LOCAL stream, but it works well with the RM in terms of\nrespecting container size limits. A NODE_LOCAL parallel partition uses\nlocal loop back for streams and is much better than using NIC. Though\nNODE_LOCAL stream fits well with similar size containers, it does need\nRM to be able to deliver two containers on the same Hadoop node. On a\nheavily used Hadoop cluster, this may not always be possible. In future\nSTRAM would do these trade-offs automatically at run-time.\n\n\nA RACK_LOCAL stream has much lower bandwidth than NODE_LOCAL\nstream, as events go through the NIC. But it still is able to better\nmanage SLA and latency. Moreover RM has much better ability to give a\nrack local container as opposed to the other two.\n\n\nParallel partitions with CONTAINER_LOCAL streams can be done by\nsetting all the intermediate streams to CONTAINER_LOCAL. Parallel\npartitions with THREAD_LOCAL streams can be done by setting all the\nintermediate streams to THREAD_LOCAL. Platform supports the following\nvia attributes.\n\n\n\n\nParallel-Partition\n\n\nParallel-Partition with THREAD_LOCAL stream\n\n\nParallel-Partition with CONTAINER_LOCAL stream\n\n\nParallel-Partition with NODE_LOCAL stream\n\n\nParallel-Partition with RACK_LOCAL stream\n\n\n\n\nThese attributes would nevertheless be initial starting point and\nSTRAM can improve on them at run time.\n\n\n\n\nSkew Balancing Partition\n\n\nSkew balancing partition is useful to manage skews in the stream\nthat is load balanced using a sticky key. Incoming events may have a\nskew, and these may change depending on various factors like time of the\nday or other special circumstances. To manage the uneven load, users can\nset a limit on the ratio of maximum load on a partition to the minimum\nload on a partition. STRAM would use this to dynamically change the\npartitions. For example suppose there are 6 partitions, and the load\nhappens to be distributed as follows: one with 40%, and the rest with\n12% each. The ratio of maximum to minimum is 3.33. If the desired ratio\nis set to 2, STRAM would partition the first instance into two\npartitions, each with 20% load to bring the ratio down to the desired\nlevel. This will be tried repeatedly till partitions are balanced. The\ntime period between each attempt is controlled via an attribute to avoid\nrebalancing too frequently. As mentioned earlier, dynamic operations\ninclude both splitting a partition as well as merging partitions with\nlow load.\n\n\nFigure 7 shows an example of skew balancing partition. An example\nof 3x1 paritition is shown. Let's say that skew balance is kept at \u201cno\npartition to take up more than 50% load. If in runtime the load type\nchanges to create a skew. For example, consider an application in the US\nthat is processing a website clickstream. At night in the US, the\nmajority of accesses come from the Far East, while in the daytime it\ncomes from the Americas. Similarly, in the early morning, the majority\nof the accesses are from east coast of the US, with the skew shifting to\nthe west coast as the day progresses. Assume operator 1 is partitioned\ninto 1a, 1b, and 1c.\n\n\nLet's see what happens if the logical operator 1 gets into a 20%,\n20%, 60% skew as shown in Figure 7. This would trigger the skew\nbalancing partition. One example of attaining balance is to merge 1a,\nand 1b to get 1a+1b in a single partition to take the load to 40%; then\nsplit 1c into two partitions 1ca and 1cb to get 30% on each of them.\nThis way STRAM is able to get back to under 50% per partition. As a live\n24x7 application, this kind of skew partitioning can be applied several\ntimes in a day. Skew-balancing at runtime is a critical feature for SLA\ncompliance; it also enables cost savings. This partitioning scheme will\nbe available in later release.\n\n\n\n\nSkew Unifier Partition\n\n\nIn this section we would take a look at another way to balance the\nskew. This method is a little less disruptive, but is useful in\naggregate operators. Let us take the same example as in Figure 7 with\nskew 20%, 20%, and 60%. To manage the load we could have either worked\non rebalancing the partition, which involves a merge and split of\npartitions to get to a new distribution or by partitioning  only\u00a0the partition with the big skew. Since the\nbest way to manage skew is to load balance, if possible, this scheme\nattempts to do so. The method is less useful than the others we discusse\n-- the main reason being that if the developer has chosen a sticky key\npartition to start with, it is unlikely that a load balancing scheme can\nhelp. Assuming that it is worthwhile to load balance, a special\none-purpose unifier can be inserted for the skew partition. If the cause\nof resource bottleneck is not the I/O, specially the I/O into the\ndownstream operator, but is the compute (memory, CPU) power of a\npartition, it makes sense to split the skew partition without having to\nchange the in-bound I/O to the upstream operator.\n\n\nTo trigger this users can set a limit on the ratio of maximum load\non a partition to the minimum load on a partition, and ask to use this\nscheme. STRAM would use this to load balance.The time period between\neach attempt is controlled via the same attribute to avoid rebalancing\ntoo frequently.\n\n\nFigure 8 shows an example of skew load balancing partition with a\ndedicated unifier. The 20%, 20%, and 60% triggers the skew load\nbalancing partition with an unifier. Partition 1c would be split into\ntwo and it would get its own dedicated unifier. Ideally these two\nadditional partitions 1ca and 1cb will get 30% load. This way STRAM is\nable to get back to under 50% per partition. This scheme is very useful\nwhen the number of partitions is very high and we still have a bad\nskew.\n\n\nIn the steady state no physical partition is computing more than\n30% of the load. Memory and CPU resources are thus well distributed. The\nunifier that was inserted has to handle 60% of the load, distributed\nmore evenly, as opposed to the final unifier that had a 60% skew to\nmanage at a much higher total load. This partitioning scheme will be\navailable in later release.\n\n\n\n\nCascading Unifier\n\n\nLet's take the case of an upstream operator oprU\u00a0that connects to a downstream operator\noprD. Let's assume the application\nis set to scale oprU by load balancing. So this could be either Nx1 or\nNxM partitioning scheme. The upstream operator oprU scales by increasing\nN. An increase in the load triggers more resource needs (CPU, Memory, or\nI/O), which in turn triggers more containers and raises N, the\ndownstream node may be impacted in a lot of situations. In this section\nwe review a method to shield oprD from dynamic changes in the execution\nplan of oprU. On aggregate operators (Sum, Count, Max, Min, Range \u2026) it\nis better to do load balanced partitioning to avoid impact of skew. This\nworks very well as each partition emits tuples at the order of number of\nkeys (range) in the incoming stream per application window. But as N\ngrows the in-bound I/O to the unifier of oprU that runs in the container\nof oprD goes up proportionately as each upstream partition sends tuples\nof the order of unique keys (range). This means that the partitioning\nwould not scale linearly. The platform has mechanisms to manage this and\nget the scale back to being linear.\n\n\nCascading unifiers are implemented by inserting a series of\nintermediate unifiers before the final unifier in the container of oprD.\nSince each unifier guarantees that the outbound I/O would be in order of\nthe number of unique keys, the unifier in the oprD container can expect\nto achieve an upper limit on the inbound I/O. The problem is the same\nirrespective of the value of M (1 or more), wherein the amount of\ninbound I/O is proportional to N, not M. Figure 8 illustrates how\ncascading unifier works.\n\n\n\n\nFigure 8 shows an example where a 4x1 partition with single\nunifier is split into three 2x1 partitions to enable the final unifier\nin oprD container to get an upper limit on inbound I/O. This is useful\nto ensure that network I/O to containers is within limits, or within a\nlimit specified by users. The platform allows setting an upper limit of\nfan-in of the stream between oprU and oprD. Let's say that this is F (in\nthe figure F=2). STRAM would plan N/F (let's call it N1) containers,\neach with one unifier. The inbound fan-in to these unifiers is F. If N1\n\n F, another level of unifiers would be inserted. Let's say at some\npoint N/(F1*F2*...Fk) \n F, where K is the level of unifiers. The\noutbound I/O of each unifier is guaranteed to be under F, specially the\nunifier for oprD. This ensures that the application scales linearly as\nthe load grows. The downside is the additional latency imposed by each\nunifier level (a few milliseconds), but the SLA is maintained, and the\napplication is able to run within the resource limits imposed by YARN.\nThe value of F can be derived from any of the following\n\n\n\n\nI/O limit on containers to allow proper behavior in an\n    multi-tenant environment\n\n\nLoad on oprD instance\n\n\nBuffer server limits on fan-in, fan-out\n\n\nSize of reservoir buffer for inbound fan-in\n\n\n\n\nA more intriguing optimization comes when cascading unifiers are\ncombined with node-local execution plan, in which the bounds of two or\nmore containers are used and much higher local loopback limits are\nleveraged. In general the first level fan-in limit (F1) and the last\nstage fan-in limit (Fk) need not be same. In fact a much open and better\nleveraged execution plan may indeed have F1 != F2 != \u2026 != Fk, as Fk\ndetermines the fan-in for oprD, while F1, \u2026 Fk-1 are fan-ins for\nunifier-only containers. The platform will have these schemes in later\nversions.\n\n\nSLA\n\n\nA Service Level Agreement translates to guaranteeing that the\napplication would meet the requirements X% of the time. For example six\nsigma X is\u00a099.99966%. For\nreal-time streaming applications this translates to requirements for\nlatency, throughput, uptime, data loss etc. and that in turn indirectly\nleads to various resource requirements, recovery mechanisms, etc. The\nplatform is designed to handle these and features would be released in\nfuture as they get developed. At a top level, STRAM monitors throughput\nper operator, computes latency per operator, manages uptime and supports\nvarious recovery mechanisms to handle data loss. A lot of this decision\nmaking and algorithms will be customizable.\n\n\n\n\nFault Tolerance\n\n\nFault tolerance in the platform is defined as the ability to\nrecognize the outage of any part of the application, get resources,\nre-initialize the failed operators, and re-compute the lost data. The\ndefault method is to bring the affected part of the DAG \u00a0back to a known\n(checkpointed) state and recompute atomic micro batches from there on.\nThus the default is  at least\nonce\u00a0processing mode. An operator can be configured for\nat most once\u00a0recovery, in which\ncase the re-initialized operator starts from next available window; or\nfor exactly once\u00a0recovery, in which\ncase the operator only recomputes the window it was processing when the\noutage happened.\n\n\nState of the Application\n\n\nThe state of the application is traditionally defined as the state\nof all operators and streams at any given time. Monitoring state as\nevery tuple is processed asynchronously in a distributed environment\nbecomes a near impossible task, and cost paid to achieve it is very\nhigh. Consequently, in the platform, state is not saved per tuple, but\nrather at window boundaries. The platform treats windows as atomic micro\nbatches. The state saving task is delegated by STRAM to the individual\noperator or container. This ensures that the bookkeeping cost is very\nlow and works in a distributed way. Thus, the state of the application\nis defined as the collection of states of every operator and the set of\nall windows stored in the buffer server. This allows STRAM to rebuild\nany part of the application from the last saved state of the impacted\noperators and the windows retained by the buffer server. The state of an\noperator is intrinsically associated with a window id. Since operators\ncan override the default checkpointing period, operators may save state\nat the end of different windows. This works because the buffer server\nsaves all windows for as long as they are needed (state in the buffer\nserver is purged once STRAM determines that it is not longer needed\nbased on checkpointing in downstream operators).\n\n\nOperators can be stateless or stateful. A stateless operator\nretains no data between windows. All results of all computations done in\na window are emitted in that window. Variables in such an operator are\neither transient or are cleared by an end_window event. Such operators\nneed no state restoration after an outage. A stateful operator retains\ndata between windows and has data in checkpointed state. This data\n(state) is used for computation in future windows. Such an operator\nneeds its state restored after an outage. By default the platform\nassumes the operator is stateful. In order to optimize recovery (skip\nprocessing related to state recovery) for a stateless operator, the\noperator needs to be declared as stateless to STRAM. Operators can\nexplicitly mark themselves stateless via an annotation or an\nattribute.\n\n\nRecovery mechanisms are explained later in this section. Operator\ndevelopers have to ensure that there is no dependency on the order of\ntuples between two different streams. As mentioned earlier in this\ndocument, the platform guarantees in-order tuple delivery within a\nsingle stream, For operators with multiple input ports, a replay may\nresult in a different relative order of tuples among the different input\nports. If the output tuple computation is affected by this relative\norder, the operator may have to wait for the endWindow call (at which\npoint it would have seen all the tuples from all input ports in the\ncurrent window), perform order-dependent computations correctly and\nfinally, emit results.\n\n\nCheckpointing\n\n\nSTRAM provides checkpointing parameters to StreamingContainer\nduring initialization. A checkpoint period is given to the containers\nthat have the window generators. A control tuple is sent at the end of\ncheckpoint interval. This tuple traverses through the data path via\nstreams and triggers each StreamingContainer in the path to instrument a\ncheckpoint of the operator that receives this tuple. This ensures that\nall the operators checkpoint at exactly the same window boundary (except\nin those cases where a different checkpoint interval was configured for\nan operator by the user).\n\n\nThe only delay is the latency of the control tuple to reach all\nthe operators. Checkpoint is thus done between the endWindow call of a\nwindow and the beginWindow call of the next window. Since most operators\nare computing in parallel (with the exception of those connected by\nTHREAD_LOCAL streams) they each checkpoint as and when they are ready\nto process the \u201ccheckpoint\u201d control tuple. The asynchronous design of\nthe platform means that there is no guarantee that two operators would\ncheckpoint at exactly the same time, but there is a guarantee that by\ndefault they would checkpoint at the same window boundary. This feature\nalso ensures that purge of old data can be efficiently done: Once the\ncheckpoint window tuple is done traversing the DAG, the checkpoint state\nof the entire DAG increments to this window id at which point prior\ncheckpoint data can be discarded.\n\n\nIn case of an operator that has an application window size that is\nlarger than the size of the streaming window, the checkpointing by\ndefault still happens at same intervals as with other operators. To\nalign checkpointing with application window boundary, the application\ndeveloper should set the attribute \u201cCHECKPOINT_WINDOW_COUNT\u201d to\n\u201cAPPLICATION_WINDOW_COUNT\u201d. This ensures that the checkpoint happens\nat the  end\u00a0of the application\nwindow and not within\u00a0that window.\nSuch operators now treat the application window as an atomic computation\nunit. The downside is that it does need the upstream buffer server to\nkeep tuples for the entire application window.\n\n\nIf an operator is completely stateless, i.e. an outbound tuple is\nonly emitted in the process\u00a0call\nand only depends on the tuple of that call, there is no need to align\ncheckpointing with application window end. If the operator is stateful\nonly within a window, the operator developer should strongly consider\ncheckpointing only on the application window boundary.\n\n\nCheckpointing involves pausing an operator, serializing the state\nto persistent storage and then resuming the operator. Thus checkpointing\nhas a latency cost that can negatively affect computational throughput;\nto minimize that impact, it is important to ensure that checkpointing is\ndone with minimal required objects. This means, as mentioned earlier,\nall data that is not part of the operator state should be declared as\ntransient so that it is not persisted.\n\n\nAn operator developer can also create a stateless operator (marked\nwith the Stateless annotation). Stateless operators are not\ncheckpointed. Obviously, in such an operator, computation should not\ndepend on state from a previous window.\n\n\nThe serialized \u00a0state of an operator is stored as a file, and is\nthe state to which that the operator is restored if an outage happens\nbefore the next checkpoint. The id of the last completed window (per\noperator) is sent back to STRAM in the next heartbeat. The default\nimplementation for serialization uses KRYO. Multiple past checkpoints\nare kept per operator. Depending on the downstream checkpoint, one of\nthese are chosen for recovery. Checkpoints and buffer server state are\npurged once STRAM sees windows as fully processed in the DAG.\n\n\nA complete recovery of an operator needs the operator to be\ncreated, its checkpointed state restored and then all the lost atomic\nwindows replayed by the upstream buffer server(s). The above design\nkeeps the bookkeeping cost low with quick catch up time. In the next\nsection we will see how this simple abstraction allows applications to\nrecover under different requirements.\n\n\nRecovery Mechanisms\n\n\nRecovery mechanism are ways to recover from a container (or an\noperator) outage. In this section we discuss a single container outage.\nMultiple container outages are handled as independent events. Recovery\nrequires the upstream buffer server to replay windows and it would\nsimply go one more level upstream if the immediate upstream container\nhas also failed. If multiple operators are in a container (THREAD_LOCAL\nor CONTAINER_LOCAL stream) the container recovery treats each operator\nas an independent object when figuring out the recovery steps.\nApplication developers can set any of the recovery mechanisms discussed\nbelow for node outage.\n\n\nIn general, the cost of recovery depends on the state of the\noperator and the recovery mechanism selected, while data loss tolerance\nis specified by the application. For example a data-loss tolerant\napplication would prefer at most\nonce\u00a0recovery. All recovery mechanisms treat a streaming\nwindow as an atomic computation unit. In all three recovery mechanisms\nthe new operator connects to the upstream buffer server and asks for\ndata from a particular window onwards. Thus all recovery methods\ntranslate to deciding which atomic units to re-compute and which state\nthe new operator resumes from. A partially computed micro-batch is\nalways dropped. Such micro-batches are re-computed in at-least-once or\nexactly-once mode and skipped in at-most-once mode. The notiion of an\natomic micro-batch is a critical guiding principle as it enables very\nlow bookkeeping costs, high throughput, low recovery times, and high\nscalability. Within an application each operator can have its own\nrecovery mechanism.\n\n\nAt Least Once\n\n\nAt least once recovery is the default recovery mechanism, i.e it\nis used when no mechanism is specified. In this method, the lost\noperator is brought back to its latest viable checkpointed state and the\nupstream buffer server is asked to replay all subsequent windows. There\nis no data loss in recovery. The viable checkpoint state is defined as\nthe one whose window id is in the past as compared to all the\ncheckpoints of all the downstream operators. All downstream operators\nare restarted at their checkpointed state. They ignore all incoming data\nthat belongs to windows prior their checkpointed window. The lost\nwindows are thus recomputed and the application catches up with live\nincoming data. This is called \" at least\nonce\"\u00a0because lost windows are recomputed. For example if\nthe streaming window is 0.5 seconds and checkpointing is being done\nevery 30 seconds, then upon node outage all windows since the last\ncheckpoint (up to 60 windows) need to be re-processed. If the\napplication can handle loss of data, then this is not the most optimal\nrecovery mechanism.\n\n\nIn general for this recovery mode, the average time lag on a node\noutage is\n\n\n= (CP/2*SW)*T + HC\n\n\nwhere\n\n\n\n\nCP\n\u00a0\u00a0- Checkpointing period (default value is 30 seconds)\n\n\nSW\n\u00a0\u00a0- Streaming window period (default value is 0.5 seconds)\n\n\nT\n\u00a0\u00a0\u00a0- \u00a0Time taken to re-compute one lost window from data in memory\n\n\nHC\n\u00a0\u00a0- Time it takes to get a new Hadoop Container, or make do with the current ones\n\n\n\n\nA lower CP is a trade off between cost of checkpointing and the\nneed to have to use it in case of outage. Input adapters cannot use\nat-least-once recovery without the support from sources outside Hadoop.\nFor an output adapter care may needed if the external system cannot\nhandle re-write of the same data.\n\n\nAt Most Once\n\n\nThis recovery mechanism is for applications that can tolerate\ndata-loss; they get the quickest recovery in return. The restarted node\nconnects to the upstream buffer server, subscribing to data from the\nstart of the next window. It then starts processing that window. The\ndownstream operators ignore the lost windows and continue to process\nincoming data normally. Thus, this mechanism forces all downstream\noperators to follow.\n\n\nFor multiple inputs, the operator waits for all ports with the\nat-most-once attribute to get responses from their respective buffer\nservers. Then, the operator starts processing till the end window of the\nlatest window id on each input port is reached. In this case the end\nwindow tuple is non-blocking till the common window id is reached. At\nthis point the input ports are now properly synchronized. Upstream nodes\nreconnect under  at most\nonce\u00a0paradigm in same way. \u00a0For example, assume an operator\nhas ports in1\u00a0and in2\u00a0and a checkpointed window of 95. Assume further that the buffer servers of\noperators upstream of  in1\u00a0and\nin2\u00a0respond with window id 100 and\n102 respectively. Then port in1\u00a0would continue to process till end window of\n101, while port  in2\u00a0will wait for in1\nto catch up to 102.\nFrom \u00a0then on, both ports process their tuples normally. So windows from\n96 to  99are lost. Window 100\nand 101 has only\nin1 active, and 102 onwards both ports are active. The other\nports of upstream nodes would also catch up till  102in a similar fashion. This operator may not\nneed to be checkpointed. Currently the option to not do checkpoint in\nsuch cases is not available.\n\n\nIn general, in this recovery mode, the average time lag on a node\noutage is\n\n\n= SW/2 + HC\n\n\nwhere\n\n\n\n\n\n\nSW\n\u00a0- Streaming window period (default value is 0.5\nseconds)\n\n\n\n\n\n\nHC\n\u00a0- Time it takes to get a new Hadoop Container, or make\ndo with the current ones\n\n\n\n\n\n\nExactly Once\n\n\nThis recovery mechanism is for applications that require no\ndata-loss as well are no recomputation. Since a window is an atomic\ncompute unit, exactly once applies to the window as a whole. In this\nrecovery mode, the operator is brought back to the start of the window\nin which the outage happened and the window is recomputed. The window is\nconsidered closed when all the data computations are done and end window\ntuple is emitted. \u00a0Exactly once requires every window to be\ncheckpointed. From then on, the operator asks the upstream buffer server\nto send data from the last checkpoint. The upstream node behaves the\nsame as in at-most-once recovery. Checkpointing after every streaming\nwindow is very costly, but users would most often do exactly once per\napplication window; if the application window size is substantially\nlarger than the streaming window size (which typically is the case) the\ncost of running an operator in this recovery mode may not be as\nhigh.\n\n\nSpeculative Execution\n\n\nIn future we looking at possibility of adding speculative execution for the applications. This would be enabled in multiple ways.\n\n\n\n\n\n\nAt an operator level: The upstream operator would emit to\n    two copies. The downstream operator would receive from both copies\n    and pick a winner. The winner (primary) would be picked in either of\n    the following ways\n\n\n\n\nStatically as dictated by STRAM\n\n\nDynamically based on whose tuple arrives first. This mode\n    needs both copies to guarantee that the computation result would\n    have identical functionality\n\n\n\n\n\n\n\n\nAt a sub-query level: A part of the application DAG would be\n    run in parallel and all upstream operators would feed to two copies\n    and all downstream operators would receive from both copies. The\n    winners would again be picked in a static or dynamic manner\n\n\n\n\nEntire DAG: Another copy of the application would be run by\n    STRAM and the winner would be decided outside the application. In\n    this mode the output adapters would both be writing\n    the result.\n\n\n\n\nIn all cases the two copies would run on different Hadoop nodes.\nSpeculative execution is under development and\nis not yet available.\n\n\n\n\n9: Dynamic Application Modifications\n\n\nDynamic application modifications are being worked on and most of\nthe features discussed here are now available. The platform supports the\nability to modify the DAG of the application as per inputs as well as\nset constraints, and will continue to provide abilities to deepen\nfeatures based on this ability. All these changes have one thing in\ncommon and that is the application does not need to be restarted as\nSTRAM will instrument the changes and the streaming will catch-up and\ncontinue.\n\n\nSome examples are\n\n\n\n\nDynamic Partitioning:\u00a0Automatic\n    changes in partitioning of computations to match constraints on a\n    run time basis. Examples includes STRAM adding resource during spike\n    in streams and returning them once spike is gone. Scale up and scale\n    down is done automatically without human intervention.\n\n\nModification via constraints: Attributes can be changed via\n    Webservices and STRAM would adapt the execution plan to meet these.\n    Examples include operations folks asking STRAM to reduce container\n    count, or changing network resource restrictions.\n\n\nModification via properties: Properties of operators can be\n    changed in run time. This enables application developers to trigger\n    a new behavior as need be. Examples include triggering an alert ON.\n    The platform supports changes to any property of an operator that\n    has a setter function defined.\n\n\nModification of DAG structure: Operators and streams can be\n    added to or removed from a running DAG, provided the code of the\n    operator being added is already in the classpath of the running\n    application master. \u00a0This enables application developers to add or\n    remove processing pipelines on the fly without having to restart\n    the application.\n\n\nQuery Insertion: Addition of sub-queries to currently\n    running application. This query would take current streams as inputs\n    and start computations as per their specs. Examples insertion of\n    SQL-queries on live data streams, dynamic query submission and\n    result from STRAM (not yet available).\n\n\n\n\nDynamic modifications to applications are foundational part of the\nplatform. They enable users to build layers over the applications. Users\ncan also save all the changes done since the application launch, and\ntherefore predictably get the application to its current state. For\ndetails refer to  \nConfiguration Guide\n\n.\n\n\n\n\nUser Interface\n\n\nThe platform provides a rich user interface. This includes tools\nto monitor the application system metrics (throughput, latency, resource\nutilization, etc.); dashboards for application data, replay, errors; and\na Developer studio for application creation, launch etc. For details\nrefer to  \nUI Console Guide\n.\n\n\nDemos\n\n\nIn this section we list some of the demos that come packaged with\ninstaller. The source code for the demos is available in the open-source\n\nApache Apex-Malhar repository\n.\nAll of these do computations in real-time. Developers are encouraged to\nreview them as they use various features of the platform and provide an\nopportunity for quick learning.\n\n\n\n\nComputation of PI:\n    Computes PI by generating a random location on X-Y plane and\n    measuring how often it lies within the unit circle centered\n    at (0,0).\n\n\nYahoo! Finance quote\u00a0computation:\n    Computes ticker quote, 1-day chart (per min), and simple moving\n    averages (per 5 min).\n\n\nEchoserver Reads messages from a\n    network connection and echoes them back out.\n\n\nTwitter top N tweeted urls: Computes\n    top N tweeted urls over last 5 minutes\n\n\nTwitter trending hashtags: Computes\n    the top Twitter Hashtags over the last 5 minutes\n\n\nTwitter top N frequent words:\n    Computes top N frequent words in a sliding window\n\n\nWord count: Computes word count for\n    all words within a large file\n\n\nMobile location tracker: Tracks\n    100,000 cell phones within an area code moving at car speed (jumping\n    cell phone towers every 1-5 seconds).\n\n\nFrauddetect: Analyzes a stream of\n    credit card merchant transactions.\n\n\nMroperator:Contains several\n    map-reduce applications.\n\n\nR: Analyzes a synthetic stream of\n    eruption event data for the Old Faithful\n    geyser (https://en.wikipedia.org/wiki/Old_Faithful).\n\n\nMachinedata: Analyzes a synthetic\n    stream of events to determine health of a machine.", 
            "title": "Applications"
        }, 
        {
            "location": "/application_development/#application-developer-guide", 
            "text": "Real-time big data processing is not only important but has become\ncritical for businesses which depend on accurate and timely analysis of\ntheir business data. A few businesses have yielded to very expensive\nsolutions like building an in-house, real-time analytics infrastructure\nsupported by an internal development team, or buying expensive\nproprietary software. A large number of businesses are dealing with the\nrequirement just by trying to make Hadoop do their batch jobs in smaller\niterations. Over the last few years, Hadoop has become ubiquitous in the\nbig data processing space, replacing expensive proprietary hardware and\nsoftware solutions for massive data processing with very cost-effective,\nfault-tolerant, open-sourced, and commodity-hardware-based solutions.\nWhile Hadoop has been a game changer for companies, it is primarily a\nbatch-oriented system, and does not yet have a viable option for\nreal-time data processing. \u00a0Most companies with real-time data\nprocessing end up having to build customized solutions in addition to\ntheir Hadoop infrastructure.  The DataTorrent platform is designed to process massive amounts of\nreal-time events natively in Hadoop. This can be event ingestion,\nprocessing, and aggregation for real-time data analytics, or can be\nreal-time business logic decisioning such as cell tower load balancing,\nreal-time ads bidding, or fraud detection. \u00a0The platform has the ability\nto repair itself in real-time (without data loss) if hardware fails, and\nadapt to changes in load by adding and removing computing resources\nautomatically.  DataTorrent is a native Hadoop application. It runs as a YARN\n(Hadoop 2.x) application and leverages Hadoop as a distributed operating\nsystem. All the basic distributed operating system capabilities of\nHadoop like resource allocation (Resource Manager, distributed file system (HDFS),\nmulti-tenancy, security, fault-tolerance, scalability,\u00a0etc.\nare supported natively in all streaming applications. \u00a0Just as Hadoop\nfor map-reduce handles all the details of the application allowing you\nto only focus on writing the application (the mapper and reducer\nfunctions), the platform handles all the details of streaming execution,\nallowing you to only focus on your business logic. Using the platform\nremoves the need to maintain separate clusters for real-time\napplications.  In the platform, building a streaming application can be extremely\neasy and intuitive. \u00a0The application is represented as a Directed\nAcyclic Graph (DAG) of computation units called  Operators  interconnected\nby the data-flow edges called   Streams .\u00a0The operators process input\nstreams and produce output streams. A library of common operators is\nprovided to enable quick application development. \u00a0In case the desired\nprocessing is not available in the Operator Library, one can easily\nwrite a custom operator. We refer those interested in creating their own\noperators to the  Operator Development Guide .", 
            "title": "Application Developer Guide"
        }, 
        {
            "location": "/application_development/#running-a-test-application", 
            "text": "This chapter will help you with a quick start on running an\napplication. If you are starting with the platform for the first time,\nit would be informative to open an existing application and see it run.\nDo the following steps to run the PI demo, which computes the value of\nPI \u00a0in a simple\nmanner:   Open up platform files in your IDE (for example NetBeans, or Eclipse)  Open Demos project  Open Test Packages and run ApplicationTest.java\u00a0in pi\u00a0package  See the results in your system console   Congratulations, you just ran your first real-time streaming demo\n:) This demo is very simple and has four operators. The first operator\nemits random integers between 0 to 30, 000. The second operator receives\nthese coefficients and emits a hashmap with x and y values each time it\nreceives two values. The third operator takes these values and computes\nx**2+y**2. The last operator counts how many computed values from\nthe previous operator were less than or equal to 30, 000**2. Assuming\nthis count is N, then PI is computed as N/number of values received.\nHere is the code snippet for the PI application. This code populates the\nDAG. Do not worry about what each line does, we will cover these\nconcepts later in this document.  // Generates random numbers\nRandomEventGenerator rand = dag.addOperator( rand , new RandomEventGenerator());\nrand.setMinvalue(0);\nrand.setMaxvalue(30000);\n\n// Generates a round robin HashMap of  x  and  y \nRoundRobinHashMap String,Object  rrhm = dag.addOperator( rrhm , new RoundRobinHashMap String, Object ());\nrrhm.setKeys(new String[] {  x ,  y  });\n\n// Calculates pi from x and y\nJavaScriptOperator calc = dag.addOperator( picalc , new Script());\ncalc.setPassThru(false);\ncalc.put( i ,0);\ncalc.put( count ,0);\ncalc.addSetupScript( function pi() { if (x*x+y*y  =  +maxValue*maxValue+ ) { i++; } count++; return i / count * 4; } );\ncalc.setInvoke( pi );\ndag.addStream( rand_rrhm , rand.integer_data, rrhm.data);\ndag.addStream( rrhm_calc , rrhm.map, calc.inBindings);\n\n// puts results on system console\nConsoleOutputOperator console = dag.addOperator( console , new ConsoleOutputOperator());\ndag.addStream( rand_console ,calc.result, console.input);  You can review the other demos and see what they do. The examples\ngiven in the Demos project cover various features of the platform and we\nstrongly encourage you to read these to familiarize yourself with the\nplatform. In the remaining part of this document we will go through\ndetails needed for you to develop and run streaming applications in\nMalhar.", 
            "title": "Running A Test Application"
        }, 
        {
            "location": "/application_development/#test-application-yahoo-finance-quotes", 
            "text": "The PI\u00a0application was to\nget you started. It is a basic application and does not fully illustrate\nthe features of the platform. For the purpose of describing concepts, we\nwill consider the test application shown in Figure 1. The application\ndownloads tick data from   Yahoo! Finance  \u00a0and computes the\nfollowing for four tickers, namely  IBM , GOOG ,  YHOO .   Quote: Consisting of last trade price, last trade time, and\n    total volume for the day  Per-minute chart data: Highest trade price, lowest trade\n    price, and volume during that minute  Simple Moving Average: trade price over 5 minutes   Total volume must ensure that all trade volume for that day is\nadded, i.e. data loss would result in wrong results. Charting data needs\nall the trades in the same minute to go to the same slot, and then on it\nstarts afresh, so again data loss would result in wrong results. The\naggregation for charting data is done over 1 minute. Simple moving\naverage computes the average price over a 5 minute sliding window; it\ntoo would produce wrong results if there is data loss. Figure 1 shows\nthe application with no partitioning.   The operator StockTickerInput:\u00a0StockTickerInput \u00a0 is\nthe input operator that reads live data from Yahoo! Finance once per\ninterval (user configurable in milliseconds), and emits the price, the\nincremental volume, and the last trade time of each stock symbol, thus\nemulating real ticks from the exchange. \u00a0We utilize the Yahoo! Finance\nCSV web service interface. \u00a0For example:  $ GET 'http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO f=sl1vt1' IBM ,203.966,1513041, 1:43pm  GOOG ,762.68,1879741, 1:43pm  AAPL ,444.3385,11738366, 1:43pm  YHOO ,19.3681,14707163, 1:43pm   Among all the operators in Figure 1, StockTickerInput is the only\noperator that requires extra code because it contains a custom mechanism\nto get the input data. \u00a0Other operators are used unchanged from the\nMalhar library.  Here is the class implementation for StockTickInput:  package com.datatorrent.demos.yahoofinance;\n\nimport au.com.bytecode.opencsv.CSVReader;\nimport com.datatorrent.annotation.OutputPortFieldAnnotation;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DefaultOutputPort;\nimport com.datatorrent.api.InputOperator;\nimport com.datatorrent.lib.util.KeyValPair;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.util.*;\nimport org.apache.commons.httpclient.HttpClient;\nimport org.apache.commons.httpclient.HttpStatus;\nimport org.apache.commons.httpclient.cookie.CookiePolicy;\nimport org.apache.commons.httpclient.methods.GetMethod;\nimport org.apache.commons.httpclient.params.DefaultHttpParams;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * This operator sends price, volume and time into separate ports and calculates incremental volume.\n */\npublic class StockTickInput implements InputOperator\n{\n  private static final Logger logger = LoggerFactory.getLogger(StockTickInput.class);\n  /**\n   * Timeout interval for reading from server. 0 or negative indicates no timeout.\n   */\n  public int readIntervalMillis = 500;\n  /**\n   * The URL of the web service resource for the POST request.\n   */\n  private String url;\n  public String[] symbols;\n  private transient HttpClient client;\n  private transient GetMethod method;\n  private HashMap String, Long  lastVolume = new HashMap String, Long ();\n  private boolean outputEvenIfZeroVolume = false;\n  /**\n   * The output port to emit price.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, Double  price = new DefaultOutputPort KeyValPair String, Double ();\n  /**\n   * The output port to emit incremental volume.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, Long  volume = new DefaultOutputPort KeyValPair String, Long ();\n  /**\n   * The output port to emit last traded time.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, String  time = new DefaultOutputPort KeyValPair String, String ();\n\n  /**\n   * Prepare URL from symbols and parameters. URL will be something like: http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO f=sl1vt1\n   *\n   * @return the URL\n   */\n  private String prepareURL()\n  {\n    String str =  http://download.finance.yahoo.com/d/quotes.csv?s= ;\n    for (int i = 0; i   symbols.length; i++) {\n      if (i != 0) {\n        str +=  , ;\n      }\n      str += symbols[i];\n    }\n    str +=  f=sl1vt1 e=.csv ;\n    return str;\n  }\n\n  @Override\n  public void setup(OperatorContext context)\n  {\n    url = prepareURL();\n    client = new HttpClient();\n    method = new GetMethod(url);\n    DefaultHttpParams.getDefaultParams().setParameter( http.protocol.cookie-policy , CookiePolicy.BROWSER_COMPATIBILITY);\n  }\n\n  @Override\n  public void teardown()\n  {\n  }\n\n  @Override\n  public void emitTuples()\n  {\n\n    try {\n      int statusCode = client.executeMethod(method);\n      if (statusCode != HttpStatus.SC_OK) {\n        System.err.println( Method failed:   + method.getStatusLine());\n      }\n      else {\n        InputStream istream = method.getResponseBodyAsStream();\n        // Process response\n        InputStreamReader isr = new InputStreamReader(istream);\n        CSVReader reader = new CSVReader(isr);\n        List String[]  myEntries = reader.readAll();\n        for (String[] stringArr: myEntries) {\n          ArrayList String  tuple = new ArrayList String (Arrays.asList(stringArr));\n          if (tuple.size() != 4) {\n            return;\n          }\n          // input csv is  Symbol , Price , Volume , Time \n          String symbol = tuple.get(0);\n          double currentPrice = Double.valueOf(tuple.get(1));\n          long currentVolume = Long.valueOf(tuple.get(2));\n          String timeStamp = tuple.get(3);\n          long vol = currentVolume;\n          // Sends total volume in first tick, and incremental volume afterwards.\n          if (lastVolume.containsKey(symbol)) {\n            vol -= lastVolume.get(symbol);\n          }\n\n          if (vol   0 || outputEvenIfZeroVolume) {\n            price.emit(new KeyValPair String, Double (symbol, currentPrice));\n            volume.emit(new KeyValPair String, Long (symbol, vol));\n            time.emit(new KeyValPair String, String (symbol, timeStamp));\n            lastVolume.put(symbol, currentVolume);\n          }\n        }\n      }\n      Thread.sleep(readIntervalMillis);\n    }\n    catch (InterruptedException ex) {\n      logger.debug(ex.toString());\n    }\n    catch (IOException ex) {\n      logger.debug(ex.toString());\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n  }\n\n  @Override\n  public void endWindow()\n  {\n  }\n\n  public void setOutputEvenIfZeroVolume(boolean outputEvenIfZeroVolume)\n  {\n       this.outputEvenIfZeroVolume = outputEvenIfZeroVolume;\n  }\n\n}  The operator has three output ports that emit the price of the\nstock, the volume of the stock and the last trade time of the stock,\ndeclared as public member variables price, volume\u00a0and  time\u00a0of the class. \u00a0The tuple of the\nprice\u00a0output port is a key-value\npair with the stock symbol being the key, and the price being the value.\n\u00a0The tuple of the volume\u00a0output\nport is a key value pair with the stock symbol being the key, and the\nincremental volume being the value. \u00a0The tuple of the  time\u00a0output port is a key value pair with the\nstock symbol being the key, and the last trade time being the\nvalue.  Important: Since operators will be\nserialized, all input and output ports need to be declared transient\nbecause they are stateless and should not be serialized.  The method\u00a0setup(OperatorContext)\ncontains the code that is necessary for setting up the HTTP\nclient for querying Yahoo! Finance.  Method\u00a0emitTuples() contains\nthe code that reads from Yahoo! Finance, and emits the data to the\noutput ports of the operator. \u00a0emitTuples()\u00a0will be called one or more times\nwithin one application window as long as time is allowed within the\nwindow.  Note that we want to emulate the tick input stream by having\nincremental volume data with Yahoo! Finance data. \u00a0We therefore subtract\nthe previous volume from the current volume to emulate incremental\nvolume for each tick.  The operator\nDailyVolume:\u00a0This operator\nreads from the input port, which contains the incremental volume tuples\nfrom StockTickInput, and\naggregates the data to provide the cumulative volume. \u00a0It uses the\nlibrary class  SumKeyVal K,V \u00a0provided in math\u00a0package. \u00a0In this case,\nSumKeyVal String,Long , where K is the stock symbol, V is the\naggregated volume, with cumulative\nset to true. (Otherwise if  cumulativewas set to false, SumKeyVal would\nprovide the sum for the application window.) \u00a0Malhar provides a number\nof built-in operators for simple operations like this so that\napplication developers do not have to write them. \u00a0More examples to\nfollow. This operator assumes that the application restarts before the\nmarket opens every day.  The operator Quote:\nThis operator has three input ports, which are price (from\nStockTickInput), daily_vol (from\nDaily Volume), and time (from\n StockTickInput). \u00a0This operator\njust consolidates the three data items and and emits the consolidated\ndata. \u00a0It utilizes the class ConsolidatorKeyVal K \u00a0from the\nstream\u00a0package.  The operator HighLow:\u00a0This operator reads from the input port,\nwhich contains the price tuples from StockTickInput, and provides the high and the\nlow price within the application window. \u00a0It utilizes the library class\n RangeKeyVal K,V \u00a0provided\nin the math\u00a0package. In this case,\nRangeKeyVal String,Double .  The operator MinuteVolume:\nThis operator reads from the input port, which contains the\nvolume tuples from StockTickInput,\nand aggregates the data to provide the sum of the volume within one\nminute. \u00a0Like the operator  DailyVolume, this operator also uses\nSumKeyVal String,Long , but\nwith cumulative set to false. \u00a0The\nApplication Window is set to one minute. We will explain how to set this\nlater.  The operator Chart:\nThis operator is very similar to the operator Quote, except that it takes inputs from\nHigh Low\u00a0and  Minute Vol\u00a0and outputs the consolidated tuples\nto the output port.  The operator PriceSMA:\nSMA stands for - Simple Moving Average. It reads from the\ninput port, which contains the price tuples from StockTickInput, and\nprovides the moving average price of the stock. \u00a0It utilizes\nSimpleMovingAverage String,Double , which is provided in the\n multiwindow\u00a0package.\nSimpleMovingAverage keeps track of the data of the previous N\napplication windows in a sliding manner. \u00a0For each end window event, it\nprovides the average of the data in those application windows.  The operator Console:\nThis operator just outputs the input tuples to the console\n(or stdout). \u00a0In this example, there are four console\u00a0operators, which connect to the output\nof  Quote, Chart, PriceSMA and VolumeSMA. \u00a0In\npractice, they should be replaced by operators that use the data to\nproduce visualization artifacts like charts.  Connecting the operators together and constructing the\nDAG:\u00a0Now that we know the\noperators used, we will create the DAG, set the streaming window size,\ninstantiate the operators, and connect the operators together by adding\nstreams that connect the output ports with the input ports among those\noperators. \u00a0This code is in the file  YahooFinanceApplication.java. Refer to Figure 1\nagain for the graphical representation of the DAG. \u00a0The last method in\nthe code, namely getApplication(),\ndoes all that. \u00a0The rest of the methods are just for setting up the\noperators.  package com.datatorrent.demos.yahoofinance;\n\nimport com.datatorrent.api.ApplicationFactory;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DAG;\nimport com.datatorrent.api.Operator.InputPort;\nimport com.datatorrent.lib.io.ConsoleOutputOperator;\nimport com.datatorrent.lib.math.RangeKeyVal;\nimport com.datatorrent.lib.math.SumKeyVal;\nimport com.datatorrent.lib.multiwindow.SimpleMovingAverage;\nimport com.datatorrent.lib.stream.ConsolidatorKeyVal;\nimport com.datatorrent.lib.util.HighLow;\nimport org.apache.hadoop.conf.Configuration;\n\n/**\n * Yahoo! Finance application demo.  p \n *\n * Get Yahoo finance feed and calculate minute price range, minute volume, simple moving average of 5 minutes.\n */\npublic class Application implements StreamingApplication\n{\n  private int streamingWindowSizeMilliSeconds = 1000; // 1 second (default is 500ms)\n  private int appWindowCountMinute = 60;   // 1 minute\n  private int appWindowCountSMA = 5 * 60;  // 5 minute\n\n  /**\n   * Get actual Yahoo finance ticks of symbol, last price, total daily volume, and last traded price.\n   */\n  public StockTickInput getStockTickInputOperator(String name, DAG dag)\n  {\n    StockTickInput oper = dag.addOperator(name, StockTickInput.class);\n    oper.readIntervalMillis = 200;\n    return oper;\n  }\n\n  /**\n   * This sends total daily volume by adding volumes from each ticks.\n   */\n  public SumKeyVal String, Long  getDailyVolumeOperator(String name, DAG dag)\n  {\n    SumKeyVal String, Long  oper = dag.addOperator(name, new SumKeyVal String, Long ());\n    oper.setType(Long.class);\n    oper.setCumulative(true);\n    return oper;\n  }\n\n  /**\n   * Get aggregated volume of 1 minute and send at the end window of 1 minute.\n   */\n  public SumKeyVal String, Long  getMinuteVolumeOperator(String name, DAG dag, int appWindowCount)\n  {\n    SumKeyVal String, Long  oper = dag.addOperator(name, new SumKeyVal String, Long ());\n    oper.setType(Long.class);\n    oper.setEmitOnlyWhenChanged(true);\ndag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    return oper;\n  }\n\n  /**\n   * Get High-low range for 1 minute.\n   */\n  public RangeKeyVal String, Double  getHighLowOperator(String name, DAG dag, int appWindowCount)\n  {\n    RangeKeyVal String, Double  oper = dag.addOperator(name, new RangeKeyVal String, Double ());\n    dag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Quote (Merge price, daily volume, time)\n   */\n  public ConsolidatorKeyVal String,Double,Long,String,?,?  getQuoteOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal String,Double,Long,String,?,?  oper = dag.addOperator(name, new ConsolidatorKeyVal String,Double,Long,String,Object,Object ());\n    return oper;\n  }\n\n  /**\n   * Chart (Merge minute volume and minute high-low)\n   */\n  public ConsolidatorKeyVal String,HighLow,Long,?,?,?  getChartOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  oper = dag.addOperator(name, new ConsolidatorKeyVal String,HighLow,Long,Object,Object,Object ());\n    return oper;\n  }\n\n  /**\n   * Get simple moving average of price.\n   */\n  public SimpleMovingAverage String, Double  getPriceSimpleMovingAverageOperator(String name, DAG dag, int appWindowCount)\n  {\n    SimpleMovingAverage String, Double  oper = dag.addOperator(name, new SimpleMovingAverage String, Double ());\n    oper.setWindowSize(appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Get console for output.\n   */\n  public InputPort Object  getConsole(String name, /*String nodeName,*/ DAG dag, String prefix)\n  {\n    ConsoleOutputOperator oper = dag.addOperator(name, ConsoleOutputOperator.class);\n    oper.setStringFormat(prefix +  : %s );\n    return oper.input;\n  }\n\n  /**\n   * Create Yahoo Finance Application DAG.\n   */\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().put(DAG.STRAM_WINDOW_SIZE_MILLIS,streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator( StockTickInput , dag);\n    SumKeyVal String, Long  dailyVolume = getDailyVolumeOperator( DailyVolume , dag);\n    ConsolidatorKeyVal String,Double,Long,String,?,?  quoteOperator = getQuoteOperator( Quote , dag);\n\n    RangeKeyVal String, Double  highlow = getHighLowOperator( HighLow , dag, appWindowCountMinute);\n    SumKeyVal String, Long  minuteVolume = getMinuteVolumeOperator( MinuteVolume , dag, appWindowCountMinute);\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  chartOperator = getChartOperator( Chart , dag);\n\n    SimpleMovingAverage String, Double  priceSMA = getPriceSimpleMovingAverageOperator( PriceSMA , dag, appWindowCountSMA);\n       DefaultPartitionCodec String, Double  codec = new DefaultPartitionCodec String, Double ();\n    dag.setInputPortAttribute(highlow.data, PortContext.STREAM_CODEC, codec);\n    dag.setInputPortAttribute(priceSMA.data, PortContext.STREAM_CODEC, codec);\n    dag.addStream( price , tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream( vol , tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream( time , tick.time, quoteOperator.in3);\n    dag.addStream( daily_vol , dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream( quote_data , quoteOperator.out, getConsole( quoteConsole , dag,  QUOTE ));\n\n    dag.addStream( high_low , highlow.range, chartOperator.in1);\n    dag.addStream( vol_1min , minuteVolume.sum, chartOperator.in2);\n    dag.addStream( chart_data , chartOperator.out, getConsole( chartConsole , dag,  CHART ));\n\n    dag.addStream( sma_price , priceSMA.doubleSMA, getConsole( priceSMAConsole , dag,  Price SMA ));\n\n    return dag;\n  }\n\n}  Note that we also set a user-specific sliding window for SMA that\nkeeps track of the previous N data points. \u00a0Do not confuse this with the\nattribute APPLICATION_WINDOW_COUNT.  In the rest of this chapter we will run through the process of\nrunning this application. We assume that \u00a0you are familiar with details\nof your Hadoop infrastructure. For installation\ndetails please refer to the  Installation Guide .", 
            "title": "Test Application: Yahoo! Finance Quotes"
        }, 
        {
            "location": "/application_development/#running-a-test-application_1", 
            "text": "We will now describe how to run the yahoo\nfinance application\u00a0described above in different modes\n(local mode, single node on Hadoop, and multi-nodes on Hadoop).  The platform runs streaming applications under the control of a\nlight-weight Streaming Application Manager (STRAM). Each application has\nits own instance of STRAM. STRAM launches the application and\ncontinually provides run time monitoring, analysis, and takes action\nsuch as load scaling or outage recovery as needed. \u00a0We will discuss\nSTRAM in more detail in the next chapter.  The instructions below assume that the platform was installed in a\ndirectory  INSTALL_DIR  and the command line interface (CLI) will\nbe used to launch the demo application. An application can be run in local mode \u00a0 (in IDE or from command line) or on a   Hadoop cluster   .  To start the dtCli run  INSTALL_DIR /bin/dtcli  The command line prompt appears.  To start the application in local mode (the actual version number in the file name may differ)  dt  launch -local  INSTALL_DIR /yahoo-finance-demo-3.2.0-SNAPSHOT.apa  To terminate the application in local mode, enter Ctrl-C  Tu run the application on the Hadoop cluster (the actual version\nnumber in the file name may differ)  dt  launch  INSTALL_DIR /yahoo-finance-demo-3.2.0-SNAPSHOT.apa  To stop the application running in Hadoop, terminate it in the dtCli:  dt  kill-app  Executing the application in either mode includes the following\nsteps. At a top level, STRAM (Streaming Application Manager) validates\nthe application (DAG), translates the logical plan to the physical plan\nand then launches the execution engine. The mode determines the\nresources needed and how how they are used.", 
            "title": "Running a Test Application"
        }, 
        {
            "location": "/application_development/#local-mode", 
            "text": "In local mode, the application is run as a single-process\u00a0with multiple threads. Although a\nfew Hadoop classes are needed, there is no dependency on a Hadoop\ncluster or Hadoop services. The local file system is used in place of\nHDFS. This mode allows a quick run of an application in a single process\nsandbox, and hence is the most suitable to debug and analyze the\napplication logic. This mode is recommended for developing the\napplication and can be used for running applications within the IDE for\nfunctional testing purposes. Due to limited resources and lack \u00a0of\nscalability an application running in this single process mode is more\nlikely to encounter throughput bottlenecks. A distributed cluster is\nrecommended for benchmarking and production testing.", 
            "title": "Local Mode"
        }, 
        {
            "location": "/application_development/#hadoop-cluster", 
            "text": "In this section we discuss various Hadoop cluster setups.", 
            "title": "Hadoop Cluster"
        }, 
        {
            "location": "/application_development/#single-node-cluster", 
            "text": "In a single node Hadoop cluster all services are deployed on a\nsingle server (a developer can use his/her development machine as a\nsingle node cluster). The platform does not distinguish between a single\nor multi-node setup and behaves exactly the same in both cases.  In this mode, the resource manager, name node, data node, and node\nmanager occupy one process each. This is an example of running a\nstreaming application as a multi-process\u00a0application on the same server.\nWith prevalence of fast, multi-core systems, this mode is effective for\ndebugging, fine tuning, and generic analysis before submitting the job\nto a larger Hadoop cluster. In this mode, execution uses the Hadoop\nservices and hence is likely to identify issues that are related to the\nHadoop environment (such issues will not be uncovered in local mode).\nThe throughput will obviously not be as high as on a multi-node Hadoop\ncluster. Additionally, since each container (i.e. Java process) requires\na significant amount of memory, you will be able to run a much smaller\nnumber of containers than on a multi-node cluster.", 
            "title": "Single Node Cluster"
        }, 
        {
            "location": "/application_development/#multi-node-cluster", 
            "text": "In a multi-node Hadoop cluster all the services of Hadoop are\ntypically distributed across multiple nodes in a production or\nproduction-level test environment. Upon launch the application is\nsubmitted to the Hadoop cluster and executes as a  multi-processapplication on\u00a0multiple nodes.  Before you start deploying, testing and troubleshooting your\napplication on a cluster, you should ensure that Hadoop (version 2.2.0\nor later)\u00a0is properly installed and\nyou have basic skills for working with it.", 
            "title": "Multi-Node Cluster"
        }, 
        {
            "location": "/application_development/#apache-apex-platform-overview", 
            "text": "", 
            "title": "Apache Apex Platform Overview"
        }, 
        {
            "location": "/application_development/#streaming-computational-model", 
            "text": "In this chapter, we describe the the basics of the real-time streaming platform and its computational model.  The platform is designed to enable completely asynchronous real time computations\u00a0done in as unblocked a way as possible with\nminimal overhead .  Applications running in the platform are represented by a Directed\nAcyclic Graph (DAG) made up of \u00a0operators and streams. All computations\nare done in memory on arrival of\nthe input data, with an option to save the output to disk (HDFS) in a\nnon-blocking way. The data that flows between operators consists of\natomic data elements. Each data element along with its type definition\n(henceforth called  schema) is\ncalled a tuple.\u00a0An application is a\ndesign of the flow of these tuples to and from\nthe appropriate compute units to enable the computation of the final\ndesired results.\u00a0A message queue (henceforth called\n\u00a0buffer server) manages tuples streaming\nbetween compute units in different processes.This server keeps track of\nall consumers, publishers, partitions, and enables replay. More\ninformation is given in later section.  The streaming application is monitored by a decision making entity\ncalled STRAM (streaming application\nmanager).\u00a0STRAM is designed to be a light weight\ncontroller that has minimal but sufficient interaction with the\napplication. This is done via periodic heartbeats. The\nSTRAM does the initial launch and periodically analyzes the system\nmetrics to decide if any run time action needs to be taken.  A fundamental building block for the streaming platform\nis the concept of breaking up a stream into equal finite time slices\ncalled streaming windows. Each window contains the ordered\nset of tuples in that time slice. A typical duration of a window is 500\nms, but can be configured per application (the Yahoo! Finance\napplication configures this value in the  properties.xml\u00a0file to be 1000ms = 1s). Each\nwindow is preceded by a begin_window\u00a0event and is terminated by an\nend_window\u00a0event, and is assigned\na unique window ID. Even though the platform performs computations at\nthe tuple level, bookkeeping is done at the window boundary, making the\ncomputations within a window an atomic event in the platform. \u00a0We can\nthink of each window as an  atomic\nmicro-batch\u00a0of tuples, to be processed together as one\natomic operation (See Figure 2). \u00a0  This atomic batching allows the platform to avoid the very steep\nper tuple bookkeeping cost and instead has a manageable per batch\nbookkeeping cost. This translates to higher throughput, low recovery\ntime, and higher scalability. Later in this document we illustrate how\nthe atomic micro-batch concept allows more efficient optimization\nalgorithms.  The platform also has in-built support for\napplication windows.\u00a0 An application window is part of the\napplication specification, and can be a small or large multiple of the\nstreaming window. \u00a0An example from our Yahoo! Finance test application\nis the moving average, calculated over a sliding application window of 5\nminutes which equates to 300 (= 5 * 60) streaming windows.  Note that these two window concepts are distinct. \u00a0A streaming\nwindow is an abstraction of many tuples into a higher atomic event for\neasier management. \u00a0An application window is a group of consecutive\nstreaming windows used for data aggregation (e.g. sum, average, maximum,\nminimum) on a per operator level.   Alongside the platform,\u00a0a set of\npredefined, benchmarked standard library operator templates is provided\nfor ease of use and rapid development of application.\u00a0These\noperators are open sourced to Apache Software Foundation under the\nproject name \u201cMalhar\u201d as part of our efforts to foster community\ninnovation. These operators can be used in a DAG as is, while others\nhave   properties  \u00a0 that can be set to specify the\ndesired computation. Those interested in details, should refer to Apex Malhar Operator Library \n.  The platform is a Hadoop YARN native\napplication. It runs in a Hadoop cluster just like any\nother YARN application (MapReduce etc.) and is designed to seamlessly\nintegrate with rest of Hadoop technology stack. It leverages Hadoop as\nmuch as possible and relies on it as its distributed operating system.\nHadoop dependencies include resource management, compute/memory/network\nallocation, HDFS, security, fault tolerance, monitoring, metrics,\nmulti-tenancy, logging etc. Hadoop classes/concepts are reused as much\nas possible.  The aim is to enable enterprises\nto leverage their existing Hadoop infrastructure for real time streaming\napplications. The platform is designed to scale with big\ndata applications and scale with Hadoop.  A streaming application is an asynchronous execution of\ncomputations across distributed nodes. All computations are done in\nparallel on a distributed cluster. The computation model is designed to\ndo as many parallel computations as possible in a non-blocking fashion.\nThe task of monitoring of the entire application is done on (streaming)\nwindow boundaries with a streaming window as an atomic entity. A window\ncompletion is a quantum of work done. There is no assumption that an\noperator can be interrupted at precisely a particular tuple or window.  An operator itself also\ncannot assume or predict the exact time a tuple that it emitted would\nget consumed by downstream operators. The operator processes the tuples\nit gets and simply emits new tuples based on its business logic. The\nonly guarantee it has is that the upstream operators are processing\neither the current or some later window, and the downstream operator is\nprocessing either the current or some earlier window. The completion of\na window (i.e. propagation of the  end_window\u00a0event through an operator) in any\noperator guarantees that all upstream operators have finished processing\nthis window. Thus, the end_window\u00a0event is blocking on an operator\nwith multiple outputs, and is a synchronization point in the DAG. The\n begin_window\u00a0event does not have\nany such restriction, a single begin_window\u00a0event from any upstream operator\ntriggers the operator to start processing tuples.", 
            "title": "Streaming Computational Model"
        }, 
        {
            "location": "/application_development/#streaming-application-manager-stram", 
            "text": "Streaming Application Manager (STRAM) is the Hadoop YARN native\napplication master. STRAM is the first process that is activated upon\napplication launch and orchestrates the streaming application on the\nplatform. STRAM is a lightweight controller process. The\nresponsibilities of STRAM include    Running the Application   Read the\u00a0logical plan\u00a0of the application (DAG) submitted by the client  Validate the logical plan  Translate the logical plan into a physical plan, where certain operators may  be partitioned (i.e. replicated) to multiple operators for  handling load.  Request resources (Hadoop containers) from Resource Manager,\n    per physical plan  Based on acquired resources and application attributes, create\n    an execution plan\u00a0by partitioning the DAG into fragments,\n    each assigned to different containers.  Executes the application by deploying each fragment to\n    its container. Containers then start stream processing and run\n    autonomously, processing one streaming window after another. Each\n    container is represented as an instance of the  StreamingContainer\u00a0class, which updates\n    STRAM via the heartbeat protocol and processes directions received\n    from STRAM.     Continually monitoring the application via heartbeats from each StreamingContainer   Collecting Application System Statistics and Logs  Logging all application-wide decisions taken  Providing system data on the state of the application via a  Web Service.   Supporting  Fault Tolerance  a.  Detecting a node outage\nb.  Requesting a replacement resource from the Resource Manager\n    and scheduling state restoration for the streaming operators\nc.  Saving state to Zookeeper    Supporting  Dynamic\n    Partitioning : \u00a0Periodically\n    evaluating the SLA and modifying the physical plan if required\n    (logical plan does not change).   Enabling  Security : \u00a0Distributing\n    security tokens for distributed components of the execution engine\n    and securing web service requests.  Enabling  Dynamic  modification \u00a0 of\n    DAG: In the future, we intend to allow for user initiated\n    modification of the logical plan to allow for changes to the\n    processing logic and functionality.   An example of the Yahoo! Finance Quote application scheduled on a\ncluster of 5 Hadoop containers (processes) is shown in Figure 3.   An example for the translation from a logical plan to a physical\nplan and an execution plan for a subset of the application is shown in\nFigure 4.", 
            "title": "Streaming Application Manager (STRAM)"
        }, 
        {
            "location": "/application_development/#hadoop-components", 
            "text": "In this section we cover some aspects of Hadoop that your\nstreaming application interacts with. This section is not meant to\neducate the reader on Hadoop, but just get the reader acquainted with\nthe terms. We strongly advise readers to learn Hadoop from other\nsources.  A streaming application runs as a native Hadoop 2.2 application.\nHadoop 2.2 does not differentiate between a map-reduce job and other\napplications, and hence as far as Hadoop is concerned, the streaming\napplication is just another job. This means that your application\nleverages all the bells and whistles Hadoop provides and is fully\nsupported within Hadoop technology stack. The platform is responsible\nfor properly integrating itself with the relevant components of Hadoop\nthat exist today and those that may emerge in the future  All investments that leverage multi-tenancy (for example quotas\nand queues), security (for example kerberos), data flow integration (for\nexample copying data in-out of HDFS), monitoring, metrics collections,\netc. will require no changes when streaming applications run on\nHadoop.", 
            "title": "Hadoop Components"
        }, 
        {
            "location": "/application_development/#yarn", 
            "text": "YARN is\nthe core library of Hadoop 2.2 that is tasked with resource management\nand works as a distributed application framework. In this section we\nwill walk through Yarn's components. In Hadoop 2.2, the old jobTracker\nhas been replaced by a combination of ResourceManager (RM) and\nApplicationMaster (AM).", 
            "title": "YARN"
        }, 
        {
            "location": "/application_development/#resource-manager-rm", 
            "text": "ResourceManager (RM)\nmanages all the distributed resources. It allocates and arbitrates all\nthe slots and the resources (cpu, memory, network) of these slots. It\nworks with per-node NodeManagers (NMs) and per-application\nApplicationMasters (AMs). Currently memory usage is monitored by RM; in\nupcoming releases it will have CPU as well as network management. RM is\nshared by map-reduce and streaming applications. Running streaming\napplications requires no changes in the RM.", 
            "title": "Resource Manager (RM)"
        }, 
        {
            "location": "/application_development/#application-master-am", 
            "text": "The AM is the watchdog or monitoring process for your application\nand has the responsibility of negotiating resources with RM and\ninteracting with NodeManagers to get the allocated containers started.\nThe AM is the starting point of your application and is considered user\ncode (not system Hadoop code). The AM itself runs in one container. All\nresource management within the application are managed by the AM. This\nis a critical feature for Hadoop 2.2 where tasks done by jobTracker in\nHadoop 1.0 have been distributed allowing Hadoop 2.2 to scale much\nbeyond Hadoop 1.0. STRAM is a native YARN ApplicationManager.", 
            "title": "Application Master (AM)"
        }, 
        {
            "location": "/application_development/#node-managers-nm", 
            "text": "There is one  NodeManager (NM)\nper node in the cluster. All the containers (i.e. processes) on that\nnode are monitored by the NM. It takes instructions from RM and manages\nresources of that node as per RM instructions. NMs interactions are same\nfor map-reduce and for streaming applications. Running streaming\napplications requires no changes in the NM.", 
            "title": "Node Managers (NM)"
        }, 
        {
            "location": "/application_development/#rpc-protocol", 
            "text": "Communication among RM, AM, and NM is done via the Hadoop RPC\nprotocol. Streaming applications use the same protocol to send their\ndata. No changes are needed in RPC support provided by Hadoop to enable\ncommunication done by components of your application.", 
            "title": "RPC Protocol"
        }, 
        {
            "location": "/application_development/#hdfs", 
            "text": "Hadoop includes a highly fault tolerant, high throughput\ndistributed file system ( HDFS ).\nIt runs on commodity hardware, and your streaming application will, by\ndefault, use it. There is no difference between files created by a\nstreaming application and those created by map-reduce.", 
            "title": "HDFS"
        }, 
        {
            "location": "/application_development/#developing-an-application", 
            "text": "In this chapter we describe the methodology to develop an\napplication using the Realtime Streaming Platform. The platform was\ndesigned to make it easy to build and launch sophisticated streaming\napplications with the developer having to deal only with the\napplication/business logic. The platform deals with details of where to\nrun what operators on which servers and how to correctly route streams\nof data among them.", 
            "title": "Developing An Application"
        }, 
        {
            "location": "/application_development/#development-process", 
            "text": "While the platform does not mandate a specific methodology or set\nof development tools, we have recommendations to maximize productivity\nfor the different phases of application development.", 
            "title": "Development Process"
        }, 
        {
            "location": "/application_development/#design", 
            "text": "Identify common, reusable operators. Use a library\n    if possible.  Identify scalability and performance requirements before\n    designing the DAG.  Leverage attributes that the platform supports for scalability\n    and performance.  Use operators that are benchmarked and tested so that later\n    surprises are minimized. If you have glue code, create appropriate\n    unit tests for it.  Use THREAD_LOCAL locality for high throughput streams. If all\n    the operators on that stream cannot fit in one container,\n    try\u00a0NODE_LOCAL\u00a0locality. Both THREAD_LOCAL and\n    NODE_LOCAL streams avoid the Network Interface Card (NIC)\n    completly. The former uses intra-process communication to also avoid\n    serialization-deserialization overhead.  The overall throughput and latencies are are not necessarily\n    correlated to the number of operators in a simple way -- the\n    relationship is more nuanced. A lot depends on how much work\n    individual operators are doing, how many are able to operate in\n    parallel, and how much data is flowing through the arcs of the DAG.\n    It is, at times, better to break a computation down into its\n    constituent simple parts and then stitch them together via streams\n    to better utilize the compute resources of the cluster. Decide on a\n    per application basis the fine line between complexity of each\n    operator vs too many streams. Doing multiple computations in one\n    operator does save network I/O, while operators that are too complex\n    are hard to maintain.  Do not use operators that depend on the order of two streams\n    as far as possible. In such cases behavior is not idempotent.  Persist key information to HDFS if possible; it may be useful\n    for debugging later.  Decide on an appropriate fault tolerance mechanism. If some\n    data loss is acceptable, use the at-most-once mechanism as it has\n    fastest recovery.", 
            "title": "Design"
        }, 
        {
            "location": "/application_development/#creating-new-project", 
            "text": "Please refer to the  Apex Application Packages \u00a0for\nthe basic steps for creating a new project.", 
            "title": "Creating New Project"
        }, 
        {
            "location": "/application_development/#writing-the-application-code", 
            "text": "Preferably use an IDE (Eclipse, Netbeans etc.) that allows you to\nmanage dependencies and assists with the Java coding. Specific benefits\ninclude ease of managing operator library jar files, individual operator\nclasses, ports and properties. It will also highlight and assist to\nrectify issues such as type mismatches when adding streams while\ntyping.", 
            "title": "Writing the application code"
        }, 
        {
            "location": "/application_development/#testing", 
            "text": "Write test cases with JUnit or similar test framework so that code\nis tested as it is written. For such testing, the DAG can run in local\nmode within the IDE. Doing this may involve writing mock input or output\noperators for the integration points with external systems. For example,\ninstead of reading from a live data stream, the application in test mode\ncan read from and write to files. This can be done with a single\napplication DAG by instrumenting a test mode using settings in the\nconfiguration that is passed to the application factory\ninterface.  Good test coverage will not only eliminate basic validation errors\nsuch as missing port connections or property constraint violations, but\nalso validate the correct processing of the data. The same tests can be\nre-run whenever the application or its dependencies change (operator\nlibraries, version of the platform etc.)", 
            "title": "Testing"
        }, 
        {
            "location": "/application_development/#running-an-application", 
            "text": "The platform provides a commandline tool called dtcli\u00a0for managing applications (launching,\nkilling, viewing, etc.). This tool was already discussed above briefly\nin the section entitled Running the Test Application. It will introspect\nthe jar file specified with the launch command for applications (classes\nthat implement ApplicationFactory) or property files that define\napplications. It will also deploy the dependency jar files from the\napplication package to the cluster.  Dtcli can run the application in local mode (i.e. outside a\ncluster). It is recommended to first run the application in local mode\nin the development environment before launching on the Hadoop cluster.\nThis way some of the external system integration and correct\nfunctionality of the application can be verified in an easier to debug\nenvironment before testing distributed mode.  For more details on CLI please refer to the  dtCli Guide .", 
            "title": "Running an application"
        }, 
        {
            "location": "/application_development/#application-api", 
            "text": "This section introduces the API to write a streaming application.\nThe work involves connecting operators via streams to form the logical\nDAG. The steps are    Instantiate an application (DAG)    (Optional) Set Attributes   Assign application name  Set any other attributes as per application requirements     Create/re-use and instantiate operators   Assign operator name that is unique within the  application  Declare schema upfront for each operator (and thereby its   ports )  (Optional) Set  properties \u00a0  and  attributes \u00a0  on the dag as per specification  Connect ports of operators via streams  Each stream connects one output port of an operator to one or  more input ports of other operators.  (Optional) Set attributes on the streams       Test the application.    There are two methods to create an application, namely Java, and\nProperties file. Java API is for applications being developed by humans,\nand properties file (Hadoop like) is more suited for DAGs generated by\ntools.", 
            "title": "Application API"
        }, 
        {
            "location": "/application_development/#java-api", 
            "text": "The Java API is the most common way to create a streaming\napplication. It is meant for application developers who prefer to\nleverage the features of Java, and the ease of use and enhanced\nproductivity provided by IDEs like NetBeans or Eclipse. Using Java to\nspecify the application provides extra validation abilities of Java\ncompiler, such as compile time checks for type safety at the time of\nwriting the code. Later in this chapter you can read more about\nvalidation support in the platform.  The developer specifies the streaming application by implementing\nthe ApplicationFactory interface, which is how platform tools (CLI etc.)\nrecognize and instantiate applications. Here we show how to create a\nYahoo! Finance application that streams the last trade price of a ticker\nand computes the high and low price in every 1 min window. Run above\n test application\u00a0to execute the\nDAG in local mode within the IDE.  Let us revisit how the Yahoo! Finance test application constructs the DAG:  public class Application implements StreamingApplication\n{\n\n  ...\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().attr(DAG.STRAM_WINDOW_SIZE_MILLIS).set(streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator( StockTickInput , dag);\n    SumKeyVal String, Long  dailyVolume = getDailyVolumeOperator( DailyVolume , dag);\n    ConsolidatorKeyVal String,Double,Long,String,?,?  quoteOperator = getQuoteOperator( Quote , dag);\n\n    RangeKeyVal String, Double  highlow = getHighLowOperator( HighLow , dag, appWindowCountMinute);\n    SumKeyVal String, Long  minuteVolume = getMinuteVolumeOperator( MinuteVolume , dag, appWindowCountMinute);\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  chartOperator = getChartOperator( Chart , dag);\n\n    SimpleMovingAverage String, Double  priceSMA = getPriceSimpleMovingAverageOperator( PriceSMA , dag, appWindowCountSMA);\n\n    dag.addStream( price , tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream( vol , tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream( time , tick.time, quoteOperator.in3);\n    dag.addStream( daily_vol , dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream( quote_data , quoteOperator.out, getConsole( quoteConsole , dag,  QUOTE ));\n\n    dag.addStream( high_low , highlow.range, chartOperator.in1);\n    dag.addStream( vol_1min , minuteVolume.sum, chartOperator.in2);\n    dag.addStream( chart_data , chartOperator.out, getConsole( chartConsole , dag,  CHART ));\n\n    dag.addStream( sma_price , priceSMA.doubleSMA, getConsole( priceSMAConsole , dag,  Price SMA ));\n\n    return dag;\n  }\n}", 
            "title": "Java API"
        }, 
        {
            "location": "/application_development/#property-file-api", 
            "text": "The platform also supports specification of a DAG via a property\nfile. The aim here to make it easy for tools to create and run an\napplication. This method of specification does not have the Java\ncompiler support of compile time check, but since these applications\nwould be created by software, they should be correct by construction.\nThe syntax is derived from Hadoop properties and should be easy for\nfolks who are used to creating software that integrated with\nHadoop.  Create an application (DAG): myApplication.properties  # input operator that reads from a file\ndt.operator.inputOp.classname=com.acme.SampleInputOperator\ndt.operator.inputOp.fileName=somefile.txt\n\n# output operator that writes to the console\ndt.operator.outputOp.classname=com.acme.ConsoleOutputOperator\n\n# stream connecting both operators\ndt.stream.inputStream.source=inputOp.outputPort\ndt.stream.inputStream.sinks=outputOp.inputPort  Above snippet is intended to convey the basic idea of specifying\nthe DAG without using Java. Operators would come from a predefined\nlibrary and referenced in the specification by class name and port names\n(obtained from the library providers documentation or runtime\nintrospection by tools). For those interested in details, see later\nsections and refer to the  Operation and\nInstallation Guide\u00a0mentioned above.", 
            "title": "Property File API"
        }, 
        {
            "location": "/application_development/#attributes", 
            "text": "Attributes impact the runtime behavior of the application. They do\nnot impact the functionality. An example of an attribute is application\nname. Setting it changes the application name. Another example is\nstreaming window size. Setting it changes the streaming window size from\nthe default value to the specified value. Users cannot add new\nattributes, they can only choose from the ones that come packaged and\npre-supported by the platform. Details of attributes are covered in the\n Operation and Installation\nGuide.", 
            "title": "Attributes"
        }, 
        {
            "location": "/application_development/#operators", 
            "text": "Operators\u00a0are basic compute units.\nOperators process each incoming tuple and emit zero or more tuples on\noutput ports as per the business logic. The data flow, connectivity,\nfault tolerance (node outage), etc. is taken care of by the platform. As\nan operator developer, all that is needed is to figure out what to do\nwith the incoming tuple and when (and which output port) to send out a\nparticular output tuple. Correctly designed operators will most likely\nget reused. Operator design needs care and foresight. For details, refer\nto the   Operator Developer Guide . As an application developer you need to connect operators\nin a way that it implements your business logic. You may also require\noperator customization for functionality and use attributes for\nperformance/scalability etc.  All operators process tuples asynchronously in a distributed\ncluster. An operator cannot assume or predict the exact time a tuple\nthat it emitted will get consumed by a downstream operator. An operator\nalso cannot predict the exact time when a tuple arrives from an upstream\noperator. The only guarantee is that the upstream operators are\nprocessing the current or a future window, i.e. the windowId of upstream\noperator is equals or exceeds its own windowId. Conversely the windowId\nof a downstream operator is less than or equals its own windowId. The\nend of a window operation, i.e. the API call to endWindow on an operator\nrequires that all upstream operators have finished processing this\nwindow. This means that completion of processing a window propagates in\na blocking fashion through an operator. Later sections provides more\ndetails on streams and data flow of tuples.  Each operator has a unique name within the DAG as provided by the\nuser. This is the name of the operator in the logical plan. The name of\nthe operator in the physical plan is an integer assigned to it by STRAM.\nThese integers are use the sequence from 1 to N, where N is total number\nof physically unique operators in the DAG. \u00a0Following the same rule,\neach partitioned instance of a logical operator has its own integer as\nan id. This id along with the Hadoop container name uniquely identifies\nthe operator in the execution plan of the DAG. The logical names and the\nphysical names are required for web service support. Operators can be\naccessed via both names. These same names are used while interacting\nwith  dtcli\u00a0to access an operator.\nIdeally these names should be self-descriptive. For example in Figure 1,\nthe node named \u201cDaily volume\u201d has a physical identifier of 2.", 
            "title": "Operators"
        }, 
        {
            "location": "/application_development/#operator-interface", 
            "text": "Operator interface in a DAG consists of  ports , \u00a0 properties , \u00a0and\n  attributes  . \u00a0Operators interact with other\ncomponents of the DAG via ports. Functional behavior of the operators\ncan be customized via parameters. Run time performance and physical\ninstantiation is controlled by attributes. Ports and parameters are\nfields (variables) of the Operator class/object, while attributes are\nmeta information that is attached to the operator object via an\nAttributeMap. An operator must have at least one port. Properties are\noptional. Attributes are provided by the platform and always have a\ndefault value that enables normal functioning of operators.", 
            "title": "Operator Interface"
        }, 
        {
            "location": "/application_development/#ports", 
            "text": "Ports are connection points by which an operator receives and\nemits tuples. These should be transient objects instantiated in the\noperator object, that implement particular interfaces. Ports should be\ntransient as they contain no state. They have a pre-defined schema and\ncan only be connected to other ports with the same schema. An input port\nneeds to implement the interface  Operator.InputPort\u00a0and\ninterface Sink. A default\nimplementation of these is provided by the abstract class DefaultInputPort. An output port needs to\nimplement the interface  Operator.OutputPort. A default implementation\nof this is provided by the concrete class DefaultOutputPort. These two are a quick way to\nimplement the above interfaces, but operator developers have the option\nof providing their own implementations.  Here are examples of an input and an output port from the operator\nSum.  @InputPortFieldAnnotation(name =  data )\npublic final transient DefaultInputPort V  data = new DefaultInputPort V () {\n  @Override\n  public void process(V tuple)\n  {\n    ...\n  }\n}\n@OutputPortFieldAnnotation(optional=true)\npublic final transient DefaultOutputPort V  sum = new DefaultOutputPort V (){ \u2026 };  The process call is in the Sink interface. An emit on an output\nport is done via emit(tuple) call. For the above example it would be\nsum.emit(t), where the type of t is the generic parameter V.  There is no limit on how many ports an operator can have. However\nany operator must have at least one port. An operator with only one port\nis called an Input Adapter if it has no input port and an Output Adapter\nif it has no output port. These are special operators needed to get/read\ndata from outside system/source into the application, or push/write data\ninto an outside system/sink. These could be in Hadoop or outside of\nHadoop. These two operators are in essence gateways for the streaming\napplication to communicate with systems outside the application.  Port connectivity can be validated during compile time by adding\nPortFieldAnnotations shown above. By default all ports have to be\nconnected, to allow a port to go unconnected, you need to add\n\u201coptional=true\u201d to the annotation.  Attributes can be specified for ports that affect the runtime\nbehavior. An example of an attribute is parallel partition that specifes\na parallel computation flow per partition. It is described in detail in\nthe  Parallel\nPartitions \u00a0 section.\nAnother example is queue capacity that specifies the buffer size for the\nport. Details of attributes are covered in  Operation and Installation Guide.", 
            "title": "Ports"
        }, 
        {
            "location": "/application_development/#properties", 
            "text": "Properties are the abstractions by which functional behavior of an\noperator can be customized. They should be non-transient objects\ninstantiated in the operator object. They need to be non-transient since\nthey are part of the operator state and re-construction of the operator\nobject from its checkpointed state must restore the operator to the\ndesired state. Properties are optional, i.e. an operator may or may not\nhave properties; they are part of user code and their values are not\ninterpreted by the platform in any way.  All non-serializable objects should be declared transient.\nExamples include sockets, session information, etc. These objects should\nbe initialized during setup call, which is called every time the\noperator is initialized.", 
            "title": "Properties"
        }, 
        {
            "location": "/application_development/#attributes_1", 
            "text": "Attributes are values assigned to the operators that impact\nrun-time. This includes things like the number of partitions, at most\nonce or at least once or exactly once recovery modes, etc. Attributes do\nnot impact functionality of the operator. Users can change certain\nattributes in runtime. Users cannot add attributes to operators; they\nare pre-defined by the platform. They are interpreted by the platform\nand thus cannot be defined in user created code (like properties).\nDetails of attributes are covered in   Configuration Guide .", 
            "title": "Attributes"
        }, 
        {
            "location": "/application_development/#operator-state", 
            "text": "The state of an operator is defined as the data that it transfers\nfrom one window to a future window. Since the computing model of the\nplatform is to treat windows like micro-batches, the operator state can\nbe  checkpointed \u00a0 every\nNth window, or every T units of time, where T is significantly greater\nthan the streaming window. \u00a0When an operator is checkpointed, the entire\nobject is written to HDFS. \u00a0The larger the amount of state in an\noperator, the longer it takes to recover from a failure. A stateless\noperator can recover much quicker than a stateful one. The needed\nwindows are preserved by the upstream buffer server and are used to\nrecompute the lost windows, and also rebuild the buffer server in the\ncurrent container.  The distinction between Stateless and Stateful is based solely on\nthe need to transfer data in the operator from one window to the next.\nThe state of an operator is independent of the number of ports.", 
            "title": "Operator State"
        }, 
        {
            "location": "/application_development/#stateless", 
            "text": "A Stateless operator is defined as one where no data is needed to\nbe kept at the end of every window. This means that all the computations\nof a window can be derived from all the tuples the operator receives\nwithin that window. This guarantees that the output of any window can be\nreconstructed by simply replaying the tuples that arrived in that\nwindow. Stateless operators are more efficient in terms of fault\ntolerance, and cost to achieve SLA.", 
            "title": "Stateless"
        }, 
        {
            "location": "/application_development/#stateful", 
            "text": "A Stateful operator is defined as one where data is needed to be\nstored at the end of a window for computations occurring in later\nwindow; a common example is the computation of a sum of values in the\ninput tuples.", 
            "title": "Stateful"
        }, 
        {
            "location": "/application_development/#operator-api", 
            "text": "The Operator API consists of methods that operator developers may\nneed to override. In this section we will discuss the Operator APIs from\nthe point of view of an application developer. Knowledge of how an\noperator works internally is critical for writing an application. Those\ninterested in the details should refer to  Malhar Operator Developer Guide.  The APIs are available in three modes, namely Single Streaming\nWindow, Sliding Application Window, and Aggregate Application Window.\nThese are not mutually exclusive, i.e. an operator can use single\nstreaming window as well as sliding application window. A physical\ninstance of an operator is always processing tuples from a single\nwindow. The processing of tuples is guaranteed to be sequential, no\nmatter which input port the tuples arrive on.  In the later part of this section we will evaluate three common\nuses of streaming windows by applications. They have different\ncharacteristics and implications on optimization and recovery mechanisms\n(i.e. algorithm used to recover a node after outage) as discussed later\nin the section.", 
            "title": "Operator API"
        }, 
        {
            "location": "/application_development/#streaming-window", 
            "text": "Streaming window is atomic micro-batch computation period. The API\nmethods relating to a streaming window are as follows  public void process( tuple_type  tuple) // Called on the input port on which the tuple arrives\npublic void beginWindow(long windowId) // Called at the start of the window as soon as the first begin_window tuple arrives\npublic void endWindow() // Called at the end of the window after end_window tuples arrive on all input ports\npublic void setup(OperatorContext context) // Called once during initialization of the operator\npublic void teardown() // Called once when the operator is being shutdown  A tuple can be emitted in any of the three streaming run-time\ncalls, namely beginWindow, process, and endWindow but not in setup or\nteardown.", 
            "title": "Streaming Window"
        }, 
        {
            "location": "/application_development/#aggregate-application-window", 
            "text": "An operator with an aggregate window is stateful within the\napplication window timeframe and possibly stateless at the end of that\napplication window. An size of an aggregate application window is an\noperator attribute and is defined as a multiple of the streaming window\nsize. The platform recognizes this attribute and optimizes the operator.\nThe beginWindow, and endWindow calls are not invoked for those streaming\nwindows that do not align with the application window. For example in\ncase of streaming window of 0.5 second and application window of 5\nminute, an application window spans 600 streaming windows (5*60*2 =\n600). At the start of the sequence of these 600 atomic streaming\nwindows, a beginWindow gets invoked, and at the end of these 600\nstreaming windows an endWindow gets invoked. All the intermediate\nstreaming windows do not invoke beginWindow or endWindow. Bookkeeping,\nnode recovery, stats, UI, etc. continue to work off streaming windows.\nFor example if operators are being checkpointed say on an average every\n30th window, then the above application window would have about 20\ncheckpoints.", 
            "title": "Aggregate Application Window"
        }, 
        {
            "location": "/application_development/#sliding-application-window", 
            "text": "A sliding window is computations that requires previous N\nstreaming windows. After each streaming window the Nth past window is\ndropped and the new window is added to the computation. An operator with\nsliding window is a stateful operator at end of any window. The sliding\nwindow period is an attribute and is a multiple of streaming window. The\nplatform recognizes this attribute and leverages it during bookkeeping.\nA sliding aggregate window with tolerance to data loss does not have a\nvery high bookkeeping cost. The cost of all three recovery mechanisms,\n at most once\u00a0(data loss tolerant),\nat least once\u00a0(data loss\nintolerant), and exactly once\u00a0(data\nloss intolerant and no extra computations) is same as recovery\nmechanisms based on streaming window. STRAM is not able to leverage this\noperator for any extra optimization.", 
            "title": "Sliding Application Window"
        }, 
        {
            "location": "/application_development/#single-vs-multi-input-operator", 
            "text": "A single-input operator by definition has a single upstream\noperator, since there can only be one writing port for a stream. \u00a0If an\noperator has a single upstream operator, then the beginWindow on the\nupstream also blocks the beginWindow of the single-input operator. For\nan operator to start processing any window at least one upstream\noperator has to start processing that window. A multi-input operator\nreads from more than one upstream ports. Such an operator would start\nprocessing as soon as the first begin_window event arrives. However the\nwindow would not close (i.e. invoke endWindow) till all ports receive\nend_window events for that windowId. Thus the end of a window is a\nblocking event. As we saw earlier, a multi-input operator is also the\npoint in the DAG where windows of all upstream operators are\nsynchronized. The windows (atomic micro-batches) from a faster (or just\nahead in processing) upstream operators are queued up till the slower\nupstream operator catches up. STRAM monitors such bottlenecks and takes\ncorrective actions. The platform ensures minimal delay, i.e processing\nstarts as long as at least one upstream operator has started\nprocessing.", 
            "title": "Single vs Multi-Input Operator"
        }, 
        {
            "location": "/application_development/#recovery-mechanisms", 
            "text": "Application developers can set any of the recovery mechanisms\nbelow to deal with node outage. In general, the cost of recovery depends\non the state of the operator, while data integrity is dependant on the\napplication. The mechanisms are per window as the platform treats\nwindows as atomic compute units. Three recovery mechanisms are\nsupported, namely   At-least-once: All atomic batches are processed at least once.\n    No data loss occurs.  At-most-once: All atomic batches are processed at most once.\n    Data loss is possible; this is the most efficient setting.  Exactly-once: All atomic batches are processed exactly once.\n    No data loss occurs; this is the least efficient setting since\n    additional work is needed to ensure proper semantics.   At-least-once is the default. During a recovery event, the\noperator connects to the upstream buffer server and asks for windows to\nbe replayed. At-least-once and exactly-once mechanisms start from its\ncheckpointed state. At-most-once starts from the next begin-window\nevent.  Recovery mechanisms can be specified per Operator while writing\nthe application as shown below.  Operator o = dag.addOperator(\u201coperator\u201d, \u2026);\ndag.setAttribute(o,  OperatorContext.PROCESSING_MODE,  ProcessingMode.AT_MOST_ONCE);  Also note that once an operator is attributed to AT_MOST_ONCE,\nall the operators downstream to it have to be AT_MOST_ONCE. The client\nwill give appropriate warnings or errors if that\u2019s not the case.  Details are explained in the chapter on Fault Tolerance\nbelow .", 
            "title": "Recovery Mechanisms"
        }, 
        {
            "location": "/application_development/#streams", 
            "text": "A stream\u00a0is a connector\n(edge) abstraction, and is a fundamental building block of the platform.\nA stream consists of tuples that flow from one port (called the\noutput\u00a0port) to one or more ports\non other operators (called  input\u00a0ports) another -- so note a potentially\nconfusing aspect of this terminology: tuples enter a stream through its\noutput port and leave via one or more input ports. A stream has the\nfollowing characteristics   Tuples are always delivered in the same order in which they\n    were emitted.  Consists of a sequence of windows one after another. Each\n    window being a collection of in-order tuples.  A stream that connects two containers passes through a\n    buffer server.  All streams can be persisted (by default in HDFS).  Exactly one output port writes to the stream.  Can be read by one or more input ports.  Connects operators within an application, not outside\n    an application.  Has an unique name within an application.  Has attributes which act as hints to STRAM.   Streams have four modes, namely in-line, in-node, in-rack,\n    and other. Modes may be overruled (for example due to lack\n    of containers). They are defined as follows:   THREAD_LOCAL: In the same thread, uses thread\n    stack (intra-thread). This mode can only be used for a downstream\n    operator which has only one input port connected; also called\n    in-line.  CONTAINER_LOCAL: In the same container (intra-process); also\n    called in-container.  NODE_LOCAL: In the same Hadoop node (inter processes, skips\n    NIC); also called in-node.  RACK_LOCAL: On nodes in the same rack; also called\n    in-rack.  unspecified: No guarantee. Could be anywhere within the\n    cluster     An example of a stream declaration is given below  DAG dag = new DAG();\n \u2026\ndag.addStream( views , viewAggregate.sum, cost.data).setLocality(CONTAINER_LOCAL); // A container local  stream\ndag.addStream(\u201cclicks\u201d, clickAggregate.sum, rev.data); // An example of unspecified locality  The platform guarantees in-order delivery of tuples in a stream.\nSTRAM views each stream as collection of ordered windows. Since no tuple\ncan exist outside a window, a replay of a stream consists of replay of a\nset of windows. When multiple input ports read the same stream, the\nexecution plan of a stream ensures that each input port is logically not\nblocked by the reading of another input port. The schema of a stream is\nsame as the schema of the tuple.  In a stream all tuples emitted by an operator in a window belong\nto that window. A replay of this window would consists of an in-order\nreplay of all the tuples. Thus the tuple order within a stream is\nguaranteed. However since an operator may receive multiple streams (for\nexample an operator with two input ports), the order of arrival of two\ntuples belonging to different streams is not guaranteed. In general in\nan asynchronous distributed architecture this is expected. Thus the\noperator (specially one with multiple input ports) should not depend on\nthe tuple order from two streams. One way to cope with this\nindeterminate order, if necessary, is to wait to get all the tuples of a\nwindow and emit results in endWindow call. All operator templates\nprovided as part of   standard operator template\nlibrary   \u00a0 follow\nthese principles.  A logical stream gets partitioned into physical streams each\nconnecting the partition to the upstream operator. If two different\nattributes are needed on the same stream, it should be split using\nStreamDuplicator\u00a0operator.  Modes of the streams are critical for performance. An in-line\nstream is the most optimal as it simply delivers the tuple as-is without\nserialization-deserialization. Streams should be marked\ncontainer_local, specially in case where there is a large tuple volume\nbetween two operators which then on drops significantly. Since the\nsetLocality call merely provides a hint, STRAM may ignore it. An In-node\nstream is not as efficient as an in-line one, but it is clearly better\nthan going off-node since it still avoids the potential bottleneck of\nthe network card.  THREAD_LOCAL and CONTAINER_LOCAL streams do not use a buffer\nserver as this stream is in a single process. The other two do.", 
            "title": "Streams"
        }, 
        {
            "location": "/application_development/#validating-an-application", 
            "text": "The platform provides various ways of validating the application\nspecification and data input. An understanding of these checks is very\nimportant for an application developer since it affects productivity.\nValidation of an application is done in three phases, namely   Compile Time: Caught during application development, and is\n    most cost effective. These checks are mainly done on declarative\n    objects and leverages the Java compiler. An example is checking that\n    the schemas specified on all ports of a stream are\n    mutually compatible.  Initialization Time: When the application is being\n    initialized, before submitting to Hadoop. These checks are related\n    to configuration/context of an application, and are done by the\n    logical DAG builder implementation. An example is the checking that\n    all non-optional ports are connected to other ports.  Run Time: Validations done when the application is running.\n    This is the costliest of all checks. These are checks that can only\n    be done at runtime as they involve data. For example divide by 0\n    check as part of business logic.", 
            "title": "Validating an Application"
        }, 
        {
            "location": "/application_development/#compile-time", 
            "text": "Compile time validations apply when an application is specified in\nJava code and include all checks that can be done by Java compiler in\nthe development environment (including IDEs like NetBeans or Eclipse).\nExamples include   Schema Validation: The tuples on ports are POJO (plain old\n    java objects) and compiler checks to ensure that all the ports on a\n    stream have the same schema.  Stream Check: Single Output Port and at least one Input port\n    per stream. A stream can only have one output port writer. This is\n    part of the addStream api. This\n    check ensures that developers only connect one output port to\n    a stream. The same signature also ensures that there is at least one\n    input port for a stream  Naming: Compile time checks ensures that applications\n    components operators, streams are named", 
            "title": "Compile Time"
        }, 
        {
            "location": "/application_development/#initializationinstantiation-time", 
            "text": "Initialization time validations include various checks that are\ndone post compile, and before the application starts running in a\ncluster (or local mode). These are mainly configuration/contextual in\nnature. These checks are as critical to proper functionality of the\napplication as the compile time validations.  Examples include    JavaBeans Validation :\n    Examples include   @Max(): Value must be less than or equal to the number  @Min(): Value must be greater than or equal to the\n    number  @NotNull: The value of the field or property must not be\n    null  @Pattern(regexp = \u201c....\u201d): Value must match the regular\n    expression  Input port connectivity: By default, every non-optional input\n    port must be connected. A port can be declared optional by using an\n    annotation: \u00a0 \u00a0 @InputPortFieldAnnotation(name = \"...\", optional\n    = true)  Output Port Connectivity: Similar. The annotation here is: \u00a0 \u00a0\n    @OutputPortFieldAnnotation(name = \"...\", optional = true)     Unique names in application scope: Operators, streams, must have\n    unique names.   Cycles in the dag: DAG cannot have a cycle.  Unique names in operator scope: Ports, properties, annotations\n    must have unique names.  One stream per port: A port can connect to only one stream.\n    This check applies to input as well as output ports even though an\n    output port can technically write to two streams. If you must have\n    two streams originating from a single output port, use \u00a0a\u00a0streamDuplicator operator.  Application Window Period: Has to be an integral multiple the\n    streaming window period.", 
            "title": "Initialization/Instantiation Time"
        }, 
        {
            "location": "/application_development/#run-time", 
            "text": "Run time checks are those that are done when the application is\nrunning. The real-time streaming platform provides rich run time error\nhandling mechanisms. The checks are exclusively done by the application\nbusiness logic, but the platform allows applications to count and audit\nthese. Some of these features are in the process of development (backend\nand UI) and this section will be updated as they are developed. Upon\ncompletion examples will be added to   demos   t o\nillustrate these.  Error ports are output ports with error annotations. Since they\nare normal ports, they can be monitored and tuples counted, persisted\nand counts shown in the UI.", 
            "title": "Run Time"
        }, 
        {
            "location": "/application_development/#multi-tenancy-and-security", 
            "text": "Hadoop is a multi-tenant distributed operating system. Security is\nan intrinsic element of multi-tenancy as without it a cluster cannot be\nreasonably be shared among enterprise applications. Streaming\napplications follow all multi-tenancy security models used in Hadoop as\nthey are native Hadoop applications. For details refer to the Operation and Installation\nGuide \n.", 
            "title": "Multi-Tenancy and Security"
        }, 
        {
            "location": "/application_development/#security", 
            "text": "The platform includes Kerberos support. Both access points, namely\nSTRAM and Bufferserver are secure. STRAM passes the token over to\nStreamingContainer, which then gives it to the Bufferserver. The most\nimportant aspect for an application developer is to note that STRAM is\nthe single point of access to ensure security measures are taken by all\ncomponents of the platform.", 
            "title": "Security"
        }, 
        {
            "location": "/application_development/#resource-limits", 
            "text": "Hadoop enforces quotas on resources. This includes hard-disk (name\nspace and total disk quota) as well as priority queues for schedulers.\nThe platform uses Hadoop resource limits to manage a streaming\napplication. In addition network I/O quotas can be enforced. An operator\ncan be dynamically partitioned if it reaches its resource limits; these\nlimits may be expressed in terms of throughput, latency, or just\naggregate resource utilization of a container.", 
            "title": "Resource Limits"
        }, 
        {
            "location": "/application_development/#scalability-and-partitioning", 
            "text": "Scalability is a foundational element of this platform and is a\nbuilding block for an eco-system where big-data meets real-time.\nEnterprises need to continually meet SLA as data grows. Without the\nability to scale as load grows, or new applications with higher loads\ncome to fruition, enterprise grade SLA cannot be met. A big issue with\nthe streaming application space is that, it is not just about high load,\nbut also the fluctuations in it. There is no way to guarantee future\nload requirements and there is a big difference between high and low\nload within a day for the same feed. Traditional streaming platforms\nsolve these two cases by simply throwing more hardware at the\nproblem.  Daily spikes are managed by ensuring enough hardware for peak\nload, which then idles during low load, and future needs are handled by\na very costly re-architecture, or investing heavily in building a\nscalable distributed operating system. Another salient and often\noverlooked cost is the need to manage SLA -- let\u2019s call it  buffer capacity. Since this means computing the\npeak load within required time, that translates to allocating enough\nresources over and above peak load as daily peaks fluctuate. For example\nan average peak load of 100 resource units (cpu and/or memory and/or\nnetwork) may mean allocating about 200 resource units to be safe. A\ndistributed cluster that cannot dynamically scale up and down, in effect\npays buffer capacity per application. Another big aspect of streaming\napplications is that the load is not just ingestion rate, more often\nthan not, the internal operators produce lot more events than the\ningestion rate. For example a dimensional data (with, say  d\u00a0dimensions) computation needs 2*d -1\u00a0computations per ingested event. A lot\nof applications have over 10 dimensions, i.e over 1000 computations per\nincoming event and these need to be distributed across the cluster,\nthereby causing an explosion in the throughput (events/sec) that needs\nto be managed.  The platform is designed to handle such cases at a very low cost.\nThe platform scales linearly with Hadoop. If applications need more\nresources, the enterprise can simply add more commodity nodes to Hadoop\nwithout any downtime, and the Hadoop native platform will take care of\nthe rest. If some nodes go bad, these can be removed without downtime.\nThe daily peaks and valleys in the load are managed by the platform by\ndynamically scaling at the peak and then giving the resources back to\nHadoop during low load. This means that a properly designed Hadoop\ncluster does several things for enterprises: (a) reduces the cost of\nhardware due to use of commodity hardware (b) shares buffer capacity\nacross all applications as peaks of all applications may not align and\n(c) raises the average CPU usage on a 24x7 basis. As a general design\nthis is similar to scale that a map-reduce application can deliver. In\nthe following sections of this chapter we will see how this is\ndone.", 
            "title": "Scalability and Partitioning"
        }, 
        {
            "location": "/application_development/#partitioning", 
            "text": "If all tuples sent through the stream(s) that are connected to the\ninput port(s) of an operator in the DAG are received by a single\nphysical instance of that operator, that operator can become a\nperformance bottleneck. This leads to scalability issues when\nthroughput, memory, or CPU needs exceed the processing capacity of that\nsingle instance.  To address the problem, the platform offers the capability to\npartition the inflow of data so that it is divided across multiple\nphysical instances of a logical operator in the DAG. There are two\nfunctional ways to partition   Load balance: Incoming load is simply partitioned\n    into stream(s) that go to separate instances of physical operators\n    and scalability is achieved via adding more physical operators. Each\n    tuple is sent to physical operator (partition) based on a\n    round-robin or other similar algorithm. This scheme scales linearly.\n    A lot of key based computations can load balance in the platform due\n    to the ability to insert  Unifiers. For many computations, the\n    endWindow and Unifier setup is similar to the combiner and reducer\n    mechanism in a Map-Reduce computation.  Sticky Key: The key assertion is that distribution of tuples\n    are sticky, i.e the data with\n    same key will always be processed by the same physical operator, no\n    matter how many times it is sent through the stream. This stickiness\n    will continue even if the number of partitions grows dynamically and\n    can eventually be leveraged for advanced features like\n    bucket testing. How this is accomplished and what is required to\n    develop compliant operators will be explained below.   We plan to add more partitioning mechanisms proactively to the\nplatform over time as needed by emerging usage patterns. The aim is to\nallow enterprises to be able to focus on their business logic, and\nsignificantly reduce the cost of operability. As an enabling technology\nfor managing high loads, this platform provides enterprises with a\nsignificant innovative edge. Scalability and Partitioning is a\nfoundational building block for this platform.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/application_development/#sticky-partition-vs-round-robin", 
            "text": "As noted above, partitioning via sticky key is data aware but\nround-robin partitioning is not. An example for non-sticky load\nbalancing would be round robin distribution over multiple instances,\nwhere for example a tuple stream of  A, A,\nA with 3 physical operator\ninstances would result in processing of a single A by each of the instances, In contrast, sticky\npartitioning means that exactly one instance of the operators will\nprocess all of the  Atuples if they\nfall into the same bucket, while B\nmay be processed by another operator. Data aware mapping of\ntuples to partitions (similar to distributed hash table) is accomplished\nvia Stream Codecs. In later sections we would show how these two\napproaches can be used in combination.", 
            "title": "Sticky Partition vs Round Robin"
        }, 
        {
            "location": "/application_development/#stream-codec", 
            "text": "The platform does not make assumptions about the tuple\ntype, it could be any Java object. The operator developer knows what\ntuple type an input port expects and is capable of processing. Each\ninput port has a stream codec \u00a0associated thatdefines how data is serialized when transmitted over a socket\nstream; it also defines another\nfunction that computes the partition hash key for the tuple. The engine\nuses that key to determine which physical instance(s) \u00a0(for a\npartitioned operator) receive that \u00a0tuple. For this to work, consistent hashing is required.\nThe default codec uses the Java Object#hashCode function, which is\nsufficient for basic types such as Integer, String etc. It will also\nwork with custom tuple classes as long as they implement hashCode\nappropriately. Reliance on hashCode may not work when generic containers\nare used that do not hash the actual data, such as standard collection\nclasses (HashMap etc.), in which case a custom stream codec must be\nassigned to the input port.", 
            "title": "Stream Codec"
        }, 
        {
            "location": "/application_development/#static-partitioning", 
            "text": "DAG designers can specify at design time how they would like\ncertain operators to be partitioned. STRAM then instantiates the DAG\nwith the physical plan which adheres to the partitioning scheme defined\nby the design. This plan is the initial partition of the application. In\nother words, Static Partitioning is used to tell STRAM to compute the\nphysical DAG from a logical DAG once, without taking into consideration\nruntime states or loads of various operators.", 
            "title": "Static Partitioning"
        }, 
        {
            "location": "/application_development/#dynamic-partitioning", 
            "text": "In streaming applications the load changes during the day, thus\ncreating situations where the number of partitioned operator instances\nneeds to adjust dynamically. The load can be measured in terms of\nprocessing within the DAG based on throughput, or latency, or\nconsiderations in external system components (time based etc.) that the\nplatform may not be aware of. Whatever the trigger, the resource\nrequirement for the current processing needs to be adjusted at run-time.\nThe platform may detect that operator instances are over or under\nutilized and may need to dynamically adjust the number of instances on\nthe fly. More instances of a logical operator may be required (partition\nsplit) or underutilized operator instances may need decommissioning\n(partition merge). We refer to either of the changes as dynamic\npartitioning. The default partitioning scheme supports split and merge\nof partitions, but without state transfer. The contract of the\nPartitioner\u00a0interface allows the operator\ndeveloper to implement split/merge and the associated state transfer, if\nnecessary.  Since partitioning is a key scalability measure, our goal is to\nmake it as simple as possible without removing the flexibility needed\nfor sophisticated applications. Basic partitioning can be enabled at\ncompile time through the DAG specification. A slightly involved\npartitioning involves writing custom codecs to calculate data aware\npartitioning scheme. More complex partitioning cases may require users\nto provide a custom implementation of Partitioner, which gives the\ndeveloper full control over state transfer between multiple instances of\nthe partitioned operator.", 
            "title": "Dynamic Partitioning"
        }, 
        {
            "location": "/application_development/#default-partitioning", 
            "text": "The platform provides a default partitioning implementation that\ncan be enabled without implementing Partitioner\u00a0(or writing any other extra Java\ncode), which is designed to support simple sticky partitioning out of\nthe box for operators with logic agnostic to the partitioning scheme\nthat can be enabled by means of DAG construction alone.  Typically an operator that can work with the default partitioning\nscheme would have a single input port. If there are multiple input\nports, only one port will be partitioned (the port first connected in\nthe DAG). The number of partitions will be calculated based on the\ninitial partition count - set as attribute on the operator in the DAG\n(if the attribute is not present, partitioning is off). Each partition\nwill handle tuples based on matching the lower bits of the hash code.\nFor example, if the tuple type was Integer and 2 partitions requested,\nall even numbers would go to one operator instance and all odd numbers\nto the other.", 
            "title": "Default Partitioning"
        }, 
        {
            "location": "/application_development/#default-dynamic-partitioning", 
            "text": "Triggering partition load evaluation and repartitioning action\nitself are separate concerns. Triggers are not specified further here,\nwe are planning to support it in a customizable fashion that, for\nexample, allows latency or SLA based implementations. Triggers calculate\na load indicator (signed number) that tells the framework that a given\npartition is either underutilized, operating normally within the\nexpected thresholds or overloaded and becoming a bottleneck. The\nindicator is then presented to the partitioning logic (default or custom\nimplementation of Partitioner) to provide the opportunity to make any\nneeded adjustments.  The default partitioning logic divides the key space\naccording to the lower bits of the hash codes that are generated by the\nstream codec, by assigning each partitioned operator instance via a bit\nmask and the respective value. For example, the operator may have\ninitially two partitions,  0and 1, each\nwith a bit mask of 1.\nIn the case where load evaluation flags partition\n0  as over utilized\n(most data tuples processed yield a hash code with lowest bit cleared),\napartition split\u00a0occurs, resulting in 00\nand  10with mask 11. Operator instance 0 will be replaced with 2 new instances and partition\n1  remains unchanged,\nresulting in three active partitions. The same process could repeat if\nmost tuples fall into the01 partition, leading to a split into 001  and101\nwith mask 111, etc.  Should load decrease in two sibling partitions, a\npartition merge\u00a0could\nreverse the split, reducing the mask length and replacing two operators\nwith one. Should only one of two sibling partitions be underutilized,\n it cannot be\u00a0merged.\nInstead, the platform can attempt to deploy the affected operator\ninstance along with other operator instances for resource sharing\namongst underutilized partitions (not implemented yet). Keeping separate\noperator instances allows\u00a0us  to\npin load increases directly to the affected instance with a single\nspecific partition key, which would not be the case had we assigned a\nshared instance to handle multiple keys.", 
            "title": "Default Dynamic Partitioning"
        }, 
        {
            "location": "/application_development/#nxm-partitions", 
            "text": "When two consecutive logical operators are partitioned a special\noptimization is done. Technically the output of the first operator\nshould be unified and streamed to the next logical node. But that can\ncreate a network bottleneck. The platform optimizes this by partitioning\nthe output stream of each partition of the first operator as per the\npartitions needed by the next operator. For example if the first\noperator has N partitions and the second operator has M partitions then\neach of the N partitions would send out M streams. The first of each of\nthese M streams would be unified and routed to the first of the M\npartitions, and so on. Such an optimization allows for higher\nscalability and eliminates a network bottleneck (one unifier in between\nthe two operators) by having M unifiers. This also enables the\napplication to perform within the resource limits enforced by YARN.\nSTRAM has a much better understanding and estimation of unifier resource\nneeds and is thus able to optimize for resource constraints.  Figure 5 shows a case where we have a 3x2 partition; the single\nintermediate unifier between operator 1\u00a0and 2\u00a0is\noptimized away. The partition computation for operator  2\u00a0is executed on outbound streams of each\npartitions of operator 1. Each\npartition of operator 2\u00a0has its own\nCONTAINER_LOCAL unifier. In such a situation, the in-bound network\ntuple flow is split between containers for  2a\u00a0and 2b\u00a0each of which take half the traffic. STRAM\ndoes this by default since it always has better performance.", 
            "title": "NxM Partitions"
        }, 
        {
            "location": "/application_development/#parallel", 
            "text": "In cases where all the downstream operators use the same\npartitioning scheme and the DAG is network bound an optimization called\nparallel partition\u00a0is very\neffective. In such a scenario all the downstream operators are also\npartitioned to create computation flow per partition. This optimization\nis extremely efficient for network bound streams, In some cases this\noptimization would also apply for CPU or RAM bounded\napplications.  In Figure 6a, operator 1\u00a0is\npartitioned into 1a\u00a0and\n1b. Both the downstream operators\n2\u00a0and  3\u00a0follow the same partition scheme as\n1, however the network I/O between\n1\u00a0and 2, and between 2\u00a0and  3\u00a0is\nhigh. Then users can decide to optimize using parallel partitions. This\nallows STRAM to completely skip the insertion of intermediate Unifier\noperators between 1 and 2 as well as between 2 and 3; a single unifier\njust before operator  4, is\nadequate by which time tuple flow volume is low.  Since operator 4 has sufficient resources to manage the combined\noutput of multiple instances of operator 3, it need not be partitioned. A further\noptimization can be done by declaring operators  1, 2, and\n3\u00a0as THREAD_LOCAL (intra-thread)\nor CONTAINER_LOCAL (intra-process) or NODE_LOCAL (intra-node).\nParallel partition is not used by default, users have to specify it\nexplicitly via an attribute of the input port (reader) of the stream as\nshown below.   The following code shows an example of creating a parallel partition.  dag.addStream( DenormalizedUserId , idAssigner.userid, uniqUserCount.data);\ndag.setInputPortAttribute(uniqUserCount.data, PortContext.PARTITION_PARALLEL, partitionParallel);  Parallel partitions can be used with other partitions, for example\na parallel partition could be sticky key or load balanced.", 
            "title": "Parallel"
        }, 
        {
            "location": "/application_development/#parallel-partitions-with-streams-modes", 
            "text": "Parallel partitions can be further optimized if the parallel\npartitions are combined with streams being in-line or in-node or in-rack\nmode. This is very powerful feature and should be used if operators have\nvery high throughput within them and the outbound merge does an\naggregation. For example in Figure 6b, if operator 3 significantly\nreduces the throughput, which usually is a reason to do parallel\npartition, then making the streams in-line or in-node within nodes\n1- 2 and 2- 3 significantly impacts the performance.  CONTAINER_LOCAL stream has high bandwidth, and can manage to\nconsume massive tuple count without taxing the NIC and networking stack.\nThe downside is that all operators (1,2,3) in this case need to be able\nto fit within the resource limits of CPU and memory enforced on a Hadoop\ncontainer. A way around this is to request RM to provide a big\ncontainer. On a highly used Hadoop grid, getting a bigger container may\nbe a problem, and operational complexities of managing a Hadoop cluster\nwith different container sizes may be higher. If THREAD_LOCAL or\nCONTAINER_LOCAL streams are needed to get the throughput, increasing\nthe partition count should be considered. In future STRAM may take this\ndecision automatically. Unless there is a very bad skew and sticky key\npartitioning is in use, the approach to partition till each container\nhas enough resources works well.  A NODE_LOCAL stream has lower bandwidth compared to a\nCONTAINER_LOCAL stream, but it works well with the RM in terms of\nrespecting container size limits. A NODE_LOCAL parallel partition uses\nlocal loop back for streams and is much better than using NIC. Though\nNODE_LOCAL stream fits well with similar size containers, it does need\nRM to be able to deliver two containers on the same Hadoop node. On a\nheavily used Hadoop cluster, this may not always be possible. In future\nSTRAM would do these trade-offs automatically at run-time.  A RACK_LOCAL stream has much lower bandwidth than NODE_LOCAL\nstream, as events go through the NIC. But it still is able to better\nmanage SLA and latency. Moreover RM has much better ability to give a\nrack local container as opposed to the other two.  Parallel partitions with CONTAINER_LOCAL streams can be done by\nsetting all the intermediate streams to CONTAINER_LOCAL. Parallel\npartitions with THREAD_LOCAL streams can be done by setting all the\nintermediate streams to THREAD_LOCAL. Platform supports the following\nvia attributes.   Parallel-Partition  Parallel-Partition with THREAD_LOCAL stream  Parallel-Partition with CONTAINER_LOCAL stream  Parallel-Partition with NODE_LOCAL stream  Parallel-Partition with RACK_LOCAL stream   These attributes would nevertheless be initial starting point and\nSTRAM can improve on them at run time.", 
            "title": "Parallel Partitions with Streams Modes"
        }, 
        {
            "location": "/application_development/#skew-balancing-partition", 
            "text": "Skew balancing partition is useful to manage skews in the stream\nthat is load balanced using a sticky key. Incoming events may have a\nskew, and these may change depending on various factors like time of the\nday or other special circumstances. To manage the uneven load, users can\nset a limit on the ratio of maximum load on a partition to the minimum\nload on a partition. STRAM would use this to dynamically change the\npartitions. For example suppose there are 6 partitions, and the load\nhappens to be distributed as follows: one with 40%, and the rest with\n12% each. The ratio of maximum to minimum is 3.33. If the desired ratio\nis set to 2, STRAM would partition the first instance into two\npartitions, each with 20% load to bring the ratio down to the desired\nlevel. This will be tried repeatedly till partitions are balanced. The\ntime period between each attempt is controlled via an attribute to avoid\nrebalancing too frequently. As mentioned earlier, dynamic operations\ninclude both splitting a partition as well as merging partitions with\nlow load.  Figure 7 shows an example of skew balancing partition. An example\nof 3x1 paritition is shown. Let's say that skew balance is kept at \u201cno\npartition to take up more than 50% load. If in runtime the load type\nchanges to create a skew. For example, consider an application in the US\nthat is processing a website clickstream. At night in the US, the\nmajority of accesses come from the Far East, while in the daytime it\ncomes from the Americas. Similarly, in the early morning, the majority\nof the accesses are from east coast of the US, with the skew shifting to\nthe west coast as the day progresses. Assume operator 1 is partitioned\ninto 1a, 1b, and 1c.  Let's see what happens if the logical operator 1 gets into a 20%,\n20%, 60% skew as shown in Figure 7. This would trigger the skew\nbalancing partition. One example of attaining balance is to merge 1a,\nand 1b to get 1a+1b in a single partition to take the load to 40%; then\nsplit 1c into two partitions 1ca and 1cb to get 30% on each of them.\nThis way STRAM is able to get back to under 50% per partition. As a live\n24x7 application, this kind of skew partitioning can be applied several\ntimes in a day. Skew-balancing at runtime is a critical feature for SLA\ncompliance; it also enables cost savings. This partitioning scheme will\nbe available in later release.", 
            "title": "Skew Balancing Partition"
        }, 
        {
            "location": "/application_development/#skew-unifier-partition", 
            "text": "In this section we would take a look at another way to balance the\nskew. This method is a little less disruptive, but is useful in\naggregate operators. Let us take the same example as in Figure 7 with\nskew 20%, 20%, and 60%. To manage the load we could have either worked\non rebalancing the partition, which involves a merge and split of\npartitions to get to a new distribution or by partitioning  only\u00a0the partition with the big skew. Since the\nbest way to manage skew is to load balance, if possible, this scheme\nattempts to do so. The method is less useful than the others we discusse\n-- the main reason being that if the developer has chosen a sticky key\npartition to start with, it is unlikely that a load balancing scheme can\nhelp. Assuming that it is worthwhile to load balance, a special\none-purpose unifier can be inserted for the skew partition. If the cause\nof resource bottleneck is not the I/O, specially the I/O into the\ndownstream operator, but is the compute (memory, CPU) power of a\npartition, it makes sense to split the skew partition without having to\nchange the in-bound I/O to the upstream operator.  To trigger this users can set a limit on the ratio of maximum load\non a partition to the minimum load on a partition, and ask to use this\nscheme. STRAM would use this to load balance.The time period between\neach attempt is controlled via the same attribute to avoid rebalancing\ntoo frequently.  Figure 8 shows an example of skew load balancing partition with a\ndedicated unifier. The 20%, 20%, and 60% triggers the skew load\nbalancing partition with an unifier. Partition 1c would be split into\ntwo and it would get its own dedicated unifier. Ideally these two\nadditional partitions 1ca and 1cb will get 30% load. This way STRAM is\nable to get back to under 50% per partition. This scheme is very useful\nwhen the number of partitions is very high and we still have a bad\nskew.  In the steady state no physical partition is computing more than\n30% of the load. Memory and CPU resources are thus well distributed. The\nunifier that was inserted has to handle 60% of the load, distributed\nmore evenly, as opposed to the final unifier that had a 60% skew to\nmanage at a much higher total load. This partitioning scheme will be\navailable in later release.", 
            "title": "Skew Unifier Partition"
        }, 
        {
            "location": "/application_development/#cascading-unifier", 
            "text": "Let's take the case of an upstream operator oprU\u00a0that connects to a downstream operator\noprD. Let's assume the application\nis set to scale oprU by load balancing. So this could be either Nx1 or\nNxM partitioning scheme. The upstream operator oprU scales by increasing\nN. An increase in the load triggers more resource needs (CPU, Memory, or\nI/O), which in turn triggers more containers and raises N, the\ndownstream node may be impacted in a lot of situations. In this section\nwe review a method to shield oprD from dynamic changes in the execution\nplan of oprU. On aggregate operators (Sum, Count, Max, Min, Range \u2026) it\nis better to do load balanced partitioning to avoid impact of skew. This\nworks very well as each partition emits tuples at the order of number of\nkeys (range) in the incoming stream per application window. But as N\ngrows the in-bound I/O to the unifier of oprU that runs in the container\nof oprD goes up proportionately as each upstream partition sends tuples\nof the order of unique keys (range). This means that the partitioning\nwould not scale linearly. The platform has mechanisms to manage this and\nget the scale back to being linear.  Cascading unifiers are implemented by inserting a series of\nintermediate unifiers before the final unifier in the container of oprD.\nSince each unifier guarantees that the outbound I/O would be in order of\nthe number of unique keys, the unifier in the oprD container can expect\nto achieve an upper limit on the inbound I/O. The problem is the same\nirrespective of the value of M (1 or more), wherein the amount of\ninbound I/O is proportional to N, not M. Figure 8 illustrates how\ncascading unifier works.   Figure 8 shows an example where a 4x1 partition with single\nunifier is split into three 2x1 partitions to enable the final unifier\nin oprD container to get an upper limit on inbound I/O. This is useful\nto ensure that network I/O to containers is within limits, or within a\nlimit specified by users. The platform allows setting an upper limit of\nfan-in of the stream between oprU and oprD. Let's say that this is F (in\nthe figure F=2). STRAM would plan N/F (let's call it N1) containers,\neach with one unifier. The inbound fan-in to these unifiers is F. If N1  F, another level of unifiers would be inserted. Let's say at some\npoint N/(F1*F2*...Fk)   F, where K is the level of unifiers. The\noutbound I/O of each unifier is guaranteed to be under F, specially the\nunifier for oprD. This ensures that the application scales linearly as\nthe load grows. The downside is the additional latency imposed by each\nunifier level (a few milliseconds), but the SLA is maintained, and the\napplication is able to run within the resource limits imposed by YARN.\nThe value of F can be derived from any of the following   I/O limit on containers to allow proper behavior in an\n    multi-tenant environment  Load on oprD instance  Buffer server limits on fan-in, fan-out  Size of reservoir buffer for inbound fan-in   A more intriguing optimization comes when cascading unifiers are\ncombined with node-local execution plan, in which the bounds of two or\nmore containers are used and much higher local loopback limits are\nleveraged. In general the first level fan-in limit (F1) and the last\nstage fan-in limit (Fk) need not be same. In fact a much open and better\nleveraged execution plan may indeed have F1 != F2 != \u2026 != Fk, as Fk\ndetermines the fan-in for oprD, while F1, \u2026 Fk-1 are fan-ins for\nunifier-only containers. The platform will have these schemes in later\nversions.", 
            "title": "Cascading Unifier"
        }, 
        {
            "location": "/application_development/#sla", 
            "text": "A Service Level Agreement translates to guaranteeing that the\napplication would meet the requirements X% of the time. For example six\nsigma X is\u00a099.99966%. For\nreal-time streaming applications this translates to requirements for\nlatency, throughput, uptime, data loss etc. and that in turn indirectly\nleads to various resource requirements, recovery mechanisms, etc. The\nplatform is designed to handle these and features would be released in\nfuture as they get developed. At a top level, STRAM monitors throughput\nper operator, computes latency per operator, manages uptime and supports\nvarious recovery mechanisms to handle data loss. A lot of this decision\nmaking and algorithms will be customizable.", 
            "title": "SLA"
        }, 
        {
            "location": "/application_development/#fault-tolerance", 
            "text": "Fault tolerance in the platform is defined as the ability to\nrecognize the outage of any part of the application, get resources,\nre-initialize the failed operators, and re-compute the lost data. The\ndefault method is to bring the affected part of the DAG \u00a0back to a known\n(checkpointed) state and recompute atomic micro batches from there on.\nThus the default is  at least\nonce\u00a0processing mode. An operator can be configured for\nat most once\u00a0recovery, in which\ncase the re-initialized operator starts from next available window; or\nfor exactly once\u00a0recovery, in which\ncase the operator only recomputes the window it was processing when the\noutage happened.", 
            "title": "Fault Tolerance"
        }, 
        {
            "location": "/application_development/#state-of-the-application", 
            "text": "The state of the application is traditionally defined as the state\nof all operators and streams at any given time. Monitoring state as\nevery tuple is processed asynchronously in a distributed environment\nbecomes a near impossible task, and cost paid to achieve it is very\nhigh. Consequently, in the platform, state is not saved per tuple, but\nrather at window boundaries. The platform treats windows as atomic micro\nbatches. The state saving task is delegated by STRAM to the individual\noperator or container. This ensures that the bookkeeping cost is very\nlow and works in a distributed way. Thus, the state of the application\nis defined as the collection of states of every operator and the set of\nall windows stored in the buffer server. This allows STRAM to rebuild\nany part of the application from the last saved state of the impacted\noperators and the windows retained by the buffer server. The state of an\noperator is intrinsically associated with a window id. Since operators\ncan override the default checkpointing period, operators may save state\nat the end of different windows. This works because the buffer server\nsaves all windows for as long as they are needed (state in the buffer\nserver is purged once STRAM determines that it is not longer needed\nbased on checkpointing in downstream operators).  Operators can be stateless or stateful. A stateless operator\nretains no data between windows. All results of all computations done in\na window are emitted in that window. Variables in such an operator are\neither transient or are cleared by an end_window event. Such operators\nneed no state restoration after an outage. A stateful operator retains\ndata between windows and has data in checkpointed state. This data\n(state) is used for computation in future windows. Such an operator\nneeds its state restored after an outage. By default the platform\nassumes the operator is stateful. In order to optimize recovery (skip\nprocessing related to state recovery) for a stateless operator, the\noperator needs to be declared as stateless to STRAM. Operators can\nexplicitly mark themselves stateless via an annotation or an\nattribute.  Recovery mechanisms are explained later in this section. Operator\ndevelopers have to ensure that there is no dependency on the order of\ntuples between two different streams. As mentioned earlier in this\ndocument, the platform guarantees in-order tuple delivery within a\nsingle stream, For operators with multiple input ports, a replay may\nresult in a different relative order of tuples among the different input\nports. If the output tuple computation is affected by this relative\norder, the operator may have to wait for the endWindow call (at which\npoint it would have seen all the tuples from all input ports in the\ncurrent window), perform order-dependent computations correctly and\nfinally, emit results.", 
            "title": "State of the Application"
        }, 
        {
            "location": "/application_development/#checkpointing", 
            "text": "STRAM provides checkpointing parameters to StreamingContainer\nduring initialization. A checkpoint period is given to the containers\nthat have the window generators. A control tuple is sent at the end of\ncheckpoint interval. This tuple traverses through the data path via\nstreams and triggers each StreamingContainer in the path to instrument a\ncheckpoint of the operator that receives this tuple. This ensures that\nall the operators checkpoint at exactly the same window boundary (except\nin those cases where a different checkpoint interval was configured for\nan operator by the user).  The only delay is the latency of the control tuple to reach all\nthe operators. Checkpoint is thus done between the endWindow call of a\nwindow and the beginWindow call of the next window. Since most operators\nare computing in parallel (with the exception of those connected by\nTHREAD_LOCAL streams) they each checkpoint as and when they are ready\nto process the \u201ccheckpoint\u201d control tuple. The asynchronous design of\nthe platform means that there is no guarantee that two operators would\ncheckpoint at exactly the same time, but there is a guarantee that by\ndefault they would checkpoint at the same window boundary. This feature\nalso ensures that purge of old data can be efficiently done: Once the\ncheckpoint window tuple is done traversing the DAG, the checkpoint state\nof the entire DAG increments to this window id at which point prior\ncheckpoint data can be discarded.  In case of an operator that has an application window size that is\nlarger than the size of the streaming window, the checkpointing by\ndefault still happens at same intervals as with other operators. To\nalign checkpointing with application window boundary, the application\ndeveloper should set the attribute \u201cCHECKPOINT_WINDOW_COUNT\u201d to\n\u201cAPPLICATION_WINDOW_COUNT\u201d. This ensures that the checkpoint happens\nat the  end\u00a0of the application\nwindow and not within\u00a0that window.\nSuch operators now treat the application window as an atomic computation\nunit. The downside is that it does need the upstream buffer server to\nkeep tuples for the entire application window.  If an operator is completely stateless, i.e. an outbound tuple is\nonly emitted in the process\u00a0call\nand only depends on the tuple of that call, there is no need to align\ncheckpointing with application window end. If the operator is stateful\nonly within a window, the operator developer should strongly consider\ncheckpointing only on the application window boundary.  Checkpointing involves pausing an operator, serializing the state\nto persistent storage and then resuming the operator. Thus checkpointing\nhas a latency cost that can negatively affect computational throughput;\nto minimize that impact, it is important to ensure that checkpointing is\ndone with minimal required objects. This means, as mentioned earlier,\nall data that is not part of the operator state should be declared as\ntransient so that it is not persisted.  An operator developer can also create a stateless operator (marked\nwith the Stateless annotation). Stateless operators are not\ncheckpointed. Obviously, in such an operator, computation should not\ndepend on state from a previous window.  The serialized \u00a0state of an operator is stored as a file, and is\nthe state to which that the operator is restored if an outage happens\nbefore the next checkpoint. The id of the last completed window (per\noperator) is sent back to STRAM in the next heartbeat. The default\nimplementation for serialization uses KRYO. Multiple past checkpoints\nare kept per operator. Depending on the downstream checkpoint, one of\nthese are chosen for recovery. Checkpoints and buffer server state are\npurged once STRAM sees windows as fully processed in the DAG.  A complete recovery of an operator needs the operator to be\ncreated, its checkpointed state restored and then all the lost atomic\nwindows replayed by the upstream buffer server(s). The above design\nkeeps the bookkeeping cost low with quick catch up time. In the next\nsection we will see how this simple abstraction allows applications to\nrecover under different requirements.", 
            "title": "Checkpointing"
        }, 
        {
            "location": "/application_development/#recovery-mechanisms_1", 
            "text": "Recovery mechanism are ways to recover from a container (or an\noperator) outage. In this section we discuss a single container outage.\nMultiple container outages are handled as independent events. Recovery\nrequires the upstream buffer server to replay windows and it would\nsimply go one more level upstream if the immediate upstream container\nhas also failed. If multiple operators are in a container (THREAD_LOCAL\nor CONTAINER_LOCAL stream) the container recovery treats each operator\nas an independent object when figuring out the recovery steps.\nApplication developers can set any of the recovery mechanisms discussed\nbelow for node outage.  In general, the cost of recovery depends on the state of the\noperator and the recovery mechanism selected, while data loss tolerance\nis specified by the application. For example a data-loss tolerant\napplication would prefer at most\nonce\u00a0recovery. All recovery mechanisms treat a streaming\nwindow as an atomic computation unit. In all three recovery mechanisms\nthe new operator connects to the upstream buffer server and asks for\ndata from a particular window onwards. Thus all recovery methods\ntranslate to deciding which atomic units to re-compute and which state\nthe new operator resumes from. A partially computed micro-batch is\nalways dropped. Such micro-batches are re-computed in at-least-once or\nexactly-once mode and skipped in at-most-once mode. The notiion of an\natomic micro-batch is a critical guiding principle as it enables very\nlow bookkeeping costs, high throughput, low recovery times, and high\nscalability. Within an application each operator can have its own\nrecovery mechanism.", 
            "title": "Recovery Mechanisms"
        }, 
        {
            "location": "/application_development/#at-least-once", 
            "text": "At least once recovery is the default recovery mechanism, i.e it\nis used when no mechanism is specified. In this method, the lost\noperator is brought back to its latest viable checkpointed state and the\nupstream buffer server is asked to replay all subsequent windows. There\nis no data loss in recovery. The viable checkpoint state is defined as\nthe one whose window id is in the past as compared to all the\ncheckpoints of all the downstream operators. All downstream operators\nare restarted at their checkpointed state. They ignore all incoming data\nthat belongs to windows prior their checkpointed window. The lost\nwindows are thus recomputed and the application catches up with live\nincoming data. This is called \" at least\nonce\"\u00a0because lost windows are recomputed. For example if\nthe streaming window is 0.5 seconds and checkpointing is being done\nevery 30 seconds, then upon node outage all windows since the last\ncheckpoint (up to 60 windows) need to be re-processed. If the\napplication can handle loss of data, then this is not the most optimal\nrecovery mechanism.  In general for this recovery mode, the average time lag on a node\noutage is  = (CP/2*SW)*T + HC  where   CP \u00a0\u00a0- Checkpointing period (default value is 30 seconds)  SW \u00a0\u00a0- Streaming window period (default value is 0.5 seconds)  T \u00a0\u00a0\u00a0- \u00a0Time taken to re-compute one lost window from data in memory  HC \u00a0\u00a0- Time it takes to get a new Hadoop Container, or make do with the current ones   A lower CP is a trade off between cost of checkpointing and the\nneed to have to use it in case of outage. Input adapters cannot use\nat-least-once recovery without the support from sources outside Hadoop.\nFor an output adapter care may needed if the external system cannot\nhandle re-write of the same data.", 
            "title": "At Least Once"
        }, 
        {
            "location": "/application_development/#at-most-once", 
            "text": "This recovery mechanism is for applications that can tolerate\ndata-loss; they get the quickest recovery in return. The restarted node\nconnects to the upstream buffer server, subscribing to data from the\nstart of the next window. It then starts processing that window. The\ndownstream operators ignore the lost windows and continue to process\nincoming data normally. Thus, this mechanism forces all downstream\noperators to follow.  For multiple inputs, the operator waits for all ports with the\nat-most-once attribute to get responses from their respective buffer\nservers. Then, the operator starts processing till the end window of the\nlatest window id on each input port is reached. In this case the end\nwindow tuple is non-blocking till the common window id is reached. At\nthis point the input ports are now properly synchronized. Upstream nodes\nreconnect under  at most\nonce\u00a0paradigm in same way. \u00a0For example, assume an operator\nhas ports in1\u00a0and in2\u00a0and a checkpointed window of 95. Assume further that the buffer servers of\noperators upstream of  in1\u00a0and\nin2\u00a0respond with window id 100 and\n102 respectively. Then port in1\u00a0would continue to process till end window of\n101, while port  in2\u00a0will wait for in1\nto catch up to 102.\nFrom \u00a0then on, both ports process their tuples normally. So windows from\n96 to  99are lost. Window 100\nand 101 has only\nin1 active, and 102 onwards both ports are active. The other\nports of upstream nodes would also catch up till  102in a similar fashion. This operator may not\nneed to be checkpointed. Currently the option to not do checkpoint in\nsuch cases is not available.  In general, in this recovery mode, the average time lag on a node\noutage is  = SW/2 + HC  where    SW \u00a0- Streaming window period (default value is 0.5\nseconds)    HC \u00a0- Time it takes to get a new Hadoop Container, or make\ndo with the current ones", 
            "title": "At Most Once"
        }, 
        {
            "location": "/application_development/#exactly-once", 
            "text": "This recovery mechanism is for applications that require no\ndata-loss as well are no recomputation. Since a window is an atomic\ncompute unit, exactly once applies to the window as a whole. In this\nrecovery mode, the operator is brought back to the start of the window\nin which the outage happened and the window is recomputed. The window is\nconsidered closed when all the data computations are done and end window\ntuple is emitted. \u00a0Exactly once requires every window to be\ncheckpointed. From then on, the operator asks the upstream buffer server\nto send data from the last checkpoint. The upstream node behaves the\nsame as in at-most-once recovery. Checkpointing after every streaming\nwindow is very costly, but users would most often do exactly once per\napplication window; if the application window size is substantially\nlarger than the streaming window size (which typically is the case) the\ncost of running an operator in this recovery mode may not be as\nhigh.", 
            "title": "Exactly Once"
        }, 
        {
            "location": "/application_development/#speculative-execution", 
            "text": "In future we looking at possibility of adding speculative execution for the applications. This would be enabled in multiple ways.    At an operator level: The upstream operator would emit to\n    two copies. The downstream operator would receive from both copies\n    and pick a winner. The winner (primary) would be picked in either of\n    the following ways   Statically as dictated by STRAM  Dynamically based on whose tuple arrives first. This mode\n    needs both copies to guarantee that the computation result would\n    have identical functionality     At a sub-query level: A part of the application DAG would be\n    run in parallel and all upstream operators would feed to two copies\n    and all downstream operators would receive from both copies. The\n    winners would again be picked in a static or dynamic manner   Entire DAG: Another copy of the application would be run by\n    STRAM and the winner would be decided outside the application. In\n    this mode the output adapters would both be writing\n    the result.   In all cases the two copies would run on different Hadoop nodes.\nSpeculative execution is under development and\nis not yet available.", 
            "title": "Speculative Execution"
        }, 
        {
            "location": "/application_development/#9-dynamic-application-modifications", 
            "text": "Dynamic application modifications are being worked on and most of\nthe features discussed here are now available. The platform supports the\nability to modify the DAG of the application as per inputs as well as\nset constraints, and will continue to provide abilities to deepen\nfeatures based on this ability. All these changes have one thing in\ncommon and that is the application does not need to be restarted as\nSTRAM will instrument the changes and the streaming will catch-up and\ncontinue.  Some examples are   Dynamic Partitioning:\u00a0Automatic\n    changes in partitioning of computations to match constraints on a\n    run time basis. Examples includes STRAM adding resource during spike\n    in streams and returning them once spike is gone. Scale up and scale\n    down is done automatically without human intervention.  Modification via constraints: Attributes can be changed via\n    Webservices and STRAM would adapt the execution plan to meet these.\n    Examples include operations folks asking STRAM to reduce container\n    count, or changing network resource restrictions.  Modification via properties: Properties of operators can be\n    changed in run time. This enables application developers to trigger\n    a new behavior as need be. Examples include triggering an alert ON.\n    The platform supports changes to any property of an operator that\n    has a setter function defined.  Modification of DAG structure: Operators and streams can be\n    added to or removed from a running DAG, provided the code of the\n    operator being added is already in the classpath of the running\n    application master. \u00a0This enables application developers to add or\n    remove processing pipelines on the fly without having to restart\n    the application.  Query Insertion: Addition of sub-queries to currently\n    running application. This query would take current streams as inputs\n    and start computations as per their specs. Examples insertion of\n    SQL-queries on live data streams, dynamic query submission and\n    result from STRAM (not yet available).   Dynamic modifications to applications are foundational part of the\nplatform. They enable users to build layers over the applications. Users\ncan also save all the changes done since the application launch, and\ntherefore predictably get the application to its current state. For\ndetails refer to   Configuration Guide \n.", 
            "title": "9: Dynamic Application Modifications"
        }, 
        {
            "location": "/application_development/#user-interface", 
            "text": "The platform provides a rich user interface. This includes tools\nto monitor the application system metrics (throughput, latency, resource\nutilization, etc.); dashboards for application data, replay, errors; and\na Developer studio for application creation, launch etc. For details\nrefer to   UI Console Guide .", 
            "title": "User Interface"
        }, 
        {
            "location": "/application_development/#demos", 
            "text": "In this section we list some of the demos that come packaged with\ninstaller. The source code for the demos is available in the open-source Apache Apex-Malhar repository .\nAll of these do computations in real-time. Developers are encouraged to\nreview them as they use various features of the platform and provide an\nopportunity for quick learning.   Computation of PI:\n    Computes PI by generating a random location on X-Y plane and\n    measuring how often it lies within the unit circle centered\n    at (0,0).  Yahoo! Finance quote\u00a0computation:\n    Computes ticker quote, 1-day chart (per min), and simple moving\n    averages (per 5 min).  Echoserver Reads messages from a\n    network connection and echoes them back out.  Twitter top N tweeted urls: Computes\n    top N tweeted urls over last 5 minutes  Twitter trending hashtags: Computes\n    the top Twitter Hashtags over the last 5 minutes  Twitter top N frequent words:\n    Computes top N frequent words in a sliding window  Word count: Computes word count for\n    all words within a large file  Mobile location tracker: Tracks\n    100,000 cell phones within an area code moving at car speed (jumping\n    cell phone towers every 1-5 seconds).  Frauddetect: Analyzes a stream of\n    credit card merchant transactions.  Mroperator:Contains several\n    map-reduce applications.  R: Analyzes a synthetic stream of\n    eruption event data for the Old Faithful\n    geyser (https://en.wikipedia.org/wiki/Old_Faithful).  Machinedata: Analyzes a synthetic\n    stream of events to determine health of a machine.", 
            "title": "Demos"
        }, 
        {
            "location": "/application_packages/", 
            "text": "Apache Apex Application Packages\n\n\nAn Apache Apex Application Package is a zip file that contains all the\nnecessary files to launch an application in Apache Apex. It is the\nstandard way for assembling and sharing an Apache Apex application.\n\n\nRequirements\n\n\nYou will need have the following installed:\n\n\n\n\nApache Maven 3.0 or later (for assembling the App Package)\n\n\nApache Apex 3.0.0 or later (for launching the App Package in your cluster)\n\n\n\n\nCreating Your First Apex App Package\n\n\nYou can create an Apex Application Package using your Linux command\nline, or using your favorite IDE.\n\n\nUsing Command Line\n\n\nFirst, change to the directory where you put your projects, and create\nan Apex application project using Maven by running the following\ncommand.  Replace \"com.example\", \"mydtapp\" and \"1.0-SNAPSHOT\" with the\nappropriate values (make sure this is all on one line):\n\n\n$ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-app-archetype -DarchetypeVersion=3.2.0-incubating \\\n -DgroupId=com.example -Dpackage=com.example.mydtapp -DartifactId=mydtapp \\\n -Dversion=1.0-SNAPSHOT\n\n\n\nThis creates a Maven project named \"mydtapp\". Open it with your favorite\nIDE (e.g. NetBeans, Eclipse, IntelliJ IDEA). In the project, there is a\nsample DAG that generates a number of tuples with a random number and\nprints out \"hello world\" and the random number in the tuples.  The code\nthat builds the DAG is in\nsrc/main/java/com/example/mydtapp/Application.java, and the code that\nruns the unit test for the DAG is in\nsrc/test/java/com/example/mydtapp/ApplicationTest.java. Try it out by\nrunning the following command:\n\n\n$cd mydtapp; mvn package\n\n\n\nThis builds the App Package runs the unit test of the DAG.  You should\nbe getting test output similar to this:\n\n\n -------------------------------------------------------\n  TESTS\n -------------------------------------------------------\n\n Running com.example.mydtapp.ApplicationTest\n hello world: 0.8015370953286478\n hello world: 0.9785359225545481\n hello world: 0.6322611586644047\n hello world: 0.8460953663451775\n hello world: 0.5719372906929072\n hello world: 0.6361174312337172\n hello world: 0.14873007534816318\n hello world: 0.8866986277418261\n hello world: 0.6346526809866057\n hello world: 0.48587295703904465\n hello world: 0.6436832429676687\n\n ...\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863\n sec\n\n Results :\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0\n\n\n\n\nThe \"mvn package\" command creates the App Package file in target\ndirectory as target/mydtapp-1.0-SNAPSHOT.apa. You will be able to use\nthat App Package file to launch this sample application in your actual\nApex installation.\n\n\nUsing IDE\n\n\nAlternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.\n\n\n\n\nThen fill the Group ID, Artifact ID, Version and Repository entries as shown below.\n\n\n\n\nGroup ID: org.apache.apex\nArtifact ID: apex-app-archetype\nVersion: 3.2.0-incubating (or any later version)\n\n\nPress Next and fill out the rest of the required information. For\nexample:\n\n\n\n\nClick Finish, and now you have created your own Apache Apex App Package\nproject, with a default unit test.  You can run the unit test, make code\nchanges or make dependency changes within your IDE.  The procedure for\nother IDEs, like Eclipse or IntelliJ, is similar.\n\n\nWriting Your Own App Package\n\n\nPlease refer to the \nCreating Apps\n on the basics on how to write an Apache Apex application.  In your AppPackage project, you can add custom operators (refer to \nOperator Development Guide\n), project dependencies, default and required configuration properties, pre-set configurations and other metadata.\n\n\nAdding (and removing) project dependencies\n\n\nUnder the project, you can add project dependencies in pom.xml, or do it\nthrough your IDE.  Here\u2019s the section that describes the dependencies in\nthe default pom.xml:\n\n\n  \ndependencies\n\n    \n!-- add your dependencies here --\n\n    \ndependency\n\n      \ngroupId\norg.apache.apex\n/groupId\n\n      \nartifactId\nmalhar-library\n/artifactId\n\n      \nversion\n${apex.version}\n/version\n\n      \n!--\n           If you know your application do not need the transitive dependencies that are pulled in by malhar-library,\n           Uncomment the following to reduce the size of your app package.\n      --\n\n      \n!--\n      \nexclusions\n\n        \nexclusion\n\n          \ngroupId\n*\n/groupId\n\n          \nartifactId\n*\n/artifactId\n\n        \n/exclusion\n\n      \n/exclusions\n\n      --\n\n    \n/dependency\n\n    \ndependency\n\n      \ngroupId\norg.apache.apex\n/groupId\n\n      \nartifactId\napex-engine\n/artifactId\n\n      \nversion\n${apex.version}\n/version\n\n      \nscope\nprovided\n/scope\n\n    \n/dependency\n\n    \ndependency\n\n      \ngroupId\njunit\n/groupId\n\n      \nartifactId\njunit\n/artifactId\n\n      \nversion\n4.10\n/version\n\n      \nscope\ntest\n/scope\n\n    \n/dependency\n\n  \n/dependencies\n\n\n\n\n\nBy default, as shown above, the default dependencies include\nmalhar-library in compile scope, dt-engine in provided scope, and junit\nin test scope.  Do not remove these three dependencies since they are\nnecessary for any Apex application.  You can, however, exclude\ntransitive dependencies from malhar-library to reduce the size of your\nApp Package, provided that none of the operators in malhar-library that\nneed the transitive dependencies will be used in your application.\n\n\nIn the sample application, it is safe to remove the transitive\ndependencies from malhar-library, by uncommenting the \"exclusions\"\nsection.  It will reduce the size of the sample App Package from 8MB to\n700KB.\n\n\nNote that if we exclude *, in some versions of Maven, you may get\nwarnings similar to the following:\n\n\n\n [WARNING] 'dependencies.dependency.exclusions.exclusion.groupId' for\n org.apache.apex:malhar-library:jar with value '*' does not match a\n valid id pattern.\n\n [WARNING]\n [WARNING] It is highly recommended to fix these problems because they\n threaten the stability of your build.\n [WARNING]\n [WARNING] For this reason, future Maven versions might no longer support\n building such malformed projects.\n [WARNING]\n\n\n\n\n\nThis is a bug in early versions of Maven 3.  The dependency exclusion is\nstill valid and it is safe to ignore these warnings.\n\n\nApplication Configuration\n\n\nA configuration file can be used to configure an application.  Different\nkinds of configuration parameters can be specified. They are application\nattributes, operator attributes and properties, port attributes, stream\nproperties and application specific properties. They are all specified\nas name value pairs, in XML format, like the following.\n\n\n?xml version=\n1.0\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome_name_1\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\nsome_name_2\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nApplication attributes\n\n\nApplication attributes are used to specify the platform behavior for the\napplication. They can be specified using the parameter\n\ndt.attr.\nattribute\n. The prefix \u201cdt\u201d is a constant, \u201cattr\u201d is a\nconstant denoting an attribute is being specified and \nattribute\n\nspecifies the name of the attribute. Below is an example snippet setting\nthe streaming windows size of the application to be 1000 milliseconds.\n\n\n  \nproperty\n\n     \nname\ndt.attr.STREAMING_WINDOW_SIZE_MILLIS\n/name\n\n     \nvalue\n1000\n/value\n\n  \n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.DAGContext and the different attributes can\nbe specified in the format described above.\n\n\nOperator attributes\n\n\nOperator attributes are used to specify the platform behavior for the\noperator. They can be specified using the parameter\n\ndt.operator.\noperator-name\n.attr.\nattribute\n. The prefix \u201cdt\u201d is a\nconstant, \u201coperator\u201d is a constant denoting that an operator is being\nspecified, \noperator-name\n denotes the name of the operator, \u201cattr\u201d is\nthe constant denoting that an attribute is being specified and\n\nattribute\n is the name of the attribute. The operator name is the\nsame name that is specified when the operator is added to the DAG using\nthe addOperator method. An example illustrating the specification is\nshown below. It specifies the number of streaming windows for one\napplication window of an operator named \u201cinput\u201d to be 10\n\n\nproperty\n\n  \nname\ndt.operator.input.attr.APPLICATION_WINDOW_COUNT\n/name\n\n  \nvalue\n10\n/value\n\n\n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.OperatorContext and the different attributes\ncan be specified in the format described above.\n\n\nOperator properties\n\n\nOperators can be configured using operator specific properties. The\nproperties can be specified using the parameter\n\ndt.operator.\noperator-name\n.prop.\nproperty-name\n. The difference\nbetween this and the operator attribute specification described above is\nthat the keyword \u201cprop\u201d is used to denote that it is a property and\n\nproperty-name\n specifies the property name.  An example illustrating\nthis is specified below. It specifies the property \u201chostname\u201d of the\nredis server for a \u201credis\u201d output operator.\n\n\n  \nproperty\n\n    \nname\ndt.operator.redis.prop.host\n/name\n\n    \nvalue\n127.0.0.1\n/value\n\n  \n/property\n\n\n\n\n\nThe name tag specifies the property and the value specifies the property\nvalue. The property name is converted to a setter method which is called\non the actual operator. The method name is composed by appending the\nword \u201cset\u201d and the property name with the first character of the name\ncapitalized. In the above example the setter method would become\nsetHost. The method is called using JAVA reflection and the property\nvalue is passed as an argument. In the above example the method setHost\nwill be called on the \u201credis\u201d operator with \u201c127.0.0.1\u201d as the argument.\n\n\nPort attributes\n\n\nPort attributes are used to specify the platform behavior for input and\noutput ports. They can be specified using the parameter \ndt.operator.\noperator-name\n.inputport.\nport-name\n.attr.\nattribute\n\nfor input port and \ndt.operator.\noperator-name\n.outputport.\nport-name\n.attr.\nattribute\n\nfor output port. The keyword \u201cinputport\u201d is used to denote an input port\nand \u201coutputport\u201d to denote an output port. The rest of the specification\nfollows the conventions described in other specifications above. An\nexample illustrating this is specified below. It specifies the queue\ncapacity for an input port named \u201cinput\u201d of an operator named \u201crange\u201d to\nbe 4k.\n\n\nproperty\n\n  \nname\ndt.operator.range.inputport.input.attr.QUEUE_CAPACITY\n/name\n\n  \nvalue\n4000\n/value\n\n\n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.PortContext and the different attributes can\nbe specified in the format described above.\n\n\nThe attributes for an output port can also be specified in a similar way\nas described above with a change that keyword \u201coutputport\u201d is used\ninstead of \u201cintputport\u201d. A generic keyword \u201cport\u201d can be used to specify\neither an input or an output port. It is useful in the wildcard\nspecification described below.\n\n\nStream properties\n\n\nStreams can be configured using stream properties. The properties can be\nspecified using the parameter\n\ndt.stream.\nstream-name\n.prop.\nproperty-name\n  The constant \u201cstream\u201d\nspecifies that it is a stream, \nstream-name\n specifies the name of the\nstream and \nproperty-name\n the name of the property. The name of the\nstream is the same name that is passed when the stream is added to the\nDAG using the addStream method. An example illustrating the\nspecification is shown below. It sets the locality of the stream named\n\u201cstream1\u201d to container local indicating that the operators the stream is\nconnecting be run in the same container.\n\n\n  \nproperty\n\n    \nname\ndt.stream.stream1.prop.locality\n/name\n\n    \nvalue\nCONTAINER_LOCAL\n/value\n\n  \n/property\n\n\n\n\n\nThe property name is converted into a set method on the stream in the\nsame way as described in operator properties section above. In this case\nthe method would be setLocality and it will be called in the stream\n\u201cstream1\u201d with the value as the argument.\n\n\nAlong with the above system defined parameters, the applications can\ndefine their own specific parameters they can be specified in the\nconfiguration file. The only condition is that the names of these\nparameters don\u2019t conflict with the system defined parameters or similar\napplication parameters defined by other applications. To this end, it is\nrecommended that the application parameters have the format\n\nfull-application-class-name\n.\nparam-name\n.\n The\nfull-application-class-name is the full JAVA class name of the\napplication including the package path and param-name is the name of the\nparameter within the application. The application will still have to\nstill read the parameter in using the configuration API of the\nconfiguration object that is passed in populateDAG.\n\n\nWildcards\n\n\nWildcards and regular expressions can be used in place of names to\nspecify a group for applications, operators, ports or streams. For\nexample, to specify an attribute for all ports of an operator it can be\ndone as follows\n\n\nproperty\n\n  \nname\ndt.operator.range.port.*.attr.QUEUE_CAPACITY\n/name\n\n  \nvalue\n4000\n/value\n\n\n/property\n\n\n\n\n\nThe wildcard \u201c*\u201d was used instead of the name of the port. Wildcard can\nalso be used for operator name, stream name or application name. Regular\nexpressions can also be used for names to specify attributes or\nproperties for a specific set.\n\n\nAdding configuration properties\n\n\nIt is common for applications to require configuration parameters to\nrun.  For example, the address and port of the database, the location of\na file for ingestion, etc.  You can specify them in\nsrc/main/resources/META-INF/properties.xml under the App Package\nproject. The properties.xml may look like:\n\n\n?xml version=\n1.0\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome_name_1\n/name\n\n  \n/property\n\n  \nproperty\n\n    \nname\nsome_name_2\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nThe name of an application-specific property takes the form of:\n\n\ndt.operator.{opName}.prop.{propName}\n\n\nThe first represents the property with name propName of operator opName.\n Or you can set the application name at run time by setting this\nproperty:\n\n\n    dt.attr.APPLICATION_NAME\n\n\n\nThere are also other properties that can be set.  For details on\nproperties, refer to the \nOperation and Installation Guide\n.\n\n\nIn this example, property some_name_1 is a required property which\nmust be set at launch time, or it must be set by a pre-set configuration\n(see next section).  Property some_name_2 is a property that is\nassigned with value some_default_value unless it is overridden at\nlaunch time.\n\n\nAdding pre-set configurations\n\n\nAt build time, you can add pre-set configurations to the App Package by\nadding configuration XML files under \nsrc/site/conf/\nconf\n.xml\nin your\nproject.  You can then specify which configuration to use at launch\ntime.  The configuration XML is of the same format of the properties.xml\nfile.\n\n\nApplication-specific properties file\n\n\nYou can also specify properties.xml per application in the application\npackage.  Just create a file with the name properties-{appName}.xml and\nit will be picked up when you launch the application with the specified\nname within the application package.  In short:\n\n\nproperties.xml: Properties that are global to the Configuration\nPackage\n\n\nproperties-{appName}.xml: Properties that are specific when launching\nan application with the specified appName.\n\n\nProperties source precedence\n\n\nIf properties with the same key appear in multiple sources (e.g. from\napp package default configuration as META-INF/properties.xml, from app\npackage configuration in the conf directory, from launch time defines,\netc), the precedence of sources, from highest to lowest, is as follows:\n\n\n\n\nLaunch time defines (using -D option in CLI, or the POST payload\n    with the Gateway REST API\u2019s launch call)\n\n\nLaunch time specified configuration file in file system (using -conf\n    option in CLI)\n\n\nLaunch time specified package configuration (using -apconf option in\n    CLI or the conf={confname} with Gateway REST API\u2019s launch call)\n\n\nConfiguration from \\$HOME/.dt/dt-site.xml\n\n\nApplication defaults within the package as\n    META-INF/properties-{appname}.xml\n\n\nPackage defaults as META-INF/properties.xml\n\n\ndt-site.xml in local DT installation\n\n\ndt-site.xml stored in HDFS\n\n\n\n\nOther meta-data\n\n\nIn a Apex App Package project, the pom.xml file contains a\nsection that looks like:\n\n\nproperties\n\n  \napex.version\n3.2.0-incubating\n/apex.version\n\n  \napex.apppackage.classpath\\\nlib*.jar\n/apex.apppackage.classpath\n\n\n/properties\n\n\n\n\n\napex.version is the Apache Apex version that are to be used\nwith this Application Package.\n\n\napex.apppackage.classpath is the classpath that is used when\nlaunching the application in the Application Package.  The default is\nlib/*.jar, where lib is where all the dependency jars are kept within\nthe Application Package.  One reason to change this field is when your\nApplication Package needs the classpath in a specific order.\n\n\nLogging configuration\n\n\nJust like other Java projects, you can change the logging configuration\nby having your log4j.properties under src/main/resources.  For example,\nif you have the following in src/main/resources/log4j.properties:\n\n\n log4j.rootLogger=WARN,CONSOLE\n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\n log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p\n %c{2} %M - %m%n\n\n\n\n\nThe root logger\u2019s level is set to WARN and the output is set to the console (stdout).\n\n\nNote that by default from project created from the maven archetype,\nthere is already a log4j.properties file under src/test/resources and\nthat file is only used for the unit test.\n\n\nZip Structure of Application Package\n\n\nApache Apex Application Package files are zip files.  You can examine the content of any Application Package by using unzip -t on your Linux command line.\n\n\nThere are four top level directories in an Application Package:\n\n\n\n\n\"app\" contains the jar files of the DAG code and any custom operators.\n\n\n\"lib\" contains all dependency jars\n\n\n\"conf\" contains all the pre-set configuration XML files.\n\n\n\"META-INF\" contains the MANIFEST.MF file and the properties.xml file.\n\n\n\u201cresources\u201d contains other files that are to be served by the Gateway on behalf of the app package.\n\n\n\n\nManaging Application Packages Through DT Gateway\n\n\nThe DT Gateway provides storing and retrieving Application Packages to\nand from your distributed file system, e.g. HDFS.\n\n\nStoring an Application Package\n\n\nYou can store your Application Packages through DT Gateway using this\nREST call:\n\n\n POST /ws/v2/appPackages\n\n\n\n\nThe payload is the raw content of your Application Package.  For\nexample, you can issue this request using curl on your Linux command\nline like this, assuming your DT Gateway is accepting requests at\nlocalhost:9090:\n\n\n$ curl -XPOST -T \napp-package-file\n http://localhost:9090/ws/v2/appPackages\n\n\n\n\nGetting Meta Information on Application Packages\n\n\nYou can get the meta information on Application Packages stored through\nDT Gateway using this call.  The information includes the logical plan\nof each application within the Application Package.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}\n\n\n\n\nGetting Available Operators In Application Package\n\n\nYou can get the list of available operators in the Application Package\nusing this call.\n\n\nGET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators?parent={parent}\n\n\n\n\nThe parent parameter is optional.  If given, parent should be the fully\nqualified class name.  It will only return operators that derive from\nthat class or interface. For example, if parent is\ncom.datatorrent.api.InputOperator, this call will only return input\noperators provided by the Application Package.\n\n\nGetting Properties of Operators in Application Package\n\n\nYou can get the list of properties of any operator in the Application\nPackage using this call.\n\n\nGET  /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators/{className}\n\n\n\n\nGetting List of Pre-Set Configurations in Application Package\n\n\nYou can get a list of pre-set configurations within the Application\nPackage using this call.\n\n\nGET /ws/v2/appPackages/{owner}/{pkgName}/{packageVersion}/configs\n\n\n\n\nYou can also get the content of a specific pre-set configuration within\nthe Application Package.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nChanging Pre-Set Configurations in Application Package\n\n\nYou can create or replace pre-set configurations within the Application\nPackage\n\n\n PUT   /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nThe payload of this PUT call is the XML file that represents the pre-set configuration.  The Content-Type of the payload is \"application/xml\" and you can delete a pre-set configuration within the Application Package.\n\n\n DELETE /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nRetrieving an Application Package\n\n\nYou can download the Application Package file.  This Application Package\nis not necessarily the same file as the one that was originally uploaded\nsince the pre-set configurations may have been modified.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/download\n\n\n\n\nLaunching an Application Package\n\n\nYou can launch an application within an Application Package.\n\n\nPOST /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/applications/{appName}/launch?config={configName}\n\n\n\n\nThe config parameter is optional.  If given, it must be one of the\npre-set configuration within the given Application Package.  The\nContent-Type of the payload of the POST request is \"application/json\"\nand should contain the properties to be launched with the application.\n It is of the form:\n\n\n {\nproperty-name\n:\nproperty-value\n, ... }\n\n\n\n\nHere is an example of launching an application through curl:\n\n\n $ curl -XPOST -d'{\ndt.operator.console.prop.stringFormat\n:\nxyz %s\n}'\n http://localhost:9090/ws/v2/appPackages/dtadmin/mydtapp/1.0-SNAPSHOT/app\n lications/MyFirstApplication/launch\n\n\n\n\nPlease refer to the \nGateway API reference\n for the complete specification of the REST API.\n\n\nExamining and Launching Application Packages Through Apex CLI\n\n\nIf you are working with Application Packages in the local filesystem and\ndo not want to deal with dtGateway, you can use the Apex Command Line Interface (dtcli).  Please refer to the \nGateway API\n\nto see samples for these commands.\n\n\nGetting Application Package Meta Information\n\n\nYou can get the meta information about the Application Package using\nthis Apex CLI command.\n\n\n dt\n get-app-package-info \napp-package-file\n\n\n\n\n\nGetting Available Operators In Application Package\n\n\nYou can get the list of available operators in the Application Package\nusing this command.\n\n\n dt\n get-app-package-operators \napp-package-file\n \npackage-prefix\n\n [parent-class]\n\n\n\n\nGetting Properties of Operators in Application Package\n\n\nYou can get the list of properties of any operator in the Application\nPackage using this command.\n\n\ndt\n get-app-package-operator-properties \n \n\n\nLaunching an Application Package\n\n\nYou can launch an application within an Application Package.\n\n\ndt\n launch [-D property-name=property-value, ...] [-conf config-name]\n [-apconf config-file-within-app-package] \napp-package-file\n\n [matching-app-name]\n\n\n\n\nNote that -conf expects a configuration file in the file system, while -apconf expects a configuration file within the app package.", 
            "title": "Application Packages"
        }, 
        {
            "location": "/application_packages/#apache-apex-application-packages", 
            "text": "An Apache Apex Application Package is a zip file that contains all the\nnecessary files to launch an application in Apache Apex. It is the\nstandard way for assembling and sharing an Apache Apex application.", 
            "title": "Apache Apex Application Packages"
        }, 
        {
            "location": "/application_packages/#requirements", 
            "text": "You will need have the following installed:   Apache Maven 3.0 or later (for assembling the App Package)  Apache Apex 3.0.0 or later (for launching the App Package in your cluster)", 
            "title": "Requirements"
        }, 
        {
            "location": "/application_packages/#creating-your-first-apex-app-package", 
            "text": "You can create an Apex Application Package using your Linux command\nline, or using your favorite IDE.", 
            "title": "Creating Your First Apex App Package"
        }, 
        {
            "location": "/application_packages/#using-command-line", 
            "text": "First, change to the directory where you put your projects, and create\nan Apex application project using Maven by running the following\ncommand.  Replace \"com.example\", \"mydtapp\" and \"1.0-SNAPSHOT\" with the\nappropriate values (make sure this is all on one line):  $ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-app-archetype -DarchetypeVersion=3.2.0-incubating \\\n -DgroupId=com.example -Dpackage=com.example.mydtapp -DartifactId=mydtapp \\\n -Dversion=1.0-SNAPSHOT  This creates a Maven project named \"mydtapp\". Open it with your favorite\nIDE (e.g. NetBeans, Eclipse, IntelliJ IDEA). In the project, there is a\nsample DAG that generates a number of tuples with a random number and\nprints out \"hello world\" and the random number in the tuples.  The code\nthat builds the DAG is in\nsrc/main/java/com/example/mydtapp/Application.java, and the code that\nruns the unit test for the DAG is in\nsrc/test/java/com/example/mydtapp/ApplicationTest.java. Try it out by\nrunning the following command:  $cd mydtapp; mvn package  This builds the App Package runs the unit test of the DAG.  You should\nbe getting test output similar to this:   -------------------------------------------------------\n  TESTS\n -------------------------------------------------------\n\n Running com.example.mydtapp.ApplicationTest\n hello world: 0.8015370953286478\n hello world: 0.9785359225545481\n hello world: 0.6322611586644047\n hello world: 0.8460953663451775\n hello world: 0.5719372906929072\n hello world: 0.6361174312337172\n hello world: 0.14873007534816318\n hello world: 0.8866986277418261\n hello world: 0.6346526809866057\n hello world: 0.48587295703904465\n hello world: 0.6436832429676687\n\n ...\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863\n sec\n\n Results :\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0  The \"mvn package\" command creates the App Package file in target\ndirectory as target/mydtapp-1.0-SNAPSHOT.apa. You will be able to use\nthat App Package file to launch this sample application in your actual\nApex installation.", 
            "title": "Using Command Line"
        }, 
        {
            "location": "/application_packages/#using-ide", 
            "text": "Alternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.   Then fill the Group ID, Artifact ID, Version and Repository entries as shown below.   Group ID: org.apache.apex\nArtifact ID: apex-app-archetype\nVersion: 3.2.0-incubating (or any later version)  Press Next and fill out the rest of the required information. For\nexample:   Click Finish, and now you have created your own Apache Apex App Package\nproject, with a default unit test.  You can run the unit test, make code\nchanges or make dependency changes within your IDE.  The procedure for\nother IDEs, like Eclipse or IntelliJ, is similar.", 
            "title": "Using IDE"
        }, 
        {
            "location": "/application_packages/#writing-your-own-app-package", 
            "text": "Please refer to the  Creating Apps  on the basics on how to write an Apache Apex application.  In your AppPackage project, you can add custom operators (refer to  Operator Development Guide ), project dependencies, default and required configuration properties, pre-set configurations and other metadata.", 
            "title": "Writing Your Own App Package"
        }, 
        {
            "location": "/application_packages/#adding-and-removing-project-dependencies", 
            "text": "Under the project, you can add project dependencies in pom.xml, or do it\nthrough your IDE.  Here\u2019s the section that describes the dependencies in\nthe default pom.xml:     dependencies \n     !-- add your dependencies here -- \n     dependency \n       groupId org.apache.apex /groupId \n       artifactId malhar-library /artifactId \n       version ${apex.version} /version \n       !--\n           If you know your application do not need the transitive dependencies that are pulled in by malhar-library,\n           Uncomment the following to reduce the size of your app package.\n      -- \n       !--\n       exclusions \n         exclusion \n           groupId * /groupId \n           artifactId * /artifactId \n         /exclusion \n       /exclusions \n      -- \n     /dependency \n     dependency \n       groupId org.apache.apex /groupId \n       artifactId apex-engine /artifactId \n       version ${apex.version} /version \n       scope provided /scope \n     /dependency \n     dependency \n       groupId junit /groupId \n       artifactId junit /artifactId \n       version 4.10 /version \n       scope test /scope \n     /dependency \n   /dependencies   By default, as shown above, the default dependencies include\nmalhar-library in compile scope, dt-engine in provided scope, and junit\nin test scope.  Do not remove these three dependencies since they are\nnecessary for any Apex application.  You can, however, exclude\ntransitive dependencies from malhar-library to reduce the size of your\nApp Package, provided that none of the operators in malhar-library that\nneed the transitive dependencies will be used in your application.  In the sample application, it is safe to remove the transitive\ndependencies from malhar-library, by uncommenting the \"exclusions\"\nsection.  It will reduce the size of the sample App Package from 8MB to\n700KB.  Note that if we exclude *, in some versions of Maven, you may get\nwarnings similar to the following:  \n [WARNING] 'dependencies.dependency.exclusions.exclusion.groupId' for\n org.apache.apex:malhar-library:jar with value '*' does not match a\n valid id pattern.\n\n [WARNING]\n [WARNING] It is highly recommended to fix these problems because they\n threaten the stability of your build.\n [WARNING]\n [WARNING] For this reason, future Maven versions might no longer support\n building such malformed projects.\n [WARNING]  This is a bug in early versions of Maven 3.  The dependency exclusion is\nstill valid and it is safe to ignore these warnings.", 
            "title": "Adding (and removing) project dependencies"
        }, 
        {
            "location": "/application_packages/#application-configuration", 
            "text": "A configuration file can be used to configure an application.  Different\nkinds of configuration parameters can be specified. They are application\nattributes, operator attributes and properties, port attributes, stream\nproperties and application specific properties. They are all specified\nas name value pairs, in XML format, like the following.  ?xml version= 1.0 ?  configuration \n   property \n     name some_name_1 /name \n     value some_default_value /value \n   /property \n   property \n     name some_name_2 /name \n     value some_default_value /value \n   /property  /configuration", 
            "title": "Application Configuration"
        }, 
        {
            "location": "/application_packages/#application-attributes", 
            "text": "Application attributes are used to specify the platform behavior for the\napplication. They can be specified using the parameter dt.attr. attribute . The prefix \u201cdt\u201d is a constant, \u201cattr\u201d is a\nconstant denoting an attribute is being specified and  attribute \nspecifies the name of the attribute. Below is an example snippet setting\nthe streaming windows size of the application to be 1000 milliseconds.     property \n      name dt.attr.STREAMING_WINDOW_SIZE_MILLIS /name \n      value 1000 /value \n   /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.DAGContext and the different attributes can\nbe specified in the format described above.", 
            "title": "Application attributes"
        }, 
        {
            "location": "/application_packages/#operator-attributes", 
            "text": "Operator attributes are used to specify the platform behavior for the\noperator. They can be specified using the parameter dt.operator. operator-name .attr. attribute . The prefix \u201cdt\u201d is a\nconstant, \u201coperator\u201d is a constant denoting that an operator is being\nspecified,  operator-name  denotes the name of the operator, \u201cattr\u201d is\nthe constant denoting that an attribute is being specified and attribute  is the name of the attribute. The operator name is the\nsame name that is specified when the operator is added to the DAG using\nthe addOperator method. An example illustrating the specification is\nshown below. It specifies the number of streaming windows for one\napplication window of an operator named \u201cinput\u201d to be 10  property \n   name dt.operator.input.attr.APPLICATION_WINDOW_COUNT /name \n   value 10 /value  /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.OperatorContext and the different attributes\ncan be specified in the format described above.", 
            "title": "Operator attributes"
        }, 
        {
            "location": "/application_packages/#operator-properties", 
            "text": "Operators can be configured using operator specific properties. The\nproperties can be specified using the parameter dt.operator. operator-name .prop. property-name . The difference\nbetween this and the operator attribute specification described above is\nthat the keyword \u201cprop\u201d is used to denote that it is a property and property-name  specifies the property name.  An example illustrating\nthis is specified below. It specifies the property \u201chostname\u201d of the\nredis server for a \u201credis\u201d output operator.     property \n     name dt.operator.redis.prop.host /name \n     value 127.0.0.1 /value \n   /property   The name tag specifies the property and the value specifies the property\nvalue. The property name is converted to a setter method which is called\non the actual operator. The method name is composed by appending the\nword \u201cset\u201d and the property name with the first character of the name\ncapitalized. In the above example the setter method would become\nsetHost. The method is called using JAVA reflection and the property\nvalue is passed as an argument. In the above example the method setHost\nwill be called on the \u201credis\u201d operator with \u201c127.0.0.1\u201d as the argument.", 
            "title": "Operator properties"
        }, 
        {
            "location": "/application_packages/#port-attributes", 
            "text": "Port attributes are used to specify the platform behavior for input and\noutput ports. They can be specified using the parameter  dt.operator. operator-name .inputport. port-name .attr. attribute \nfor input port and  dt.operator. operator-name .outputport. port-name .attr. attribute \nfor output port. The keyword \u201cinputport\u201d is used to denote an input port\nand \u201coutputport\u201d to denote an output port. The rest of the specification\nfollows the conventions described in other specifications above. An\nexample illustrating this is specified below. It specifies the queue\ncapacity for an input port named \u201cinput\u201d of an operator named \u201crange\u201d to\nbe 4k.  property \n   name dt.operator.range.inputport.input.attr.QUEUE_CAPACITY /name \n   value 4000 /value  /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.PortContext and the different attributes can\nbe specified in the format described above.  The attributes for an output port can also be specified in a similar way\nas described above with a change that keyword \u201coutputport\u201d is used\ninstead of \u201cintputport\u201d. A generic keyword \u201cport\u201d can be used to specify\neither an input or an output port. It is useful in the wildcard\nspecification described below.", 
            "title": "Port attributes"
        }, 
        {
            "location": "/application_packages/#stream-properties", 
            "text": "Streams can be configured using stream properties. The properties can be\nspecified using the parameter dt.stream. stream-name .prop. property-name   The constant \u201cstream\u201d\nspecifies that it is a stream,  stream-name  specifies the name of the\nstream and  property-name  the name of the property. The name of the\nstream is the same name that is passed when the stream is added to the\nDAG using the addStream method. An example illustrating the\nspecification is shown below. It sets the locality of the stream named\n\u201cstream1\u201d to container local indicating that the operators the stream is\nconnecting be run in the same container.     property \n     name dt.stream.stream1.prop.locality /name \n     value CONTAINER_LOCAL /value \n   /property   The property name is converted into a set method on the stream in the\nsame way as described in operator properties section above. In this case\nthe method would be setLocality and it will be called in the stream\n\u201cstream1\u201d with the value as the argument.  Along with the above system defined parameters, the applications can\ndefine their own specific parameters they can be specified in the\nconfiguration file. The only condition is that the names of these\nparameters don\u2019t conflict with the system defined parameters or similar\napplication parameters defined by other applications. To this end, it is\nrecommended that the application parameters have the format full-application-class-name . param-name .  The\nfull-application-class-name is the full JAVA class name of the\napplication including the package path and param-name is the name of the\nparameter within the application. The application will still have to\nstill read the parameter in using the configuration API of the\nconfiguration object that is passed in populateDAG.", 
            "title": "Stream properties"
        }, 
        {
            "location": "/application_packages/#wildcards", 
            "text": "Wildcards and regular expressions can be used in place of names to\nspecify a group for applications, operators, ports or streams. For\nexample, to specify an attribute for all ports of an operator it can be\ndone as follows  property \n   name dt.operator.range.port.*.attr.QUEUE_CAPACITY /name \n   value 4000 /value  /property   The wildcard \u201c*\u201d was used instead of the name of the port. Wildcard can\nalso be used for operator name, stream name or application name. Regular\nexpressions can also be used for names to specify attributes or\nproperties for a specific set.", 
            "title": "Wildcards"
        }, 
        {
            "location": "/application_packages/#adding-configuration-properties", 
            "text": "It is common for applications to require configuration parameters to\nrun.  For example, the address and port of the database, the location of\na file for ingestion, etc.  You can specify them in\nsrc/main/resources/META-INF/properties.xml under the App Package\nproject. The properties.xml may look like:  ?xml version= 1.0 ?  configuration \n   property \n     name some_name_1 /name \n   /property \n   property \n     name some_name_2 /name \n     value some_default_value /value \n   /property  /configuration   The name of an application-specific property takes the form of:  dt.operator.{opName}.prop.{propName}  The first represents the property with name propName of operator opName.\n Or you can set the application name at run time by setting this\nproperty:      dt.attr.APPLICATION_NAME  There are also other properties that can be set.  For details on\nproperties, refer to the  Operation and Installation Guide .  In this example, property some_name_1 is a required property which\nmust be set at launch time, or it must be set by a pre-set configuration\n(see next section).  Property some_name_2 is a property that is\nassigned with value some_default_value unless it is overridden at\nlaunch time.", 
            "title": "Adding configuration properties"
        }, 
        {
            "location": "/application_packages/#adding-pre-set-configurations", 
            "text": "At build time, you can add pre-set configurations to the App Package by\nadding configuration XML files under  src/site/conf/ conf .xml in your\nproject.  You can then specify which configuration to use at launch\ntime.  The configuration XML is of the same format of the properties.xml\nfile.", 
            "title": "Adding pre-set configurations"
        }, 
        {
            "location": "/application_packages/#application-specific-properties-file", 
            "text": "You can also specify properties.xml per application in the application\npackage.  Just create a file with the name properties-{appName}.xml and\nit will be picked up when you launch the application with the specified\nname within the application package.  In short:  properties.xml: Properties that are global to the Configuration\nPackage  properties-{appName}.xml: Properties that are specific when launching\nan application with the specified appName.", 
            "title": "Application-specific properties file"
        }, 
        {
            "location": "/application_packages/#properties-source-precedence", 
            "text": "If properties with the same key appear in multiple sources (e.g. from\napp package default configuration as META-INF/properties.xml, from app\npackage configuration in the conf directory, from launch time defines,\netc), the precedence of sources, from highest to lowest, is as follows:   Launch time defines (using -D option in CLI, or the POST payload\n    with the Gateway REST API\u2019s launch call)  Launch time specified configuration file in file system (using -conf\n    option in CLI)  Launch time specified package configuration (using -apconf option in\n    CLI or the conf={confname} with Gateway REST API\u2019s launch call)  Configuration from \\$HOME/.dt/dt-site.xml  Application defaults within the package as\n    META-INF/properties-{appname}.xml  Package defaults as META-INF/properties.xml  dt-site.xml in local DT installation  dt-site.xml stored in HDFS", 
            "title": "Properties source precedence"
        }, 
        {
            "location": "/application_packages/#other-meta-data", 
            "text": "In a Apex App Package project, the pom.xml file contains a\nsection that looks like:  properties \n   apex.version 3.2.0-incubating /apex.version \n   apex.apppackage.classpath\\ lib*.jar /apex.apppackage.classpath  /properties   apex.version is the Apache Apex version that are to be used\nwith this Application Package.  apex.apppackage.classpath is the classpath that is used when\nlaunching the application in the Application Package.  The default is\nlib/*.jar, where lib is where all the dependency jars are kept within\nthe Application Package.  One reason to change this field is when your\nApplication Package needs the classpath in a specific order.", 
            "title": "Other meta-data"
        }, 
        {
            "location": "/application_packages/#logging-configuration", 
            "text": "Just like other Java projects, you can change the logging configuration\nby having your log4j.properties under src/main/resources.  For example,\nif you have the following in src/main/resources/log4j.properties:   log4j.rootLogger=WARN,CONSOLE\n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\n log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p\n %c{2} %M - %m%n  The root logger\u2019s level is set to WARN and the output is set to the console (stdout).  Note that by default from project created from the maven archetype,\nthere is already a log4j.properties file under src/test/resources and\nthat file is only used for the unit test.", 
            "title": "Logging configuration"
        }, 
        {
            "location": "/application_packages/#zip-structure-of-application-package", 
            "text": "Apache Apex Application Package files are zip files.  You can examine the content of any Application Package by using unzip -t on your Linux command line.  There are four top level directories in an Application Package:   \"app\" contains the jar files of the DAG code and any custom operators.  \"lib\" contains all dependency jars  \"conf\" contains all the pre-set configuration XML files.  \"META-INF\" contains the MANIFEST.MF file and the properties.xml file.  \u201cresources\u201d contains other files that are to be served by the Gateway on behalf of the app package.", 
            "title": "Zip Structure of Application Package"
        }, 
        {
            "location": "/application_packages/#managing-application-packages-through-dt-gateway", 
            "text": "The DT Gateway provides storing and retrieving Application Packages to\nand from your distributed file system, e.g. HDFS.", 
            "title": "Managing Application Packages Through DT Gateway"
        }, 
        {
            "location": "/application_packages/#storing-an-application-package", 
            "text": "You can store your Application Packages through DT Gateway using this\nREST call:   POST /ws/v2/appPackages  The payload is the raw content of your Application Package.  For\nexample, you can issue this request using curl on your Linux command\nline like this, assuming your DT Gateway is accepting requests at\nlocalhost:9090:  $ curl -XPOST -T  app-package-file  http://localhost:9090/ws/v2/appPackages", 
            "title": "Storing an Application Package"
        }, 
        {
            "location": "/application_packages/#getting-meta-information-on-application-packages", 
            "text": "You can get the meta information on Application Packages stored through\nDT Gateway using this call.  The information includes the logical plan\nof each application within the Application Package.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}", 
            "title": "Getting Meta Information on Application Packages"
        }, 
        {
            "location": "/application_packages/#getting-available-operators-in-application-package", 
            "text": "You can get the list of available operators in the Application Package\nusing this call.  GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators?parent={parent}  The parent parameter is optional.  If given, parent should be the fully\nqualified class name.  It will only return operators that derive from\nthat class or interface. For example, if parent is\ncom.datatorrent.api.InputOperator, this call will only return input\noperators provided by the Application Package.", 
            "title": "Getting Available Operators In Application Package"
        }, 
        {
            "location": "/application_packages/#getting-properties-of-operators-in-application-package", 
            "text": "You can get the list of properties of any operator in the Application\nPackage using this call.  GET  /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators/{className}", 
            "title": "Getting Properties of Operators in Application Package"
        }, 
        {
            "location": "/application_packages/#getting-list-of-pre-set-configurations-in-application-package", 
            "text": "You can get a list of pre-set configurations within the Application\nPackage using this call.  GET /ws/v2/appPackages/{owner}/{pkgName}/{packageVersion}/configs  You can also get the content of a specific pre-set configuration within\nthe Application Package.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}", 
            "title": "Getting List of Pre-Set Configurations in Application Package"
        }, 
        {
            "location": "/application_packages/#changing-pre-set-configurations-in-application-package", 
            "text": "You can create or replace pre-set configurations within the Application\nPackage   PUT   /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}  The payload of this PUT call is the XML file that represents the pre-set configuration.  The Content-Type of the payload is \"application/xml\" and you can delete a pre-set configuration within the Application Package.   DELETE /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}", 
            "title": "Changing Pre-Set Configurations in Application Package"
        }, 
        {
            "location": "/application_packages/#retrieving-an-application-package", 
            "text": "You can download the Application Package file.  This Application Package\nis not necessarily the same file as the one that was originally uploaded\nsince the pre-set configurations may have been modified.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/download", 
            "title": "Retrieving an Application Package"
        }, 
        {
            "location": "/application_packages/#launching-an-application-package", 
            "text": "You can launch an application within an Application Package.  POST /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/applications/{appName}/launch?config={configName}  The config parameter is optional.  If given, it must be one of the\npre-set configuration within the given Application Package.  The\nContent-Type of the payload of the POST request is \"application/json\"\nand should contain the properties to be launched with the application.\n It is of the form:   { property-name : property-value , ... }  Here is an example of launching an application through curl:   $ curl -XPOST -d'{ dt.operator.console.prop.stringFormat : xyz %s }'\n http://localhost:9090/ws/v2/appPackages/dtadmin/mydtapp/1.0-SNAPSHOT/app\n lications/MyFirstApplication/launch  Please refer to the  Gateway API reference  for the complete specification of the REST API.", 
            "title": "Launching an Application Package"
        }, 
        {
            "location": "/application_packages/#examining-and-launching-application-packages-through-apex-cli", 
            "text": "If you are working with Application Packages in the local filesystem and\ndo not want to deal with dtGateway, you can use the Apex Command Line Interface (dtcli).  Please refer to the  Gateway API \nto see samples for these commands.", 
            "title": "Examining and Launching Application Packages Through Apex CLI"
        }, 
        {
            "location": "/application_packages/#getting-application-package-meta-information", 
            "text": "You can get the meta information about the Application Package using\nthis Apex CLI command.   dt  get-app-package-info  app-package-file", 
            "title": "Getting Application Package Meta Information"
        }, 
        {
            "location": "/application_packages/#getting-available-operators-in-application-package_1", 
            "text": "You can get the list of available operators in the Application Package\nusing this command.   dt  get-app-package-operators  app-package-file   package-prefix \n [parent-class]", 
            "title": "Getting Available Operators In Application Package"
        }, 
        {
            "location": "/application_packages/#getting-properties-of-operators-in-application-package_1", 
            "text": "You can get the list of properties of any operator in the Application\nPackage using this command.  dt  get-app-package-operator-properties", 
            "title": "Getting Properties of Operators in Application Package"
        }, 
        {
            "location": "/application_packages/#launching-an-application-package_1", 
            "text": "You can launch an application within an Application Package.  dt  launch [-D property-name=property-value, ...] [-conf config-name]\n [-apconf config-file-within-app-package]  app-package-file \n [matching-app-name]  Note that -conf expects a configuration file in the file system, while -apconf expects a configuration file within the app package.", 
            "title": "Launching an Application Package"
        }, 
        {
            "location": "/configuration_packages/", 
            "text": "Apache Apex Configuration Packages\n\n\nAn Apache Apex Application Configuration Package is a zip file that contains\nconfiguration files and additional files to be launched with an\n\nApplication Package\n using \nDTCLI or REST API.  This guide assumes the reader\u2019s familiarity of\nApplication Package.  Please read the Application Package document to\nget yourself familiar with the concept first if you have not done so.\n\n\nRequirements\n\n\nYou will need have the following installed:\n\n\n\n\nApache Maven 3.0 or later (for assembling the Config Package)\n\n\nApex 3.0.0 or later (for launching the App Package with the Config\n    Package in your cluster)\n\n\n\n\nCreating Your First Configuration Package\n\n\nYou can create a Configuration Package using your Linux command line, or\nusing your favorite IDE.  \n\n\nUsing Command Line\n\n\nFirst, change to the directory where you put your projects, and create a\nDT configuration project using Maven by running the following command.\n Replace \"com.example\", \"mydtconfig\" and \"1.0-SNAPSHOT\" with the\nappropriate values:\n\n\n$ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-conf-archetype -DarchetypeVersion=3.2.0-incubating \\\n -DgroupId=com.example -Dpackage=com.example.mydtconfig -DartifactId=mydtconfig \\\n -Dversion=1.0-SNAPSHOT\n\n\n\nThis creates a Maven project named \"mydtconfig\". Open it with your\nfavorite IDE (e.g. NetBeans, Eclipse, IntelliJ IDEA).  Try it out by\nrunning the following command:\n\n\n$ mvn package                                                         \n\n\n\n\nThe \"mvn package\" command creates the Config Package file in target\ndirectory as target/mydtconfig.apc. You will be able to use that\nConfiguration Package file to launch an Apache Apex application.\n\n\nUsing IDE\n\n\nAlternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.\n\n\n\n\nThen fill the Group ID, Artifact ID, Version and Repository entries as\nshown below.\n\n\n\n\nGroup ID: org.apache.apex\nArtifact ID: apex-conf-archetype\nVersion: 3.2.0-incubating (or any later version)\n\n\nPress Next and fill out the rest of the required information. For\nexample:\n\n\n\n\nClick Finish, and now you have created your own Apex\nConfiguration Package project.  The procedure for other IDEs, like\nEclipse or IntelliJ, is similar.\n\n\nAssembling your own configuration package\n\n\nInside the project created by the archetype, these are the files that\nyou should know about when assembling your own configuration package:\n\n\n./pom.xml\n./src/main/resources/classpath\n./src/main/resources/files\n./src/main/resources/META-INF/properties.xml\n./src/main/resources/META-INF/properties-{appname}.xml\n\n\n\npom.xml\n\n\nExample:\n\n\n  \ngroupId\ncom.example\n/groupId\n\n  \nversion\n1.0.0\n/version\n\n  \nartifactId\nmydtconf\n/artifactId\n\n  \npackaging\njar\n/packaging\n\n  \n!-- change these to the appropriate values --\n\n  \nname\nMy DataTorrent Application Configuration\n/name\n\n  \ndescription\nMy DataTorrent Application Configuration Description\n/description\n\n  \nproperties\n\n    \ndatatorrent.apppackage.name\nmydtapp\n/datatorrent.apppackage.name\n\n    \ndatatorrent.apppackage.minversion\n1.0.0\n/datatorrent.apppackage.minversion\n\n   \ndatatorrent.apppackage.maxversion\n1.9999.9999\n/datatorrent.apppackage.maxversion\n\n    \ndatatorrent.appconf.classpath\nclasspath/*\n/datatorrent.appconf.classpath\n\n    \ndatatorrent.appconf.files\nfiles/*\n/datatorrent.appconf.files\n\n  \n/properties\n \n\n\n\n\n\nIn pom.xml, you can change the following keys to your desired values\n\n\n\n\ngroupId\n\n\nversion\n\n\nartifactId\n\n\nname\n\n\ndescription\n\n\n\n\nYou can also change the values of \n\n\n\n\ndatatorrent.apppackage.name\n\n\ndatatorrent.apppackage.minversion\n\n\ndatatorrent.apppackage.maxversion\n\n\n\n\nto reflect what app packages should be used with this configuration package.  Apex will use this information to check whether a\nconfiguration package is compatible with the application package when you issue a launch command.\n\n\n./src/main/resources/classpath\n\n\nPlace any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application and included in the\nclasspath of the application.  Example of such files are Java properties\nfiles and jar files.\n\n\n./src/main/resources/files\n\n\nPlace any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application but not included in the\nclasspath of the application.\n\n\nProperties XML file\n\n\nA properties xml file consists of a set of key-value pairs.  The set of\nkey-value pairs specifies the configuration options the application\nshould be launched with.  \n\n\nExample:\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome-property-name\n/name\n\n    \nvalue\nsome-property-value\n/value\n\n  \n/property\n\n   ...\n\n/configuration\n\n\n\n\n\nNames of properties XML file:\n\n\n\n\nproperties.xml:\n Properties that are global to the Configuration\nPackage\n\n\nproperties-{appName}.xml:\n Properties that are specific when launching\nan application with the specified appName within the Application\nPackage.\n\n\n\n\nAfter you are done with the above, remember to do mvn package to\ngenerate a new configuration package, which will be located in the\ntarget directory in your project.\n\n\nZip structure of configuration package\n\n\nApex Application Configuration Package files are zip files.  You\ncan examine the content of any Application Configuration Package by\nusing unzip -t on your Linux command line.  The structure of the zip\nfile is as follow:\n\n\nMETA-INF\n  MANIFEST.MF\n  properties.xml\n  properties-{appname}.xml\nclasspath\n  {classpath files}\nfiles\n  {files} \n\n\n\n\nLaunching with CLI\n\n\n-conf\n option of the launch command in CLI supports specifying configuration package in the local filesystem.  Example:\n\n\ndt\\\n launch DTApp-mydtapp-1.0.0.jar -conf DTConfig-mydtconfig-1.0.0.jar\n\n\n\nThis command expects both the application package and the configuration package to be in the local file system.\n\n\nRelated REST API\n\n\nPOST /ws/v2/configPackages\n\n\nPayload: Raw content of configuration package zip\n\n\nFunction: Creates or replace a configuration package zip file in HDFS\n\n\nCurl example:\n\n\n$ curl -XPOST -T DTConfig-{name}.jar http://{yourhost:port}/ws/v2/configPackages\n\n\n\nGET /ws/v2/configPackages?appPackageName=...\nappPackageVersion=...\n\n\nAll query parameters are optional\n\n\nFunction: Returns the configuration packages that the user is authorized to use and that are compatible with the specified appPackageName, appPackageVersion and appName. \n\n\nGET /ws/v2/configPackages/\nuser\n?appPackageName=...\nappPackageVersion=...\n\n\nAll query parameters are optional\n\n\nFunction: Returns the configuration packages under the specified user and that are compatible with the specified appPackageName, appPackageVersion and appName.\n\n\nGET /ws/v2/configPackages/\nuser\n/\nname\n\n\nFunction: Returns the information of the specified configuration package\n\n\nGET /ws/v2/configPackages/\nuser\n/\nname\n/download\n\n\nFunction: Returns the raw config package file\n\n\nCurl example:\n\n\n$ curl http://{yourhost:port}/ws/v2/configPackages/{user}/{name}/download \\\n DTConfig-xyz.jar\n$ unzip -t DTConfig-xyz.jar\n\n\n\n\nPOST /ws/v2/appPackages/\nuser\n/\napp-pkg-name\n/\napp-pkg-version\n/applications/{app-name}/launch?configPackage=\nuser\n/\nconfpkgname\n\n\nFunction: Launches the app package with the specified configuration package stored in HDFS.\n\n\nCurl example:\n\n\n$ curl -XPOST -d \u2019{}\u2019 http://{yourhost:port}/ws/v2/appPackages/{user}/{app-pkg-name}/{app-pkg-version}/applications/{app-name}/launch?configPackage={user}/{confpkgname}", 
            "title": "Configuration Packages"
        }, 
        {
            "location": "/configuration_packages/#apache-apex-configuration-packages", 
            "text": "An Apache Apex Application Configuration Package is a zip file that contains\nconfiguration files and additional files to be launched with an Application Package  using \nDTCLI or REST API.  This guide assumes the reader\u2019s familiarity of\nApplication Package.  Please read the Application Package document to\nget yourself familiar with the concept first if you have not done so.", 
            "title": "Apache Apex Configuration Packages"
        }, 
        {
            "location": "/configuration_packages/#requirements", 
            "text": "You will need have the following installed:   Apache Maven 3.0 or later (for assembling the Config Package)  Apex 3.0.0 or later (for launching the App Package with the Config\n    Package in your cluster)", 
            "title": "Requirements"
        }, 
        {
            "location": "/configuration_packages/#creating-your-first-configuration-package", 
            "text": "You can create a Configuration Package using your Linux command line, or\nusing your favorite IDE.", 
            "title": "Creating Your First Configuration Package"
        }, 
        {
            "location": "/configuration_packages/#using-command-line", 
            "text": "First, change to the directory where you put your projects, and create a\nDT configuration project using Maven by running the following command.\n Replace \"com.example\", \"mydtconfig\" and \"1.0-SNAPSHOT\" with the\nappropriate values:  $ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-conf-archetype -DarchetypeVersion=3.2.0-incubating \\\n -DgroupId=com.example -Dpackage=com.example.mydtconfig -DartifactId=mydtconfig \\\n -Dversion=1.0-SNAPSHOT  This creates a Maven project named \"mydtconfig\". Open it with your\nfavorite IDE (e.g. NetBeans, Eclipse, IntelliJ IDEA).  Try it out by\nrunning the following command:  $ mvn package                                                           The \"mvn package\" command creates the Config Package file in target\ndirectory as target/mydtconfig.apc. You will be able to use that\nConfiguration Package file to launch an Apache Apex application.", 
            "title": "Using Command Line"
        }, 
        {
            "location": "/configuration_packages/#using-ide", 
            "text": "Alternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.   Then fill the Group ID, Artifact ID, Version and Repository entries as\nshown below.   Group ID: org.apache.apex\nArtifact ID: apex-conf-archetype\nVersion: 3.2.0-incubating (or any later version)  Press Next and fill out the rest of the required information. For\nexample:   Click Finish, and now you have created your own Apex\nConfiguration Package project.  The procedure for other IDEs, like\nEclipse or IntelliJ, is similar.", 
            "title": "Using IDE"
        }, 
        {
            "location": "/configuration_packages/#assembling-your-own-configuration-package", 
            "text": "Inside the project created by the archetype, these are the files that\nyou should know about when assembling your own configuration package:  ./pom.xml\n./src/main/resources/classpath\n./src/main/resources/files\n./src/main/resources/META-INF/properties.xml\n./src/main/resources/META-INF/properties-{appname}.xml", 
            "title": "Assembling your own configuration package"
        }, 
        {
            "location": "/configuration_packages/#pomxml", 
            "text": "Example:     groupId com.example /groupId \n   version 1.0.0 /version \n   artifactId mydtconf /artifactId \n   packaging jar /packaging \n   !-- change these to the appropriate values -- \n   name My DataTorrent Application Configuration /name \n   description My DataTorrent Application Configuration Description /description \n   properties \n     datatorrent.apppackage.name mydtapp /datatorrent.apppackage.name \n     datatorrent.apppackage.minversion 1.0.0 /datatorrent.apppackage.minversion \n    datatorrent.apppackage.maxversion 1.9999.9999 /datatorrent.apppackage.maxversion \n     datatorrent.appconf.classpath classpath/* /datatorrent.appconf.classpath \n     datatorrent.appconf.files files/* /datatorrent.appconf.files \n   /properties    In pom.xml, you can change the following keys to your desired values   groupId  version  artifactId  name  description   You can also change the values of    datatorrent.apppackage.name  datatorrent.apppackage.minversion  datatorrent.apppackage.maxversion   to reflect what app packages should be used with this configuration package.  Apex will use this information to check whether a\nconfiguration package is compatible with the application package when you issue a launch command.", 
            "title": "pom.xml"
        }, 
        {
            "location": "/configuration_packages/#srcmainresourcesclasspath", 
            "text": "Place any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application and included in the\nclasspath of the application.  Example of such files are Java properties\nfiles and jar files.", 
            "title": "./src/main/resources/classpath"
        }, 
        {
            "location": "/configuration_packages/#srcmainresourcesfiles", 
            "text": "Place any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application but not included in the\nclasspath of the application.", 
            "title": "./src/main/resources/files"
        }, 
        {
            "location": "/configuration_packages/#properties-xml-file", 
            "text": "A properties xml file consists of a set of key-value pairs.  The set of\nkey-value pairs specifies the configuration options the application\nshould be launched with.    Example:  configuration \n   property \n     name some-property-name /name \n     value some-property-value /value \n   /property \n   ... /configuration   Names of properties XML file:   properties.xml:  Properties that are global to the Configuration\nPackage  properties-{appName}.xml:  Properties that are specific when launching\nan application with the specified appName within the Application\nPackage.   After you are done with the above, remember to do mvn package to\ngenerate a new configuration package, which will be located in the\ntarget directory in your project.", 
            "title": "Properties XML file"
        }, 
        {
            "location": "/configuration_packages/#zip-structure-of-configuration-package", 
            "text": "Apex Application Configuration Package files are zip files.  You\ncan examine the content of any Application Configuration Package by\nusing unzip -t on your Linux command line.  The structure of the zip\nfile is as follow:  META-INF\n  MANIFEST.MF\n  properties.xml\n  properties-{appname}.xml\nclasspath\n  {classpath files}\nfiles\n  {files}", 
            "title": "Zip structure of configuration package"
        }, 
        {
            "location": "/configuration_packages/#launching-with-cli", 
            "text": "-conf  option of the launch command in CLI supports specifying configuration package in the local filesystem.  Example:  dt\\  launch DTApp-mydtapp-1.0.0.jar -conf DTConfig-mydtconfig-1.0.0.jar  This command expects both the application package and the configuration package to be in the local file system.", 
            "title": "Launching with CLI"
        }, 
        {
            "location": "/configuration_packages/#related-rest-api", 
            "text": "", 
            "title": "Related REST API"
        }, 
        {
            "location": "/configuration_packages/#post-wsv2configpackages", 
            "text": "Payload: Raw content of configuration package zip  Function: Creates or replace a configuration package zip file in HDFS  Curl example:  $ curl -XPOST -T DTConfig-{name}.jar http://{yourhost:port}/ws/v2/configPackages", 
            "title": "POST /ws/v2/configPackages"
        }, 
        {
            "location": "/configuration_packages/#get-wsv2configpackagesapppackagenameapppackageversion", 
            "text": "All query parameters are optional  Function: Returns the configuration packages that the user is authorized to use and that are compatible with the specified appPackageName, appPackageVersion and appName.", 
            "title": "GET /ws/v2/configPackages?appPackageName=...&amp;appPackageVersion=..."
        }, 
        {
            "location": "/configuration_packages/#get-wsv2configpackagesuserapppackagenameapppackageversion", 
            "text": "All query parameters are optional  Function: Returns the configuration packages under the specified user and that are compatible with the specified appPackageName, appPackageVersion and appName.", 
            "title": "GET /ws/v2/configPackages/&lt;user&gt;?appPackageName=...&amp;appPackageVersion=..."
        }, 
        {
            "location": "/configuration_packages/#get-wsv2configpackagesusername", 
            "text": "Function: Returns the information of the specified configuration package", 
            "title": "GET /ws/v2/configPackages/&lt;user&gt;/&lt;name&gt;"
        }, 
        {
            "location": "/configuration_packages/#get-wsv2configpackagesusernamedownload", 
            "text": "Function: Returns the raw config package file  Curl example:  $ curl http://{yourhost:port}/ws/v2/configPackages/{user}/{name}/download \\  DTConfig-xyz.jar\n$ unzip -t DTConfig-xyz.jar", 
            "title": "GET /ws/v2/configPackages/&lt;user&gt;/&lt;name&gt;/download"
        }, 
        {
            "location": "/configuration_packages/#post-wsv2apppackagesuserapp-pkg-nameapp-pkg-versionapplicationsapp-namelaunchconfigpackageuserconfpkgname", 
            "text": "Function: Launches the app package with the specified configuration package stored in HDFS.  Curl example:  $ curl -XPOST -d \u2019{}\u2019 http://{yourhost:port}/ws/v2/appPackages/{user}/{app-pkg-name}/{app-pkg-version}/applications/{app-name}/launch?configPackage={user}/{confpkgname}", 
            "title": "POST /ws/v2/appPackages/&lt;user&gt;/&lt;app-pkg-name&gt;/&lt;app-pkg-version&gt;/applications/{app-name}/launch?configPackage=&lt;user&gt;/&lt;confpkgname&gt;"
        }, 
        {
            "location": "/operator_development/", 
            "text": "Operator Development Guide\n\n\nOperators are basic building blocks of an application built to run on\nApache Apex\u00a0platform. An application may consist of one or more\noperators each of which define some logical operation to be done on the\ntuples arriving at the operator. These operators are connected together\nusing streams forming a Directed Acyclic Graph (DAG). In other words, a streaming\napplication is represented by a DAG that consists of operations (called operators) and\ndata flow (called streams).\n\n\nIn this document we will discuss details on how an operator works and\nits internals. This document is intended to serve the following purposes\n\n\n\n\nApache Apex Operators\n\u00a0- Introduction to operator terminology and concepts.\n\n\nWriting Custom Operators\n\u00a0- Designing, coding and testing new operators from scratch.  Includes code examples.\n\n\nOperator Reference\n - Details of operator internals, lifecycle, and best practices and optimizations.\n\n\n\n\n\n\nApache Apex Operators \n\n\nOperators - \u201cWhat\u201d in a nutshell\n\n\nOperators are independent units of logical operations which can\ncontribute in executing the business logic of a use case. For example,\nin an ETL workflow, a filtering operation can be represented by a single\noperator. This filtering operator will be responsible for doing just one\ntask in the ETL pipeline, i.e. filter incoming tuples. Operators do not\nimpose any restrictions on what can or cannot be done as part of a\noperator. An operator may as well contain the entire business logic.\nHowever, it is recommended, that the operators are light weight\nindependent tasks, in\norder to take advantage of the distributed framework that Apache Apex\nprovides.\u00a0The structure of a streaming application shares resemblance\nwith the way CPU pipelining works. CPU pipelining breaks down the\ncomputation engine into different stages viz. instruction fetch,\ninstruction decode, etc. so that each of them can perform their task on\ndifferent instructions\nparallely. Similarly,\nApache Apex APIs allow the user to break down their tasks into different\nstages so that all of the tasks can be executed on different tuples\nparallely.\n\n\n\n\nOperators - \u201cHow\u201d in a nutshell\n\n\nAn Apache Apex application runs as a YARN application. Hence, each of\nthe operators that the application DAG contains, runs in one of the\ncontainers provisioned by YARN.\u00a0Further, Apache Apex exposes APIs to\nallow the user to request bundling multiple operators in a single node,\na single container or even a single thread. We shall look at these calls\nin the reference sections [cite reference sections]. For now, consider\nan operator as some piece of code that runs on some machine of a YARN\ncluster.\n\n\nTypes of Operators\n\n\nAn operator works on one tuple at a time. These tuples may be supplied\nby other operators in the application or by external sources,\nsuch as a database or a message bus. Similarly, after the tuples are\nprocessed, these may be passed on to other operators, or stored into an external system. \nTherea are 3 type of operators based on function: \n\n\n\n\nInput Adapter\n - This is one of the starting points in\n    the\u00a0application DAG and is responsible for getting tuples from an\n    external system. At the same time, such data may also be generated\n    by the operator itself, without interacting with the outside\n    world.\u00a0These input tuples will form the initial universe of\n    data\u00a0that the application works on.\n\n\nGeneric Operator\n - This type of operator accepts input tuples from\n    the previous operators and passes\u00a0them on to the following operators\n    in the DAG.\n\n\nOutput Adapter\n - This is one of the ending points in the application\n    DAG and is responsible for writing the data out to some external\n    system.\n\n\n\n\nNote: There can be multiple operators of all types in an application\nDAG.\n\n\nOperators Position in a DAG\n\n\nWe may refer to operators depending on their position with respect to\none another. For any operator opr (see image below), there are two types of operators.\n\n\n\n\nUpstream operators\n - These are the operators from which there is a\n    directed path to opr\u00a0in the application DAG.\n\n\nDownstream operators\n - These are the operators to which there is a\n    directed path from opr\u00a0in the application DAG.\n\n\n\n\nNote that there are no cycles formed in the application\u00a0DAG.\n\n\n\n\nPorts\n\n\nOperators in a DAG are connected together via directed flows\ncalled streams. Each\u00a0stream\u00a0has end-points located on the operators\ncalled ports. Therea are 2 types of ports.\n\n\n\n\nInput Port\n - This is a\u00a0port through which an operator accepts input\n    tuples\u00a0from an upstream operator.\n\n\nOutput port\n - This is a\u00a0port through which an operator passes on the\n    processed data to downstream operators.\n\n\n\n\nLooking at the number of input ports, an Input Adapter is an operator\nwith no input ports, a Generic operator has both input and output ports,\nwhile an Output Adapter has no output ports. At the same time, note that\nan operator may act as an Input Adapter while at the same time have an\ninput port. In such cases, the operator is getting data from two\ndifferent sources, viz.\u00a0the input stream from the input port and an\nexternal source.\n\n\n\n\n\n\nHow Operator Works\n\n\nAn operator passes through various stages during its lifetime. Each\nstage is an API call that the Streaming Application Master makes for an\noperator. \u00a0The following figure illustrates the stages through which an\noperator passes.\n\n\n\n\n\n\nThe \nsetup()\n call initializes the operator and prepares itself to\n    start processing tuples.\n\n\nThe \nbeginWindow()\n call marks the beginning\u00a0of an\u00a0application window\n    and allows for any processing to be done before a window starts.\n\n\nThe \nprocess()\n call belongs to the \nInputPort\n and gets triggered when\n    any tuple arrives at the Input port of the operator. This call is\n    specific only to Generic and Output adapters, since Input Adapters\n    do not have an input port. This is made for all the tuples at the\n    input port until the end window marker tuple is received on the\n    input port.\n\n\nThe \nemitTuples()\n is the counterpart of \nprocess()\n call for Input\n    Adapters.\n    This call is used by Input adapters to emit any tuples that are\n    fetched from the external systems, or generated by the operator.\n    This method is called continuously until the pre-configured window\n    time is elapsed, at which the end window marker tuple is sent out on\n    the output port.\n\n\nThe \nendWindow()\n call marks the end of the window and allows for any\n    processing to be done after the window ends.\n\n\nThe \nteardown()\n call is used for gracefully shutting down the\n    operator and releasing any resources held by the operator.\n\n\n\n\nDeveloping Custom Operators \n\n\nAbout this tutorial\n\n\nThis tutorial will guide the user towards developing a operator from\nscratch. It includes all aspects of writing an operator including\ndesign, code and unit testing.\n\n\nIntroduction\n\n\nIn this tutorial, we will design and write, from scratch, an operator\ncalled Word Count. This operator will accept tuples of type String,\ncount the number of occurrences for each word appearing in the tuple and\nsend out the updated counts for all the words encountered in the tuple.\nFurther, the operator will also accept a file path on HDFS which will\ncontain the stop-words which need to be ignored when counting\noccurrences.\n\n\nDesign\n\n\nDesign of the operator must be finalized before starting to write an\noperator. Many aspects including the functionality, the data sources,\nthe types involved etc. need to be first finalized before writing the\noperator. Let us dive into each of these while considering the Word\nCount\u00a0operator.\n\n\nFunctionality\n\n\nWe can define the scope of operator functionality using the following\ntasks:\n\n\n\n\nParse the input tuple to identify the words in the tuple\n\n\nIdentify the stop-words in the tuple by looking up the stop-word\n    file as configured\n\n\nFor each non-stop-word in the tuple, count the occurrences in that\n    tuple and add it to a global counts\n\n\n\n\nLet\u2019s consider an example. Suppose we have the following tuples flow\ninto the Word Count operator.\n\n\n\n\nHumpty dumpty sat on a wall\n\n\nHumpty dumpty had a great fall\n\n\n\n\nInitially counts for all words\u00a0is 0. Once the first tuple is processed,\nthe counts that must be emitted are:\n\n\nhumpty - 1\ndumpty - 1\nsat - 1\nwall - 1\n\n\n\n\nNote that we are ignoring the stop-words, \u201con\u201d and \u201ca\u201d in this case.\nAlso note that as a rule, we\u2019ll ignore the case of the words when\ncounting occurrences.\n\n\nSimilarly, after the second tuple is processed, the counts that must be\nemitted are:\n\n\nhumpty - 2\ndumpty - 2\ngreat - 1\nfall - 1\n\n\n\n\nAgain, we ignore the words \n\u201chad\u201d\n and \n\u201ca\u201d\n since these are stop-words.\n\n\nNote that the most recent count for any word is correct count for that\nword. In other words, any new output for a word, invalidated all the\nprevious counts for that word.\n\n\nInputs\n\n\nAs seen from the example\u00a0above, the following\u00a0inputs are expected for\nthe operator:\n\n\n\n\nInput stream whose tuple type is String\n\n\nInput HDFS file path, pointing to a file containing stop-words\n\n\n\n\nOnly one input port is needed. The stop-word file will be small enough\nto be read completely in a single read. In addition this will be a one\ntime activity for the lifetime of the operator. This does not need a\nseparate input port.\n\n\n\n\nOutputs\n\n\nWe can define the output for this operator in multiple ways.\n\n\n\n\nThe operator may send out the set of counts for which the counts\n    have changed after processing each tuple.\n\n\nSome applications might not need an update after every tuple, but\n    only after\u00a0a certain time duration.\n\n\n\n\nLet us try and implement both these options depending on the\nconfiguration. Let us define a\u00a0boolean configuration parameter\n\n\u201csendPerTuple\u201d\n. The value of this parameter will indicate whether the\nupdated counts for words need to be emitted after processing each\ntuple\u00a0(true)\u00a0or after a certain time duration (false).\n\n\nThe type of information the operator will be sending out on the output\nport is the same for all the cases. This will be a \n key, value \n\u00a0pair,\nwhere the key\u00a0is the word while, the value is the latest count for that\nword. This means we just need one output port on which this information\nwill go out.\n\n\n\n\nConfiguration\n\n\nWe have the following configuration parameters:\n\n\n\n\nstopWordFilePath\n\u00a0- This parameter will store the path to the stop\n    word file on HDFS as configured by the user.\n\n\nsendPerTuple\n\u00a0- This parameter decides whether we send out the\n    updated counts after processing each tuple or at the end of a\n    window. When set to true, the operator will send out the updated\n    counts after each tuple, else it will send at the end of\n    each\u00a0window.\n\n\n\n\nCode\n\n\nThe source code for the tutorial can be found here:\n\n\nhttps://github.com/DataTorrent/examples/tree/master/tutorials/operatorTutorial\n\n\nOperator Reference \n\n\nThe Operator Class\n\n\nThe operator will exist physically as a class which implements the\nOperator\u00a0interface. This interface will require implementations for the\nfollowing method calls:\n\n\n\n\nsetup(OperatorContext context)\n\n\nbeginWindow(long windowId)\n\n\nendWindow()\n\n\ntearDown()\n\n\n\n\nIn order to simplify the creation of an operator, Apache\u00a0Apex\nlibrary also provides a base class \u201cBaseOperator\u201d which has empty\nimplementations for these methods. Please refer to the \nApex Operators\n\u00a0section and the\n\nReference\n\u00a0section for details on these.\n\n\nWe extend the class \u201cBaseOperator\u201d to create our own operator\n\u201cWordCountOperator\u201d.\n\n\npublic class WordCountOperator extends BaseOperator\n{\n}\n\n\n\n\nClass (Operator) properties\n\n\nWe define the following class variables:\n\n\n\n\nsendPerTuple\n\u00a0- Configures the output frequency from the operator\n\n\n\n\nprivate boolean sendPerTuple = true; // default\n\n\n\n\n\n\nstopWordFilePath\n\u00a0- Stores the path to the stop words file on HDFS\n\n\n\n\nprivate String stopWordFilePath;\u00a0// no default\n\n\n\n\n\n\nstopWords\n\u00a0- Stores the stop words read from the configured file\n\n\n\n\nprivate transient String[] stopWords;\n\n\n\n\n\n\nglobalCounts\n\u00a0- A Map which stores the counts of all the words\n    encountered so far. Note that this variable is non transient, which\n    means that this variable is saved as part of the checkpoint and can be recovered in event of a crash.\n\n\n\n\nprivate Map\nString, Long\n globalCounts;\n\n\n\n\n\n\nupdatedCounts\n\u00a0- A Map which stores the counts for only the most\n    recent tuple(s). sendPerTuple configuration determines whether to store the most recent or the recent\n    window worth of tuples.\n\n\n\n\nprivate transient Map\nString, Long\n updatedCounts;\n\n\n\n\n\n\ninput\n - The input port for the operator. The type of this input port\n    is String\u00a0which means it will only accept tuples of type String. The\n    definition of an input port requires implementation of a method\n    called process(String tuple), which should\u00a0have the processing logic\n    for the input tuple which \u00a0arrives at this input port. We delegate\n    this task to another method called processTuple(String tuple). This\n    helps in keeping the operator classes extensible by overriding the\n    processing logic for the input tuples.\n\n\n\n\npublic transient DefaultInputPort\nString\n input = new \u00a0 \u00a0\nDefaultInputPort\nString\n()\n{\n\u00a0\u00a0\u00a0\u00a0@Override\n\u00a0\u00a0\u00a0\u00a0public void process(String tuple)\n\u00a0\u00a0\u00a0\u00a0{\n    \u00a0\u00a0\u00a0\u00a0processTuple(tuple);\n\u00a0\u00a0\u00a0\u00a0}\n};\n\n\n\n\n\n\noutput - The output port for the operator. The type of this port is\n    Entry \n String, Long \n, which means the operator will emit \n word,\n    count \n pairs for the updated counts.\n\n\n\n\npublic transient DefaultOutputPort \nEntry\nString, Long\n output = new\nDefaultOutputPort\nEntry\nString,Long\n();\n\n\n\n\nThe Constructor\n\n\nThe constructor is the place where we initialize the non-transient data\nstructures,\u00a0since\nconstructor is called just once per activation of an operator. With regards to Word Count\u00a0operator, we initialize the globalCounts variable in the constructor.\n\n\nglobalCounts = Maps.newHashMap();\n\n\n\n\nSetup call\n\n\nThe setup method is called only once during an operator lifetime and its purpose is to allow \nthe operator to set itself up for processing incoming streams. Transient objects in the operator are\nnot serialized and checkpointed. Hence, it is essential that such objects initialized in the setup call. \nIn case of operator failure, the operator will be redeployed (most likely on a different container). The setup method called by the Apache Apex engine allows the operator to prepare for execution in the new container.\n\n\nThe following tasks are executed as part of the setup call:\n\n\n\n\nRead the stop-word list from HDFS and store it in the\n    stopWords\u00a0array\n\n\nInitialize updatedCounts\u00a0variable. This will store the updated\n    counts for words in most recent tuples processed by the operator.\n    As a transient variable, the value will be lost when operator fails.\n\n\n\n\nBegin Window call\n\n\nThe begin window call signals the start of an application window. With \nregards to Word Count Operator, we are expecting updated counts for the most recent window of\ndata if the sendPerTuple\u00a0is set to false. Hence, we clear the updatedCounts\u00a0variable in the begin window\ncall and start accumulating the counts till the end window call.\n\n\nProcess Tuple call\n\n\nThe processTuple\u00a0method is called by the process\u00a0method of the input\nport, input. This method defines the processing logic for the current\ntuple that is received at the input port. As part of this method, we\nidentify the words in the current tuple and update the globalCounts\u00a0and\nthe updatedCounts\u00a0variables. In addition, if the sendPerTuple\u00a0variable\nis set to true, we also emit the words\u00a0and corresponding counts in\nupdatedCounts\u00a0to the output port. Note\u00a0that in this case (sendPerTuple =\ntrue), we clear the updatedCounts\u00a0variable in every call to\nprocessTuple.\n\n\nEnd Window call\n\n\nThis call signals the end of an application window. With regards to Word\nCount Operator, we emit the updatedCounts\u00a0to the output port if the\nsendPerTuple\u00a0flag is set to false.\n\n\nTeardown call\n\n\nThis method allows the operator to gracefully shut down itself after\nreleasing the resources that it has acquired. With regards to our operator,\nwe call the shutDown\u00a0method which shuts down the operator along with any\ndownstream operators.\n\n\nTesting your Operator\n\n\nAs part of testing our operator, we test the following two facets:\n\n\n\n\nTest output of the operator after processing a single tuple\n\n\nTest output of the operator after processing of a window of tuples\n\n\n\n\nThe unit tests for the WordCount operator are available in the class\nWordCountOperatorTest.java. We simulate the behavior of the engine by\nusing the test utilities provided by Apache Apex libraries. We simulate\nthe setup, beginWindow, process\u00a0method of the input port and\nendWindow\u00a0calls and compare the output received at the simulated output\nports.\n\n\n\n\nInvoke constructor; non-transients initialized.\n\n\nCopy state from checkpoint -- initialized values from step 1 are\nreplaced.", 
            "title": "Operators"
        }, 
        {
            "location": "/operator_development/#operator-development-guide", 
            "text": "Operators are basic building blocks of an application built to run on\nApache Apex\u00a0platform. An application may consist of one or more\noperators each of which define some logical operation to be done on the\ntuples arriving at the operator. These operators are connected together\nusing streams forming a Directed Acyclic Graph (DAG). In other words, a streaming\napplication is represented by a DAG that consists of operations (called operators) and\ndata flow (called streams).  In this document we will discuss details on how an operator works and\nits internals. This document is intended to serve the following purposes   Apache Apex Operators \u00a0- Introduction to operator terminology and concepts.  Writing Custom Operators \u00a0- Designing, coding and testing new operators from scratch.  Includes code examples.  Operator Reference  - Details of operator internals, lifecycle, and best practices and optimizations.", 
            "title": "Operator Development Guide"
        }, 
        {
            "location": "/operator_development/#apache-apex-operators", 
            "text": "", 
            "title": "Apache Apex Operators "
        }, 
        {
            "location": "/operator_development/#operators-what-in-a-nutshell", 
            "text": "Operators are independent units of logical operations which can\ncontribute in executing the business logic of a use case. For example,\nin an ETL workflow, a filtering operation can be represented by a single\noperator. This filtering operator will be responsible for doing just one\ntask in the ETL pipeline, i.e. filter incoming tuples. Operators do not\nimpose any restrictions on what can or cannot be done as part of a\noperator. An operator may as well contain the entire business logic.\nHowever, it is recommended, that the operators are light weight\nindependent tasks, in\norder to take advantage of the distributed framework that Apache Apex\nprovides.\u00a0The structure of a streaming application shares resemblance\nwith the way CPU pipelining works. CPU pipelining breaks down the\ncomputation engine into different stages viz. instruction fetch,\ninstruction decode, etc. so that each of them can perform their task on\ndifferent instructions\nparallely. Similarly,\nApache Apex APIs allow the user to break down their tasks into different\nstages so that all of the tasks can be executed on different tuples\nparallely.", 
            "title": "Operators - \u201cWhat\u201d in a nutshell"
        }, 
        {
            "location": "/operator_development/#operators-how-in-a-nutshell", 
            "text": "An Apache Apex application runs as a YARN application. Hence, each of\nthe operators that the application DAG contains, runs in one of the\ncontainers provisioned by YARN.\u00a0Further, Apache Apex exposes APIs to\nallow the user to request bundling multiple operators in a single node,\na single container or even a single thread. We shall look at these calls\nin the reference sections [cite reference sections]. For now, consider\nan operator as some piece of code that runs on some machine of a YARN\ncluster.", 
            "title": "Operators - \u201cHow\u201d in a nutshell"
        }, 
        {
            "location": "/operator_development/#types-of-operators", 
            "text": "An operator works on one tuple at a time. These tuples may be supplied\nby other operators in the application or by external sources,\nsuch as a database or a message bus. Similarly, after the tuples are\nprocessed, these may be passed on to other operators, or stored into an external system. \nTherea are 3 type of operators based on function:    Input Adapter  - This is one of the starting points in\n    the\u00a0application DAG and is responsible for getting tuples from an\n    external system. At the same time, such data may also be generated\n    by the operator itself, without interacting with the outside\n    world.\u00a0These input tuples will form the initial universe of\n    data\u00a0that the application works on.  Generic Operator  - This type of operator accepts input tuples from\n    the previous operators and passes\u00a0them on to the following operators\n    in the DAG.  Output Adapter  - This is one of the ending points in the application\n    DAG and is responsible for writing the data out to some external\n    system.   Note: There can be multiple operators of all types in an application\nDAG.", 
            "title": "Types of Operators"
        }, 
        {
            "location": "/operator_development/#operators-position-in-a-dag", 
            "text": "We may refer to operators depending on their position with respect to\none another. For any operator opr (see image below), there are two types of operators.   Upstream operators  - These are the operators from which there is a\n    directed path to opr\u00a0in the application DAG.  Downstream operators  - These are the operators to which there is a\n    directed path from opr\u00a0in the application DAG.   Note that there are no cycles formed in the application\u00a0DAG.", 
            "title": "Operators Position in a DAG"
        }, 
        {
            "location": "/operator_development/#ports", 
            "text": "Operators in a DAG are connected together via directed flows\ncalled streams. Each\u00a0stream\u00a0has end-points located on the operators\ncalled ports. Therea are 2 types of ports.   Input Port  - This is a\u00a0port through which an operator accepts input\n    tuples\u00a0from an upstream operator.  Output port  - This is a\u00a0port through which an operator passes on the\n    processed data to downstream operators.   Looking at the number of input ports, an Input Adapter is an operator\nwith no input ports, a Generic operator has both input and output ports,\nwhile an Output Adapter has no output ports. At the same time, note that\nan operator may act as an Input Adapter while at the same time have an\ninput port. In such cases, the operator is getting data from two\ndifferent sources, viz.\u00a0the input stream from the input port and an\nexternal source.", 
            "title": "Ports"
        }, 
        {
            "location": "/operator_development/#how-operator-works", 
            "text": "An operator passes through various stages during its lifetime. Each\nstage is an API call that the Streaming Application Master makes for an\noperator. \u00a0The following figure illustrates the stages through which an\noperator passes.    The  setup()  call initializes the operator and prepares itself to\n    start processing tuples.  The  beginWindow()  call marks the beginning\u00a0of an\u00a0application window\n    and allows for any processing to be done before a window starts.  The  process()  call belongs to the  InputPort  and gets triggered when\n    any tuple arrives at the Input port of the operator. This call is\n    specific only to Generic and Output adapters, since Input Adapters\n    do not have an input port. This is made for all the tuples at the\n    input port until the end window marker tuple is received on the\n    input port.  The  emitTuples()  is the counterpart of  process()  call for Input\n    Adapters.\n    This call is used by Input adapters to emit any tuples that are\n    fetched from the external systems, or generated by the operator.\n    This method is called continuously until the pre-configured window\n    time is elapsed, at which the end window marker tuple is sent out on\n    the output port.  The  endWindow()  call marks the end of the window and allows for any\n    processing to be done after the window ends.  The  teardown()  call is used for gracefully shutting down the\n    operator and releasing any resources held by the operator.", 
            "title": "How Operator Works"
        }, 
        {
            "location": "/operator_development/#developing-custom-operators", 
            "text": "", 
            "title": "Developing Custom Operators "
        }, 
        {
            "location": "/operator_development/#about-this-tutorial", 
            "text": "This tutorial will guide the user towards developing a operator from\nscratch. It includes all aspects of writing an operator including\ndesign, code and unit testing.", 
            "title": "About this tutorial"
        }, 
        {
            "location": "/operator_development/#introduction", 
            "text": "In this tutorial, we will design and write, from scratch, an operator\ncalled Word Count. This operator will accept tuples of type String,\ncount the number of occurrences for each word appearing in the tuple and\nsend out the updated counts for all the words encountered in the tuple.\nFurther, the operator will also accept a file path on HDFS which will\ncontain the stop-words which need to be ignored when counting\noccurrences.", 
            "title": "Introduction"
        }, 
        {
            "location": "/operator_development/#design", 
            "text": "Design of the operator must be finalized before starting to write an\noperator. Many aspects including the functionality, the data sources,\nthe types involved etc. need to be first finalized before writing the\noperator. Let us dive into each of these while considering the Word\nCount\u00a0operator.", 
            "title": "Design"
        }, 
        {
            "location": "/operator_development/#functionality", 
            "text": "We can define the scope of operator functionality using the following\ntasks:   Parse the input tuple to identify the words in the tuple  Identify the stop-words in the tuple by looking up the stop-word\n    file as configured  For each non-stop-word in the tuple, count the occurrences in that\n    tuple and add it to a global counts   Let\u2019s consider an example. Suppose we have the following tuples flow\ninto the Word Count operator.   Humpty dumpty sat on a wall  Humpty dumpty had a great fall   Initially counts for all words\u00a0is 0. Once the first tuple is processed,\nthe counts that must be emitted are:  humpty - 1\ndumpty - 1\nsat - 1\nwall - 1  Note that we are ignoring the stop-words, \u201con\u201d and \u201ca\u201d in this case.\nAlso note that as a rule, we\u2019ll ignore the case of the words when\ncounting occurrences.  Similarly, after the second tuple is processed, the counts that must be\nemitted are:  humpty - 2\ndumpty - 2\ngreat - 1\nfall - 1  Again, we ignore the words  \u201chad\u201d  and  \u201ca\u201d  since these are stop-words.  Note that the most recent count for any word is correct count for that\nword. In other words, any new output for a word, invalidated all the\nprevious counts for that word.", 
            "title": "Functionality"
        }, 
        {
            "location": "/operator_development/#inputs", 
            "text": "As seen from the example\u00a0above, the following\u00a0inputs are expected for\nthe operator:   Input stream whose tuple type is String  Input HDFS file path, pointing to a file containing stop-words   Only one input port is needed. The stop-word file will be small enough\nto be read completely in a single read. In addition this will be a one\ntime activity for the lifetime of the operator. This does not need a\nseparate input port.", 
            "title": "Inputs"
        }, 
        {
            "location": "/operator_development/#outputs", 
            "text": "We can define the output for this operator in multiple ways.   The operator may send out the set of counts for which the counts\n    have changed after processing each tuple.  Some applications might not need an update after every tuple, but\n    only after\u00a0a certain time duration.   Let us try and implement both these options depending on the\nconfiguration. Let us define a\u00a0boolean configuration parameter \u201csendPerTuple\u201d . The value of this parameter will indicate whether the\nupdated counts for words need to be emitted after processing each\ntuple\u00a0(true)\u00a0or after a certain time duration (false).  The type of information the operator will be sending out on the output\nport is the same for all the cases. This will be a   key, value  \u00a0pair,\nwhere the key\u00a0is the word while, the value is the latest count for that\nword. This means we just need one output port on which this information\nwill go out.", 
            "title": "Outputs"
        }, 
        {
            "location": "/operator_development/#configuration", 
            "text": "We have the following configuration parameters:   stopWordFilePath \u00a0- This parameter will store the path to the stop\n    word file on HDFS as configured by the user.  sendPerTuple \u00a0- This parameter decides whether we send out the\n    updated counts after processing each tuple or at the end of a\n    window. When set to true, the operator will send out the updated\n    counts after each tuple, else it will send at the end of\n    each\u00a0window.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operator_development/#code", 
            "text": "The source code for the tutorial can be found here:  https://github.com/DataTorrent/examples/tree/master/tutorials/operatorTutorial", 
            "title": "Code"
        }, 
        {
            "location": "/operator_development/#operator-reference", 
            "text": "", 
            "title": "Operator Reference "
        }, 
        {
            "location": "/operator_development/#the-operator-class", 
            "text": "The operator will exist physically as a class which implements the\nOperator\u00a0interface. This interface will require implementations for the\nfollowing method calls:   setup(OperatorContext context)  beginWindow(long windowId)  endWindow()  tearDown()   In order to simplify the creation of an operator, Apache\u00a0Apex\nlibrary also provides a base class \u201cBaseOperator\u201d which has empty\nimplementations for these methods. Please refer to the  Apex Operators \u00a0section and the Reference \u00a0section for details on these.  We extend the class \u201cBaseOperator\u201d to create our own operator\n\u201cWordCountOperator\u201d.  public class WordCountOperator extends BaseOperator\n{\n}", 
            "title": "The Operator Class"
        }, 
        {
            "location": "/operator_development/#class-operator-properties", 
            "text": "We define the following class variables:   sendPerTuple \u00a0- Configures the output frequency from the operator   private boolean sendPerTuple = true; // default   stopWordFilePath \u00a0- Stores the path to the stop words file on HDFS   private String stopWordFilePath;\u00a0// no default   stopWords \u00a0- Stores the stop words read from the configured file   private transient String[] stopWords;   globalCounts \u00a0- A Map which stores the counts of all the words\n    encountered so far. Note that this variable is non transient, which\n    means that this variable is saved as part of the checkpoint and can be recovered in event of a crash.   private Map String, Long  globalCounts;   updatedCounts \u00a0- A Map which stores the counts for only the most\n    recent tuple(s). sendPerTuple configuration determines whether to store the most recent or the recent\n    window worth of tuples.   private transient Map String, Long  updatedCounts;   input  - The input port for the operator. The type of this input port\n    is String\u00a0which means it will only accept tuples of type String. The\n    definition of an input port requires implementation of a method\n    called process(String tuple), which should\u00a0have the processing logic\n    for the input tuple which \u00a0arrives at this input port. We delegate\n    this task to another method called processTuple(String tuple). This\n    helps in keeping the operator classes extensible by overriding the\n    processing logic for the input tuples.   public transient DefaultInputPort String  input = new \u00a0 \u00a0\nDefaultInputPort String ()\n{\n\u00a0\u00a0\u00a0\u00a0@Override\n\u00a0\u00a0\u00a0\u00a0public void process(String tuple)\n\u00a0\u00a0\u00a0\u00a0{\n    \u00a0\u00a0\u00a0\u00a0processTuple(tuple);\n\u00a0\u00a0\u00a0\u00a0}\n};   output - The output port for the operator. The type of this port is\n    Entry   String, Long  , which means the operator will emit   word,\n    count   pairs for the updated counts.   public transient DefaultOutputPort  Entry String, Long  output = new\nDefaultOutputPort Entry String,Long ();", 
            "title": "Class (Operator) properties"
        }, 
        {
            "location": "/operator_development/#the-constructor", 
            "text": "The constructor is the place where we initialize the non-transient data\nstructures,\u00a0since\nconstructor is called just once per activation of an operator. With regards to Word Count\u00a0operator, we initialize the globalCounts variable in the constructor.  globalCounts = Maps.newHashMap();", 
            "title": "The Constructor"
        }, 
        {
            "location": "/operator_development/#setup-call", 
            "text": "The setup method is called only once during an operator lifetime and its purpose is to allow \nthe operator to set itself up for processing incoming streams. Transient objects in the operator are\nnot serialized and checkpointed. Hence, it is essential that such objects initialized in the setup call. \nIn case of operator failure, the operator will be redeployed (most likely on a different container). The setup method called by the Apache Apex engine allows the operator to prepare for execution in the new container.  The following tasks are executed as part of the setup call:   Read the stop-word list from HDFS and store it in the\n    stopWords\u00a0array  Initialize updatedCounts\u00a0variable. This will store the updated\n    counts for words in most recent tuples processed by the operator.\n    As a transient variable, the value will be lost when operator fails.", 
            "title": "Setup call"
        }, 
        {
            "location": "/operator_development/#begin-window-call", 
            "text": "The begin window call signals the start of an application window. With \nregards to Word Count Operator, we are expecting updated counts for the most recent window of\ndata if the sendPerTuple\u00a0is set to false. Hence, we clear the updatedCounts\u00a0variable in the begin window\ncall and start accumulating the counts till the end window call.", 
            "title": "Begin Window call"
        }, 
        {
            "location": "/operator_development/#process-tuple-call", 
            "text": "The processTuple\u00a0method is called by the process\u00a0method of the input\nport, input. This method defines the processing logic for the current\ntuple that is received at the input port. As part of this method, we\nidentify the words in the current tuple and update the globalCounts\u00a0and\nthe updatedCounts\u00a0variables. In addition, if the sendPerTuple\u00a0variable\nis set to true, we also emit the words\u00a0and corresponding counts in\nupdatedCounts\u00a0to the output port. Note\u00a0that in this case (sendPerTuple =\ntrue), we clear the updatedCounts\u00a0variable in every call to\nprocessTuple.", 
            "title": "Process Tuple call"
        }, 
        {
            "location": "/operator_development/#end-window-call", 
            "text": "This call signals the end of an application window. With regards to Word\nCount Operator, we emit the updatedCounts\u00a0to the output port if the\nsendPerTuple\u00a0flag is set to false.", 
            "title": "End Window call"
        }, 
        {
            "location": "/operator_development/#teardown-call", 
            "text": "This method allows the operator to gracefully shut down itself after\nreleasing the resources that it has acquired. With regards to our operator,\nwe call the shutDown\u00a0method which shuts down the operator along with any\ndownstream operators.", 
            "title": "Teardown call"
        }, 
        {
            "location": "/operator_development/#testing-your-operator", 
            "text": "As part of testing our operator, we test the following two facets:   Test output of the operator after processing a single tuple  Test output of the operator after processing of a window of tuples   The unit tests for the WordCount operator are available in the class\nWordCountOperatorTest.java. We simulate the behavior of the engine by\nusing the test utilities provided by Apache Apex libraries. We simulate\nthe setup, beginWindow, process\u00a0method of the input port and\nendWindow\u00a0calls and compare the output received at the simulated output\nports.   Invoke constructor; non-transients initialized.  Copy state from checkpoint -- initialized values from step 1 are\nreplaced.", 
            "title": "Testing your Operator"
        }, 
        {
            "location": "/operators/kafkaInputOperator/", 
            "text": "KAFKA INPUT OPERATOR\n\n\nIntroduction: About Kafka Input Operator\n\n\nThis is an input operator that consumes data from Kafka messaging system for further processing in Apex. Kafka Input Operator is an fault-tolerant and scalable Malhar Operator.\n\n\nWhy is it needed ?\n\n\nKafka is a pull-based and distributed publish subscribe messaging system, topics are partitioned and replicated across\nnodes. Kafka input operator is needed when you want to read data from multiple\npartitions of a Kafka topic in parallel in an Apex application.\n\n\nAbstractKafkaInputOperator\n\n\nThis is the abstract implementation that serves as base class for consuming messages from Kafka messaging system. This class doesn\u2019t have any ports.\n\n\n\n\nConfiguration Parameters\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\nmaxTuplesPerWindow\n\n\nControls the maximum number of messages emitted in each streaming window from this operator. Minimum value is 1. Default value = MAX_VALUE \n\n\n\n\n\n\nidempotentStorageManager\n\n\nThis is an instance of IdempotentStorageManager. Idempotency ensures that the operator will process the same set of messages in a window before and after a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same messages again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Default Value = com.datatorrent.lib.io.IdempotentStorageManager.\nNoopIdempotentStorageManager\n\n\n\n\n\n\nstrategy\n\n\nOperator supports two types of partitioning strategies, ONE_TO_ONE and ONE_TO_MANY.\n\n\nONE_TO_ONE: If this is enabled, the AppMaster creates one input operator instance per Kafka topic partition. So the number of Kafka topic partitions equals the number of operator instances.\n\n\nONE_TO_MANY: The AppMaster creates K = min(initialPartitionCount, N) Kafka input operator instances where N is the number of Kafka topic partitions. If K is less than N, the remaining topic partitions are assigned to the K operator instances in round-robin fashion. If K is less than initialPartitionCount, the AppMaster creates one input operator instance per Kafka topic partition. For example, if initialPartitionCount = 5 and number of Kafka partitions(N) = 2 then AppMaster creates 2 Kafka input operator instances.\nDefault Value = ONE_TO_ONE\n\n\n\n\n\n\nmsgRateUpperBound\n\n\nMaximum messages upper bound. Operator repartitions when the \nmsgProcessedPS\n exceeds this bound. \nmsgProcessedPS\n is the average number of messages processed per second by this operator.\n\n\n\n\n\n\nbyteRateUpperBound\n\n\nMaximum bytes upper bound. Operator repartitions when the \nbytesPS\n exceeds this bound. \nbytesPS\n is the average number of bytes processed per second by this operator.\n\n\n\n\n\n\n\n\noffsetManager\n\n\nThis is an optional parameter that is useful when the application restarts or start at specific offsets (offsets are explained below)\n\n\n\n\n\n\nrepartitionInterval\n\n\nInterval specified in milliseconds. This value specifies the minimum time required between two repartition actions. Default Value = 30 Seconds\n\n\n\n\n\n\nrepartitionCheckInterval\n\n\nInterval specified in milliseconds. This value specifies the minimum interval between two offset updates. Default Value = 5 Seconds\n\n\n\n\n\n\ninitialPartitionCount\n\n\nWhen the ONE_TO_MANY partition strategy is enabled, this value indicates the number of Kafka input operator instances. Default Value = 1\n\n\n\n\n\n\nconsumer\n\n\nThis is an instance of com.datatorrent.contrib.kafka.KafkaConsumer. Default Value = Instance of SimpleKafkaConsumer.\n\n\n\n\n\n\n\n\nAbstract Methods\n\n\nvoid emitTuple(Message message): Abstract method that emits tuples\nextracted from Kafka message.\n\n\nKafkaConsumer\n\n\nThis is an abstract implementation of Kafka consumer. It sends the fetch\nrequests to the leading brokers of Kafka partitions. For each request,\nit receives the set of messages and stores them into the buffer which is\nArrayBlockingQueue. SimpleKafkaConsumer\u00a0which extends\nKafkaConsumer and serves the functionality of Simple Consumer API and\nHighLevelKafkaConsumer which extends KafkaConsumer and \u00a0serves the\nfunctionality of High Level Consumer API.\n\n\nPre-requisites\n\n\nThis operator referred the Kafka Consumer API of version\n0.8.1.1. So, this operator will work with any 0.8.x and 0.7.x version of Apache Kafka.\n\n\nConfiguration Parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\nzookeeper\n\n\nString\n\n\n\n\nSpecifies the zookeeper quorum of Kafka clusters that you want to consume messages from. zookeeper \u00a0is a string in the form of hostname1:port1,hostname2:port2,hostname3:port3 \u00a0where hostname1,hostname2,hostname3 are hosts and port1,port2,port3 are ports of zookeeper server. \u00a0If the topic name is the same across the Kafka clusters and want to consume data from these clusters, then configure the zookeeper as follows: c1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6\n\n\nwhere\n\n\nc1,c2,c3 indicates the cluster names, hs1,hs2,hs3,hs4,hs5,hs6 are zookeeper hosts and p1,p2,p3,p4,p5,p6 are corresponding ports. Here, cluster name is optional in case of single cluster\n\n\n\n\n\n\ncacheSize\n\n\nint\n\n\n1024\n\n\nMaximum of buffered messages hold in memory.\n\n\n\n\n\n\ntopic\n\n\nString\n\n\ndefault_topic\n\n\nIndicates the name of the topic.\n\n\n\n\n\n\ninitialOffset\n\n\nString\n\n\nlatest\n\n\nIndicates the type of offset i.e, \u201cearliest or latest\u201d. If initialOffset is \u201clatest\u201d, then the operator consumes messages from latest point of Kafka queue. If initialOffset is \u201cearliest\u201d, then the operator consumes messages starting from message queue. This can be overridden by OffsetManager.\n\n\n\n\n\n\n\n\n\nAbstract Methods\n\n\n\n\nvoid commitOffset(): Commit the offsets at checkpoint.\n\n\nMap \nKafkaPartition, Long\n getCurrentOffsets(): Return the current\n    offset status.\n\n\nresetPartitionsAndOffset(Set \nKafkaPartition\n partitionIds,\n    Map \nKafkaPartition, Long\n startOffset): Reset the partitions with\n    parittionIds and offsets with startOffset.\n\n\n\n\nConfiguration Parameters\u00a0for SimpleKafkaConsumer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\nbufferSize\n\n\nint\n\n\n1 MB\n\n\nSpecifies the maximum total size of messages for each fetch request.\n\n\n\n\n\n\nmetadataRefreshInterval\n\n\nint\n\n\n30 Seconds\n\n\nInterval in between refresh the metadata change(broker change) in milliseconds. Enabling metadata refresh guarantees an automatic reconnect when a new broker is elected as the host. A value of -1 disables this feature.\n\n\n\n\n\n\nmetadataRefreshRetryLimit\n\n\nint\n\n\n-1\n\n\nSpecifies the maximum brokers' metadata refresh retry limit. -1 means unlimited retry.\n\n\n\n\n\n\n\n\n\nOffsetManager\n\n\nThis is an interface for offset management and is useful when consuming data\nfrom specified offsets. Updates the offsets for all the Kafka partitions\nperiodically. Below is the code snippet:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\npublic interface OffsetManager\n{\n  public Map\nKafkaPartition, Long\n loadInitialOffsets();\n  public void updateOffsets(Map\nKafkaPartition, Long\n offsetsOfPartitions);\n}\n\n\n\n\nAbstract Methods\n\n\nMap \nKafkaPartition, Long\n loadInitialOffsets(): Specifies the initial offset for consuming messages; called at the activation stage.\n\n\nupdateOffsets(Map \nKafkaPartition, Long\n offsetsOfPartitions): \u00a0This\nmethod is called at every repartitionCheckInterval to update offsets.\n\n\nPartitioning\n\n\nThe logical instance of the KafkaInputOperator acts as the Partitioner\nas well as a StatsListener. This is because the\nAbstractKafkaInputOperator implements both the\ncom.datatorrent.api.Partitioner and com.datatorrent.api.StatsListener\ninterfaces and provides an implementation of definePartitions(...) and\nprocessStats(...) which makes it auto-scalable.\n\n\nResponse processStats(BatchedOperatorStats stats)\n\n\nThe application master invokes this method on the logical instance with\nthe stats (tuplesProcessedPS, bytesPS, etc.) of each partition.\nRe-partitioning happens based on whether any new Kafka partitions added for\nthe topic or bytesPS and msgPS cross their respective upper bounds.\n\n\nDefinePartitions\n\n\nBased on the repartitionRequired field of the Response object which is\nreturned by processStats(...) method, the application master invokes\ndefinePartitions(...) on the logical instance which is also the\npartitioner instance. Dynamic partition can be disabled by setting the\nparameter repartitionInterval value to a negative value.\n\n\nAbstractSinglePortKafkaInputOperator\n\n\nThis class extends AbstractKafkaInputOperator and having single output\nport, will emit the messages through this port.\n\n\nPorts\n\n\noutputPort \nT\n: Tuples extracted from Kafka messages are emitted through\nthis port.\n\n\nAbstract Methods\n\n\nT getTuple(Message msg) : Converts the Kafka message to tuple.\n\n\nConcrete Classes\n\n\n\n\n\n\nKafkaSinglePortStringInputOperator :\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts string from Kafka message.\n\n\n\n\n\n\nKafkaSinglePortByteArrayInputOperator:\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts byte array from Kafka message.\n\n\n\n\n\n\nApplication Example\n\n\nThis section builds an Apex application using Kafka input operator.\nBelow is the code snippet:\n\n\n@ApplicationAnnotation(name = \nKafkaApp\n)\npublic class ExampleKafkaApplication implements StreamingApplication\n{\n@Override\npublic void populateDAG(DAG dag, Configuration entries)\n{\n  KafkaSinglePortByteArrayInputOperator input =  dag.addOperator(\nMessageReader\n, new KafkaSinglePortByteArrayInputOperator());\n\n  ConsoleOutputOperator output = dag.addOperator(\nOutput\n, new ConsoleOutputOperator());\n\n  dag.addStream(\nMessageData\n, input.outputPort, output.input);\n}\n}\n\n\n\n\nBelow is the configuration for \u201ctest\u201d Kafka topic name and\n\u201clocalhost:2181\u201d is the zookeeper forum:\n\n\nproperty\n\n\nname\ndt.operator.MessageReader.prop.topic\n/name\n\n\nvalue\ntest\n/value\n\n\n/property\n\n\n\nproperty\n\n\nname\ndt.operator.KafkaInputOperator.prop.zookeeper\n/nam\n\n\nvalue\nlocalhost:2181\n/value\n\n\n/property", 
            "title": "Kafka Input"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#kafka-input-operator", 
            "text": "", 
            "title": "KAFKA INPUT OPERATOR"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#introduction-about-kafka-input-operator", 
            "text": "This is an input operator that consumes data from Kafka messaging system for further processing in Apex. Kafka Input Operator is an fault-tolerant and scalable Malhar Operator.", 
            "title": "Introduction: About Kafka Input Operator"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#why-is-it-needed", 
            "text": "Kafka is a pull-based and distributed publish subscribe messaging system, topics are partitioned and replicated across\nnodes. Kafka input operator is needed when you want to read data from multiple\npartitions of a Kafka topic in parallel in an Apex application.", 
            "title": "Why is it needed ?"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstractkafkainputoperator", 
            "text": "This is the abstract implementation that serves as base class for consuming messages from Kafka messaging system. This class doesn\u2019t have any ports.", 
            "title": "AbstractKafkaInputOperator"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#configuration-parameters", 
            "text": "Parameter  Description    maxTuplesPerWindow  Controls the maximum number of messages emitted in each streaming window from this operator. Minimum value is 1. Default value = MAX_VALUE     idempotentStorageManager  This is an instance of IdempotentStorageManager. Idempotency ensures that the operator will process the same set of messages in a window before and after a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same messages again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Default Value = com.datatorrent.lib.io.IdempotentStorageManager. NoopIdempotentStorageManager    strategy  Operator supports two types of partitioning strategies, ONE_TO_ONE and ONE_TO_MANY.  ONE_TO_ONE: If this is enabled, the AppMaster creates one input operator instance per Kafka topic partition. So the number of Kafka topic partitions equals the number of operator instances.  ONE_TO_MANY: The AppMaster creates K = min(initialPartitionCount, N) Kafka input operator instances where N is the number of Kafka topic partitions. If K is less than N, the remaining topic partitions are assigned to the K operator instances in round-robin fashion. If K is less than initialPartitionCount, the AppMaster creates one input operator instance per Kafka topic partition. For example, if initialPartitionCount = 5 and number of Kafka partitions(N) = 2 then AppMaster creates 2 Kafka input operator instances.\nDefault Value = ONE_TO_ONE    msgRateUpperBound  Maximum messages upper bound. Operator repartitions when the  msgProcessedPS  exceeds this bound.  msgProcessedPS  is the average number of messages processed per second by this operator.    byteRateUpperBound  Maximum bytes upper bound. Operator repartitions when the  bytesPS  exceeds this bound.  bytesPS  is the average number of bytes processed per second by this operator.     offsetManager  This is an optional parameter that is useful when the application restarts or start at specific offsets (offsets are explained below)    repartitionInterval  Interval specified in milliseconds. This value specifies the minimum time required between two repartition actions. Default Value = 30 Seconds    repartitionCheckInterval  Interval specified in milliseconds. This value specifies the minimum interval between two offset updates. Default Value = 5 Seconds    initialPartitionCount  When the ONE_TO_MANY partition strategy is enabled, this value indicates the number of Kafka input operator instances. Default Value = 1    consumer  This is an instance of com.datatorrent.contrib.kafka.KafkaConsumer. Default Value = Instance of SimpleKafkaConsumer.", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods", 
            "text": "void emitTuple(Message message): Abstract method that emits tuples\nextracted from Kafka message.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#kafkaconsumer", 
            "text": "This is an abstract implementation of Kafka consumer. It sends the fetch\nrequests to the leading brokers of Kafka partitions. For each request,\nit receives the set of messages and stores them into the buffer which is\nArrayBlockingQueue. SimpleKafkaConsumer\u00a0which extends\nKafkaConsumer and serves the functionality of Simple Consumer API and\nHighLevelKafkaConsumer which extends KafkaConsumer and \u00a0serves the\nfunctionality of High Level Consumer API.", 
            "title": "KafkaConsumer"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#pre-requisites", 
            "text": "This operator referred the Kafka Consumer API of version\n0.8.1.1. So, this operator will work with any 0.8.x and 0.7.x version of Apache Kafka.", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#configuration-parameters_1", 
            "text": "Parameter  Type  Default  Description    zookeeper  String   Specifies the zookeeper quorum of Kafka clusters that you want to consume messages from. zookeeper \u00a0is a string in the form of hostname1:port1,hostname2:port2,hostname3:port3 \u00a0where hostname1,hostname2,hostname3 are hosts and port1,port2,port3 are ports of zookeeper server. \u00a0If the topic name is the same across the Kafka clusters and want to consume data from these clusters, then configure the zookeeper as follows: c1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6  where  c1,c2,c3 indicates the cluster names, hs1,hs2,hs3,hs4,hs5,hs6 are zookeeper hosts and p1,p2,p3,p4,p5,p6 are corresponding ports. Here, cluster name is optional in case of single cluster    cacheSize  int  1024  Maximum of buffered messages hold in memory.    topic  String  default_topic  Indicates the name of the topic.    initialOffset  String  latest  Indicates the type of offset i.e, \u201cearliest or latest\u201d. If initialOffset is \u201clatest\u201d, then the operator consumes messages from latest point of Kafka queue. If initialOffset is \u201cearliest\u201d, then the operator consumes messages starting from message queue. This can be overridden by OffsetManager.", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods_1", 
            "text": "void commitOffset(): Commit the offsets at checkpoint.  Map  KafkaPartition, Long  getCurrentOffsets(): Return the current\n    offset status.  resetPartitionsAndOffset(Set  KafkaPartition  partitionIds,\n    Map  KafkaPartition, Long  startOffset): Reset the partitions with\n    parittionIds and offsets with startOffset.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#configuration-parameters-for-simplekafkaconsumer", 
            "text": "Parameter  Type  Default  Description    bufferSize  int  1 MB  Specifies the maximum total size of messages for each fetch request.    metadataRefreshInterval  int  30 Seconds  Interval in between refresh the metadata change(broker change) in milliseconds. Enabling metadata refresh guarantees an automatic reconnect when a new broker is elected as the host. A value of -1 disables this feature.    metadataRefreshRetryLimit  int  -1  Specifies the maximum brokers' metadata refresh retry limit. -1 means unlimited retry.", 
            "title": "Configuration Parameters\u00a0for SimpleKafkaConsumer"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#offsetmanager", 
            "text": "This is an interface for offset management and is useful when consuming data\nfrom specified offsets. Updates the offsets for all the Kafka partitions\nperiodically. Below is the code snippet:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  public interface OffsetManager\n{\n  public Map KafkaPartition, Long  loadInitialOffsets();\n  public void updateOffsets(Map KafkaPartition, Long  offsetsOfPartitions);\n}", 
            "title": "OffsetManager"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods_2", 
            "text": "Map  KafkaPartition, Long  loadInitialOffsets(): Specifies the initial offset for consuming messages; called at the activation stage.  updateOffsets(Map  KafkaPartition, Long  offsetsOfPartitions): \u00a0This\nmethod is called at every repartitionCheckInterval to update offsets.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#partitioning", 
            "text": "The logical instance of the KafkaInputOperator acts as the Partitioner\nas well as a StatsListener. This is because the\nAbstractKafkaInputOperator implements both the\ncom.datatorrent.api.Partitioner and com.datatorrent.api.StatsListener\ninterfaces and provides an implementation of definePartitions(...) and\nprocessStats(...) which makes it auto-scalable.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#response-processstatsbatchedoperatorstats-stats", 
            "text": "The application master invokes this method on the logical instance with\nthe stats (tuplesProcessedPS, bytesPS, etc.) of each partition.\nRe-partitioning happens based on whether any new Kafka partitions added for\nthe topic or bytesPS and msgPS cross their respective upper bounds.", 
            "title": "Response processStats(BatchedOperatorStats stats)"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#definepartitions", 
            "text": "Based on the repartitionRequired field of the Response object which is\nreturned by processStats(...) method, the application master invokes\ndefinePartitions(...) on the logical instance which is also the\npartitioner instance. Dynamic partition can be disabled by setting the\nparameter repartitionInterval value to a negative value.", 
            "title": "DefinePartitions"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstractsingleportkafkainputoperator", 
            "text": "This class extends AbstractKafkaInputOperator and having single output\nport, will emit the messages through this port.", 
            "title": "AbstractSinglePortKafkaInputOperator"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#ports", 
            "text": "outputPort  T : Tuples extracted from Kafka messages are emitted through\nthis port.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods_3", 
            "text": "T getTuple(Message msg) : Converts the Kafka message to tuple.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#concrete-classes", 
            "text": "KafkaSinglePortStringInputOperator :\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts string from Kafka message.    KafkaSinglePortByteArrayInputOperator:\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts byte array from Kafka message.", 
            "title": "Concrete Classes"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#application-example", 
            "text": "This section builds an Apex application using Kafka input operator.\nBelow is the code snippet:  @ApplicationAnnotation(name =  KafkaApp )\npublic class ExampleKafkaApplication implements StreamingApplication\n{\n@Override\npublic void populateDAG(DAG dag, Configuration entries)\n{\n  KafkaSinglePortByteArrayInputOperator input =  dag.addOperator( MessageReader , new KafkaSinglePortByteArrayInputOperator());\n\n  ConsoleOutputOperator output = dag.addOperator( Output , new ConsoleOutputOperator());\n\n  dag.addStream( MessageData , input.outputPort, output.input);\n}\n}  Below is the configuration for \u201ctest\u201d Kafka topic name and\n\u201clocalhost:2181\u201d is the zookeeper forum:  property  name dt.operator.MessageReader.prop.topic /name  value test /value  /property  property  name dt.operator.KafkaInputOperator.prop.zookeeper /nam  value localhost:2181 /value  /property", 
            "title": "Application Example"
        }, 
        {
            "location": "/operators/dimensions_computation/", 
            "text": "Dimensions Computation\n\n\nBig data scenarios often have use-case where huge volumes of data flowing through the big data systems need to be observed for historical trends.\n\nMany applications addressing such scenarios can greatly benefit if they are equipped with the functionality of viewing historical data aggregated across time buckets. The process of receiving individual events, aggregating them over a duration, and using parameters to observe trends is referred to as \nDimensions Computation\n.\n\n\nThis document provides an overview of \nDimensions Computation\n, as well as instructions for using DataTorrent's operators to easily add dimensions computation to an Apache Apex application.\n\n\nOverview\n\n\nWhat is Dimensions Computation?\n\n\nDimensions Computation is a powerful mechanism that allows for spotting trends in streaming data in real-time. This tutorial will cover the concepts behind Dimensions Computation and provide details on the process of performing Dimensions Computation. We will also show you how to use Data Torrent's out of the box operators to easily add Dimensions Computation to an application.\n\n\nDimensions Computation\n provides a way for businesses to perform aggregations on configured numeric data. The \nDimensionsComputation\n operator works along with the \nDimensionStore\n operator, which provides the capability for applications to display historical data and trends.\n\n\nKey Concepts\n\n\nKey set\n\n\nA key set is a set of fields in the incoming tuple that is used to combine data for aggregation.\n\n\nValue set\n\n\nA value set is the set of fields in the incoming tuple on which \nAggregator\n(s)  are applied.\n\n\nAggregator\n\n\nAn aggregator is a mathematical function that is  applied on value fields in an incoming tuple. Examples of aggregators are SUM, COUNT, MAX, MIN, and AVERAGE.\n\n\nAggregates\n\n\nAggregates are objects containing the aggregated values for a configured value set and key combination.\n\n\nTime buckets\n\n\nTime buckets indicate the duration for which the floor time is calculated. For example, a for a time bucket of 1 minute, the floor time-value for both * 12:\n01:34\n PM * and * 12:\n01:59\n PM * will be * 12:\n01:00\n PM\n. Similarly, for an hourly time bucket, floor time-value for * \n15\n:02:34 PM\n and * \n15\n:34:00 PM\n will be * \n15:00:00\n PM\n.\n\n\nAfter calculating the floor value for a duration, the time-value becomes a key. Currently supported time buckets are:  1 second, 1 minute, 1 hour, and 1 day.\n\n\nCombinations\n\n\nCombinations indicate a group of the keys that are used for aggregate computations.\n\n\nIncremental \nAggregators\n\n\nIncremental aggregators are aggregate functions for which computations are possible only by using previous aggregate value and the new value. For example,\n\n\nSUM = {Previous_SUM} + {Current_Value}\nCOUNT = {Previous_COUNT}  + 1\nMIN = ( {Current_Value} \n {Previous_MIN} ) ? {Current_Value} : {Previous_MIN}\n\n\n\n\nOn-the-fly \nAggregators\n (OTF Aggregators)\n\n\nOn-the-fly aggregators are the aggregate functions that use the result of multiple incremental aggregators, and can be calculated on-the-fly if necessary. For example,\n\n\nAVERAGE = {Current_SUM} / {Current_COUNT}\n\n\n\n\nDimensionsComputation operator\n\n\nThe DimensionsComputation operator  is an Operator that performs intermediate aggregations using incremental aggregators.\n\n\nDimensionsStore operator\n\n\nThe DimensionsStore operator  is an Operator that performs transient and final aggregations on the data generated by the Dimensions Computations operator. It maintains the historical data for aggregates to ensure a meaningful  historical view.\n\n\nDimensions Computation use cases\n\n\nConsider the case of a digital advertising publisher who receives thousands of click events every second. The history of individual clicks and impressions doesn't divulge details about users and the advertisements. A technique for deriving meaning out of such data is to observe the total number of clicks and impressions every second, minute, hour, and day. Such a technique might be  helpful for determining global trends in the advertising system, but may not provide enough granularity for localized trends. For example, the total clicks and impressions over a duration might lack in usefulness, however, the total clicks and impressions for a particular advertiser, a particular geographical area, or a combination of the two can provide actionable insight.\n\n\nArchitecture\n\n\nDimensions Computation requires 4 operators working in sync: DimensionsComputation, DimensionsStore, Query, and QueryResult. Given \nDAG\n is for a complete Dimensions Computation application.\n\n\nThe operators within Dimensions Computation are described in detail.\n\n\nDimensionsComputation\n\n\nThe DimensionsComputation operator works only with incremental aggregates. The incoming data stream contains tuples that are Plain Old Java Objects (POJO), which contain data required for aggregations.\nDepending on the \nconfiguration\n, the DimensionsComputation operator applies incremental aggregators on the value set of the tuple data within a boundary of an application window. At the end of the application window, the aggregate value is reset to calculate the new aggregates. Thus, discrete aggregates are generated by DimensionsComputation operator for every application window. This output is used by the DimensionStore operator (labeled as Store in \nDAG\n) for calculating cumulative aggregates.\n\n\nDimensionsStore\n\n\nThe DimensionsStore operator uses the discrete aggregates generated by the DimensionsComputation operator to generate cumulative aggregates in turn. Because the aggregates generated by the DimensionsComputation operator are incremental aggregates, the sum of multiple such aggregates provides cumulative aggregates as follows:\n\n\nSUM1 = SUM(Value11, Value12, ...)\nSUM2 = SUM(Value21, Value22, ...)\n\n{Cumulative_SUM} = SUM1 + SUM2\n\n\n\n\nThe DimensionsStore operator also stores transient aggregates in a persistent proprietary store called HDHT. The DimensionsStore operator uses HDHT to present the requested historical data.\n\n\nQuery\n\n\nThe Query operator interfaces with the \npubsub server\n of \nDataTorrent Gateway\n. The browser creates a websocket connection with the  pubsub server hosted by DataTorrent Gateway. The Dashboard UI Widgets send queries to the pubsub server via this connection. The Query operator subscribes to the configured pubsub topic for receiving queries. These queries are parsed by the Query operator and passed onto DimensionsStore to fetch data from HDHT Store.\n\n\nQueryResult\n\n\nThe QueryResult operator gets the result from the DimensionsStore operator for a given query. The results are reconstructed into a format that is understood by a widget. After the results are reconstructed into the required format, they are sent to the \npubsub server\n for publishing to UI widgets.\n\n\nDAG \n\n\n\n\nDimensions Computation Configuration \n\n\nConfiguration Definitions\n\n\nThe configuration of Dimensions Computation is divided into: Dimensions Computation Schema Configuration and Operator Configurations as follows:\n\n\nDimensions Computation Schema Configuration\n\n\nThe Dimensions Computation Schema provides the Dimensions Computation operator with information about the aggregations. The schema looks like this:\n\n\n{\nkeys\n:[{\nname\n:\npublisher\n,\ntype\n:\nstring\n,\nenumValues\n:[\ntwitter\n,\nfacebook\n,\nyahoo\n]},\n         {\nname\n:\nadvertiser\n,\ntype\n:\nstring\n,\nenumValues\n:[\nstarbucks\n,\nsafeway\n,\nmcdonalds\n]},\n         {\nname\n:\nlocation\n,\ntype\n:\nstring\n,\nenumValues\n:[\nN\n,\nLREC\n,\nSKY\n,\nAL\n,\nAK\n]}],\n \ntimeBuckets\n:[\n1m\n,\n1h\n,\n1d\n],\n \nvalues\n:\n  [{\nname\n:\nimpressions\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]},\n   {\nname\n:\nclicks\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]},\n   {\nname\n:\ncost\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]},\n   {\nname\n:\nrevenue\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]}],\n \ndimensions\n:\n  [{\ncombination\n:[]},\n   {\ncombination\n:[\nlocation\n]},\n   {\ncombination\n:[\nadvertiser\n], \nadditionalValues\n:[\nimpressions:MIN\n, \nclicks:MIN\n, \ncost:MIN\n, \nrevenue:MIN\n, \nimpressions:MAX\n, \nclicks:MAX\n, \ncost:MAX\n, \nrevenue:MAX\n]},\n   {\ncombination\n:[\npublisher\n], \nadditionalValues\n:[\nimpressions:MIN\n, \nclicks:MIN\n, \ncost:MIN\n, \nrevenue:MIN\n, \nimpressions:MAX\n, \nclicks:MAX\n, \ncost:MAX\n, \nrevenue:MAX\n]},\n   {\ncombination\n:[\nadvertiser\n,\nlocation\n]},\n   {\ncombination\n:[\npublisher\n,\nlocation\n]},\n   {\ncombination\n:[\npublisher\n,\nadvertiser\n]},\n   {\ncombination\n:[\npublisher\n,\nadvertiser\n,\nlocation\n]}]\n}\n\n\n\n\nThe schema configuration is a JSON string that contains the following information:\n\n\n\n\n\n\nkeys:\n - This contains the set of keys derived from an input tuple. The \nname\n field stands for the name of the field from input tuple. The \ntype\n can be defined for individual keys. The probable values for individual keys can be provided using \nenumValues\n.\n\n\n\n\n\n\nvalues:\n - This contains the set of fields from an input tuple on which aggregates are calculated. The \nname\n field stands for the name of the field from an input tuple. The \ntype\n can be defined for individual keys. The \naggregators\n can be defined separately for individual values. Only configured aggregator functions are executed on values.\n\n\n\n\n\n\ntimeBuckets:\n - This can be used to specify the time bucket over which aggregations occur. Possible values for timeBuckets are \n\"1m\", \"1h\", \"1d\"\n\n\n\n\n\n\ndimensions:\n - This defines the combinations of keys that are used for grouping data for aggregate calculations. This can be mentioned in \ncombination\n with the JSON key. \nadditionalValues\n can be used for mentioning additional aggregators for any \nvalue\n. For example, \nimpressions:MIN\n indicates that for a given combination, calculate \"\nMIN\n\" for value \"\nimpression\n\" as well.\nBy default, the down time rounded off to the next value as per time bucket is always considered as one of the keys.\n\n\n\n\n\n\nOperator Configurations\n\n\nOperator configurations is another set of configuration that can be used to configure individual operators.\n\n\nProperties\n\n\n\n\ndt.operator.QueryResult.topic:\n - This is the name of the topic on which UI widgets listen for results.\n\n\ndt.operator.Query.topic:\n - his is the name of the topic on which Query operator listen for queries.\n\n\ndt.operator.QueryResult.numRetries\n - This property indicates the maximum number of times the QueryResult operator should retry sending data. This value is usually high.\n\n\n\n\nAttributes\n\n\n\n\ndt.operator.DimensionsComputation.attr.PARTITIONER:\n - This attribute determines the number of  partitions for DimensionsComputation. Adding more partitions means data is  processed in parallel. If this attribute is not provided, a single partition is created. Refer to the \nPartitioning\n section for details on partitioning.\n\n\ndt.operator.DimensionsComputation.attr.MEMORY_MB:\n - This attribute determines the  memory that should be assigned to the DimensionsComputations operator. If this attribute is not provided, the default value of  1 GB is used.\n\n\ndt.operator.Store.attr.MEMORY_MB:\n - This attribute determines the memory that should be assigned to DimensionsStore operator. If this attribute is not provided, the default value of 1 GB is used.\n\n\ndt.port.*.attr.QUEUE_CAPACITY\n - This attribute determines the number of tuples the buffer server can cache without blocking the input stream to the port. For peak activity, we  recommend increasing QUEUE_CAPACITY to a higher value such as 32000. If this attribute is not provided, the default value of 1024 is used.\n\n\n\n\nVisualizing Dimensions Computation\n\n\nWhen Dimension Computation  is launched, the visualization of aggregates over a duration can be seen by adding a widget to a dtDashboard. dtDashboard is the self-service real-time and historical data visualization interface. Rapidly gaining insight and reducing time to action provides the greatest value to an organization. DataTorrent RTS provides self-service data visualization for the business user enabling them to not only see dashboards and reports an order of magnitude faster, but to also create and share customer reports.To derive more value out of dashboards, you can add widgets to the dashboards. Widgets are charts in addition to the default charts that you can see on the dashboard.\n\n\nAnd example of a Dashboard UI Widget is as follows:\n\n\n\n\nCreating Dimensions Computation Application\n\n\nConsider an example of the advertising publisher. Typically, an advertising publisher receives a packet of information for every event related to their  advertisements.\n\n\nSample publisher event\n\n\nAn event might look like this:\n\n\npublic class AdEvent\n{\n  //The name of the company that is advertising\n  public String advertiser;\n  //The geographical location of the person initiating the event\n  public String location;\n  //How much the advertiser was charged for the event\n  public double cost;\n  //How much revenue was generated for the advertiser\n  public double revenue;\n  //The number of impressions the advertiser received from this event\n  public long impressions;\n  //The number of clicks the advertiser received from this event\n  public long clicks;\n  //The timestamp of the event in milliseconds\n  public long time;\n\n  public AdEvent() {}\n\n  public AdEvent(String advertiser, String location, double cost, double revenue,\n                 long impressions, long clicks, long time)\n  {\n    this.advertiser = advertiser;\n    this.location = location;\n    this.cost = cost;\n    this.revenue = revenue;\n    this.impressions = impressions;\n    this.clicks = clicks;\n    this.time = time;\n  }\n\n  /* Getters and setters go here */\n}\n\n\n\n\nCreating an Application using out-of-the-box operators\n\n\nDimensions Computation can be created using out-of-the-box operators from the Megh and Malhar library. A sample is given below:\n\n\n@ApplicationAnnotation(name=\nAdEventDemo\n)\npublic class AdEventDemo implements StreamingApplication\n{\n  public static final String EVENT_SCHEMA = \nadsGenericEventSchema.json\n;\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    //This loads the eventSchema.json file which is a jar resource file.\n    // eventSchema.json contains the Dimensions Schema using which aggregations is configured.\n    String eventSchema = SchemaUtils.jarResourceFileToString(\neventSchema.json\n);\n\n    // Operator that receives Ad Events\n    // This can be coming from any source as long as the operator can convert the data into AdEventReceiver oject.\n    AdEventReceiver receiver = dag.addOperator(\nEvent Receiver\n, AdEventReceiver.class);\n\n    //Adding Dimensions Computation operator into DAG.\n    DimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator(\nDimensionsComputation\n, DimensionsComputationFlexibleSingleSchemaPOJO.class);\n\n    // This defines the name present in input tuple to the name of the getter method to be used to get the value of the field.\n    Map\nString, String\n keyToExpression = Maps.newHashMap();\n    keyToExpression.put(\nadvertiser\n, \ngetAdvertiser()\n);\n    keyToExpression.put(\nlocation\n, \ngetLocation()\n);\n    keyToExpression.put(\ntime\n, \ngetTime()\n);\n\n    // This defines value to expression mapping for value field name to the name of the getter method to get the value of the field.\n    Map\nString, String\n valueToExpression = Maps.newHashMap();\n    valueToExpression.put(\ncost\n, \ngetCost()\n);\n    valueToExpression.put(\nrevenue\n, \ngetRevenue()\n);\n    valueToExpression.put(\nimpressions\n, \ngetImpressions()\n);\n    valueToExpression.put(\nclicks\n, \ngetClicks()\n);\n\n    dimensions.setKeyToExpression(keyToExpression);\n    dimensions.setAggregateToExpression(aggregateToExpression);\n    dimensions.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the unifier. The purpose of this unifier is to combine the partial aggregates from different partitions of DimensionsComputation operator into single stream.\n    dimensions.setUnifier(new DimensionsComputationUnifierImpl\nInputEvent, Aggregate\n());\n\n    // Add Dimension Store operator to DAG.\n    AppDataSingleSchemaDimensionStoreHDHT store = dag.addOperator(\nStore\n, AppDataSingleSchemaDimensionStoreHDHT.class);\n\n    // This configure the Backend HDHT store of DimensionStore operator. This backend will be used to persist the Historical Aggregates Data.\n    TFileImpl hdsFile = new TFileImpl.DTFileImpl();\n    hdsFile.setBasePath(\ndataStorePath\n);\n    store.setFileStore(hdsFile);\n    store.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the Query and QueryResult operators to the gateway address. This is needs for pubsub communication of queries/results between operators and pubsub server.\n    String gatewayAddress = dag.getValue(DAG.GATEWAY_CONNECT_ADDRESS);\n    URI uri = URI.create(\nws://\n + gatewayAddress + \n/pubsub\n);\n    PubSubWebSocketAppDataQuery wsIn = dag.addOperator(\nQuery\n, PubSubWebSocketAppDataQuery.class);\n    wsIn.setUri(uri);\n    PubSubWebSocketAppDataResult wsOut = dag.addOperator(\nQueryResult\n, PubSubWebSocketAppDataResult.class);\n    wsOut.setUri(uri);\n\n    // Connecting all together.\n    dag.addStream(\nQuery\n, wsIn.outputPort, store.query);\n    dag.addStream(\nQueryResult\n, store.queryResult, wsOut.input);\n    dag.addStream(\nInputStream\n, receiver.output, dimensions.input);\n    dag.addStream(\nDimensionalData\n, dimensions.output, store.input);\n  }\n}\n\n\n\n\nConfiguration for Sample Predefined Use Cases\n\n\nThe following configuration can be used for the predefined use case of the advertiser-publisher.\n\n\nDimensions Schema Configuration\n\n\n{\nkeys\n:[{\nname\n:\nadvertiser\n,\ntype\n:\nstring\n},\n         {\nname\n:\nlocation\n,\ntype\n:\nstring\n}],\n \ntimeBuckets\n:[\n1m\n,\n1h\n,\n1d\n],\n \nvalues\n:\n  [{\nname\n:\nimpressions\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]},\n   {\nname\n:\nclicks\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]},\n   {\nname\n:\ncost\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]},\n   {\nname\n:\nrevenue\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]}],\n \ndimensions\n:\n  [{\ncombination\n:[]},\n   {\ncombination\n:[\nlocation\n]},\n   {\ncombination\n:[\nadvertiser\n]},\n   {\ncombination\n:[\nadvertiser\n,\nlocation\n]}]\n}\n\n\n\n\nOperator Configuration\n\n\nNote:\n This operator configuration is used for an application where the input data rate is high. To sustain the load, the Dimensions Computation operator is partitioned 8 times, and the queue capacity is increased.\n\n\n?xml version=\n1.0\n encoding=\nUTF-8\n standalone=\nno\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\ndt.operator.DimensionsComputation.attr.PARTITIONER\n/name\n\n    \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:8\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.DimensionsComputation.attr.MEMORY_MB\n/name\n\n    \nvalue\n16384\n/value\n\n  \n/property\n\n  \nproperty\n\n     \nname\ndt.port.*.attr.QUEUE_CAPACITY\n/name\n\n     \nvalue\n32000\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.Query.topic\n/name\n\n    \nvalue\nAdsEventQuery\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.QueryResult.topic\n/name\n\n    \nvalue\nAdsEventQueryResult\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nThe above operator configuration is to be used for a highly loaded application where the input rate is quite high. To sustain the load, the Dimensions Computation operator is partitioned 8 times and the queue capacity is also increased.\n\n\nAdvanced Concepts\n\n\nPartitioning \n\n\nThe Dimensions Computation operator can be statically partitioned for higher processing throughput. This can be done by adding the following attributes in the \nproperties.xml\n file, or in the \ndt-site.xml\n file.\n\n\nproperty\n\n  \nname\ndt.operator.DimensionsComputations.attr.PARTITIONER\n/name\n\n  \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:8\n/value\n\n\n/property\n\n\n\n\n\nThis adds the PARTITIONER attribute. This attribute creates a StatelessPartitioner for the DimensionsComputation operator. The parameter of \n8\n is going to partition the operator 8 times.\nThe StatelessPartitioner ensures that the operators are clones of each other. The tuples passed to individual clones of operators are decided based on the hashCode of the tuple.\n\n\nAlong with partitioned DimensionsComputation, there also comes a unifier which combines all the intermediate results from individual DimensionsComputation operators into a single stream. This stream is then passed to Dimensions Store.\nFollowing code needs to be added in populateDAG for adding an Unifier:\n\n\nDimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator(\nDimensionsComputation\n, DimensionsComputationFlexibleSingleSchemaPOJO.class);\ndimensions.setUnifier(new DimensionsComputationUnifierImpl\nInputEvent, Aggregate\n());\n\n\n\n\nHere the unifier used is \nDimensionsComputationUnifierImpl\n which is an out-of-the-box operator present in the DataTorrent distribution.\n\n\nConclusion\n\n\nAggregating huge amounts of data in real time is a major challenge that many enterprises face today. Dimension Computation provides a valuable way in which to think about the problem of aggregating data, and Data Torrent provides an implementation of of Dimension Computation that allows users to integrate data aggregation with their applications with minimal effort.", 
            "title": "Dimension Computation"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation", 
            "text": "Big data scenarios often have use-case where huge volumes of data flowing through the big data systems need to be observed for historical trends. \nMany applications addressing such scenarios can greatly benefit if they are equipped with the functionality of viewing historical data aggregated across time buckets. The process of receiving individual events, aggregating them over a duration, and using parameters to observe trends is referred to as  Dimensions Computation .  This document provides an overview of  Dimensions Computation , as well as instructions for using DataTorrent's operators to easily add dimensions computation to an Apache Apex application.", 
            "title": "Dimensions Computation"
        }, 
        {
            "location": "/operators/dimensions_computation/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/operators/dimensions_computation/#what-is-dimensions-computation", 
            "text": "Dimensions Computation is a powerful mechanism that allows for spotting trends in streaming data in real-time. This tutorial will cover the concepts behind Dimensions Computation and provide details on the process of performing Dimensions Computation. We will also show you how to use Data Torrent's out of the box operators to easily add Dimensions Computation to an application.  Dimensions Computation  provides a way for businesses to perform aggregations on configured numeric data. The  DimensionsComputation  operator works along with the  DimensionStore  operator, which provides the capability for applications to display historical data and trends.", 
            "title": "What is Dimensions Computation?"
        }, 
        {
            "location": "/operators/dimensions_computation/#key-concepts", 
            "text": "", 
            "title": "Key Concepts"
        }, 
        {
            "location": "/operators/dimensions_computation/#key-set", 
            "text": "A key set is a set of fields in the incoming tuple that is used to combine data for aggregation.", 
            "title": "Key set"
        }, 
        {
            "location": "/operators/dimensions_computation/#value-set", 
            "text": "A value set is the set of fields in the incoming tuple on which  Aggregator (s)  are applied.", 
            "title": "Value set"
        }, 
        {
            "location": "/operators/dimensions_computation/#aggregator", 
            "text": "An aggregator is a mathematical function that is  applied on value fields in an incoming tuple. Examples of aggregators are SUM, COUNT, MAX, MIN, and AVERAGE.", 
            "title": "Aggregator"
        }, 
        {
            "location": "/operators/dimensions_computation/#aggregates", 
            "text": "Aggregates are objects containing the aggregated values for a configured value set and key combination.", 
            "title": "Aggregates"
        }, 
        {
            "location": "/operators/dimensions_computation/#time-buckets", 
            "text": "Time buckets indicate the duration for which the floor time is calculated. For example, a for a time bucket of 1 minute, the floor time-value for both * 12: 01:34  PM * and * 12: 01:59  PM * will be * 12: 01:00  PM . Similarly, for an hourly time bucket, floor time-value for *  15 :02:34 PM  and *  15 :34:00 PM  will be *  15:00:00  PM .  After calculating the floor value for a duration, the time-value becomes a key. Currently supported time buckets are:  1 second, 1 minute, 1 hour, and 1 day.", 
            "title": "Time buckets"
        }, 
        {
            "location": "/operators/dimensions_computation/#combinations", 
            "text": "Combinations indicate a group of the keys that are used for aggregate computations.", 
            "title": "Combinations"
        }, 
        {
            "location": "/operators/dimensions_computation/#incremental-aggregators", 
            "text": "Incremental aggregators are aggregate functions for which computations are possible only by using previous aggregate value and the new value. For example,  SUM = {Previous_SUM} + {Current_Value}\nCOUNT = {Previous_COUNT}  + 1\nMIN = ( {Current_Value}   {Previous_MIN} ) ? {Current_Value} : {Previous_MIN}", 
            "title": "Incremental Aggregators"
        }, 
        {
            "location": "/operators/dimensions_computation/#on-the-fly-aggregators-otf-aggregators", 
            "text": "On-the-fly aggregators are the aggregate functions that use the result of multiple incremental aggregators, and can be calculated on-the-fly if necessary. For example,  AVERAGE = {Current_SUM} / {Current_COUNT}", 
            "title": "On-the-fly Aggregators (OTF Aggregators)"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensionscomputation-operator", 
            "text": "The DimensionsComputation operator  is an Operator that performs intermediate aggregations using incremental aggregators.", 
            "title": "DimensionsComputation operator"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensionsstore-operator", 
            "text": "The DimensionsStore operator  is an Operator that performs transient and final aggregations on the data generated by the Dimensions Computations operator. It maintains the historical data for aggregates to ensure a meaningful  historical view.", 
            "title": "DimensionsStore operator"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation-use-cases", 
            "text": "Consider the case of a digital advertising publisher who receives thousands of click events every second. The history of individual clicks and impressions doesn't divulge details about users and the advertisements. A technique for deriving meaning out of such data is to observe the total number of clicks and impressions every second, minute, hour, and day. Such a technique might be  helpful for determining global trends in the advertising system, but may not provide enough granularity for localized trends. For example, the total clicks and impressions over a duration might lack in usefulness, however, the total clicks and impressions for a particular advertiser, a particular geographical area, or a combination of the two can provide actionable insight.", 
            "title": "Dimensions Computation use cases"
        }, 
        {
            "location": "/operators/dimensions_computation/#architecture", 
            "text": "Dimensions Computation requires 4 operators working in sync: DimensionsComputation, DimensionsStore, Query, and QueryResult. Given  DAG  is for a complete Dimensions Computation application.  The operators within Dimensions Computation are described in detail.", 
            "title": "Architecture"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensionscomputation", 
            "text": "The DimensionsComputation operator works only with incremental aggregates. The incoming data stream contains tuples that are Plain Old Java Objects (POJO), which contain data required for aggregations.\nDepending on the  configuration , the DimensionsComputation operator applies incremental aggregators on the value set of the tuple data within a boundary of an application window. At the end of the application window, the aggregate value is reset to calculate the new aggregates. Thus, discrete aggregates are generated by DimensionsComputation operator for every application window. This output is used by the DimensionStore operator (labeled as Store in  DAG ) for calculating cumulative aggregates.", 
            "title": "DimensionsComputation"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensionsstore", 
            "text": "The DimensionsStore operator uses the discrete aggregates generated by the DimensionsComputation operator to generate cumulative aggregates in turn. Because the aggregates generated by the DimensionsComputation operator are incremental aggregates, the sum of multiple such aggregates provides cumulative aggregates as follows:  SUM1 = SUM(Value11, Value12, ...)\nSUM2 = SUM(Value21, Value22, ...)\n\n{Cumulative_SUM} = SUM1 + SUM2  The DimensionsStore operator also stores transient aggregates in a persistent proprietary store called HDHT. The DimensionsStore operator uses HDHT to present the requested historical data.", 
            "title": "DimensionsStore"
        }, 
        {
            "location": "/operators/dimensions_computation/#query", 
            "text": "The Query operator interfaces with the  pubsub server  of  DataTorrent Gateway . The browser creates a websocket connection with the  pubsub server hosted by DataTorrent Gateway. The Dashboard UI Widgets send queries to the pubsub server via this connection. The Query operator subscribes to the configured pubsub topic for receiving queries. These queries are parsed by the Query operator and passed onto DimensionsStore to fetch data from HDHT Store.", 
            "title": "Query"
        }, 
        {
            "location": "/operators/dimensions_computation/#queryresult", 
            "text": "The QueryResult operator gets the result from the DimensionsStore operator for a given query. The results are reconstructed into a format that is understood by a widget. After the results are reconstructed into the required format, they are sent to the  pubsub server  for publishing to UI widgets.", 
            "title": "QueryResult"
        }, 
        {
            "location": "/operators/dimensions_computation/#dag", 
            "text": "", 
            "title": "DAG "
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation-configuration", 
            "text": "", 
            "title": "Dimensions Computation Configuration "
        }, 
        {
            "location": "/operators/dimensions_computation/#configuration-definitions", 
            "text": "The configuration of Dimensions Computation is divided into: Dimensions Computation Schema Configuration and Operator Configurations as follows:", 
            "title": "Configuration Definitions"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation-schema-configuration", 
            "text": "The Dimensions Computation Schema provides the Dimensions Computation operator with information about the aggregations. The schema looks like this:  { keys :[{ name : publisher , type : string , enumValues :[ twitter , facebook , yahoo ]},\n         { name : advertiser , type : string , enumValues :[ starbucks , safeway , mcdonalds ]},\n         { name : location , type : string , enumValues :[ N , LREC , SKY , AL , AK ]}],\n  timeBuckets :[ 1m , 1h , 1d ],\n  values :\n  [{ name : impressions , type : long , aggregators :[ SUM , COUNT , AVG ]},\n   { name : clicks , type : long , aggregators :[ SUM , COUNT , AVG ]},\n   { name : cost , type : double , aggregators :[ SUM , COUNT , AVG ]},\n   { name : revenue , type : double , aggregators :[ SUM , COUNT , AVG ]}],\n  dimensions :\n  [{ combination :[]},\n   { combination :[ location ]},\n   { combination :[ advertiser ],  additionalValues :[ impressions:MIN ,  clicks:MIN ,  cost:MIN ,  revenue:MIN ,  impressions:MAX ,  clicks:MAX ,  cost:MAX ,  revenue:MAX ]},\n   { combination :[ publisher ],  additionalValues :[ impressions:MIN ,  clicks:MIN ,  cost:MIN ,  revenue:MIN ,  impressions:MAX ,  clicks:MAX ,  cost:MAX ,  revenue:MAX ]},\n   { combination :[ advertiser , location ]},\n   { combination :[ publisher , location ]},\n   { combination :[ publisher , advertiser ]},\n   { combination :[ publisher , advertiser , location ]}]\n}  The schema configuration is a JSON string that contains the following information:    keys:  - This contains the set of keys derived from an input tuple. The  name  field stands for the name of the field from input tuple. The  type  can be defined for individual keys. The probable values for individual keys can be provided using  enumValues .    values:  - This contains the set of fields from an input tuple on which aggregates are calculated. The  name  field stands for the name of the field from an input tuple. The  type  can be defined for individual keys. The  aggregators  can be defined separately for individual values. Only configured aggregator functions are executed on values.    timeBuckets:  - This can be used to specify the time bucket over which aggregations occur. Possible values for timeBuckets are  \"1m\", \"1h\", \"1d\"    dimensions:  - This defines the combinations of keys that are used for grouping data for aggregate calculations. This can be mentioned in  combination  with the JSON key.  additionalValues  can be used for mentioning additional aggregators for any  value . For example,  impressions:MIN  indicates that for a given combination, calculate \" MIN \" for value \" impression \" as well.\nBy default, the down time rounded off to the next value as per time bucket is always considered as one of the keys.", 
            "title": "Dimensions Computation Schema Configuration"
        }, 
        {
            "location": "/operators/dimensions_computation/#operator-configurations", 
            "text": "Operator configurations is another set of configuration that can be used to configure individual operators.", 
            "title": "Operator Configurations"
        }, 
        {
            "location": "/operators/dimensions_computation/#properties", 
            "text": "dt.operator.QueryResult.topic:  - This is the name of the topic on which UI widgets listen for results.  dt.operator.Query.topic:  - his is the name of the topic on which Query operator listen for queries.  dt.operator.QueryResult.numRetries  - This property indicates the maximum number of times the QueryResult operator should retry sending data. This value is usually high.", 
            "title": "Properties"
        }, 
        {
            "location": "/operators/dimensions_computation/#attributes", 
            "text": "dt.operator.DimensionsComputation.attr.PARTITIONER:  - This attribute determines the number of  partitions for DimensionsComputation. Adding more partitions means data is  processed in parallel. If this attribute is not provided, a single partition is created. Refer to the  Partitioning  section for details on partitioning.  dt.operator.DimensionsComputation.attr.MEMORY_MB:  - This attribute determines the  memory that should be assigned to the DimensionsComputations operator. If this attribute is not provided, the default value of  1 GB is used.  dt.operator.Store.attr.MEMORY_MB:  - This attribute determines the memory that should be assigned to DimensionsStore operator. If this attribute is not provided, the default value of 1 GB is used.  dt.port.*.attr.QUEUE_CAPACITY  - This attribute determines the number of tuples the buffer server can cache without blocking the input stream to the port. For peak activity, we  recommend increasing QUEUE_CAPACITY to a higher value such as 32000. If this attribute is not provided, the default value of 1024 is used.", 
            "title": "Attributes"
        }, 
        {
            "location": "/operators/dimensions_computation/#visualizing-dimensions-computation", 
            "text": "When Dimension Computation  is launched, the visualization of aggregates over a duration can be seen by adding a widget to a dtDashboard. dtDashboard is the self-service real-time and historical data visualization interface. Rapidly gaining insight and reducing time to action provides the greatest value to an organization. DataTorrent RTS provides self-service data visualization for the business user enabling them to not only see dashboards and reports an order of magnitude faster, but to also create and share customer reports.To derive more value out of dashboards, you can add widgets to the dashboards. Widgets are charts in addition to the default charts that you can see on the dashboard.  And example of a Dashboard UI Widget is as follows:", 
            "title": "Visualizing Dimensions Computation"
        }, 
        {
            "location": "/operators/dimensions_computation/#creating-dimensions-computation-application", 
            "text": "Consider an example of the advertising publisher. Typically, an advertising publisher receives a packet of information for every event related to their  advertisements.", 
            "title": "Creating Dimensions Computation Application"
        }, 
        {
            "location": "/operators/dimensions_computation/#sample-publisher-event", 
            "text": "An event might look like this:  public class AdEvent\n{\n  //The name of the company that is advertising\n  public String advertiser;\n  //The geographical location of the person initiating the event\n  public String location;\n  //How much the advertiser was charged for the event\n  public double cost;\n  //How much revenue was generated for the advertiser\n  public double revenue;\n  //The number of impressions the advertiser received from this event\n  public long impressions;\n  //The number of clicks the advertiser received from this event\n  public long clicks;\n  //The timestamp of the event in milliseconds\n  public long time;\n\n  public AdEvent() {}\n\n  public AdEvent(String advertiser, String location, double cost, double revenue,\n                 long impressions, long clicks, long time)\n  {\n    this.advertiser = advertiser;\n    this.location = location;\n    this.cost = cost;\n    this.revenue = revenue;\n    this.impressions = impressions;\n    this.clicks = clicks;\n    this.time = time;\n  }\n\n  /* Getters and setters go here */\n}", 
            "title": "Sample publisher event"
        }, 
        {
            "location": "/operators/dimensions_computation/#creating-an-application-using-out-of-the-box-operators", 
            "text": "Dimensions Computation can be created using out-of-the-box operators from the Megh and Malhar library. A sample is given below:  @ApplicationAnnotation(name= AdEventDemo )\npublic class AdEventDemo implements StreamingApplication\n{\n  public static final String EVENT_SCHEMA =  adsGenericEventSchema.json ;\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    //This loads the eventSchema.json file which is a jar resource file.\n    // eventSchema.json contains the Dimensions Schema using which aggregations is configured.\n    String eventSchema = SchemaUtils.jarResourceFileToString( eventSchema.json );\n\n    // Operator that receives Ad Events\n    // This can be coming from any source as long as the operator can convert the data into AdEventReceiver oject.\n    AdEventReceiver receiver = dag.addOperator( Event Receiver , AdEventReceiver.class);\n\n    //Adding Dimensions Computation operator into DAG.\n    DimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator( DimensionsComputation , DimensionsComputationFlexibleSingleSchemaPOJO.class);\n\n    // This defines the name present in input tuple to the name of the getter method to be used to get the value of the field.\n    Map String, String  keyToExpression = Maps.newHashMap();\n    keyToExpression.put( advertiser ,  getAdvertiser() );\n    keyToExpression.put( location ,  getLocation() );\n    keyToExpression.put( time ,  getTime() );\n\n    // This defines value to expression mapping for value field name to the name of the getter method to get the value of the field.\n    Map String, String  valueToExpression = Maps.newHashMap();\n    valueToExpression.put( cost ,  getCost() );\n    valueToExpression.put( revenue ,  getRevenue() );\n    valueToExpression.put( impressions ,  getImpressions() );\n    valueToExpression.put( clicks ,  getClicks() );\n\n    dimensions.setKeyToExpression(keyToExpression);\n    dimensions.setAggregateToExpression(aggregateToExpression);\n    dimensions.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the unifier. The purpose of this unifier is to combine the partial aggregates from different partitions of DimensionsComputation operator into single stream.\n    dimensions.setUnifier(new DimensionsComputationUnifierImpl InputEvent, Aggregate ());\n\n    // Add Dimension Store operator to DAG.\n    AppDataSingleSchemaDimensionStoreHDHT store = dag.addOperator( Store , AppDataSingleSchemaDimensionStoreHDHT.class);\n\n    // This configure the Backend HDHT store of DimensionStore operator. This backend will be used to persist the Historical Aggregates Data.\n    TFileImpl hdsFile = new TFileImpl.DTFileImpl();\n    hdsFile.setBasePath( dataStorePath );\n    store.setFileStore(hdsFile);\n    store.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the Query and QueryResult operators to the gateway address. This is needs for pubsub communication of queries/results between operators and pubsub server.\n    String gatewayAddress = dag.getValue(DAG.GATEWAY_CONNECT_ADDRESS);\n    URI uri = URI.create( ws://  + gatewayAddress +  /pubsub );\n    PubSubWebSocketAppDataQuery wsIn = dag.addOperator( Query , PubSubWebSocketAppDataQuery.class);\n    wsIn.setUri(uri);\n    PubSubWebSocketAppDataResult wsOut = dag.addOperator( QueryResult , PubSubWebSocketAppDataResult.class);\n    wsOut.setUri(uri);\n\n    // Connecting all together.\n    dag.addStream( Query , wsIn.outputPort, store.query);\n    dag.addStream( QueryResult , store.queryResult, wsOut.input);\n    dag.addStream( InputStream , receiver.output, dimensions.input);\n    dag.addStream( DimensionalData , dimensions.output, store.input);\n  }\n}", 
            "title": "Creating an Application using out-of-the-box operators"
        }, 
        {
            "location": "/operators/dimensions_computation/#configuration-for-sample-predefined-use-cases", 
            "text": "The following configuration can be used for the predefined use case of the advertiser-publisher.", 
            "title": "Configuration for Sample Predefined Use Cases"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-schema-configuration", 
            "text": "{ keys :[{ name : advertiser , type : string },\n         { name : location , type : string }],\n  timeBuckets :[ 1m , 1h , 1d ],\n  values :\n  [{ name : impressions , type : long , aggregators :[ SUM , MAX , MIN ]},\n   { name : clicks , type : long , aggregators :[ SUM , MAX , MIN ]},\n   { name : cost , type : double , aggregators :[ SUM , MAX , MIN ]},\n   { name : revenue , type : double , aggregators :[ SUM , MAX , MIN ]}],\n  dimensions :\n  [{ combination :[]},\n   { combination :[ location ]},\n   { combination :[ advertiser ]},\n   { combination :[ advertiser , location ]}]\n}", 
            "title": "Dimensions Schema Configuration"
        }, 
        {
            "location": "/operators/dimensions_computation/#operator-configuration", 
            "text": "Note:  This operator configuration is used for an application where the input data rate is high. To sustain the load, the Dimensions Computation operator is partitioned 8 times, and the queue capacity is increased.  ?xml version= 1.0  encoding= UTF-8  standalone= no ?  configuration \n   property \n     name dt.operator.DimensionsComputation.attr.PARTITIONER /name \n     value com.datatorrent.common.partitioner.StatelessPartitioner:8 /value \n   /property \n   property \n     name dt.operator.DimensionsComputation.attr.MEMORY_MB /name \n     value 16384 /value \n   /property \n   property \n      name dt.port.*.attr.QUEUE_CAPACITY /name \n      value 32000 /value \n   /property \n   property \n     name dt.operator.Query.topic /name \n     value AdsEventQuery /value \n   /property \n   property \n     name dt.operator.QueryResult.topic /name \n     value AdsEventQueryResult /value \n   /property  /configuration   The above operator configuration is to be used for a highly loaded application where the input rate is quite high. To sustain the load, the Dimensions Computation operator is partitioned 8 times and the queue capacity is also increased.", 
            "title": "Operator Configuration"
        }, 
        {
            "location": "/operators/dimensions_computation/#advanced-concepts", 
            "text": "", 
            "title": "Advanced Concepts"
        }, 
        {
            "location": "/operators/dimensions_computation/#partitioning", 
            "text": "The Dimensions Computation operator can be statically partitioned for higher processing throughput. This can be done by adding the following attributes in the  properties.xml  file, or in the  dt-site.xml  file.  property \n   name dt.operator.DimensionsComputations.attr.PARTITIONER /name \n   value com.datatorrent.common.partitioner.StatelessPartitioner:8 /value  /property   This adds the PARTITIONER attribute. This attribute creates a StatelessPartitioner for the DimensionsComputation operator. The parameter of  8  is going to partition the operator 8 times.\nThe StatelessPartitioner ensures that the operators are clones of each other. The tuples passed to individual clones of operators are decided based on the hashCode of the tuple.  Along with partitioned DimensionsComputation, there also comes a unifier which combines all the intermediate results from individual DimensionsComputation operators into a single stream. This stream is then passed to Dimensions Store.\nFollowing code needs to be added in populateDAG for adding an Unifier:  DimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator( DimensionsComputation , DimensionsComputationFlexibleSingleSchemaPOJO.class);\ndimensions.setUnifier(new DimensionsComputationUnifierImpl InputEvent, Aggregate ());  Here the unifier used is  DimensionsComputationUnifierImpl  which is an out-of-the-box operator present in the DataTorrent distribution.", 
            "title": "Partitioning "
        }, 
        {
            "location": "/operators/dimensions_computation/#conclusion", 
            "text": "Aggregating huge amounts of data in real time is a major challenge that many enterprises face today. Dimension Computation provides a valuable way in which to think about the problem of aggregating data, and Data Torrent provides an implementation of of Dimension Computation that allows users to integrate data aggregation with their applications with minimal effort.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/operators/hdht/", 
            "text": "HDHT\n\n\nSome applications need to compute values based not only on current event flow but also on historical data. HDHT provides a simple interface to store\nand access historical data in an operator. HDHT is an embedded state store with key value interface on top of the Hadoop file system. It is fully integrated into the Apache Apex operator model and provides persistent storage with exactly-once guarantee.\n\n\nThe programming model of a key-value store or hash table can be applied to a wide range of common use cases. Within most streaming applications, ingested events or computed data already carry the key that can be used for storage and retrieval. Many operations performed during computation require key based access. HDHT provides an embedded key value store for the application. The advantage of HDHT over other key value stores in streaming applications are\n\n\n\n\nEmbedded, Hadoop native solution. Does not require install/manage of other services.\n\n\nIntegrates with Apex check-pointing mechanism to provide exactly once guarantee.\n\n\n\n\nThis document provides overview of HDHT and instructions for using HDHT in an operator for storing\nand retrieving state of the operator.\n\n\nConcepts\n\n\nWrite Ahead Log\n\n\nEach tuple written to HDHT is written to Write Ahead Log (WAL) first. The WAL is used\nto recover in-memory state and provide exactly once processing after failure of an operator. HDHT\nstores WAL on HDFS to prevent data loss during node failure.\n\n\nUncommitted Cache\n\n\nUncommitted cache is in-memory key value store. Initially updates are written to this memory store, to avoid disk I/Os on every update. When data in Uncommitted cached reaches a configured limit, it is written on the disk. It avoids frequent data flushes and small file creation by writing data in bulk from the cache to disk thereby also improving throughput.\n\n\nData Files\n\n\nHDHT flushes memory to persisted storage periodically. The data is kept indexed for efficient retrieval of given key. HDHT supports multiple data file backends. Default backend used is DTFile, which is a modified version of Hadoop TFile with block cache for speedy lookups.\nAvailable backends are\n- \nTFile\n: Files are written in hadoop \nTfile\n format\n- \nDTFile\n: Files are written in TFile format; during lookup HDHT maintains a block cache to reduce disk I/Os.\n- \nHFile\n : Files are written in HBase format.\n\n\nMetadata\n\n\nMetadata file keeps information about data files. Each data file record contains start key and name of the file. Metadata file also contains WAL recovery information, which is used during recovery after failure.\n\n\nPartition (HDHT Bucket)\n\n\nBy default, when the operator is partitioned, the partitioning is reflected by HDHT in the filesystem by using a separate directory for each operator partition. Each directory is accessed only by the associated operator partition. Each partition has its own WAL and metadata file. Each\nHDHT partition is identified by bucketKey, which is also the name of the subdirectory used for\nstoring data for the partition.\n\n\nInterface\n\n\nHDHT supports two basic operations \nget\n and \nput\n, they are wrapped by interfaces HDHT.Writer, HDHT.Reader and an abstract implementation is provided by the HDHTReader and HDHTWriter classes.\n\n\nOperations supported by HDHT are.\n\n\nbyte[] get(long bucketKey, Slice key) throws IOException;\nvoid put(long bucketKey, Slice key, byte[] value) throws IOException;\nbyte[] getUncommitted(long bucketKey, Slice key);\n\n\n\n\nAll methods takes bucketKey as the first argument. The bucketKey is used as a partition key within HDHT.\n\n\n\n\nput\n store data in HDHT. The data written is written to the WAL first and then stored in uncommitted cache.\nAfter enough dirty data is accumulated in cache or enough time has elapsed from last flush, this cache is flushed to the data files.\n\n\ngetUncommitted\n does a lookup in uncommitted cache. Uncommitted cache is in-memory key value store.\n\n\nget\n does a lookup in persisted storage file and return the data. \nNote\n \nget\n does not\n return data from uncommitted cache.\n\n\n\n\nArchitecture\n\n\nPlease refer to \nHDHT Blog\n\nfor the architecture of HDHT.\n\n\nCodec\n\n\nHDHT provides an abstract implementation \nAbstractSinglePortHDHTWriter\n, which uses a user defined codec for serialization and de-serialization.\n\n\npublic interface HDHTCodec\nEVENT\n extends StreamCodec\nEVENT\n\n{\n  byte[] getKeyBytes(EVENT event);\n  byte[] getValueBytes(EVENT event);\n  EVENT fromKeyValue(Slice key, byte[] value);\n}\n\n\n\n\nIt has following methods\n- \ngetKeyBytes\n Return key as a byte array from the event.\n- \ngetValueBytes\n Return value as a byte array from event.\n- \nfromKeyValue\n HDHT will use this function to deserialize key and value byte arrays to reconstruct the user event object.\n- \ngetPartition\n This method is inherited from StreamCodec, its return value is used to determine HDHT bucket where this event will be written. The same stream codec is used\n for partition of the input port which make sure that data for same event goes to a single partition\n of the operator.\n\n\nConfiguration\n\n\nfileStore\n\n\nThis setting determines the format in which files are written. Default is DTFileImpl.\n\n\nbasePath\n\n\nLocation in HDFS where data files are stored. This is required configuration parameter.\n\n\nProperty File Syntax\n\n\n   \nproperty\n\n     \nname\ndt.operator.{name}.store.basePath\n/name\n\n     \nvalue\n/home/hdhtdatadir\n/value\n\n  \n/property\n\n\n\n\n\nJava API.\n\n\n/* select DTFile backend with basePath set to HDHTdata */\nTFileImpl.DTFileImpl hdsFile = new TFileImpl.DTFileImpl();\nhdsFile.setBasePath(\nHDHTdata\n);\nstore.setFileStore(hdsFile);\n\n\n\n\nmaxFileSize\n\n\nSize of each file. Default value is 134217728134217728 (i.e 128MB).\n\n\nProperty File Syntax\n\n\n \nproperty\n\n   \nname\ndt.operator.{name}.maxFileSize\n/name\n\n   \nvalue\n{value in bytes}\n/value\n\n \n/property\n\n\n\n\n\nJava API.\n\n\nstore.setMaxFileSize(64 * 1024 * 1024);\n\n\n\n\nflushSize\n\n\nHDHT will flush data to files after number of unwritten tuples crosses this limit. Default value is 1000000.\n\n\nProperty File Syntax\n\n\nproperty\n\n  \nname\ndt.operator.{name}.flushSize\n/name\n\n  \nvalue\n{number}\n/value\n\n\n/property\n\n\n\n\n\nJava API.\n\n\nstore.setFlushSize(1000000);\n\n\n\n\nflushIntervalCount\n\n\nThis setting will force data flush even if number of tuples are below \nflushSize\n. Default value is 120 windows.\n\n\nProperty File Syntax\n\n\n \nproperty\n\n   \nname\ndt.operator.{name}.flushIntervalCount\n/name\n\n   \nvalue\n{number of windows}\n/value\n\n \n/property\n\n\n\n\n\nJava API.\n\n\nstore.setFlushIntervalCount(120);\n\n\n\n\nmaxWalFileSize\n\n\nWrite Ahead Log segment size. Older segments are deleted once data is written to the data files. Default value is 67108864 (64MB)\n\n\nProperty File Syntax\n\n\nproperty\n\n  \nname\ndt.operator.{name}.maxWalFileSize\n/name\n\n  \nvalue\n{value in bytes}\n/value\n\n\n/property\n\n\n\n\n\nJava API.\n\n\nstore.setMaxWalFileSize(128 * 1024 * 1024);\n\n\n\n\nExample\n\n\nThis is a sample reference implementation, which computes how many times a word was seen in an\n application. The partial count is stored in the HDHT. The application does a lookup for\nthe previous count and writes back the incremented count in HDHT.\n\n\nStore Operator\n\n\nHDHT provides following abstract implementations\n\n \nHDHTReader\n - This class implements functionality required for \nget\n, It access HDHT\nin read-only mode.\n\n \nHDHTWriter\n - This class extends functionality of \nHDHTReader\n by adding support for \nput\n,\n  this class also maintains \nuncommitted cache\n, which can be accessed through \ngetUncommitted\n\n  method.\n* \nAbstractSinglePortHDHTWriter\n - This class extends from \nHDHTWriter\n and provides common functionality\nrequired for the operator. This class support code for operator partitioning. Also it provides an input port with a default implementation of\nstoring value received on the port to the HDHT using the coded provided.\n\n\nFor this example we will use \nAbstractSinglePortHDHTWriter\n for the store, we need to\nimplement codec which is used by \nAbstractSinglePortHDHTWriter\n for serialization and deserialization. Following is\na simple serializer which serializes key and ignores the value part, as the input to the operator is only keys.\n\n\nImplement a Codec\n\n\n  public static class StringCodec extends KryoSerializableStreamCodec\nString\n implements HDHTCodec\nString\n {\n    @Override\n    public byte[] getKeyBytes(String s)\n    {\n      return s.getBytes();\n    }\n\n    @Override\n    public byte[] getValueBytes(String s)\n    {\n      return s.getBytes();\n    }\n\n    @Override\n    public String fromKeyValue(Slice key, byte[] value)\n    {\n      return new String(key.buffer, key.offset, key.length);\n    }\n  }\n\n\n\n\nThe store operator is implemented as shown below, we will need to provide an implementation of\n\ngetCodec\n, and override \nprocessEvent\n to change default behavior of storing data in HDHT\ndirectly.\n\n\npublic class HDHTWordCounter extends AbstractSinglePortHDHTWriter\nString\n\n{\n  public transient DefaultOutputPort\nPair\nString, Long\n out = new DefaultOutputPort\n();\n  private transient HashMap\nString, AtomicLong\n cache;\n\n  @Override\n  protected HDHTCodec\nString\n getCodec()\n  {\n    return new StringCodec();\n  }\n\n  @Override\n  protected void processEvent(String word) throws IOException\n  {\n    AtomicLong count = cache.get(word);\n    if (count == null) {\n      count = new AtomicLong(0L);\n      cache.put(word, count);\n    }\n    count.incrementAndGet();\n  }\n\n  private void updateCount() throws IOException\n  {\n    for(Map.Entry\nString, AtomicLong\n entry : cache.entrySet()) {\n      String word = entry.getKey();\n      long prevCount = 0;\n      byte[] key = codec.getKeyBytes(word);\n      Slice keySlice = new Slice(key);\n      long bucketKey = getBucketKey(word);\n      /** First look for cached data */\n      byte[] value = getUncommitted(bucketKey, keySlice);\n      if (value == null) {\n        /** look into persisted data files */\n        value = get(bucketKey, keySlice);\n        if (value == null) {\n          value = ByteBuffer.allocate(8).putLong(0).array();\n        }\n      }\n\n      prevCount = ByteBuffer.wrap(value).getLong();\n\n      /** update count by taking new event into account */\n      prevCount += entry.getValue().get();\n\n      /** save computed result back to HDHT */\n      put(bucketKey, keySlice, ByteBuffer.wrap(value).putLong(prevCount).array());\n\n      /* emit updated counts on the output port */\n      out.emit(new Pair\n(word, prevCount));\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n    super.beginWindow(windowId);\n    cache = new HashMap\n();\n  }\n\n  @Override\n  public void endWindow()\n  {\n    try {\n      updateCount();\n      super.endWindow();\n    } catch (IOException e) {\n\n      throw new RuntimeException(\nUnable to flush to HDHT\n);\n    }\n  }\n}\n\n\n\n\nSample Application.\n\n\n@ApplicationAnnotation(name=\nHDHTWordCount\n)\npublic class HDHTWordCountApp implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    AbstractFileInputOperator.FileLineInputOperator gen = dag.addOperator(\nReader\n, new AbstractFileInputOperator.FileLineInputOperator());\n    gen.setDirectory(\n/home/data\n);\n\n    WordSplitter splitter = dag.addOperator(\nSplitter\n, new WordSplitter());\n\n    HDHTWordCounter store = dag.addOperator(\nStore\n, new HDHTWordCounter());\n    ConsoleOutputOperator console = dag.addOperator(\nConsole\n, new ConsoleOutputOperator());\n\n    TFileImpl.DTFileImpl hdsFile = new TFileImpl.DTFileImpl();\n    hdsFile.setBasePath(\nWALBenchMarkDir\n);\n    store.setFileStore(hdsFile);\n\n    dag.addStream(\nlines\n, gen.output, splitter.input);\n    dag.addStream(\ns1\n, splitter.output, store.input);\n    dag.addStream(\ns2\n, store.out, console.input);\n  }\n}\n\n\n\n\nPerformance tuning\n\n\nEffect of frequent WAL flushes.\n\n\nHDHT stores Write Ahead Log (WAL) on HDFS, WAL is flushed at end of every application window. Operator will be blocked till WAL is persisted on the disks. This flush will add additional delay\nto the operator. To avoid frequent delay we can reduce the frequency of flush by increasing APPLICATION_WINDOW_SIZE.\n\n\nApplication Level Cache\n\n\nMaintain a cache to avoid frequent serialization and de-serialization of events while accessing HDHT. For example in the provided example the operator keeps computed counts till the endWindow and flushes the data to HDHT at end of the application window. If duplicate keys are seen within an application window we will save on serialization and de-serialization time.\n\n\nKey design\n\n\nHDHT gives best performance if keys are monotonically increasing, In this case\nHDHT does not have to overwrite existing files, which avoids expensive disk I/O thus yielding\noptimal performance. Overwriting existing file is costly operation, as it involves reading file data\nto memory and applying new changes from committed cached which falls within the key range of file, and\nwriting back changes to disk again. If you are storing time series data in HDHT, it is best to\nuse timestamp as the leading field in the key.\n\n\nFor keys which are not monotonically increasing, key design should be such that hot\nkeys falls in small number of files. For example, suppose each file size is \nS\n bytes and flush is\ntriggered every \nT\n seconds, and HDFS write bandwidth per container is \nB\n bytes per second, in this case we can sustain the write throughput, if keys processed within 30 seconds span at most \n(T / (S/B))\n files. \n\n\nIf S, T and B are 128MB, 30 seconds, and 40MB respectively, this expression evaluates to 10, so if your keys span more than 10 files with 30 seconds, the write cannot be sustained.\n\n\nFile Backend\n\n\nPrefer \nDTFile\n backend implementation over \nTFile\n backend implementation if you are going to issue frequent \nget\n operations. DTFile backend caches file blocks\nin memory which reduces disk I/O during cache hit.\n\n\nLimitations\n\n\n\n\nDynamic Partitioning is not supported yet.\n\n\nWrite to same bucket from multiple operator is not supported. The default implementation use derives bucketKey based on number of operator partitions and hashcode of the event. If user chooses to use different bucketKey he needs to make sure that a single bucketKey is handled by only one operator partition.", 
            "title": "HDHT"
        }, 
        {
            "location": "/operators/hdht/#hdht", 
            "text": "Some applications need to compute values based not only on current event flow but also on historical data. HDHT provides a simple interface to store\nand access historical data in an operator. HDHT is an embedded state store with key value interface on top of the Hadoop file system. It is fully integrated into the Apache Apex operator model and provides persistent storage with exactly-once guarantee.  The programming model of a key-value store or hash table can be applied to a wide range of common use cases. Within most streaming applications, ingested events or computed data already carry the key that can be used for storage and retrieval. Many operations performed during computation require key based access. HDHT provides an embedded key value store for the application. The advantage of HDHT over other key value stores in streaming applications are   Embedded, Hadoop native solution. Does not require install/manage of other services.  Integrates with Apex check-pointing mechanism to provide exactly once guarantee.   This document provides overview of HDHT and instructions for using HDHT in an operator for storing\nand retrieving state of the operator.", 
            "title": "HDHT"
        }, 
        {
            "location": "/operators/hdht/#concepts", 
            "text": "", 
            "title": "Concepts"
        }, 
        {
            "location": "/operators/hdht/#write-ahead-log", 
            "text": "Each tuple written to HDHT is written to Write Ahead Log (WAL) first. The WAL is used\nto recover in-memory state and provide exactly once processing after failure of an operator. HDHT\nstores WAL on HDFS to prevent data loss during node failure.", 
            "title": "Write Ahead Log"
        }, 
        {
            "location": "/operators/hdht/#uncommitted-cache", 
            "text": "Uncommitted cache is in-memory key value store. Initially updates are written to this memory store, to avoid disk I/Os on every update. When data in Uncommitted cached reaches a configured limit, it is written on the disk. It avoids frequent data flushes and small file creation by writing data in bulk from the cache to disk thereby also improving throughput.", 
            "title": "Uncommitted Cache"
        }, 
        {
            "location": "/operators/hdht/#data-files", 
            "text": "HDHT flushes memory to persisted storage periodically. The data is kept indexed for efficient retrieval of given key. HDHT supports multiple data file backends. Default backend used is DTFile, which is a modified version of Hadoop TFile with block cache for speedy lookups.\nAvailable backends are\n-  TFile : Files are written in hadoop  Tfile  format\n-  DTFile : Files are written in TFile format; during lookup HDHT maintains a block cache to reduce disk I/Os.\n-  HFile  : Files are written in HBase format.", 
            "title": "Data Files"
        }, 
        {
            "location": "/operators/hdht/#metadata", 
            "text": "Metadata file keeps information about data files. Each data file record contains start key and name of the file. Metadata file also contains WAL recovery information, which is used during recovery after failure.", 
            "title": "Metadata"
        }, 
        {
            "location": "/operators/hdht/#partition-hdht-bucket", 
            "text": "By default, when the operator is partitioned, the partitioning is reflected by HDHT in the filesystem by using a separate directory for each operator partition. Each directory is accessed only by the associated operator partition. Each partition has its own WAL and metadata file. Each\nHDHT partition is identified by bucketKey, which is also the name of the subdirectory used for\nstoring data for the partition.", 
            "title": "Partition (HDHT Bucket)"
        }, 
        {
            "location": "/operators/hdht/#interface", 
            "text": "HDHT supports two basic operations  get  and  put , they are wrapped by interfaces HDHT.Writer, HDHT.Reader and an abstract implementation is provided by the HDHTReader and HDHTWriter classes.  Operations supported by HDHT are.  byte[] get(long bucketKey, Slice key) throws IOException;\nvoid put(long bucketKey, Slice key, byte[] value) throws IOException;\nbyte[] getUncommitted(long bucketKey, Slice key);  All methods takes bucketKey as the first argument. The bucketKey is used as a partition key within HDHT.   put  store data in HDHT. The data written is written to the WAL first and then stored in uncommitted cache.\nAfter enough dirty data is accumulated in cache or enough time has elapsed from last flush, this cache is flushed to the data files.  getUncommitted  does a lookup in uncommitted cache. Uncommitted cache is in-memory key value store.  get  does a lookup in persisted storage file and return the data.  Note   get  does not\n return data from uncommitted cache.", 
            "title": "Interface"
        }, 
        {
            "location": "/operators/hdht/#architecture", 
            "text": "Please refer to  HDHT Blog \nfor the architecture of HDHT.", 
            "title": "Architecture"
        }, 
        {
            "location": "/operators/hdht/#codec", 
            "text": "HDHT provides an abstract implementation  AbstractSinglePortHDHTWriter , which uses a user defined codec for serialization and de-serialization.  public interface HDHTCodec EVENT  extends StreamCodec EVENT \n{\n  byte[] getKeyBytes(EVENT event);\n  byte[] getValueBytes(EVENT event);\n  EVENT fromKeyValue(Slice key, byte[] value);\n}  It has following methods\n-  getKeyBytes  Return key as a byte array from the event.\n-  getValueBytes  Return value as a byte array from event.\n-  fromKeyValue  HDHT will use this function to deserialize key and value byte arrays to reconstruct the user event object.\n-  getPartition  This method is inherited from StreamCodec, its return value is used to determine HDHT bucket where this event will be written. The same stream codec is used\n for partition of the input port which make sure that data for same event goes to a single partition\n of the operator.", 
            "title": "Codec"
        }, 
        {
            "location": "/operators/hdht/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/hdht/#filestore", 
            "text": "This setting determines the format in which files are written. Default is DTFileImpl.", 
            "title": "fileStore"
        }, 
        {
            "location": "/operators/hdht/#basepath", 
            "text": "Location in HDFS where data files are stored. This is required configuration parameter.  Property File Syntax      property \n      name dt.operator.{name}.store.basePath /name \n      value /home/hdhtdatadir /value \n   /property   Java API.  /* select DTFile backend with basePath set to HDHTdata */\nTFileImpl.DTFileImpl hdsFile = new TFileImpl.DTFileImpl();\nhdsFile.setBasePath( HDHTdata );\nstore.setFileStore(hdsFile);", 
            "title": "basePath"
        }, 
        {
            "location": "/operators/hdht/#maxfilesize", 
            "text": "Size of each file. Default value is 134217728134217728 (i.e 128MB).  Property File Syntax    property \n    name dt.operator.{name}.maxFileSize /name \n    value {value in bytes} /value \n  /property   Java API.  store.setMaxFileSize(64 * 1024 * 1024);", 
            "title": "maxFileSize"
        }, 
        {
            "location": "/operators/hdht/#flushsize", 
            "text": "HDHT will flush data to files after number of unwritten tuples crosses this limit. Default value is 1000000.  Property File Syntax  property \n   name dt.operator.{name}.flushSize /name \n   value {number} /value  /property   Java API.  store.setFlushSize(1000000);", 
            "title": "flushSize"
        }, 
        {
            "location": "/operators/hdht/#flushintervalcount", 
            "text": "This setting will force data flush even if number of tuples are below  flushSize . Default value is 120 windows.  Property File Syntax    property \n    name dt.operator.{name}.flushIntervalCount /name \n    value {number of windows} /value \n  /property   Java API.  store.setFlushIntervalCount(120);", 
            "title": "flushIntervalCount"
        }, 
        {
            "location": "/operators/hdht/#maxwalfilesize", 
            "text": "Write Ahead Log segment size. Older segments are deleted once data is written to the data files. Default value is 67108864 (64MB)  Property File Syntax  property \n   name dt.operator.{name}.maxWalFileSize /name \n   value {value in bytes} /value  /property   Java API.  store.setMaxWalFileSize(128 * 1024 * 1024);", 
            "title": "maxWalFileSize"
        }, 
        {
            "location": "/operators/hdht/#example", 
            "text": "This is a sample reference implementation, which computes how many times a word was seen in an\n application. The partial count is stored in the HDHT. The application does a lookup for\nthe previous count and writes back the incremented count in HDHT.", 
            "title": "Example"
        }, 
        {
            "location": "/operators/hdht/#store-operator", 
            "text": "HDHT provides following abstract implementations   HDHTReader  - This class implements functionality required for  get , It access HDHT\nin read-only mode.   HDHTWriter  - This class extends functionality of  HDHTReader  by adding support for  put ,\n  this class also maintains  uncommitted cache , which can be accessed through  getUncommitted \n  method.\n*  AbstractSinglePortHDHTWriter  - This class extends from  HDHTWriter  and provides common functionality\nrequired for the operator. This class support code for operator partitioning. Also it provides an input port with a default implementation of\nstoring value received on the port to the HDHT using the coded provided.  For this example we will use  AbstractSinglePortHDHTWriter  for the store, we need to\nimplement codec which is used by  AbstractSinglePortHDHTWriter  for serialization and deserialization. Following is\na simple serializer which serializes key and ignores the value part, as the input to the operator is only keys.", 
            "title": "Store Operator"
        }, 
        {
            "location": "/operators/hdht/#implement-a-codec", 
            "text": "public static class StringCodec extends KryoSerializableStreamCodec String  implements HDHTCodec String  {\n    @Override\n    public byte[] getKeyBytes(String s)\n    {\n      return s.getBytes();\n    }\n\n    @Override\n    public byte[] getValueBytes(String s)\n    {\n      return s.getBytes();\n    }\n\n    @Override\n    public String fromKeyValue(Slice key, byte[] value)\n    {\n      return new String(key.buffer, key.offset, key.length);\n    }\n  }  The store operator is implemented as shown below, we will need to provide an implementation of getCodec , and override  processEvent  to change default behavior of storing data in HDHT\ndirectly.  public class HDHTWordCounter extends AbstractSinglePortHDHTWriter String \n{\n  public transient DefaultOutputPort Pair String, Long  out = new DefaultOutputPort ();\n  private transient HashMap String, AtomicLong  cache;\n\n  @Override\n  protected HDHTCodec String  getCodec()\n  {\n    return new StringCodec();\n  }\n\n  @Override\n  protected void processEvent(String word) throws IOException\n  {\n    AtomicLong count = cache.get(word);\n    if (count == null) {\n      count = new AtomicLong(0L);\n      cache.put(word, count);\n    }\n    count.incrementAndGet();\n  }\n\n  private void updateCount() throws IOException\n  {\n    for(Map.Entry String, AtomicLong  entry : cache.entrySet()) {\n      String word = entry.getKey();\n      long prevCount = 0;\n      byte[] key = codec.getKeyBytes(word);\n      Slice keySlice = new Slice(key);\n      long bucketKey = getBucketKey(word);\n      /** First look for cached data */\n      byte[] value = getUncommitted(bucketKey, keySlice);\n      if (value == null) {\n        /** look into persisted data files */\n        value = get(bucketKey, keySlice);\n        if (value == null) {\n          value = ByteBuffer.allocate(8).putLong(0).array();\n        }\n      }\n\n      prevCount = ByteBuffer.wrap(value).getLong();\n\n      /** update count by taking new event into account */\n      prevCount += entry.getValue().get();\n\n      /** save computed result back to HDHT */\n      put(bucketKey, keySlice, ByteBuffer.wrap(value).putLong(prevCount).array());\n\n      /* emit updated counts on the output port */\n      out.emit(new Pair (word, prevCount));\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n    super.beginWindow(windowId);\n    cache = new HashMap ();\n  }\n\n  @Override\n  public void endWindow()\n  {\n    try {\n      updateCount();\n      super.endWindow();\n    } catch (IOException e) {\n\n      throw new RuntimeException( Unable to flush to HDHT );\n    }\n  }\n}  Sample Application.  @ApplicationAnnotation(name= HDHTWordCount )\npublic class HDHTWordCountApp implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    AbstractFileInputOperator.FileLineInputOperator gen = dag.addOperator( Reader , new AbstractFileInputOperator.FileLineInputOperator());\n    gen.setDirectory( /home/data );\n\n    WordSplitter splitter = dag.addOperator( Splitter , new WordSplitter());\n\n    HDHTWordCounter store = dag.addOperator( Store , new HDHTWordCounter());\n    ConsoleOutputOperator console = dag.addOperator( Console , new ConsoleOutputOperator());\n\n    TFileImpl.DTFileImpl hdsFile = new TFileImpl.DTFileImpl();\n    hdsFile.setBasePath( WALBenchMarkDir );\n    store.setFileStore(hdsFile);\n\n    dag.addStream( lines , gen.output, splitter.input);\n    dag.addStream( s1 , splitter.output, store.input);\n    dag.addStream( s2 , store.out, console.input);\n  }\n}", 
            "title": "Implement a Codec"
        }, 
        {
            "location": "/operators/hdht/#performance-tuning", 
            "text": "", 
            "title": "Performance tuning"
        }, 
        {
            "location": "/operators/hdht/#effect-of-frequent-wal-flushes", 
            "text": "HDHT stores Write Ahead Log (WAL) on HDFS, WAL is flushed at end of every application window. Operator will be blocked till WAL is persisted on the disks. This flush will add additional delay\nto the operator. To avoid frequent delay we can reduce the frequency of flush by increasing APPLICATION_WINDOW_SIZE.", 
            "title": "Effect of frequent WAL flushes."
        }, 
        {
            "location": "/operators/hdht/#application-level-cache", 
            "text": "Maintain a cache to avoid frequent serialization and de-serialization of events while accessing HDHT. For example in the provided example the operator keeps computed counts till the endWindow and flushes the data to HDHT at end of the application window. If duplicate keys are seen within an application window we will save on serialization and de-serialization time.", 
            "title": "Application Level Cache"
        }, 
        {
            "location": "/operators/hdht/#key-design", 
            "text": "HDHT gives best performance if keys are monotonically increasing, In this case\nHDHT does not have to overwrite existing files, which avoids expensive disk I/O thus yielding\noptimal performance. Overwriting existing file is costly operation, as it involves reading file data\nto memory and applying new changes from committed cached which falls within the key range of file, and\nwriting back changes to disk again. If you are storing time series data in HDHT, it is best to\nuse timestamp as the leading field in the key.  For keys which are not monotonically increasing, key design should be such that hot\nkeys falls in small number of files. For example, suppose each file size is  S  bytes and flush is\ntriggered every  T  seconds, and HDFS write bandwidth per container is  B  bytes per second, in this case we can sustain the write throughput, if keys processed within 30 seconds span at most  (T / (S/B))  files.   If S, T and B are 128MB, 30 seconds, and 40MB respectively, this expression evaluates to 10, so if your keys span more than 10 files with 30 seconds, the write cannot be sustained.", 
            "title": "Key design"
        }, 
        {
            "location": "/operators/hdht/#file-backend", 
            "text": "Prefer  DTFile  backend implementation over  TFile  backend implementation if you are going to issue frequent  get  operations. DTFile backend caches file blocks\nin memory which reduces disk I/O during cache hit.", 
            "title": "File Backend"
        }, 
        {
            "location": "/operators/hdht/#limitations", 
            "text": "Dynamic Partitioning is not supported yet.  Write to same bucket from multiple operator is not supported. The default implementation use derives bucketKey based on number of operator partitions and hashcode of the event. If user chooses to use different bucketKey he needs to make sure that a single bucketKey is handled by only one operator partition.", 
            "title": "Limitations"
        }, 
        {
            "location": "/operators/deduper/", 
            "text": "Dedup Operator\n\n\nThis document is intended as a guide for understanding and using the\nDedup operator/module.\n\n\nDedup - \u201cWhat\u201d in a Nutshell\n\n\nDedup is actually a shortened form of Deduplication. Duplicates are\nomnipresent and can be found in almost any kind of data. Most of the\ntimes it is essential to discard, or at the very least separate out the\ndata into unique\u00a0and duplicate\u00a0components. The entire purpose of this\noperator is to de-duplicate data. In other words, when data passes\nthrough this operator, it will be segregated into two different data\nsets, one containing all unique tuples, and the other containing duplicates.\n\n\n\n\nDedup - \u201cHow\u201d in a Nutshell\n\n\nIn order to quickly decide whether an incoming tuple is duplicate or\nunique, it has to store each incoming tuple (or a signature, like key\nfor example) to be used for comparison later. A plain storage for such a\nhuge data is hardly scalable. Deduper employs a large scale distributed\nhashing mechanism (known as the Bucket Store) which allows it to\nidentify if a particular tuple is duplicate or unique. Each time it\nidentifies a tuple as a unique tuple, it also stores it into a\npersistent store called the Bucket Store for lookup in the future.\n\n\n\n\nFollowing are the different components of the Deduper Operator\n\n\n\n\nDedup Operator\n - This is responsible for the overall functionality\n    of the operator. This in turn makes use of other components to\n    establish the end goal of deciding whether a tuple is a duplicate of\n    some earlier tuple, or is a unique tuple.\n\n\nBucket Store\n - This is responsible for storing the unique tuples as\n    supplied by the Deduper and storing them into Buckets in HDFS.\n\n\nBucket Manager\n - Since, all of the data cannot be stored in memory,\n    this component is responsible for loading and unloading of the\n    buckets to and from the memory as requested by the Deduper.\n\n\n\n\nThis was a very small introduction to the functioning of the Deduper.\nFollowing sections will go into more detail on each of the components.\n\n\nUse case - Basic Dedup\n\n\nDedup Key\n\n\nA dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates.\n\n\nConsider an example schema and two sample tuples\n\n\n{Name, Phone, Email, Date, State, Zip, Country}\n\nTuple 1:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  ausaunders@semperegestasurna.com,\n  2015-11-09 13:38:38,\n  Texas,\n  73301,\n  United States\n}\n\nTuple 2:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  austin@semperegestasurna.com,\n  2015-11-09 13:39:38,\n  Texas,\n  73301,\n  United States\n}\n\n\n\n\nLet us assume that the Dedup Key is\n\n\n{Name, Phone}\n\n\n\n\nIn this case, the two\ntuples are duplicates because the key fields are same in both the\ntuples. However, if we plan to make the Dedup Key as\n\n\n{Phone, Email}\n\n\n\n\nthen in this case, the two are unique tuples as the keys of both tuples\ndo not match.\n\n\nUse case Details\n\n\nConsider the case of de-duplicating a master data set which is stored in\na file. Further also consider the following schema for tuples in the\ndata set.\n\n\n{Name, Phone, Email, Date, City, Zip, Country}\n\n\n\n\nAlso consider that we need to identify unique customers from the master\ndata set. So, ultimately the output needed for the use case is two data\nsets - Unique Records\u00a0and Duplicate Records.\n\n\nAs part of configuring the operator for this use case, we need to set\nthe following parameters:\n\n\nDedup Key - This can be set as the primary key which can be used to uniquely identify a Customer. For example, we can set it to\n\n\n{Name,Email}\n\n\n\n\nThe above configuration is sufficient to resolve the use case.\n\n\n\n\nUse case - Dedup with Expiry\n\n\nMotivation\n\n\nThe Basic Dedup use case is the most straightforward and is usually\napplied when the amount of data to be processed is not huge. However, if\nthe incoming data is huge, or even never-ending, it is usually not\nnecessary to keep storing all the data. This is because in most real\nworld use cases, the duplicates occur only a short distance apart.\nHence, after a while, it is usually okay to forget the part of the\nhistory and consider only limited history for identifying duplicates, in\nthe interest of efficiency. In other words, we expire some tuples which\nare (or were supposed to be) delivered long back. Doing so, reduces the\nload on the Bucket Store which effectively deletes part of the history,\nthus making the whole process more efficient. We call this use case,\nDedup with expiry.\n\n\nExpiry Key\n\n\nThe easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a natural expiry key and is in line\nwith the concept of expiry. Formally, an expiry field is a field in the\ninput tuple which can be used to discard incoming tuples as expired.\nThis expiry key usually works with another parameter called Expiry\nPeriod defined next.\n\n\nExpiry Period\n\n\nThe expiry period is the value supplied by the user to define the extent\nof history which should be considered while expiring tuples.\n\n\nUse case Details\n\n\nConsider an incoming stream of system logs. The use case requires us to\nidentify duplicate log messages and pass on only the unique ones.\nAnother relaxation in the use case is that the log messages which are\nolder than a day, may not be considered and must be filtered out as\nexpired. The expiry must be measured with respect to the time stamp in\nthe logs. For example, if the timestamp in the incoming message is\n\u201c30-12-2014 00:00:00\u201d and the latest message that the system has\nencountered had the time stamp \u201c31-12-2014 00:00:00\u201d, then the incoming\nmessage must be considered as expired. However, if the incoming message\nhad any timestamp like \u201c30-12-2014 00:11:00\u201d, it must be accepted into\nthe system and check for a possible duplicate.\n\n\nThe expiry facet in the use case above gives us an advantage in that we\ndo not have to compare the incoming record with all the data to check if\nit is a duplicate. At the same time, all the data need not be stored.\nJust a day worth of data needs to be stored in order to address the\nabove use case.\n\n\nConfiguring the below parameters will solve the problem for this use\ncase:\n\n\n\n\nDedup Key\n - This is the dedup key for the incoming tuples (similar\n    to the Basic Dedup use case). This can be any key which can uniquely\n    identify a record. For log messages this can be a serial number\n    attached in the log.\n\n\nExpiry Key\n - This is the key which can help identify the expired\n    records, as explained above. In this particular use case, it can be\n    a timestamp field which indicates when the log message was\n    generated.\n\n\nExpiry Period\n - This is the period of expiry as explained above. In\n    our particular use case this will be 24 hours.\n\n\n\n\nConfiguration of these parameters would resolve this use case.\n\n\nUse cases - Summary\n\n\n\n\nBasic Dedup\n - Deduplication of bounded datasets. Data is assumed to be bounded. This use case is not meant for never ending streams of data. For example: Deduplication of master data like customer records, product catalogs etc.\n\n\nDedup with Expiry\n - Deduplication of unlimited streams of data. This use case handles unbounded streams of data and can run forever. An expiry key and criterion is expected as part of the input which helps avoid storing all the unique data. This helps speed up performance. Following expiry keys are supported:\n\n\nTime based\n - Timestamp fields, system date, creation date, load date etc. are examples of the fields that can be used as a time based expiry key. Additionally an option can be provided so as to maintain time with respect to System time, or Tuple time\n\n\nWith respect to system time\n - Time progresses with system time. Any expiry criterions are executed with this notion of system time.\n\n\nWith respect to tuple time\n - Time progresses based on the time in the incoming tuples. Expiry criterions are executed with the notion of time indicated by the incoming tuple.\n\n\nAny Ordered Key\n - Similar to time, any non-time field can also be used as an expiry key, provided the key is also ordered (analogous to the time field). Examples include Transaction ids, Sequence Ids etc. The expiry criterion must also be in the domain of the key.\n\n\nCategorical Key\n - Any categorical key can be used for expiry, provided that data is grouped by the key. Examples include City name, Circle Id etc. In case of City name, for example, the records tend to appear for City 1 first, followed by City 2, then City 3 and so on. Any out of order cities may be considered as expired based on the configuration of the expiry criterion.\n\n\n\n\n\n\nTechnical Architecture\n\n\nBlock Diagram\n\n\n\n\nThe deduper has a single input port and multiple output ports.\n\n\n\n\ninput\n - This is the input port through which the tuples arrive at\n    the Deduper.\n\n\nunique\n\u00a0- This is the output port on which unique tuples are sent out\n    by the Deduper.\n\n\nduplicate\n\u00a0- This is the output port on which duplicate tuples are\n    sent out by the Deduper.\n\n\nexpired\n\u00a0- This is the output port on which expired tuples are sent\n    out by the Deduper.\n\n\nerror\n\u00a0- This is the output port on which the error tuples are sent\n    out by the Deduper.\n\n\n\n\n\n\nConcepts\n\n\nDedup Key\n\n\nA dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates. If\nDedup Key of two tuples match, then they are duplicates, else they are\nunique.\n\n\nExpiry Key\n\n\nA tuple may or may not have an Expiry Key. Dedup operator cannot keep\nstoring all the data that is flowing into the operator. At some point it\nbecomes essential to discard some of the historical tuples in interest\nof memory and efficiency.\n\n\nAt the same time, tuples are expected to arrive at the Dedup operator\nwithin some time after they are generated. After this time, the tuples\nmay be considered as stale or obsolete.\n\n\nIn such cases, the Deduper chooses to consider these tuples expired\u00a0and\ntakes no action but to separate out these tuples on a different port in\norder to be processed by some other operator or offline analysis.\n\n\nIn order to create a criterion for discarding such tuples, we introduce\nan Expiry Key. Looking at the value of the Expiry Key in each tuple, we\ncan decide whether or not to discard this tuple as expired.\n\n\nThe easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a very good and general example of\nan expiry key and is in line with the concept of expiry. Formally, an\nexpiry field is a field in the input tuple which can be used to discard\nincoming tuples as expired. There are some criteria for a field to be\nconsidered an Expiry Field. At-least one\u00a0of the following must hold for\nan Expiry Key.\n\n\n\n\nThe domain of the key must be ordered. Example - Timestamp field\n\n\nThe domain of the key must be categorical and sorted. Example - City\n    names grouped together\n\n\n\n\nThis expiry key usually works with another parameter called Expiry\nPeriod defined next.\n\n\nExpiry Period\n\n\nThe Expiry Period is the value supplied by the user which decides when a\nparticular tuple expires.\n\n\nTime Points\n\n\nFor every dataset that the deduper processes, it maintains a set of time\npoints:\n\n\n\n\nLatest Point\n\u00a0- This is the maximum time point observed in all the\n    processed tuples.\n\n\nExpiry Point\n\u00a0- This is given by: \nExpiry Point = Latest Point -\n    Expiry Period\n\n\n\n\nThese points help the deduper to make decisions related to expiry of a\ntuple.\n\n\nExample - Expiry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTuple Id\n\n\nExpiry Key\n\n\n(Expiry Period = 10)\n\n\nLatest Point\n\n\nExpiry Point\n\n\nDecision for Tuple\n\n\n\n\n\n\n1\n\n\n10\n\n\n10\n\n\n1\n\n\nNot Expired\n\n\n\n\n\n\n2\n\n\n20\n\n\n20\n\n\n11\n\n\nNot Expired\n\n\n\n\n\n\n3\n\n\n25\n\n\n25\n\n\n16\n\n\nNot Expired\n\n\n\n\n\n\n4\n\n\n40\n\n\n40\n\n\n31\n\n\nNot Expired\n\n\n\n\n\n\n5\n\n\n21\n\n\n40\n\n\n31\n\n\nExpired\n\n\n\n\n\n\n6\n\n\n35\n\n\n40\n\n\n31\n\n\nNot Expired\n\n\n\n\n\n\n7\n\n\n45\n\n\n45\n\n\n36\n\n\nNot Expired\n\n\n\n\n\n\n8\n\n\n57\n\n\n57\n\n\n48\n\n\nNot Expired\n\n\n\n\n\n\n\n\n\nBuckets\n\n\nOne of the requirements of the Deduper is to store all the unique tuples\n(actually, just the dedup keys of tuples). Keeping an ever growing cache\nin memory is not scalable. So what we need was a limited cache backed by\na persistent store. Now to reduce cache misses, we load a chunk of data\n(called Buckets), together into memory. Buckets help narrow down the\nsearch of duplicates for incoming tuples. A Bucket is an abstraction for\na collection of tuples all of which share a common hash value based on\nsome hash function. A bucket is identified using a Bucket Key, defined\nbelow. A Bucket\u00a0has a span called Bucket Span.\n\n\nBucket Span\n\n\nBucket span is simply the range of the domain that is covered by the\nBucket. This span is specified in the domain of the Expiry key. If the\nExpiry Key is time, \u00a0then the Bucket span will be specified in seconds.\nIt is only defined in case tuples have an Expiry Key.\n\n\nBucket Key\n\n\nBucket Key acts as the identifier for a Bucket. It is derived using the\nDedup Key or Expiry Key of the tuple along with the Bucket Span.\n\n\nWe define Bucket Key differently in case of Basic Dedup\u00a0and Dedup with\nExpiry.\n\n\nIn case of Basic Dedup:\n\n\nBucket Key = Hash(Dedup Key) % Number of Buckets\n\n\n\n\nIn case of Dedup with Expiry:\n\n\nBucket Key = Expiry Key / Bucket Span\n\n\n\n\nNumber of Buckets\n\n\nThe number of buckets can be given by\n\n\nNum Buckets = Expiry Period / Bucket Span.\n\n\n\n\nThis is because at any point of time, we need only store Expiry Period\nworth of data. As soon as we get a new tuple, we can forget about the\nleast recent tuple in our store, since this tuple will be expired due to\nthe most recent tuple.\n\n\nBucket Index\n\n\nA Bucket Index iterates over number of buckets. In contrast to Bucket\nKey, which continuously keeps on increasing, Bucket Index will loop\naround to 0, once it has reached value (Number of Buckets - 1).\n\n\nExample - Buckets\n\n\n\n\nAssumptions\n\n\nAssumption 1\n\n\nThis assumption is only applicable in case of Dedup with Expiry.\n\n\nFor any two tuples, t1 and t2 having dedup keys d1 and d2, and expiry\nkeys e1 and e2, respectively, the following holds:\nIf d1 = d2, then e1 = e2\n\n\n\n\nIn other words, there may never be\u00a0two tuples t1 and t2 such that\n\n\nTuple 1: d1, e1\nTuple 2: d2, e2\nd1 = d2 and e1 != e2\n\n\n\n\nThis assumption was made with respect to certain use cases. These use\ncases follow this assumption in that the records which are duplicates\nare exactly identical. An example use case is when log messages are\nreplayed erroneously, and we want to identify the duplicate log\nmessages. In such cases, we need not worry about two different log\nmessages having the same identifier but different timestamps. Since its\na replay of the same data, the duplicate records are assumed to be\nexactly identical.\n\n\nThe reason for making this assumption was to simplify and architect the\noperator to suit only such use cases. The backend architecture for use\ncases where this assumption does not hold, is very different from the\none where this assumption holds. Hence handling the generic case could\nhave been much more complicated and inefficient.\n\n\nFlow of a Tuple through Dedup Operator\n\n\nTuples flow through the Dedup operator one by one. Deduper may choose to\nprocess a tuple immediately, or store it in some data structure for\nlater processing. We break down the processing of the Deduper by various\nstages as follows.\n\n\nDeduper View\n\n\nA tuple always arrives at the input port\u00a0of the Dedup operator. Once\narrived, the Deduper does the following tasks.\n\n\nIdentify Bucket Key\n\n\nIdentify the Bucket Key\u00a0of the tuple. Bucket key identifies the Bucket\nto which this tuple belongs. In case of the basic dedup use case, the\nBucket Key will be calculated as follows:\n\n\nBucket Key = Hash(Dedup Key) % Number of Buckets\n\n\n\n\nIn case of Dedup with expiry, we calculate the Bucket key as\n\n\nBucket Key = Expiry Key / Bucket Span\n\n\n\n\nCheck if tuple is Expired\n\n\nThis is only applicable in case of Dedup with expiry. The following\ncondition can be used to check if the tuple is expired.\n\n\nif ( Latest Point - Expiry Key \n Expiry Point ) then Expired\n\n\n\n\nIf the tuple is expired, then put it out to the expired port.\n\n\nCheck if tuple is a Duplicate or Unique\n\n\nOnce a tuple passes the check of expiry, we proceed to check if the\ntuple already has a duplicate tuple which is not expired. Note that if\nthe tuple in question is not expired, the duplicate will also not have\nexpired due to the assumption listed \nhere\n.\n\n\nDuplicates of the tuple being processed, if any, will be available only\nin the bucket identified by the Bucket Key identified in the \nfirst\nstep\n. The amount of physical memory available with the\nDedup operator may not be sufficient to hold all the buckets in memory.\nHence at any given point in time, only a configured maximum number of\nbuckets can be kept in memory. The Deduper follows different paths\ndepending on the availability of the required bucket in memory.\n\n\nCase I - Bucket available in memory\n\n\nIn case the bucket with key Bucket Key\u00a0is available in memory, the\nDeduper simply checks if there is a tuple in the bucket with the same\nDedup Key as the one currently being processed. If, so, then the tuple\nbeing processed is deemed to be a duplicate and put out on the\nDuplicate\u00a0port. If not, then the tuple being processed is deemed to be\nunique and put out on the Unique\u00a0port. If the tuple is unique,\nadditionally it is also added to the bucket for future references.\n\n\nCase II - Bucket not in memory\n\n\nIn case the bucket with key Bucket Key\u00a0is not available in memory, the\nDeduper requests the Bucket Manager to load the bucket with key Bucket\nKey\u00a0from the Bucket Store. This request is processed by the Bucket\nManager in a separate asynchronous thread as detailed\n\nhere\n. Additionally the Deduper also inserts the tuple\nbeing processed into a waiting events\u00a0queue for later processing. After\nthis, the Deduper cannot proceed until the bucket is loaded by the\nBucket Manager and hence proceeds to process another tuple. Next section\ndetails the process after the bucket is loaded by the Bucket Manager.\n\n\nHandling tuples after Buckets are loaded\n\n\nThe Bucket Manager would load all the buckets requested by the Deduper\nand add them to a fetched buckets\u00a0queue. During the span of one\napplication window of the Deduper, it will process all the tuples on its\ninput port. Processing here could mean one of the below:\n\n\n\n\nThe bucket for a tuple was already available in memory and hence the\n    deduper could conclude whether a tuple is a duplicate or unique.\n\n\nThe bucket for a tuple was not available in memory and a request was\n    made to the Bucket Manager for asynchronously loading that\n    particular bucket in memory.\n\n\n\n\nAfter processing all the tuples as above, the Deduper starts processing\nthe tuples in the waiting events\u00a0queue as mentioned in section \nCase II\n- Bucket not in memory\n. For each of these waiting\ntuples, a corresponding bucket is loaded by the Bucket Manager in the\nfetched buckets\u00a0queue. Using these fetched buckets, the Deduper can\nresolve the pending tuples as duplicate or unique. This is done in the\nsame way as for \nBuckets available in memory\n.\n\n\nBucket Manager\n\n\nBucket manager is responsible for loading and unloading of buckets to\nand from memory. Bucket manager maintains a requested buckets\u00a0queue\nwhich holds the requests (in form of bucket keys) from the Deduper,\nindicating which buckets need to be loaded from the Bucket Store. The\nrequests are processed by the Bucket Manager one by one. The first step\nis to identify the Bucket Index for bucket key.\n\n\nIdentify Bucket Index\n\n\nBucket index is discussed \nhere\n. Bucket index can be\ncalculated as follows:\n\n\nBucket Index = Requested Bucket Key % Number of Buckets,\n\n\n\n\nwhere Number of Buckets is as defined \nhere\n.\n\n\nRequest Bucket Load from Store\n\n\nOnce we have the Bucket Index, the Bucket Store is requested to fetch\nthe corresponding bucket \u00a0and load it into memory. This is a blocking\ncall and the Bucket Manager waits while the Bucket Store fetches the\ndata from the store. Once the data is available, the Bucket Manager\nbundles the data into a bucket and adds it to the fetched buckets\u00a0queue\nmentioned \nhere\n. We detail the process of fetching the\nbucket data from the store in \nthis\n\u00a0section.\n\n\nBucket Eviction\n\n\nIt may not be efficient or even possible in some cases to keep all the\nbuckets into memory. This is the reason the buckets are persisted to the\nHDFS store every window. This makes it essential to off load some of the\nbuckets from memory so that new buckets can be loaded. The policy\nfollowed by the Bucket Manager is the least recently used policy.\nWhenever the Bucket Manager needs to load a particular bucket into\nmemory, it identifies a bucket in memory which has been accessed least\nrecently and unloads it from memory. No other processing has to be done\nin this case. Upon unloading, it informs the listeners (the Deduper\nthread) that the particular bucket has been off loaded from memory and\nis no longer available.\n\n\nBucket Store\n\n\nThe Bucket Store is responsible for fetching and storing the data from a\npersistent store. In this case, HDFS is used as the persistent store and\nHDFSBucketStore is responsible for interacting with the store.\n\n\nData Format\n\n\nBucket store persists the buckets onto the HDFS. This is typically done\nevery window, although this can be configured to be done every\ncheckpoint window. This data is stored as files on HDFS. Every write\ni.e. new unique records generated per window (or per checkpoint window)\nis written into a new file on HDFS. The format of the file is given\nbelow.\n\n\n\n\nAll the unique records (actually, just keys, since dedup requires just\nstorage of keys) that are received in a window, are collected in a set\nof buckets and written one after the other serially in a file on HDFS.\nThis data is indexed in case it needs to be read back. Index structures\nare described in the \nData Structures\n\u00a0section.\n\n\nData Structures\n\n\nHDFS Bucket Store keeps the information about the buckets and their\nlocations in various data structures.\n\n\n\n\nBucket Positions\n - This data structure stores for each bucket index,\n    the files and the offset within those files where the data for the\n    bucket is stored. Since the data for a single bucket may be spread\n    across multiple files, the number of files and their offsets may be\n    multiple.\n\n\nWindow To Buckets\n - This data structure keeps track of what buckets\n    were modified in which window. This is essentially a multi map of\n    window id to the set of bucket indexes that were written in that\n    window.\n\n\nWindow to Timestamp\n - This data structure keeps track of the maximum\n    timestamp within a particular window file. This gives an indication\n    as to how old is the data in the window file and helps in\n    identifying window files that can be deleted entirely.\n\n\n\n\nBucket Fetch\n\n\nFor fetching a particular bucket, a bucket index is passed to the HDFS\nbucket store. Using the bucket index, a list of window files is\nidentified which contains the data for this index. The bucket positions\ndata structure is used for this purpose. For each such window file, the\nBucket Store forks off a thread to fetch that particular window file\nfrom HDFS. Effectively all the window files which contain data for a\nparticular bucket index, are fetched in parallel from the HDFS. Once\nfetched, the data is bundled together and returned to the calling\nfunction i.e. the Bucket Manager.", 
            "title": "Deduper"
        }, 
        {
            "location": "/operators/deduper/#dedup-operator", 
            "text": "This document is intended as a guide for understanding and using the\nDedup operator/module.", 
            "title": "Dedup Operator"
        }, 
        {
            "location": "/operators/deduper/#dedup-what-in-a-nutshell", 
            "text": "Dedup is actually a shortened form of Deduplication. Duplicates are\nomnipresent and can be found in almost any kind of data. Most of the\ntimes it is essential to discard, or at the very least separate out the\ndata into unique\u00a0and duplicate\u00a0components. The entire purpose of this\noperator is to de-duplicate data. In other words, when data passes\nthrough this operator, it will be segregated into two different data\nsets, one containing all unique tuples, and the other containing duplicates.", 
            "title": "Dedup - \u201cWhat\u201d in a Nutshell"
        }, 
        {
            "location": "/operators/deduper/#dedup-how-in-a-nutshell", 
            "text": "In order to quickly decide whether an incoming tuple is duplicate or\nunique, it has to store each incoming tuple (or a signature, like key\nfor example) to be used for comparison later. A plain storage for such a\nhuge data is hardly scalable. Deduper employs a large scale distributed\nhashing mechanism (known as the Bucket Store) which allows it to\nidentify if a particular tuple is duplicate or unique. Each time it\nidentifies a tuple as a unique tuple, it also stores it into a\npersistent store called the Bucket Store for lookup in the future.", 
            "title": "Dedup - \u201cHow\u201d in a Nutshell"
        }, 
        {
            "location": "/operators/deduper/#use-case-basic-dedup", 
            "text": "", 
            "title": "Use case - Basic Dedup"
        }, 
        {
            "location": "/operators/deduper/#dedup-key", 
            "text": "A dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates.  Consider an example schema and two sample tuples  {Name, Phone, Email, Date, State, Zip, Country}\n\nTuple 1:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  ausaunders@semperegestasurna.com,\n  2015-11-09 13:38:38,\n  Texas,\n  73301,\n  United States\n}\n\nTuple 2:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  austin@semperegestasurna.com,\n  2015-11-09 13:39:38,\n  Texas,\n  73301,\n  United States\n}  Let us assume that the Dedup Key is  {Name, Phone}  In this case, the two\ntuples are duplicates because the key fields are same in both the\ntuples. However, if we plan to make the Dedup Key as  {Phone, Email}  then in this case, the two are unique tuples as the keys of both tuples\ndo not match.", 
            "title": "Dedup Key"
        }, 
        {
            "location": "/operators/deduper/#use-case-details", 
            "text": "Consider the case of de-duplicating a master data set which is stored in\na file. Further also consider the following schema for tuples in the\ndata set.  {Name, Phone, Email, Date, City, Zip, Country}  Also consider that we need to identify unique customers from the master\ndata set. So, ultimately the output needed for the use case is two data\nsets - Unique Records\u00a0and Duplicate Records.  As part of configuring the operator for this use case, we need to set\nthe following parameters:  Dedup Key - This can be set as the primary key which can be used to uniquely identify a Customer. For example, we can set it to  {Name,Email}  The above configuration is sufficient to resolve the use case.", 
            "title": "Use case Details"
        }, 
        {
            "location": "/operators/deduper/#use-case-dedup-with-expiry", 
            "text": "", 
            "title": "Use case - Dedup with Expiry"
        }, 
        {
            "location": "/operators/deduper/#motivation", 
            "text": "The Basic Dedup use case is the most straightforward and is usually\napplied when the amount of data to be processed is not huge. However, if\nthe incoming data is huge, or even never-ending, it is usually not\nnecessary to keep storing all the data. This is because in most real\nworld use cases, the duplicates occur only a short distance apart.\nHence, after a while, it is usually okay to forget the part of the\nhistory and consider only limited history for identifying duplicates, in\nthe interest of efficiency. In other words, we expire some tuples which\nare (or were supposed to be) delivered long back. Doing so, reduces the\nload on the Bucket Store which effectively deletes part of the history,\nthus making the whole process more efficient. We call this use case,\nDedup with expiry.", 
            "title": "Motivation"
        }, 
        {
            "location": "/operators/deduper/#expiry-key", 
            "text": "The easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a natural expiry key and is in line\nwith the concept of expiry. Formally, an expiry field is a field in the\ninput tuple which can be used to discard incoming tuples as expired.\nThis expiry key usually works with another parameter called Expiry\nPeriod defined next.", 
            "title": "Expiry Key"
        }, 
        {
            "location": "/operators/deduper/#expiry-period", 
            "text": "The expiry period is the value supplied by the user to define the extent\nof history which should be considered while expiring tuples.", 
            "title": "Expiry Period"
        }, 
        {
            "location": "/operators/deduper/#use-case-details_1", 
            "text": "Consider an incoming stream of system logs. The use case requires us to\nidentify duplicate log messages and pass on only the unique ones.\nAnother relaxation in the use case is that the log messages which are\nolder than a day, may not be considered and must be filtered out as\nexpired. The expiry must be measured with respect to the time stamp in\nthe logs. For example, if the timestamp in the incoming message is\n\u201c30-12-2014 00:00:00\u201d and the latest message that the system has\nencountered had the time stamp \u201c31-12-2014 00:00:00\u201d, then the incoming\nmessage must be considered as expired. However, if the incoming message\nhad any timestamp like \u201c30-12-2014 00:11:00\u201d, it must be accepted into\nthe system and check for a possible duplicate.  The expiry facet in the use case above gives us an advantage in that we\ndo not have to compare the incoming record with all the data to check if\nit is a duplicate. At the same time, all the data need not be stored.\nJust a day worth of data needs to be stored in order to address the\nabove use case.  Configuring the below parameters will solve the problem for this use\ncase:   Dedup Key  - This is the dedup key for the incoming tuples (similar\n    to the Basic Dedup use case). This can be any key which can uniquely\n    identify a record. For log messages this can be a serial number\n    attached in the log.  Expiry Key  - This is the key which can help identify the expired\n    records, as explained above. In this particular use case, it can be\n    a timestamp field which indicates when the log message was\n    generated.  Expiry Period  - This is the period of expiry as explained above. In\n    our particular use case this will be 24 hours.   Configuration of these parameters would resolve this use case.", 
            "title": "Use case Details"
        }, 
        {
            "location": "/operators/deduper/#use-cases-summary", 
            "text": "Basic Dedup  - Deduplication of bounded datasets. Data is assumed to be bounded. This use case is not meant for never ending streams of data. For example: Deduplication of master data like customer records, product catalogs etc.  Dedup with Expiry  - Deduplication of unlimited streams of data. This use case handles unbounded streams of data and can run forever. An expiry key and criterion is expected as part of the input which helps avoid storing all the unique data. This helps speed up performance. Following expiry keys are supported:  Time based  - Timestamp fields, system date, creation date, load date etc. are examples of the fields that can be used as a time based expiry key. Additionally an option can be provided so as to maintain time with respect to System time, or Tuple time  With respect to system time  - Time progresses with system time. Any expiry criterions are executed with this notion of system time.  With respect to tuple time  - Time progresses based on the time in the incoming tuples. Expiry criterions are executed with the notion of time indicated by the incoming tuple.  Any Ordered Key  - Similar to time, any non-time field can also be used as an expiry key, provided the key is also ordered (analogous to the time field). Examples include Transaction ids, Sequence Ids etc. The expiry criterion must also be in the domain of the key.  Categorical Key  - Any categorical key can be used for expiry, provided that data is grouped by the key. Examples include City name, Circle Id etc. In case of City name, for example, the records tend to appear for City 1 first, followed by City 2, then City 3 and so on. Any out of order cities may be considered as expired based on the configuration of the expiry criterion.", 
            "title": "Use cases - Summary"
        }, 
        {
            "location": "/operators/deduper/#technical-architecture", 
            "text": "", 
            "title": "Technical Architecture"
        }, 
        {
            "location": "/operators/deduper/#block-diagram", 
            "text": "", 
            "title": "Block Diagram"
        }, 
        {
            "location": "/operators/deduper/#concepts", 
            "text": "", 
            "title": "Concepts"
        }, 
        {
            "location": "/operators/deduper/#dedup-key_1", 
            "text": "A dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates. If\nDedup Key of two tuples match, then they are duplicates, else they are\nunique.", 
            "title": "Dedup Key"
        }, 
        {
            "location": "/operators/deduper/#expiry-key_1", 
            "text": "A tuple may or may not have an Expiry Key. Dedup operator cannot keep\nstoring all the data that is flowing into the operator. At some point it\nbecomes essential to discard some of the historical tuples in interest\nof memory and efficiency.  At the same time, tuples are expected to arrive at the Dedup operator\nwithin some time after they are generated. After this time, the tuples\nmay be considered as stale or obsolete.  In such cases, the Deduper chooses to consider these tuples expired\u00a0and\ntakes no action but to separate out these tuples on a different port in\norder to be processed by some other operator or offline analysis.  In order to create a criterion for discarding such tuples, we introduce\nan Expiry Key. Looking at the value of the Expiry Key in each tuple, we\ncan decide whether or not to discard this tuple as expired.  The easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a very good and general example of\nan expiry key and is in line with the concept of expiry. Formally, an\nexpiry field is a field in the input tuple which can be used to discard\nincoming tuples as expired. There are some criteria for a field to be\nconsidered an Expiry Field. At-least one\u00a0of the following must hold for\nan Expiry Key.   The domain of the key must be ordered. Example - Timestamp field  The domain of the key must be categorical and sorted. Example - City\n    names grouped together   This expiry key usually works with another parameter called Expiry\nPeriod defined next.", 
            "title": "Expiry Key"
        }, 
        {
            "location": "/operators/deduper/#expiry-period_1", 
            "text": "The Expiry Period is the value supplied by the user which decides when a\nparticular tuple expires.", 
            "title": "Expiry Period"
        }, 
        {
            "location": "/operators/deduper/#time-points", 
            "text": "For every dataset that the deduper processes, it maintains a set of time\npoints:   Latest Point \u00a0- This is the maximum time point observed in all the\n    processed tuples.  Expiry Point \u00a0- This is given by:  Expiry Point = Latest Point -\n    Expiry Period   These points help the deduper to make decisions related to expiry of a\ntuple.", 
            "title": "Time Points"
        }, 
        {
            "location": "/operators/deduper/#example-expiry", 
            "text": "Tuple Id  Expiry Key  (Expiry Period = 10)  Latest Point  Expiry Point  Decision for Tuple    1  10  10  1  Not Expired    2  20  20  11  Not Expired    3  25  25  16  Not Expired    4  40  40  31  Not Expired    5  21  40  31  Expired    6  35  40  31  Not Expired    7  45  45  36  Not Expired    8  57  57  48  Not Expired", 
            "title": "Example - Expiry"
        }, 
        {
            "location": "/operators/deduper/#buckets", 
            "text": "One of the requirements of the Deduper is to store all the unique tuples\n(actually, just the dedup keys of tuples). Keeping an ever growing cache\nin memory is not scalable. So what we need was a limited cache backed by\na persistent store. Now to reduce cache misses, we load a chunk of data\n(called Buckets), together into memory. Buckets help narrow down the\nsearch of duplicates for incoming tuples. A Bucket is an abstraction for\na collection of tuples all of which share a common hash value based on\nsome hash function. A bucket is identified using a Bucket Key, defined\nbelow. A Bucket\u00a0has a span called Bucket Span.", 
            "title": "Buckets"
        }, 
        {
            "location": "/operators/deduper/#bucket-span", 
            "text": "Bucket span is simply the range of the domain that is covered by the\nBucket. This span is specified in the domain of the Expiry key. If the\nExpiry Key is time, \u00a0then the Bucket span will be specified in seconds.\nIt is only defined in case tuples have an Expiry Key.", 
            "title": "Bucket Span"
        }, 
        {
            "location": "/operators/deduper/#bucket-key", 
            "text": "Bucket Key acts as the identifier for a Bucket. It is derived using the\nDedup Key or Expiry Key of the tuple along with the Bucket Span.  We define Bucket Key differently in case of Basic Dedup\u00a0and Dedup with\nExpiry.  In case of Basic Dedup:  Bucket Key = Hash(Dedup Key) % Number of Buckets  In case of Dedup with Expiry:  Bucket Key = Expiry Key / Bucket Span", 
            "title": "Bucket Key"
        }, 
        {
            "location": "/operators/deduper/#number-of-buckets", 
            "text": "The number of buckets can be given by  Num Buckets = Expiry Period / Bucket Span.  This is because at any point of time, we need only store Expiry Period\nworth of data. As soon as we get a new tuple, we can forget about the\nleast recent tuple in our store, since this tuple will be expired due to\nthe most recent tuple.", 
            "title": "Number of Buckets"
        }, 
        {
            "location": "/operators/deduper/#bucket-index", 
            "text": "A Bucket Index iterates over number of buckets. In contrast to Bucket\nKey, which continuously keeps on increasing, Bucket Index will loop\naround to 0, once it has reached value (Number of Buckets - 1).", 
            "title": "Bucket Index"
        }, 
        {
            "location": "/operators/deduper/#example-buckets", 
            "text": "", 
            "title": "Example - Buckets"
        }, 
        {
            "location": "/operators/deduper/#assumptions", 
            "text": "", 
            "title": "Assumptions"
        }, 
        {
            "location": "/operators/deduper/#assumption-1", 
            "text": "This assumption is only applicable in case of Dedup with Expiry.  For any two tuples, t1 and t2 having dedup keys d1 and d2, and expiry\nkeys e1 and e2, respectively, the following holds:\nIf d1 = d2, then e1 = e2  In other words, there may never be\u00a0two tuples t1 and t2 such that  Tuple 1: d1, e1\nTuple 2: d2, e2\nd1 = d2 and e1 != e2  This assumption was made with respect to certain use cases. These use\ncases follow this assumption in that the records which are duplicates\nare exactly identical. An example use case is when log messages are\nreplayed erroneously, and we want to identify the duplicate log\nmessages. In such cases, we need not worry about two different log\nmessages having the same identifier but different timestamps. Since its\na replay of the same data, the duplicate records are assumed to be\nexactly identical.  The reason for making this assumption was to simplify and architect the\noperator to suit only such use cases. The backend architecture for use\ncases where this assumption does not hold, is very different from the\none where this assumption holds. Hence handling the generic case could\nhave been much more complicated and inefficient.", 
            "title": "Assumption 1"
        }, 
        {
            "location": "/operators/deduper/#flow-of-a-tuple-through-dedup-operator", 
            "text": "Tuples flow through the Dedup operator one by one. Deduper may choose to\nprocess a tuple immediately, or store it in some data structure for\nlater processing. We break down the processing of the Deduper by various\nstages as follows.", 
            "title": "Flow of a Tuple through Dedup Operator"
        }, 
        {
            "location": "/operators/deduper/#deduper-view", 
            "text": "A tuple always arrives at the input port\u00a0of the Dedup operator. Once\narrived, the Deduper does the following tasks.", 
            "title": "Deduper View"
        }, 
        {
            "location": "/operators/deduper/#identify-bucket-key", 
            "text": "Identify the Bucket Key\u00a0of the tuple. Bucket key identifies the Bucket\nto which this tuple belongs. In case of the basic dedup use case, the\nBucket Key will be calculated as follows:  Bucket Key = Hash(Dedup Key) % Number of Buckets  In case of Dedup with expiry, we calculate the Bucket key as  Bucket Key = Expiry Key / Bucket Span", 
            "title": "Identify Bucket Key"
        }, 
        {
            "location": "/operators/deduper/#check-if-tuple-is-expired", 
            "text": "This is only applicable in case of Dedup with expiry. The following\ncondition can be used to check if the tuple is expired.  if ( Latest Point - Expiry Key   Expiry Point ) then Expired  If the tuple is expired, then put it out to the expired port.", 
            "title": "Check if tuple is Expired"
        }, 
        {
            "location": "/operators/deduper/#check-if-tuple-is-a-duplicate-or-unique", 
            "text": "Once a tuple passes the check of expiry, we proceed to check if the\ntuple already has a duplicate tuple which is not expired. Note that if\nthe tuple in question is not expired, the duplicate will also not have\nexpired due to the assumption listed  here .  Duplicates of the tuple being processed, if any, will be available only\nin the bucket identified by the Bucket Key identified in the  first\nstep . The amount of physical memory available with the\nDedup operator may not be sufficient to hold all the buckets in memory.\nHence at any given point in time, only a configured maximum number of\nbuckets can be kept in memory. The Deduper follows different paths\ndepending on the availability of the required bucket in memory.", 
            "title": "Check if tuple is a Duplicate or Unique"
        }, 
        {
            "location": "/operators/deduper/#case-i-bucket-available-in-memory", 
            "text": "In case the bucket with key Bucket Key\u00a0is available in memory, the\nDeduper simply checks if there is a tuple in the bucket with the same\nDedup Key as the one currently being processed. If, so, then the tuple\nbeing processed is deemed to be a duplicate and put out on the\nDuplicate\u00a0port. If not, then the tuple being processed is deemed to be\nunique and put out on the Unique\u00a0port. If the tuple is unique,\nadditionally it is also added to the bucket for future references.", 
            "title": "Case I - Bucket available in memory"
        }, 
        {
            "location": "/operators/deduper/#case-ii-bucket-not-in-memory", 
            "text": "In case the bucket with key Bucket Key\u00a0is not available in memory, the\nDeduper requests the Bucket Manager to load the bucket with key Bucket\nKey\u00a0from the Bucket Store. This request is processed by the Bucket\nManager in a separate asynchronous thread as detailed here . Additionally the Deduper also inserts the tuple\nbeing processed into a waiting events\u00a0queue for later processing. After\nthis, the Deduper cannot proceed until the bucket is loaded by the\nBucket Manager and hence proceeds to process another tuple. Next section\ndetails the process after the bucket is loaded by the Bucket Manager.", 
            "title": "Case II - Bucket not in memory"
        }, 
        {
            "location": "/operators/deduper/#handling-tuples-after-buckets-are-loaded", 
            "text": "The Bucket Manager would load all the buckets requested by the Deduper\nand add them to a fetched buckets\u00a0queue. During the span of one\napplication window of the Deduper, it will process all the tuples on its\ninput port. Processing here could mean one of the below:   The bucket for a tuple was already available in memory and hence the\n    deduper could conclude whether a tuple is a duplicate or unique.  The bucket for a tuple was not available in memory and a request was\n    made to the Bucket Manager for asynchronously loading that\n    particular bucket in memory.   After processing all the tuples as above, the Deduper starts processing\nthe tuples in the waiting events\u00a0queue as mentioned in section  Case II\n- Bucket not in memory . For each of these waiting\ntuples, a corresponding bucket is loaded by the Bucket Manager in the\nfetched buckets\u00a0queue. Using these fetched buckets, the Deduper can\nresolve the pending tuples as duplicate or unique. This is done in the\nsame way as for  Buckets available in memory .", 
            "title": "Handling tuples after Buckets are loaded"
        }, 
        {
            "location": "/operators/deduper/#bucket-manager", 
            "text": "Bucket manager is responsible for loading and unloading of buckets to\nand from memory. Bucket manager maintains a requested buckets\u00a0queue\nwhich holds the requests (in form of bucket keys) from the Deduper,\nindicating which buckets need to be loaded from the Bucket Store. The\nrequests are processed by the Bucket Manager one by one. The first step\nis to identify the Bucket Index for bucket key.", 
            "title": "Bucket Manager"
        }, 
        {
            "location": "/operators/deduper/#identify-bucket-index", 
            "text": "Bucket index is discussed  here . Bucket index can be\ncalculated as follows:  Bucket Index = Requested Bucket Key % Number of Buckets,  where Number of Buckets is as defined  here .", 
            "title": "Identify Bucket Index"
        }, 
        {
            "location": "/operators/deduper/#request-bucket-load-from-store", 
            "text": "Once we have the Bucket Index, the Bucket Store is requested to fetch\nthe corresponding bucket \u00a0and load it into memory. This is a blocking\ncall and the Bucket Manager waits while the Bucket Store fetches the\ndata from the store. Once the data is available, the Bucket Manager\nbundles the data into a bucket and adds it to the fetched buckets\u00a0queue\nmentioned  here . We detail the process of fetching the\nbucket data from the store in  this \u00a0section.", 
            "title": "Request Bucket Load from Store"
        }, 
        {
            "location": "/operators/deduper/#bucket-eviction", 
            "text": "It may not be efficient or even possible in some cases to keep all the\nbuckets into memory. This is the reason the buckets are persisted to the\nHDFS store every window. This makes it essential to off load some of the\nbuckets from memory so that new buckets can be loaded. The policy\nfollowed by the Bucket Manager is the least recently used policy.\nWhenever the Bucket Manager needs to load a particular bucket into\nmemory, it identifies a bucket in memory which has been accessed least\nrecently and unloads it from memory. No other processing has to be done\nin this case. Upon unloading, it informs the listeners (the Deduper\nthread) that the particular bucket has been off loaded from memory and\nis no longer available.", 
            "title": "Bucket Eviction"
        }, 
        {
            "location": "/operators/deduper/#bucket-store", 
            "text": "The Bucket Store is responsible for fetching and storing the data from a\npersistent store. In this case, HDFS is used as the persistent store and\nHDFSBucketStore is responsible for interacting with the store.", 
            "title": "Bucket Store"
        }, 
        {
            "location": "/operators/deduper/#data-format", 
            "text": "Bucket store persists the buckets onto the HDFS. This is typically done\nevery window, although this can be configured to be done every\ncheckpoint window. This data is stored as files on HDFS. Every write\ni.e. new unique records generated per window (or per checkpoint window)\nis written into a new file on HDFS. The format of the file is given\nbelow.   All the unique records (actually, just keys, since dedup requires just\nstorage of keys) that are received in a window, are collected in a set\nof buckets and written one after the other serially in a file on HDFS.\nThis data is indexed in case it needs to be read back. Index structures\nare described in the  Data Structures \u00a0section.", 
            "title": "Data Format"
        }, 
        {
            "location": "/operators/deduper/#data-structures", 
            "text": "HDFS Bucket Store keeps the information about the buckets and their\nlocations in various data structures.   Bucket Positions  - This data structure stores for each bucket index,\n    the files and the offset within those files where the data for the\n    bucket is stored. Since the data for a single bucket may be spread\n    across multiple files, the number of files and their offsets may be\n    multiple.  Window To Buckets  - This data structure keeps track of what buckets\n    were modified in which window. This is essentially a multi map of\n    window id to the set of bucket indexes that were written in that\n    window.  Window to Timestamp  - This data structure keeps track of the maximum\n    timestamp within a particular window file. This gives an indication\n    as to how old is the data in the window file and helps in\n    identifying window files that can be deleted entirely.", 
            "title": "Data Structures"
        }, 
        {
            "location": "/operators/deduper/#bucket-fetch", 
            "text": "For fetching a particular bucket, a bucket index is passed to the HDFS\nbucket store. Using the bucket index, a list of window files is\nidentified which contains the data for this index. The bucket positions\ndata structure is used for this purpose. For each such window file, the\nBucket Store forks off a thread to fetch that particular window file\nfrom HDFS. Effectively all the window files which contain data for a\nparticular bucket index, are fetched in parallel from the HDFS. Once\nfetched, the data is bundled together and returned to the calling\nfunction i.e. the Bucket Manager.", 
            "title": "Bucket Fetch"
        }, 
        {
            "location": "/operators/file_splitter/", 
            "text": "File Splitter\n\n\nThis is a simple operator whose main function is to split a file virtually and create metadata describing the files and the splits. \n\n\nWhy is it needed?\n\n\nIt is a common operation to read a file and parse it. This operation can be parallelized by having multiple partitions of such operators and each partition operating on different files. However, at times when a file is large then a single partition reading it can become a bottleneck.\nIn these cases, throughput can be increased if instances of the partitioned operator can read and parse non-overlapping sets of file blocks. This is where file splitter comes in handy. It creates metadata of blocks of file which serves as tasks handed out to downstream operator partitions. \nThe downstream partitions can read/parse the block without the need of interacting with other partitions.\n\n\nClass Diagram\n\n\n\n\nAbstractFileSplitter\n\n\nThe abstract implementation defines the logic of processing \nFileInfo\n. This comprises the following tasks -  \n\n\n\n\n\n\nbuilding \nFileMetadata\n per file and emitting it. This metadata contains the file information such as filepath, no. of blocks in it, length of the file, all the block ids, etc.\n\n\n\n\n\n\ncreating \nBlockMetadataIterator\n from \nFileMetadata\n. The iterator lazy-loads the block metadata when needed. We use an iterator because the no. of blocks in a file can be huge if the block size is small and loading all of them at once in memory may cause out of memory errors.\n\n\n\n\n\n\nretrieving \nBlockMetadata.FileBlockMetadata\n from the block metadata iterator and emitting it. The FileBlockMetadata contains the block id, start offset of the block, length of file in the block, etc. The number of block metadata emitted per window are controlled by \nblocksThreshold\n setting which by default is 1.  \n\n\n\n\n\n\nThe main utility method that performs all the above tasks is the \nprocess()\n method. Concrete implementations can invoke this method whenever they have data to process.\n\n\nPorts\n\n\nDeclares only output ports on which file metadata and block metadata are emitted.\n\n\n\n\nfilesMetadataOutput: metadata for each file is emitted on this port. \n\n\nblocksMetadataOutput: metadata for each block is emitted on this port. \n\n\n\n\nprocess()\n method\n\n\nWhen process() is invoked, any pending blocks from the current file are emitted on the 'blocksMetadataOutput' port. If the threshold for blocks per window is still not met then a new input file is processed - corresponding metadata is emitted on 'filesMetadataOutput' and more of its blocks are emitted. This operation is repeated until the \nblocksThreshold\n is reached or there are no more new files.\n\n\n  protected void process()\n  {\n    if (blockMetadataIterator != null \n blockCount \n blocksThreshold) {\n      emitBlockMetadata();\n    }\n\n    FileInfo fileInfo;\n    while (blockCount \n blocksThreshold \n (fileInfo = getFileInfo()) != null) {\n      if (!processFileInfo(fileInfo)) {\n        break;\n      }\n    }\n  }\n\n\n\n\nAbstract methods\n\n\n\n\n\n\nFileInfo getFileInfo()\n: called from within the \nprocess()\n and provides the next file to process.\n\n\n\n\n\n\nlong getDefaultBlockSize()\n: provides the block size which is used when user hasn't configured the size.\n\n\n\n\n\n\nFileStatus getFileStatus(Path path)\n: provides the \norg.apache.hadoop.fs.FileStatus\n instance for a path.   \n\n\n\n\n\n\nConfiguration\n\n\n\n\nblockSize\n: size of a block.\n\n\nblocksThreshold\n: threshold on the number of blocks emitted by file splitter every window. This setting is used for throttling the work for downstream operators.\n\n\n\n\nFileSplitterBase\n\n\nSimple operator that receives tuples of type \nFileInfo\n on its \ninput\n port. \nFileInfo\n contains the information (currently just the file path) about the file which this operator uses to create file metadata and block metadata.\n\n\nExample application\n\n\nThis is a simple sub-dag that demonstrates how FileSplitterBase can be plugged into an application.\n\n\n\nThe upstream operator emits tuples of type \nFileInfo\n on its output port which is connected to splitter input port. The downstream receives tuples of type \nBlockMetadata.FileBlockMetadata\n from the splitter's block metadata output port.\n\n\npublic class ApplicationWithBaseSplitter implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    JMSInput input = dag.addOperator(\nInput\n, new JMSInput());\n    FileSplitterBase splitter = dag.addOperator(\nSplitter\n, new FileSplitterBase());\n    FSSliceReader blockReader = dag.addOperator(\nBlockReader\n, new FSSliceReader());\n    ...\n    dag.addStream(\nfile-info\n, input.output, splitter.input);\n    dag.addStream(\nblock-metadata\n, splitter.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    ...\n  }\n\n  public static class JMSInput extends AbstractJMSInputOperator\nAbstractFileSplitter.FileInfo\n\n  {\n\n    public final transient DefaultOutputPort\nAbstractFileSplitter.FileInfo\n output = new DefaultOutputPort\n();\n\n    @Override\n    protected AbstractFileSplitter.FileInfo convert(Message message) throws JMSException\n    {\n      //assuming the message is a text message containing the absolute path of the file.\n      return new AbstractFileSplitter.FileInfo(null, ((TextMessage)message).getText());\n    }\n\n    @Override\n    protected void emit(AbstractFileSplitter.FileInfo payload)\n    {\n      output.emit(payload);\n    }\n  }\n}\n\n\n\n\nPorts\n\n\nDeclares an input port on which it receives tuples from the upstream operator. Output ports are inherited from AbstractFileSplitter.\n\n\n\n\ninput: non optional port on which tuples of type \nFileInfo\n are received.\n\n\n\n\nConfiguration\n\n\n\n\nfile\n: path of the file from which the filesystem is inferred. FileSplitter creates an instance of \norg.apache.hadoop.fs.FileSystem\n which is why this path is needed.  \n\n\n\n\nFileSystem.newInstance(new Path(file).toUri(), new Configuration());\n\n\n\n\nThe fs instance is then used to fetch the default block size and \norg.apache.hadoop.fs.FileStatus\n for each file path.\n\n\nFileSplitterInput\n\n\nThis is an input operator that discovers files itself. The scanning of the directories for new files is asynchronous which is handled by \nTimeBasedDirectoryScanner\n. The function of TimeBasedDirectoryScanner is to periodically scan specified directories and find files which were newly added or modified. The interaction between the operator and the scanner is depicted in the diagram below.\n\n\n\n\nExample application\n\n\nThis is a simple sub-dag that demonstrates how FileSplitterInput can be plugged into an application.\n\n\n\n\nSplitter is the input operator here that sends block metadata to the downstream BlockReader.\n\n\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator(\nInput\n, new FileSplitterInput());\n    FSSliceReader reader = dag.addOperator(\nBlock Reader\n, new FSSliceReader());\n    ...\n    dag.addStream(\nblock-metadata\n, input.blocksMetadataOutput, reader.blocksMetadataInput);\n    ...\n  }\n\n\n\n\n\nPorts\n\n\nSince it is an input operator there are no input ports and output ports are inherited from AbstractFileSplitter.\n\n\nConfiguration\n\n\n\n\nscanner\n: the component that scans directories asynchronously. It is of type \ncom.datatorrent.lib.io.fs.FileSplitter.TimeBasedDirectoryScanner\n. The basic implementation of TimeBasedDirectoryScanner can be customized by users.  \n\n\n\n\na. \nfiles\n: comma separated list of directories to scan.  \n\n\nb. \nrecursive\n: flag that controls whether the directories should be scanned recursively.  \n\n\nc. \nscanIntervalMillis\n: interval specified in milliseconds after which another scan iteration is triggered.  \n\n\nd. \nfilePatternRegularExp\n: regular expression for accepted file names.  \n\n\ne. \ntrigger\n: a flag that triggers a scan iteration instantly. If the scanner thread is idling then it will initiate a scan immediately otherwise if a scan is in progress, then the new iteration will be triggered immediately after the completion of current one.\n2. \nidempotentStorageManager\n: by default FileSplitterInput is idempotent. \nIdempotency ensures that the operator will process the same set of files/blocks in a window if it has seen that window previously, i.e., before a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same file/block again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Therefore, if one doesn't care about idempotency then they can set this property to be an instance of \ncom.datatorrent.lib.io.IdempotentStorageManager.NoopIdempotentStorageManager\n.\n\n\nHandling of split records\n\n\nSplitting of files to create tasks for downstream operator needs to be a simple operation that doesn't consume a lot of resources and is fast. This is why the file splitter doesn't open files to read. The downside of that is if the file contains records then a record may split across adjacent blocks. Handling of this is left to the downstream operator.\n\n\nWe have created Block readers in Apex-malhar library that handle line splits efficiently. The 2 line readers- \nAbstractFSLineReader\n and \nAbstractFSReadAheadLineReader\n can be found here \nAbstractFSBlockReader\n.", 
            "title": "File Splitter"
        }, 
        {
            "location": "/operators/file_splitter/#file-splitter", 
            "text": "This is a simple operator whose main function is to split a file virtually and create metadata describing the files and the splits.", 
            "title": "File Splitter"
        }, 
        {
            "location": "/operators/file_splitter/#why-is-it-needed", 
            "text": "It is a common operation to read a file and parse it. This operation can be parallelized by having multiple partitions of such operators and each partition operating on different files. However, at times when a file is large then a single partition reading it can become a bottleneck.\nIn these cases, throughput can be increased if instances of the partitioned operator can read and parse non-overlapping sets of file blocks. This is where file splitter comes in handy. It creates metadata of blocks of file which serves as tasks handed out to downstream operator partitions. \nThe downstream partitions can read/parse the block without the need of interacting with other partitions.", 
            "title": "Why is it needed?"
        }, 
        {
            "location": "/operators/file_splitter/#class-diagram", 
            "text": "", 
            "title": "Class Diagram"
        }, 
        {
            "location": "/operators/file_splitter/#abstractfilesplitter", 
            "text": "The abstract implementation defines the logic of processing  FileInfo . This comprises the following tasks -      building  FileMetadata  per file and emitting it. This metadata contains the file information such as filepath, no. of blocks in it, length of the file, all the block ids, etc.    creating  BlockMetadataIterator  from  FileMetadata . The iterator lazy-loads the block metadata when needed. We use an iterator because the no. of blocks in a file can be huge if the block size is small and loading all of them at once in memory may cause out of memory errors.    retrieving  BlockMetadata.FileBlockMetadata  from the block metadata iterator and emitting it. The FileBlockMetadata contains the block id, start offset of the block, length of file in the block, etc. The number of block metadata emitted per window are controlled by  blocksThreshold  setting which by default is 1.      The main utility method that performs all the above tasks is the  process()  method. Concrete implementations can invoke this method whenever they have data to process.", 
            "title": "AbstractFileSplitter"
        }, 
        {
            "location": "/operators/file_splitter/#ports", 
            "text": "Declares only output ports on which file metadata and block metadata are emitted.   filesMetadataOutput: metadata for each file is emitted on this port.   blocksMetadataOutput: metadata for each block is emitted on this port.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_splitter/#abstract-methods", 
            "text": "FileInfo getFileInfo() : called from within the  process()  and provides the next file to process.    long getDefaultBlockSize() : provides the block size which is used when user hasn't configured the size.    FileStatus getFileStatus(Path path) : provides the  org.apache.hadoop.fs.FileStatus  instance for a path.", 
            "title": "Abstract methods"
        }, 
        {
            "location": "/operators/file_splitter/#configuration", 
            "text": "blockSize : size of a block.  blocksThreshold : threshold on the number of blocks emitted by file splitter every window. This setting is used for throttling the work for downstream operators.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/file_splitter/#filesplitterbase", 
            "text": "Simple operator that receives tuples of type  FileInfo  on its  input  port.  FileInfo  contains the information (currently just the file path) about the file which this operator uses to create file metadata and block metadata.", 
            "title": "FileSplitterBase"
        }, 
        {
            "location": "/operators/file_splitter/#example-application", 
            "text": "This is a simple sub-dag that demonstrates how FileSplitterBase can be plugged into an application.  The upstream operator emits tuples of type  FileInfo  on its output port which is connected to splitter input port. The downstream receives tuples of type  BlockMetadata.FileBlockMetadata  from the splitter's block metadata output port.  public class ApplicationWithBaseSplitter implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    JMSInput input = dag.addOperator( Input , new JMSInput());\n    FileSplitterBase splitter = dag.addOperator( Splitter , new FileSplitterBase());\n    FSSliceReader blockReader = dag.addOperator( BlockReader , new FSSliceReader());\n    ...\n    dag.addStream( file-info , input.output, splitter.input);\n    dag.addStream( block-metadata , splitter.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    ...\n  }\n\n  public static class JMSInput extends AbstractJMSInputOperator AbstractFileSplitter.FileInfo \n  {\n\n    public final transient DefaultOutputPort AbstractFileSplitter.FileInfo  output = new DefaultOutputPort ();\n\n    @Override\n    protected AbstractFileSplitter.FileInfo convert(Message message) throws JMSException\n    {\n      //assuming the message is a text message containing the absolute path of the file.\n      return new AbstractFileSplitter.FileInfo(null, ((TextMessage)message).getText());\n    }\n\n    @Override\n    protected void emit(AbstractFileSplitter.FileInfo payload)\n    {\n      output.emit(payload);\n    }\n  }\n}", 
            "title": "Example application"
        }, 
        {
            "location": "/operators/file_splitter/#ports_1", 
            "text": "Declares an input port on which it receives tuples from the upstream operator. Output ports are inherited from AbstractFileSplitter.   input: non optional port on which tuples of type  FileInfo  are received.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_splitter/#configuration_1", 
            "text": "file : path of the file from which the filesystem is inferred. FileSplitter creates an instance of  org.apache.hadoop.fs.FileSystem  which is why this path is needed.     FileSystem.newInstance(new Path(file).toUri(), new Configuration());  The fs instance is then used to fetch the default block size and  org.apache.hadoop.fs.FileStatus  for each file path.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/file_splitter/#filesplitterinput", 
            "text": "This is an input operator that discovers files itself. The scanning of the directories for new files is asynchronous which is handled by  TimeBasedDirectoryScanner . The function of TimeBasedDirectoryScanner is to periodically scan specified directories and find files which were newly added or modified. The interaction between the operator and the scanner is depicted in the diagram below.", 
            "title": "FileSplitterInput"
        }, 
        {
            "location": "/operators/file_splitter/#example-application_1", 
            "text": "This is a simple sub-dag that demonstrates how FileSplitterInput can be plugged into an application.   Splitter is the input operator here that sends block metadata to the downstream BlockReader.    @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator( Input , new FileSplitterInput());\n    FSSliceReader reader = dag.addOperator( Block Reader , new FSSliceReader());\n    ...\n    dag.addStream( block-metadata , input.blocksMetadataOutput, reader.blocksMetadataInput);\n    ...\n  }", 
            "title": "Example application"
        }, 
        {
            "location": "/operators/file_splitter/#ports_2", 
            "text": "Since it is an input operator there are no input ports and output ports are inherited from AbstractFileSplitter.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_splitter/#configuration_2", 
            "text": "scanner : the component that scans directories asynchronously. It is of type  com.datatorrent.lib.io.fs.FileSplitter.TimeBasedDirectoryScanner . The basic implementation of TimeBasedDirectoryScanner can be customized by users.     a.  files : comma separated list of directories to scan.    b.  recursive : flag that controls whether the directories should be scanned recursively.    c.  scanIntervalMillis : interval specified in milliseconds after which another scan iteration is triggered.    d.  filePatternRegularExp : regular expression for accepted file names.    e.  trigger : a flag that triggers a scan iteration instantly. If the scanner thread is idling then it will initiate a scan immediately otherwise if a scan is in progress, then the new iteration will be triggered immediately after the completion of current one.\n2.  idempotentStorageManager : by default FileSplitterInput is idempotent. \nIdempotency ensures that the operator will process the same set of files/blocks in a window if it has seen that window previously, i.e., before a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same file/block again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Therefore, if one doesn't care about idempotency then they can set this property to be an instance of  com.datatorrent.lib.io.IdempotentStorageManager.NoopIdempotentStorageManager .", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/file_splitter/#handling-of-split-records", 
            "text": "Splitting of files to create tasks for downstream operator needs to be a simple operation that doesn't consume a lot of resources and is fast. This is why the file splitter doesn't open files to read. The downside of that is if the file contains records then a record may split across adjacent blocks. Handling of this is left to the downstream operator.  We have created Block readers in Apex-malhar library that handle line splits efficiently. The 2 line readers-  AbstractFSLineReader  and  AbstractFSReadAheadLineReader  can be found here  AbstractFSBlockReader .", 
            "title": "Handling of split records"
        }, 
        {
            "location": "/operators/block_reader/", 
            "text": "Block Reader\n\n\nThis is a scalable operator that reads and parses blocks of data sources into records. A data source can be a file or a message bus that contains records and a block defines a chunk of data in the source by specifying the block offset and the length of the source belonging to the block. \n\n\nWhy is it needed?\n\n\nA Block Reader is needed to parallelize reading and parsing of a single data source, for example a file. Simple parallelism of reading data sources can be achieved by multiple partitions reading different source of same type (for files see \nAbstractFileInputOperator\n) but Block Reader partitions can read blocks of same source in parallel and parse them for records ensuring that no record is duplicated or missed.\n\n\nClass Diagram\n\n\n\n\nAbstractBlockReader\n\n\nThis is the abstract implementation that serves as the base for different types of data sources. It defines how a block metadata is processed. The flow diagram below describes the processing of a block metadata.\n\n\n\n\nPorts\n\n\n\n\n\n\nblocksMetadataInput: input port on which block metadata are received.\n\n\n\n\n\n\nblocksMetadataOutput: output port on which block metadata are emitted if the port is connected. This port is useful when a downstream operator that receives records from block reader may also be interested to know the details of the corresponding blocks.\n\n\n\n\n\n\nmessages: output port on which tuples of type \ncom.datatorrent.lib.io.block.AbstractBlockReader.ReaderRecord\n are emitted. This class encapsulates a \nrecord\n and the \nblockId\n of the corresponding block.\n\n\n\n\n\n\nreaderContext\n\n\nThis is one of the most important fields in the block reader. It is of type \ncom.datatorrent.lib.io.block.ReaderContext\n and is responsible for fetching bytes that make a record. It also lets the reader know how many total bytes were consumed which may not be equal to the total bytes in a record because consumed bytes also include bytes for the record delimiter which may not be a part of the actual record.\n\n\nOnce the reader creates an input stream for the block (or uses the previous opened stream if the current block is successor of the previous block) it initializes the reader context by invoking \nreaderContext.initialize(stream, blockMetadata, consecutiveBlock);\n. Initialize method is where any implementation of \nReaderContext\n can perform all the operations which have to be executed just before reading the block or create states which are used during the lifetime of reading the block.\n\n\nOnce the initialization is done, \nreaderContext.next()\n is called repeatedly until it returns \nnull\n. It is left to the \nReaderContext\n implementations to decide when a block is completely processed. In cases when a record is split across adjacent blocks, reader context may decide to read ahead of the current block boundary to completely fetch the split record (examples- \nLineReaderContext\n and \nReadAheadLineReaderContext\n). In other cases when there isn't a possibility of split record (example- \nFixedBytesReaderContext\n), it returns \nnull\n immediately when the block boundary is reached. The return type of \nreaderContext.next()\n is of type \ncom.datatorrent.lib.io.block.ReaderContext.Entity\n which is just a wrapper for a \nbyte[]\n that represents the record and total bytes used in fetching the record.\n\n\nAbstract methods\n\n\n\n\n\n\nSTREAM setupStream(B block)\n: creating a stream for a block is dependent on the type of source which is not known to AbstractBlockReader. Sub-classes which deal with a specific data source provide this implementation.\n\n\n\n\n\n\nR convertToRecord(byte[] bytes)\n: this converts the array of bytes into the actual instance of record type.\n\n\n\n\n\n\nAuto-scalability\n\n\nBlock reader can auto-scale, that is, depending on the backlog (total number of all the blocks which are waiting in the \nblocksMetadataInput\n port queue of all partitions) it can create more partitions or reduce them. Details are discussed in the last section which covers the \npartitioner and stats-listener\n.\n\n\nConfiguration\n\n\n\n\nmaxReaders\n: when auto-scaling is enabled, this controls the maximum number of block reader partitions that can be created.\n\n\nminReaders\n: when auto-scaling is enabled, this controls the minimum number of block reader partitions that should always exist.\n\n\ncollectStats\n: this enables or disables auto-scaling. When it is set to \ntrue\n the stats (number of blocks in the queue) are collected and this triggers partitioning; otherwise auto-scaling is disabled.\n\n\nintervalMillis\n: when auto-scaling is enabled, this specifies the interval at which the reader will trigger the logic of computing the backlog and auto-scale.\n\n\n\n\n AbstractFSBlockReader\n\n\nThis abstract implementation deals with files. Different types of file systems that are implementations of \norg.apache.hadoop.fs.FileSystem\n are supported. The user can override \ngetFSInstance()\n method to create an instance of a specific \nFileSystem\n. By default, filesystem instance is created from the filesytem URI that comes from the default hadoop configuration.\n\n\nprotected FileSystem getFSInstance() throws IOException\n{\n  return FileSystem.newInstance(configuration);\n}\n\n\n\n\nIt uses this filesystem instance to setup a stream of type \norg.apache.hadoop.fs.FSDataInputStream\n to read the block.\n\n\n@Override\nprotected FSDataInputStream setupStream(BlockMetadata.FileBlockMetadata block) throws IOException\n{\n  return fs.open(new Path(block.getFilePath()));\n}\n\n\n\n\nAll the ports and configurations are derived from the super class. It doesn't provide an implementation of \nconvertToRecord(byte[] bytes)\n method which is delegated to concrete sub-classes.\n\n\nExample Application\n\n\nThis simple dag demonstrates how any concrete implementation of \nAbstractFSBlockReader\n can be plugged into an application. \n\n\n\n\nIn the above application, file splitter creates block metadata for files which are sent to block reader. Partitions of the block reader parses the file blocks for records which are filtered, transformed and then persisted to a file (created per block). Therefore block reader is parallel partitioned with the 2 downstream operators - filter/converter and record output operator. The code which implements this dag is below.\n\n\npublic class ExampleApplication implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator(\nFile-splitter\n, new FileSplitterInput());\n    //any concrete implementation of AbstractFSBlockReader based on the use-case can be added here.\n    LineReader blockReader = dag.addOperator(\nBlock-reader\n, new LineReader());\n    Filter filter = dag.addOperator(\nFilter\n, new Filter());\n    RecordOutputOperator recordOutputOperator = dag.addOperator(\nRecord-writer\n, new RecordOutputOperator());\n\n    dag.addStream(\nfile-block metadata\n, input.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    dag.addStream(\nrecords\n, blockReader.messages, filter.input);\n    dag.addStream(\nfiltered-records\n, filter.output, recordOutputOperator.input);\n  }\n\n  /**\n   * Concrete implementation of {@link AbstractFSBlockReader} for which a record is a line in the file.\n   */\n  public static class LineReader extends AbstractFSBlockReader.AbstractFSReadAheadLineReader\nString\n\n  {\n\n    @Override\n    protected String convertToRecord(byte[] bytes)\n    {\n      return new String(bytes);\n    }\n  }\n\n  /**\n   * Considers any line starting with a '.' as invalid. Emits the valid records.\n   */\n  public static class Filter extends BaseOperator\n  {\n    public final transient DefaultOutputPort\nAbstractBlockReader.ReaderRecord\nString\n output = new DefaultOutputPort\n();\n    public final transient DefaultInputPort\nAbstractBlockReader.ReaderRecord\nString\n input = new DefaultInputPort\nAbstractBlockReader.ReaderRecord\nString\n()\n    {\n      @Override\n      public void process(AbstractBlockReader.ReaderRecord\nString\n stringRecord)\n      {\n        //filter records and transform\n        //if the string starts with a '.' ignore the string.\n        if (!StringUtils.startsWith(stringRecord.getRecord(), \n.\n)) {\n          output.emit(stringRecord);\n        }\n      }\n    };\n  }\n\n  /**\n   * Persists the valid records to corresponding block files.\n   */\n  public static class RecordOutputOperator extends AbstractFileOutputOperator\nAbstractBlockReader.ReaderRecord\nString\n\n  {\n    @Override\n    protected String getFileName(AbstractBlockReader.ReaderRecord\nString\n tuple)\n    {\n      return Long.toHexString(tuple.getBlockId());\n    }\n\n    @Override\n    protected byte[] getBytesForTuple(AbstractBlockReader.ReaderRecord\nString\n tuple)\n    {\n      return tuple.getRecord().getBytes();\n    }\n  }\n}\n\n\n\n\nConfiguration to parallel partition block reader with its downstream operators.\n\n\n  \nproperty\n\n    \nname\ndt.operator.Filter.port.input.attr.PARTITION_PARALLEL\n/name\n\n    \nvalue\ntrue\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.Record-writer.port.input.attr.PARTITION_PARALLEL\n/name\n\n    \nvalue\ntrue\n/value\n\n  \n/property\n\n\n\n\n\nAbstractFSReadAheadLineReader\n\n\nThis extension of \nAbstractFSBlockReader\n parses lines from a block and binds the \nreaderContext\n field to an instance of \nReaderContext.ReadAheadLineReaderContext\n.\n\n\nIt is abstract because it doesn't provide an implementation of \nconvertToRecord(byte[] bytes)\n since the user may want to convert the bytes that make a line into some other type. \n\n\nReadAheadLineReaderContext\n\n\nIn order to handle a line split across adjacent blocks, ReadAheadLineReaderContext always reads beyond the block boundary and ignores the bytes till the first end-of-line character of all the blocks except the first block of the file. This ensures that no line is missed or incomplete.\n\n\nThis is one of the most common ways of handling a split record. It doesn't require any further information to decide if a line is complete. However, the cost of this consistent way to handle a line split is that it always reads from the next block.\n\n\nAbstractFSLineReader\n\n\nSimilar to \nAbstractFSReadAheadLineReader\n, even this parses lines from a block. However, it binds the \nreaderContext\n field to an instance of \nReaderContext.LineReaderContext\n.\n\n\nLineReaderContext\n\n\nThis handles the line split differently from \nReadAheadLineReaderContext\n. It doesn't always read from the next block. If the end of the last line is aligned with the block boundary then it stops processing the block. It does read from the next block when the boundaries are not aligned, that is, last line extends beyond the block boundary. The result of this is an inconsistency in reading the next block.\n\n\nWhen the boundary of the last line of the previous block was aligned with its block, then the first line of the current block is a valid line. However, in the other case the bytes from the block start offset to the first end-of-line character should be ignored. Therefore, this means that any record formed by this reader context has to be validated. For example, if the lines are of fixed size then size of each record can be validated or if each line begins with a special field then that knowledge can be used to check if a record is complete.\n\n\nIf the validations of completeness fails for a line then \nconvertToRecord(byte[] bytes)\n should return null.\n\n\nFSSliceReader\n\n\nA concrete extension of \nAbstractFSBlockReader\n that reads fixed-size \nbyte[]\n from a block and emits the byte array wrapped in \ncom.datatorrent.netlet.util.Slice\n.\n\n\nThis operator binds the \nreaderContext\n to an instance of \nReaderContext.FixedBytesReaderContext\n.\n\n\nFixedBytesReaderContext\n\n\nThis implementation of \nReaderContext\n never reads beyond a block boundary which can result in the last \nbyte[]\n of a block to be of a shorter length than the rest of the records.\n\n\nConfiguration\n\n\nreaderContext.length\n: length of each record. By default, this is initialized to the default hdfs block size.\n\n\nPartitioner and StatsListener\n\n\nThe logical instance of the block reader acts as the Partitioner (unless a custom partitioner is set using the operator attribute - \nPARTITIONER\n) as well as a StatsListener. This is because the \n\nAbstractBlockReader\n implements both the \ncom.datatorrent.api.Partitioner\n and \ncom.datatorrent.api.StatsListener\n interfaces and provides an implementation of \ndefinePartitions(...)\n and \nprocessStats(...)\n which make it auto-scalable.\n\n\nprocessStats \n\n\nThe application master invokes \nResponse processStats(BatchedOperatorStats stats)\n method on the logical instance with the stats (\ntuplesProcessedPSMA\n, \ntuplesEmittedPSMA\n, \nlatencyMA\n, etc.) of each partition. The data which this operator is interested in is the \nqueueSize\n of the input port \nblocksMetadataInput\n.\n\n\nUsually the \nqueueSize\n of an input port gives the count of waiting control tuples plus data tuples. However, if a stats listener is interested only in the count of data tuples then that can be expressed by annotating the class with \n@DataQueueSize\n. In this case \nAbstractBlockReader\n itself is the \nStatsListener\n which is why it is annotated with \n@DataQueueSize\n.\n\n\nThe logical instance caches the queue size per partition and at regular intervals (configured by \nintervalMillis\n) sums these values to find the total backlog which is then used to decide whether re-partitioning is needed. The flow-diagram below describes this logic.\n\n\n\n\nThe goal of this logic is to create as many partitions within bounds (see \nmaxReaders\n and \nminReaders\n above) to quickly reduce this backlog or if the backlog is small then remove any idle partitions.\n\n\ndefinePartitions\n\n\nBased on the \nrepartitionRequired\n field of the \nResponse\n object which is returned by \nprocessStats\n method, the application master invokes \n\n\nCollection\nPartition\nAbstractBlockReader\n...\n definePartitions(Collection\nPartition\nAbstractBlockReader\n...\n partitions, PartitioningContext context)\n\n\n\n\non the logical instance which is also the partitioner instance. The implementation calculates the difference between required partitions and the existing count of partitions. If this difference is negative, then equivalent number of partitions are removed otherwise new partitions are created. \n\n\nPlease note auto-scaling can be disabled by setting \ncollectStats\n to \nfalse\n. If the use-case requires only static partitioning, then that can be achieved by setting \nStatelessPartitioner\n as the operator attribute- \nPARTITIONER\n on the block reader.", 
            "title": "Block Reader"
        }, 
        {
            "location": "/operators/block_reader/#block-reader", 
            "text": "This is a scalable operator that reads and parses blocks of data sources into records. A data source can be a file or a message bus that contains records and a block defines a chunk of data in the source by specifying the block offset and the length of the source belonging to the block.", 
            "title": "Block Reader"
        }, 
        {
            "location": "/operators/block_reader/#why-is-it-needed", 
            "text": "A Block Reader is needed to parallelize reading and parsing of a single data source, for example a file. Simple parallelism of reading data sources can be achieved by multiple partitions reading different source of same type (for files see  AbstractFileInputOperator ) but Block Reader partitions can read blocks of same source in parallel and parse them for records ensuring that no record is duplicated or missed.", 
            "title": "Why is it needed?"
        }, 
        {
            "location": "/operators/block_reader/#class-diagram", 
            "text": "", 
            "title": "Class Diagram"
        }, 
        {
            "location": "/operators/block_reader/#abstractblockreader", 
            "text": "This is the abstract implementation that serves as the base for different types of data sources. It defines how a block metadata is processed. The flow diagram below describes the processing of a block metadata.", 
            "title": "AbstractBlockReader"
        }, 
        {
            "location": "/operators/block_reader/#ports", 
            "text": "blocksMetadataInput: input port on which block metadata are received.    blocksMetadataOutput: output port on which block metadata are emitted if the port is connected. This port is useful when a downstream operator that receives records from block reader may also be interested to know the details of the corresponding blocks.    messages: output port on which tuples of type  com.datatorrent.lib.io.block.AbstractBlockReader.ReaderRecord  are emitted. This class encapsulates a  record  and the  blockId  of the corresponding block.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/block_reader/#readercontext", 
            "text": "This is one of the most important fields in the block reader. It is of type  com.datatorrent.lib.io.block.ReaderContext  and is responsible for fetching bytes that make a record. It also lets the reader know how many total bytes were consumed which may not be equal to the total bytes in a record because consumed bytes also include bytes for the record delimiter which may not be a part of the actual record.  Once the reader creates an input stream for the block (or uses the previous opened stream if the current block is successor of the previous block) it initializes the reader context by invoking  readerContext.initialize(stream, blockMetadata, consecutiveBlock); . Initialize method is where any implementation of  ReaderContext  can perform all the operations which have to be executed just before reading the block or create states which are used during the lifetime of reading the block.  Once the initialization is done,  readerContext.next()  is called repeatedly until it returns  null . It is left to the  ReaderContext  implementations to decide when a block is completely processed. In cases when a record is split across adjacent blocks, reader context may decide to read ahead of the current block boundary to completely fetch the split record (examples-  LineReaderContext  and  ReadAheadLineReaderContext ). In other cases when there isn't a possibility of split record (example-  FixedBytesReaderContext ), it returns  null  immediately when the block boundary is reached. The return type of  readerContext.next()  is of type  com.datatorrent.lib.io.block.ReaderContext.Entity  which is just a wrapper for a  byte[]  that represents the record and total bytes used in fetching the record.", 
            "title": "readerContext"
        }, 
        {
            "location": "/operators/block_reader/#abstract-methods", 
            "text": "STREAM setupStream(B block) : creating a stream for a block is dependent on the type of source which is not known to AbstractBlockReader. Sub-classes which deal with a specific data source provide this implementation.    R convertToRecord(byte[] bytes) : this converts the array of bytes into the actual instance of record type.", 
            "title": "Abstract methods"
        }, 
        {
            "location": "/operators/block_reader/#auto-scalability", 
            "text": "Block reader can auto-scale, that is, depending on the backlog (total number of all the blocks which are waiting in the  blocksMetadataInput  port queue of all partitions) it can create more partitions or reduce them. Details are discussed in the last section which covers the  partitioner and stats-listener .", 
            "title": "Auto-scalability"
        }, 
        {
            "location": "/operators/block_reader/#configuration", 
            "text": "maxReaders : when auto-scaling is enabled, this controls the maximum number of block reader partitions that can be created.  minReaders : when auto-scaling is enabled, this controls the minimum number of block reader partitions that should always exist.  collectStats : this enables or disables auto-scaling. When it is set to  true  the stats (number of blocks in the queue) are collected and this triggers partitioning; otherwise auto-scaling is disabled.  intervalMillis : when auto-scaling is enabled, this specifies the interval at which the reader will trigger the logic of computing the backlog and auto-scale.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/block_reader/#example-application", 
            "text": "This simple dag demonstrates how any concrete implementation of  AbstractFSBlockReader  can be plugged into an application.    In the above application, file splitter creates block metadata for files which are sent to block reader. Partitions of the block reader parses the file blocks for records which are filtered, transformed and then persisted to a file (created per block). Therefore block reader is parallel partitioned with the 2 downstream operators - filter/converter and record output operator. The code which implements this dag is below.  public class ExampleApplication implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator( File-splitter , new FileSplitterInput());\n    //any concrete implementation of AbstractFSBlockReader based on the use-case can be added here.\n    LineReader blockReader = dag.addOperator( Block-reader , new LineReader());\n    Filter filter = dag.addOperator( Filter , new Filter());\n    RecordOutputOperator recordOutputOperator = dag.addOperator( Record-writer , new RecordOutputOperator());\n\n    dag.addStream( file-block metadata , input.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    dag.addStream( records , blockReader.messages, filter.input);\n    dag.addStream( filtered-records , filter.output, recordOutputOperator.input);\n  }\n\n  /**\n   * Concrete implementation of {@link AbstractFSBlockReader} for which a record is a line in the file.\n   */\n  public static class LineReader extends AbstractFSBlockReader.AbstractFSReadAheadLineReader String \n  {\n\n    @Override\n    protected String convertToRecord(byte[] bytes)\n    {\n      return new String(bytes);\n    }\n  }\n\n  /**\n   * Considers any line starting with a '.' as invalid. Emits the valid records.\n   */\n  public static class Filter extends BaseOperator\n  {\n    public final transient DefaultOutputPort AbstractBlockReader.ReaderRecord String  output = new DefaultOutputPort ();\n    public final transient DefaultInputPort AbstractBlockReader.ReaderRecord String  input = new DefaultInputPort AbstractBlockReader.ReaderRecord String ()\n    {\n      @Override\n      public void process(AbstractBlockReader.ReaderRecord String  stringRecord)\n      {\n        //filter records and transform\n        //if the string starts with a '.' ignore the string.\n        if (!StringUtils.startsWith(stringRecord.getRecord(),  . )) {\n          output.emit(stringRecord);\n        }\n      }\n    };\n  }\n\n  /**\n   * Persists the valid records to corresponding block files.\n   */\n  public static class RecordOutputOperator extends AbstractFileOutputOperator AbstractBlockReader.ReaderRecord String \n  {\n    @Override\n    protected String getFileName(AbstractBlockReader.ReaderRecord String  tuple)\n    {\n      return Long.toHexString(tuple.getBlockId());\n    }\n\n    @Override\n    protected byte[] getBytesForTuple(AbstractBlockReader.ReaderRecord String  tuple)\n    {\n      return tuple.getRecord().getBytes();\n    }\n  }\n}  Configuration to parallel partition block reader with its downstream operators.     property \n     name dt.operator.Filter.port.input.attr.PARTITION_PARALLEL /name \n     value true /value \n   /property \n   property \n     name dt.operator.Record-writer.port.input.attr.PARTITION_PARALLEL /name \n     value true /value \n   /property", 
            "title": "Example Application"
        }, 
        {
            "location": "/operators/block_reader/#abstractfsreadaheadlinereader", 
            "text": "This extension of  AbstractFSBlockReader  parses lines from a block and binds the  readerContext  field to an instance of  ReaderContext.ReadAheadLineReaderContext .  It is abstract because it doesn't provide an implementation of  convertToRecord(byte[] bytes)  since the user may want to convert the bytes that make a line into some other type.", 
            "title": "AbstractFSReadAheadLineReader"
        }, 
        {
            "location": "/operators/block_reader/#readaheadlinereadercontext", 
            "text": "In order to handle a line split across adjacent blocks, ReadAheadLineReaderContext always reads beyond the block boundary and ignores the bytes till the first end-of-line character of all the blocks except the first block of the file. This ensures that no line is missed or incomplete.  This is one of the most common ways of handling a split record. It doesn't require any further information to decide if a line is complete. However, the cost of this consistent way to handle a line split is that it always reads from the next block.", 
            "title": "ReadAheadLineReaderContext"
        }, 
        {
            "location": "/operators/block_reader/#abstractfslinereader", 
            "text": "Similar to  AbstractFSReadAheadLineReader , even this parses lines from a block. However, it binds the  readerContext  field to an instance of  ReaderContext.LineReaderContext .", 
            "title": "AbstractFSLineReader"
        }, 
        {
            "location": "/operators/block_reader/#linereadercontext", 
            "text": "This handles the line split differently from  ReadAheadLineReaderContext . It doesn't always read from the next block. If the end of the last line is aligned with the block boundary then it stops processing the block. It does read from the next block when the boundaries are not aligned, that is, last line extends beyond the block boundary. The result of this is an inconsistency in reading the next block.  When the boundary of the last line of the previous block was aligned with its block, then the first line of the current block is a valid line. However, in the other case the bytes from the block start offset to the first end-of-line character should be ignored. Therefore, this means that any record formed by this reader context has to be validated. For example, if the lines are of fixed size then size of each record can be validated or if each line begins with a special field then that knowledge can be used to check if a record is complete.  If the validations of completeness fails for a line then  convertToRecord(byte[] bytes)  should return null.", 
            "title": "LineReaderContext"
        }, 
        {
            "location": "/operators/block_reader/#fsslicereader", 
            "text": "A concrete extension of  AbstractFSBlockReader  that reads fixed-size  byte[]  from a block and emits the byte array wrapped in  com.datatorrent.netlet.util.Slice .  This operator binds the  readerContext  to an instance of  ReaderContext.FixedBytesReaderContext .", 
            "title": "FSSliceReader"
        }, 
        {
            "location": "/operators/block_reader/#fixedbytesreadercontext", 
            "text": "This implementation of  ReaderContext  never reads beyond a block boundary which can result in the last  byte[]  of a block to be of a shorter length than the rest of the records.", 
            "title": "FixedBytesReaderContext"
        }, 
        {
            "location": "/operators/block_reader/#configuration_1", 
            "text": "readerContext.length : length of each record. By default, this is initialized to the default hdfs block size.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/block_reader/#partitioner-and-statslistener", 
            "text": "The logical instance of the block reader acts as the Partitioner (unless a custom partitioner is set using the operator attribute -  PARTITIONER ) as well as a StatsListener. This is because the  AbstractBlockReader  implements both the  com.datatorrent.api.Partitioner  and  com.datatorrent.api.StatsListener  interfaces and provides an implementation of  definePartitions(...)  and  processStats(...)  which make it auto-scalable.", 
            "title": "Partitioner and StatsListener"
        }, 
        {
            "location": "/operators/block_reader/#processstats", 
            "text": "The application master invokes  Response processStats(BatchedOperatorStats stats)  method on the logical instance with the stats ( tuplesProcessedPSMA ,  tuplesEmittedPSMA ,  latencyMA , etc.) of each partition. The data which this operator is interested in is the  queueSize  of the input port  blocksMetadataInput .  Usually the  queueSize  of an input port gives the count of waiting control tuples plus data tuples. However, if a stats listener is interested only in the count of data tuples then that can be expressed by annotating the class with  @DataQueueSize . In this case  AbstractBlockReader  itself is the  StatsListener  which is why it is annotated with  @DataQueueSize .  The logical instance caches the queue size per partition and at regular intervals (configured by  intervalMillis ) sums these values to find the total backlog which is then used to decide whether re-partitioning is needed. The flow-diagram below describes this logic.   The goal of this logic is to create as many partitions within bounds (see  maxReaders  and  minReaders  above) to quickly reduce this backlog or if the backlog is small then remove any idle partitions.", 
            "title": "processStats "
        }, 
        {
            "location": "/operators/block_reader/#definepartitions", 
            "text": "Based on the  repartitionRequired  field of the  Response  object which is returned by  processStats  method, the application master invokes   Collection Partition AbstractBlockReader ...  definePartitions(Collection Partition AbstractBlockReader ...  partitions, PartitioningContext context)  on the logical instance which is also the partitioner instance. The implementation calculates the difference between required partitions and the existing count of partitions. If this difference is negative, then equivalent number of partitions are removed otherwise new partitions are created.   Please note auto-scaling can be disabled by setting  collectStats  to  false . If the use-case requires only static partitioning, then that can be achieved by setting  StatelessPartitioner  as the operator attribute-  PARTITIONER  on the block reader.", 
            "title": "definePartitions"
        }, 
        {
            "location": "/operators/file_output/", 
            "text": "AbstractFileOutputOperator\n\n\nThe abstract file output operator in Apache Apex Malhar library \n \nAbstractFileOutputOperator\n writes streaming data to files. The main features of this operator are:\n\n\n\n\nPersisting data to files.\n\n\nAutomatic rotation of files based on:\n\n  a. maximum length of a file.\n\n  b. time-based rotation where time is specified using a count of application windows.\n\n\nFault-tolerance.\n\n\nCompression and encryption of data before it is persisted.\n\n\n\n\nIn this tutorial we will cover the details of the basic structure and implementation of all the above features in \nAbstractFileOutputOperator\n. Configuration items related to each feature are discussed as they are introduced in the section of that feature.\n\n\nPersisting data to files\n\n\nThe principal function of this operator is to persist tuples to files efficiently. These files are created under a specific directory on the file system. The relevant configuration item is:\n\n\nfilePath\n: path specifying the directory where files are written.\n\n\nDifferent types of file system that are implementations of \norg.apache.hadoop.fs.FileSystem\n are supported. The file system instance which is used for creating streams is constructed from the \nfilePath\n URI.\n\n\nFileSystem.newInstance(new Path(filePath).toUri(), new Configuration())\n\n\n\n\nTuples may belong to different files therefore expensive IO operations like creating multiple output streams, flushing of data to disk, and closing streams are handled carefully.\n\n\nPorts\n\n\n\n\ninput\n: the input port on which tuples to be persisted are received.\n\n\n\n\nstreamsCache\n\n\nThis transient state caches output streams per file in memory. The file to which the data is appended may change with incoming tuples. It will be highly inefficient to keep re-opening streams for a file just because tuples for that file are interleaved with tuples for another file. Therefore, the operator maintains a cache of limited size with open output streams.\n\n\nstreamsCache\n is of type \ncom.google.common.cache.LoadingCache\n. A \nLoadingCache\n has an attached \nCacheLoader\n which is responsible to load value of a key when the key is not present in the cache. Details are explained here- \nCachesExplained\n.\n\n\nThe operator constructs this cache in \nsetup(...)\n. It is built with the following configuration items:\n\n\n\n\nmaxOpenFiles\n: maximum size of the cache. The cache evicts entries that haven't been used recently when the cache size is approaching this limit. \nDefault\n: 100\n\n\nexpireStreamAfterAcessMillis\n: expires streams after the specified duration has passed since the stream was last accessed. \nDefault\n: value of attribute- \nOperatorContext.SPIN_MILLIS\n.\n\n\n\n\nAn important point to note here is that the guava cache does not perform cleanup and evict values asynchronously, that is, instantly after a value expires. Instead, it performs small amounts of maintenance during write operations, or during occasional read operations if writes are rare.\n\n\nCacheLoader\n\n\nstreamsCache\n is created with a \nCacheLoader\n that opens an \nFSDataOutputStream\n for a file which is not in the cache. The output stream is opened in either \nappend\n or \ncreate\n mode and the basic logic to determine this is explained by the simple diagram below.\n\n\n\n\nThis process gets complicated when fault-tolerance (writing to temporary files)  and rotation is added.\n\n\nFollowing are few configuration items used for opening the streams:\n\n\n\n\nreplication\n: specifies the replication factor of the output files. \nDefault\n: \nfs.getDefaultReplication(new Path(filePath))\n\n\nfilePermission\n: specifies the permission of the output files. The permission is an octal number similar to that used by the Unix chmod command. \nDefault\n: 0777\n\n\n\n\nRemovalListener\n\n\nA \nGuava\n cache also allows specification of removal listener which can perform some operation when an entry is removed from the cache. Since \nstreamsCache\n is of limited size and also has time-based expiry enabled, it is imperative that when a stream is evicted from the cache it is closed properly. Therefore, we attach a removal listener to \nstreamsCache\n which closes the stream when it is evicted.\n\n\nsetup(OperatorContext context)\n\n\nDuring setup the following main tasks are performed:\n\n\n\n\nFileSystem instance is created.\n\n\nThe cache of streams is created.\n\n\nFiles are recovered (see Fault-tolerance section).\n\n\nStray part files are cleaned (see Automatic rotation section).\n\n\n\n\nprocessTuple(INPUT tuple)\n\n\nThe code snippet below highlights the basic steps of processing a tuple.\n\n\nprotected void processTuple(INPUT tuple)\n{  \n  //which file to write to is derived from the tuple.\n  String fileName = getFileName(tuple);  \n\n  //streamsCache is queried for the output stream. If the stream is already opened then it is returned immediately otherwise the cache loader creates one.\n  FilterOutputStream fsOutput = streamsCache.get(fileName).getFilterStream();\n\n  byte[] tupleBytes = getBytesForTuple(tuple);\n\n  fsOutput.write(tupleBytes);\n}\n\n\n\n\nendWindow()\n\n\nIt should be noted that while processing a tuple we do not flush the stream after every write. Since flushing is expensive it is done periodically for all the open streams in the operator's \nendWindow()\n.\n\n\nMap\nString, FSFilterStreamContext\n openStreams = streamsCache.asMap();\nfor (FSFilterStreamContext streamContext: openStreams.values()) {\n  ...\n  //this flushes the stream\n  streamContext.finalizeContext();\n  ...\n}\n\n\n\n\nFSFilterStreamContext\n will be explained with compression and encryption.\n\n\nteardown()\n\n\nWhen any operator in a DAG fails then the application master invokes \nteardown()\n for that operator and its downstream operators. In \nAbstractFileOutputOperator\n we have a bunch of open streams in the cache and the operator (acting as HDFS client) holds leases for all the corresponding files. It is important to release these leases for clean re-deployment. Therefore, we try to close all the open streams in \nteardown()\n.\n\n\nAutomatic rotation\n\n\nIn a streaming application where data is being continuously processed, when this output operator is used, data will be continuously written to an output file. The users may want to be able to take the data from time to time to use it, copy it out of Hadoop or do some other processing. Having all the data in a single file makes it difficult as the user needs to keep track of how much data has been read from the file each time so that the same data is not read again. Also users may already have processes and scripts in place that work with full files and not partial data from a file.\n\n\nTo help solve these problems the operator supports creating many smaller files instead of writing to just one big file. Data is written to a file and when some condition is met the file is finalized and data is written to a new file. This is called file rotation. The user can determine when the file gets rotated. Each of these files is called a part file as they contain portion of the data.\n\n\nPart filename\n\n\nThe filename for a part file is formed by using the original file name and the part number. The part number starts from 0 and is incremented each time a new part file created. The default filename has the format, assuming origfile represents the original filename and partnum represents the part number,\n\n\norigfile.partnum\n\n\nThis naming scheme can be changed by the user. It can be done so by overriding the following method\n\n\nprotected String getPartFileName(String fileName, int part)\n\n\n\n\nThis method is passed the original filename and part number as arguments and should return the part filename.\n\n\nMechanisms\n\n\nThe user has a couple of ways to specify when a file gets rotated. First is based on size and second on time. In the first case the files are limited by size and in the second they are rotated by time.\n\n\nSize Based\n\n\nWith size based rotation the user specifies a size limit. Once the size of the currently file reaches this limit the file is rotated. The size limit can be specified by setting the following property\n\n\nmaxLength\n\n\nLike any other property this can be set in Java application code or in the property file.\n\n\nTime Based\n\n\nIn time based rotation user specifies a time interval. This interval is specified as number of application windows. The files are rotated periodically once the specified number of application windows have elapsed. Since the interval is application window based it is not always exactly constant time. The interval can be specified using the following property\n\n\nrotationWindows\n\n\nsetup(OperatorContext context)\n\n\nWhen an operator is being started there may be stray part files and they need to be cleaned up. One common scenario, when these could be present, is in the case of failure, where a node running the operator failed and a previous instance of the operator was killed. This cleanup and other initial processing for the part files happens in the operator setup. The following diagram describes this process\n\n\n\n\nFault-tolerance\n\n\nThere are two issues that should be addressed in order to make the operator fault-tolerant:\n\n\n\n\n\n\nThe operator flushes data to the filesystem every application window. This implies that after a failure when the operator is re-deployed and tuples of a window are replayed, then duplicate data will be saved to the files. This is handled by recording how much the operator has written to each file every window in a state that is checkpointed and truncating files back to the recovery checkpoint after re-deployment.\n\n\n\n\n\n\nWhile writing to HDFS, if the operator gets killed and didn't have the opportunity to close a file, then later when it is redeployed it will attempt to truncate/restore that file. Restoring a file may fail because the lease that the previous process (operator instance before failure) had acquired from namenode to write to a file may still linger and therefore there can be exceptions in acquiring the lease again by the new process (operator instance after failure). This is handled by always writing data to temporary files and renaming these files to actual files when a file is finalized (closed) for writing, that is, we are sure that no more data will be written to it. The relevant configuration item is:  \n\n\n\n\nalwaysWriteToTmp\n: enables/disables writing to a temporary file. \nDefault\n: true.\n\n\n\n\nMost of the complexity in the code comes from making this operator fault-tolerant.\n\n\nCheckpointed states needed for fault-tolerance\n\n\n\n\n\n\nendOffsets\n: contains the size of each file as it is being updated by the operator. It helps the operator to restore a file during recovery in operator \nsetup(...)\n and is also used while loading a stream to find out if the operator has seen a file before.\n\n\n\n\n\n\nfileNameToTmpName\n: contains the name of the temporary file per actual file. It is needed because the name of a temporary file is random. They are named based on the timestamp when the stream is created. During recovery the operator needs to know the temp file which it was writing to and if it needs restoration then it creates a new temp file and updates this mapping.\n\n\n\n\n\n\nfinalizedFiles\n: contains set of files which were requested to be finalized per window id.\n\n\n\n\n\n\nfinalizedPart\n: contains the latest \npart\n of each file which was requested to be finalized.\n\n\n\n\n\n\nThe use of \nfinalizedFiles\n and \nfinalizedPart\n are explained in detail under \nrequestFinalize(...)\n method.\n\n\nRecovering files\n\n\nWhen the operator is re-deployed, it checks in its \nsetup(...)\n method if the state of a file which it has seen before the failure is consistent with the file's state on the file system, that is, the size of the file on the file system should match the size in the \nendOffsets\n. When it doesn't the operator truncates the file.\n\n\nFor example, let's say the operator wrote 100 bytes to test1.txt by the end of window 10. It wrote another 20 bytes by the end of window 12 but failed in window 13. When the operator gets re-deployed it is restored with window 10 (recovery checkpoint) state. In the previous run, by the end of window 10, the size of file on the filesystem was 100 bytes but now it is 120 bytes. Tuples for windows 11 and 12 are going to be replayed. Therefore, in order to avoid writing duplicates to test1.txt, the operator truncates the file to 100 bytes (size at the end of window 10) discarding the last 20 bytes.\n\n\nrequestFinalize(String fileName)\n\n\nWhen the operator is always writing to temporary files (in order to avoid HDFS Lease exceptions), then it is necessary to rename the temporary files to the actual files once it has been determined that the files are closed. This is refered to as \nfinalization\n of files and the method allows the user code to specify when a file is ready for finalization.\n\n\nIn this method, the requested file (or in the case of rotation \n all the file parts including the latest open part which have not yet been requested for finalization) are registered for finalization. Registration is basically adding the file names to \nfinalizedFiles\n state and updating \nfinalizedPart\n.\n\n\nThe process of \nfinalization\n of all the files which were requested till the window \nw\n is deferred till window \nw\n is committed. This is because until a window is committed it can be replayed after a failure which means that a file can be open for writing even after it was requested for finalization.\n\n\nWhen rotation is enabled, part files as and when they get completed are requested for finalization. However, when rotation is not enabled user code needs to invoke this method as the knowledge that when a file is closed is unknown to this abstract operator.", 
            "title": "File Output"
        }, 
        {
            "location": "/operators/file_output/#abstractfileoutputoperator", 
            "text": "The abstract file output operator in Apache Apex Malhar library    AbstractFileOutputOperator  writes streaming data to files. The main features of this operator are:   Persisting data to files.  Automatic rotation of files based on: \n  a. maximum length of a file. \n  b. time-based rotation where time is specified using a count of application windows.  Fault-tolerance.  Compression and encryption of data before it is persisted.   In this tutorial we will cover the details of the basic structure and implementation of all the above features in  AbstractFileOutputOperator . Configuration items related to each feature are discussed as they are introduced in the section of that feature.", 
            "title": "AbstractFileOutputOperator"
        }, 
        {
            "location": "/operators/file_output/#persisting-data-to-files", 
            "text": "The principal function of this operator is to persist tuples to files efficiently. These files are created under a specific directory on the file system. The relevant configuration item is:  filePath : path specifying the directory where files are written.  Different types of file system that are implementations of  org.apache.hadoop.fs.FileSystem  are supported. The file system instance which is used for creating streams is constructed from the  filePath  URI.  FileSystem.newInstance(new Path(filePath).toUri(), new Configuration())  Tuples may belong to different files therefore expensive IO operations like creating multiple output streams, flushing of data to disk, and closing streams are handled carefully.", 
            "title": "Persisting data to files"
        }, 
        {
            "location": "/operators/file_output/#ports", 
            "text": "input : the input port on which tuples to be persisted are received.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_output/#streamscache", 
            "text": "This transient state caches output streams per file in memory. The file to which the data is appended may change with incoming tuples. It will be highly inefficient to keep re-opening streams for a file just because tuples for that file are interleaved with tuples for another file. Therefore, the operator maintains a cache of limited size with open output streams.  streamsCache  is of type  com.google.common.cache.LoadingCache . A  LoadingCache  has an attached  CacheLoader  which is responsible to load value of a key when the key is not present in the cache. Details are explained here-  CachesExplained .  The operator constructs this cache in  setup(...) . It is built with the following configuration items:   maxOpenFiles : maximum size of the cache. The cache evicts entries that haven't been used recently when the cache size is approaching this limit.  Default : 100  expireStreamAfterAcessMillis : expires streams after the specified duration has passed since the stream was last accessed.  Default : value of attribute-  OperatorContext.SPIN_MILLIS .   An important point to note here is that the guava cache does not perform cleanup and evict values asynchronously, that is, instantly after a value expires. Instead, it performs small amounts of maintenance during write operations, or during occasional read operations if writes are rare.", 
            "title": "streamsCache"
        }, 
        {
            "location": "/operators/file_output/#cacheloader", 
            "text": "streamsCache  is created with a  CacheLoader  that opens an  FSDataOutputStream  for a file which is not in the cache. The output stream is opened in either  append  or  create  mode and the basic logic to determine this is explained by the simple diagram below.   This process gets complicated when fault-tolerance (writing to temporary files)  and rotation is added.  Following are few configuration items used for opening the streams:   replication : specifies the replication factor of the output files.  Default :  fs.getDefaultReplication(new Path(filePath))  filePermission : specifies the permission of the output files. The permission is an octal number similar to that used by the Unix chmod command.  Default : 0777", 
            "title": "CacheLoader"
        }, 
        {
            "location": "/operators/file_output/#removallistener", 
            "text": "A  Guava  cache also allows specification of removal listener which can perform some operation when an entry is removed from the cache. Since  streamsCache  is of limited size and also has time-based expiry enabled, it is imperative that when a stream is evicted from the cache it is closed properly. Therefore, we attach a removal listener to  streamsCache  which closes the stream when it is evicted.", 
            "title": "RemovalListener"
        }, 
        {
            "location": "/operators/file_output/#setupoperatorcontext-context", 
            "text": "During setup the following main tasks are performed:   FileSystem instance is created.  The cache of streams is created.  Files are recovered (see Fault-tolerance section).  Stray part files are cleaned (see Automatic rotation section).", 
            "title": "setup(OperatorContext context)"
        }, 
        {
            "location": "/operators/file_output/#automatic-rotation", 
            "text": "In a streaming application where data is being continuously processed, when this output operator is used, data will be continuously written to an output file. The users may want to be able to take the data from time to time to use it, copy it out of Hadoop or do some other processing. Having all the data in a single file makes it difficult as the user needs to keep track of how much data has been read from the file each time so that the same data is not read again. Also users may already have processes and scripts in place that work with full files and not partial data from a file.  To help solve these problems the operator supports creating many smaller files instead of writing to just one big file. Data is written to a file and when some condition is met the file is finalized and data is written to a new file. This is called file rotation. The user can determine when the file gets rotated. Each of these files is called a part file as they contain portion of the data.", 
            "title": "Automatic rotation"
        }, 
        {
            "location": "/operators/file_output/#part-filename", 
            "text": "The filename for a part file is formed by using the original file name and the part number. The part number starts from 0 and is incremented each time a new part file created. The default filename has the format, assuming origfile represents the original filename and partnum represents the part number,  origfile.partnum  This naming scheme can be changed by the user. It can be done so by overriding the following method  protected String getPartFileName(String fileName, int part)  This method is passed the original filename and part number as arguments and should return the part filename.", 
            "title": "Part filename"
        }, 
        {
            "location": "/operators/file_output/#mechanisms", 
            "text": "The user has a couple of ways to specify when a file gets rotated. First is based on size and second on time. In the first case the files are limited by size and in the second they are rotated by time.", 
            "title": "Mechanisms"
        }, 
        {
            "location": "/operators/file_output/#size-based", 
            "text": "With size based rotation the user specifies a size limit. Once the size of the currently file reaches this limit the file is rotated. The size limit can be specified by setting the following property  maxLength  Like any other property this can be set in Java application code or in the property file.", 
            "title": "Size Based"
        }, 
        {
            "location": "/operators/file_output/#time-based", 
            "text": "In time based rotation user specifies a time interval. This interval is specified as number of application windows. The files are rotated periodically once the specified number of application windows have elapsed. Since the interval is application window based it is not always exactly constant time. The interval can be specified using the following property  rotationWindows", 
            "title": "Time Based"
        }, 
        {
            "location": "/operators/file_output/#setupoperatorcontext-context_1", 
            "text": "When an operator is being started there may be stray part files and they need to be cleaned up. One common scenario, when these could be present, is in the case of failure, where a node running the operator failed and a previous instance of the operator was killed. This cleanup and other initial processing for the part files happens in the operator setup. The following diagram describes this process", 
            "title": "setup(OperatorContext context)"
        }, 
        {
            "location": "/operators/file_output/#fault-tolerance", 
            "text": "There are two issues that should be addressed in order to make the operator fault-tolerant:    The operator flushes data to the filesystem every application window. This implies that after a failure when the operator is re-deployed and tuples of a window are replayed, then duplicate data will be saved to the files. This is handled by recording how much the operator has written to each file every window in a state that is checkpointed and truncating files back to the recovery checkpoint after re-deployment.    While writing to HDFS, if the operator gets killed and didn't have the opportunity to close a file, then later when it is redeployed it will attempt to truncate/restore that file. Restoring a file may fail because the lease that the previous process (operator instance before failure) had acquired from namenode to write to a file may still linger and therefore there can be exceptions in acquiring the lease again by the new process (operator instance after failure). This is handled by always writing data to temporary files and renaming these files to actual files when a file is finalized (closed) for writing, that is, we are sure that no more data will be written to it. The relevant configuration item is:     alwaysWriteToTmp : enables/disables writing to a temporary file.  Default : true.   Most of the complexity in the code comes from making this operator fault-tolerant.", 
            "title": "Fault-tolerance"
        }, 
        {
            "location": "/operators/file_output/#checkpointed-states-needed-for-fault-tolerance", 
            "text": "endOffsets : contains the size of each file as it is being updated by the operator. It helps the operator to restore a file during recovery in operator  setup(...)  and is also used while loading a stream to find out if the operator has seen a file before.    fileNameToTmpName : contains the name of the temporary file per actual file. It is needed because the name of a temporary file is random. They are named based on the timestamp when the stream is created. During recovery the operator needs to know the temp file which it was writing to and if it needs restoration then it creates a new temp file and updates this mapping.    finalizedFiles : contains set of files which were requested to be finalized per window id.    finalizedPart : contains the latest  part  of each file which was requested to be finalized.    The use of  finalizedFiles  and  finalizedPart  are explained in detail under  requestFinalize(...)  method.", 
            "title": "Checkpointed states needed for fault-tolerance"
        }, 
        {
            "location": "/operators/file_output/#recovering-files", 
            "text": "When the operator is re-deployed, it checks in its  setup(...)  method if the state of a file which it has seen before the failure is consistent with the file's state on the file system, that is, the size of the file on the file system should match the size in the  endOffsets . When it doesn't the operator truncates the file.  For example, let's say the operator wrote 100 bytes to test1.txt by the end of window 10. It wrote another 20 bytes by the end of window 12 but failed in window 13. When the operator gets re-deployed it is restored with window 10 (recovery checkpoint) state. In the previous run, by the end of window 10, the size of file on the filesystem was 100 bytes but now it is 120 bytes. Tuples for windows 11 and 12 are going to be replayed. Therefore, in order to avoid writing duplicates to test1.txt, the operator truncates the file to 100 bytes (size at the end of window 10) discarding the last 20 bytes.", 
            "title": "Recovering files"
        }, 
        {
            "location": "/dtgateway_api/", 
            "text": "DataTorrent dtGateway API v2 Specification\n\n\nREST API\n\n\nReturn codes\n\n\n\n\n200\n: OK\n\n\n400\n: The request is not in the format that the server expects\n\n\n404\n: The resource is not found\n\n\n500\n: Something is wrong on the server side\n\n\n\n\nREST URI Specification\n\n\nGET /ws/v2/about\n\n\nFunction:\n\n\nReturn:\n\n\n{\n    \nbuildVersion\n: \n{buildVersion}\n,\n    \nbuildDate\n: \n{date and time}\n,\n    \nbuildRevision\n: \n{revision}\n,\n    \nbuildUser\n: \n{user}\n,\n    \nversion\n: \n{version}\n,\n    \ngatewayUser\n: \n{user}\n,\n    \njavaVersion\n: \n{java_version}\n,\n    \nhadoopLocation\n: \n{hadoop_location}\n,\n    \njvmName\n: \n{pid}@{hostname}\n,\n    \nconfigDirectory\n: \n{configDir}\n,\n    \nhostname\n: \n{hostname}\n,\n    \nhadoopIsSecurityEnabled\n: \n{true/false}\n\n}\n\n\n\n\nGET /ws/v2/cluster/metrics\n\n\nFunction: List metrics that are relevant to the entire cluster\n\n\nReturn:\n\n\n{\n    \naverageAge\n: \n{average running application age in milliseconds}\n,\n    \ncpuPercentage\n: \n{cpuPercentage}\n,\n    \ncurrentMemoryAllocatedMB\n: \n{currentMemoryAllocatedMB}\n,\n    \nmaxMemoryAllocatedMB\n: \n{maxMemoryAllocatedMB}\n,\n    \nnumAppsFailed\n: \n{numAppsFailed}\n,\n    \nnumAppsFinished\n: \n{numAppsFinished}\n,\n    \nnumAppsKilled\n: \n{numAppsKilled}\n,\n    \nnumAppsPending\n: \n{numAppsPending}\n,\n    \nnumAppsRunning\n: \n{numAppsRunning}\n,\n    \nnumAppsSubmitted\n: \n{numAppsSubmitted}\n,\n    \nnumContainers\n: \n{numContainers}\n,\n    \nnumOperators\n: \n{numOperators}\n,\n    \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n    \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n\n}\n\n\n\n\nGET /ws/v2/applications[?states={STATE_FILTER}\nname={NAME_FILTER}\nuser={USER_FILTER]\n\n\nFunction: List IDs of all streaming applications\n\n\nReturn:\n\n\n{\n    \napps\n: [\n        {\n            \ndiagnostics\n: \n{diagnostics}\n,\n            \nelapsedTime\n: \n{elapsedTime}\n,\n            \nfinalStatus\n: \n{finalStatus}\n,\n            \nfinishedTime\n: \n{finishedTime}\n,\n            \nid\n: \n{appId}\n,\n            \nname\n: \n{name}\n,\n            \nqueue\n: \n{queue}\n,\n            \nstartedTime\n: \n{startedTime}\n,\n            \nstate\n: \n{state}\n,\n            \ntrackingUrl\n: \n{trackingUrl}\n,\n            \nuser\n: \n{user}\n\n        },  \n        \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}\n\n\nFunction: Get the information for the specified application\n\n\nReturn:\n\n\n{\n    \nid\n: \n{appid}\n,\n    \nname\n: \n{name}\n,\n    \nstate\n: \n{state}\n,\n    \ntrackingUrl\n: \n{tracking url}\n,\n    \nfinalStatus\n: {finalStatus},\n    \nappPath\n: \n{appPath}\n,\n    \ngatewayAddress\n: \n{gatewayAddress}\n,\n    \nelapsedTime\n: \n{elapsedTime}\n,\n    \nstartedTime\n: \n{startTime}\n,\n    \nuser\n: \n{user}\n,\n    \nversion\n: \n{stram version}\n,\n    \nremainingLicensedMB\n: \n{remainingLicensedMB}\n,\n    \nallocatedMB\n: \n{allocatedMB}\n,\n    \ngatewayConnected\n: \ntrue/false\n,\n    \nconnectedToThisGateway\n: \ntrue/false\n,\n    \nattributes\n: {\n           \n{attributeName}\n: \n{attributeValue}\n, \n           \n{attributeName-n}\n: \n{attributeValue-n}\n, \n    },\n    \nstats\n: {\n        \nallocatedContainers\n: \n{allocatedContainer}\n,\n        \ntotalMemoryAllocated\n: \n{totalMemoryAllocated}\n,\n        \nlatency\n: \n{overall latency}\n,\n        \ncriticalPath\n: \n{list of operator id that represents the critical path}\n,\n        \nfailedContainers\n: \n{failedContainers}\n,\n        \nnumOperators\n: \n{numOperators}\n,\n        \nplannedContainers\n: \n{plannedContainers}\n,\n        \ncurrentWindowId\n: \n{min of operators:currentWindowId}\n,\n        \nrecoveryWindowId\n: \n{min of operators:recoveryWindowId}\n,\n        \ntuplesProcessedPSMA\n: \n{sum of operators:tuplesProcessedPSMA}\n,\n        \ntotalTuplesProcessed\n:\n{sum of operators:totalTuplesProcessed}\n,\n        \ntuplesEmittedPSMA\n:\n{sum of operators:tuplesEmittedPSMA}\n,\n        \ntotalTuplesEmitted\n:\n{sum of operators:totalTuplesEmitted}\n,\n        \ntotalBufferServerReadBytesPSMA\n: \n{totalBufferServerReadBytesPSMA}\n,\n        \ntotalBufferServerWriteBytesPSMA\n: \n{totalBufferServerWriteBytesPSMA}\n\n    }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan\n\n\nFunction: Return the physical plan for the given application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainer\n: \n{containerId}\n,\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \nports\n: [\n                {\n                    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n                    \nname\n: \n{name}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n                    \ntype\n: \ninput/output\n,\n                    \nrecordingStartTime\n: \n{recordingStartTime}\n\n                },\n                ...\n            ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: \n{status}\n,\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nlogicalName\n: \n{logicalName}\n,\n            \nisUnifier\n: true/false\n        },\n         \u2026\n     ],\n     \nstreams\n: [        \n        {\n            \nlogicalName\n: \n{logicalName}\n,\n            \nsinks\n: [\n                {\n                    \noperatorId\n: \n{operatorId}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorId\n: \n{operatorId}\n,\n                \nportName\n: \n{portName}\n\n            },\n            \nlocality\n: \n{locality}\n\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators\n\n\nFunction: Return list of operators for the given application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainer\n: \n{containerId}\n,\n            \ncounters\n: {\n                \n{counterName}\n: \n{counterValue}\n, \n                ...\n             },\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \nports\n: [\n                {\n                    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n                    \nname\n: \n{name}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n                    \ntype\n: \ninput/output\n,\n                    \nrecordingStartTime\n: \n{recordingStartTime}\n\n                },\n                ...\n            ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: \n{status}\n,\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nlogicalName\n: \n{logicalName}\n,\n            \nunifierClass\n: \n{unifierClass}\n\n        },\n         \u2026\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/streams\n\n\nFunction: Return physical streams\n\n\nReturn:\n\n\n{\n     \nstreams\n: [        \n        {\n            \nlogicalName\n: \n{logicalName}\n,\n            \nsinks\n: [\n                {\n                    \noperatorId\n: \n{operatorId}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorId\n: \n{operatorId}\n,\n                \nportName\n: \n{portName}\n\n            },\n            \nlocality\n: \n{locality}\n\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}\n\n\nFunction: Return information of the given operator for the given application\n\n\nReturn:\n\n\n{\n    \nclassName\n: \n{className}\n,\n    \ncontainer\n: \n{containerId}\n,\n    \ncounters\n: {\n      \n{counterName}: \n{counterValue}\n, ...            \n    }\n    \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n    \ncurrentWindowId\n: \n{currentWindowId}\n,\n    \nfailureCount\n: \n{failureCount}\n,\n    \nhost\n: \n{host}\n,\n    \nid\n: \n{id}\n,\n    \nports\n: [\n       {\n          \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n          \nname\n: \n{name}\n,\n          \ntotalTuples\n: \n{totalTuples}\n,\n          \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n          \ntype\n: \ninput/output\n\n       }, ...\n    ],\n    \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n    \nlatencyMA\n: \n{latencyMA}\n,\n    \nname\n: \n{name}\n,\n    \nrecordingStartTime\n: \n{recordingStartTime}\n,\n    \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n    \nstatus\n: \n{status}\n,\n    \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n    \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n    \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n    \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/deployHistory\n\n\nFunction: Return container deploy history of this operator\nSince: 1.0.6\n\n\nReturn:\n\n\n{\n   \ncontainers\n: [  \n        {  \n            \ncontainer\n: \n{containerId}\n,   \n            \nstartTime\n: \n{startTime}\n  \n        }, ...  \n    ],   \n    \nname\n: \n{operatorName}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports\n\n\nFunction: Get the information of all ports of the given operator of the\ngiven application\n\n\nReturn:\n\n\n{  \n    \nports\n: [\n        {  \n            \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,   \n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,  \n            \ntotalTuples\n: \n{totalTuples}\n,   \n            \ntuplesPSMA\n: \n{tuplesPSMA}\n,   \n            \ntype\n: \noutput\n  \n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}\n\n\nFunction: Get the information of a specified port\n\n\nReturn:\n\n\n{  \n    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,   \n    \nname\n: \n{name}\n,   \n    \ntotalTuples\n: \n{totalTuples}\n,   \n    \ntuplesPSMA\n: \n{tuplesPSMA}\n,   \n    \ntype\n: \n{type}\n  \n}\n\n\n\n\nGET /ws/v2/applications/{appid}/operatorClasses[?parent={parent}\nq={searchTerm}\npackagePrefixes={comma-separated-package-prefixes}]\n\n\nFunction: Get the classes of operators, if given the parent parameter,\nall classes that inherits from parent\n\n\nReturn:\n\n\n{  \n    \noperatorClasses\n: [  \n        { \nname\n:\n{className}\n },\n       \u2026\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/operatorClasses/{operatorClass}\n\n\nFunction: Get the description of the given operator class\n\n\nReturn:\n\n\n{\n    \ninputPorts\n: [\n        {\n            \nname\n: \n{name}\n,\n            \noptional\n: {boolean}\n        },\n          ...\n    ],\n    \noutputPorts\n: [\n        {\n            \nname\n: \n{name}\n,\n            \noptional\n: {boolean}\n        },\n        \u2026\n    ],\n    \nproperties\n: [  \n        {\n          \nname\n:\n{className}\n,\n          \ncanGet\n: {canGet},\n          \ncanSet\n: {canSet},\n          \ntype\n:\n{type}\n,\n          \ndescription\n:\n{description}\n,\n          \nproperties\n: ...\n        },\n       \u2026\n     ]\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/shutdown\n\n\nFunction: Shut down the application\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/kill\n\n\nFunction: Kill the given application\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/start\n\n\nFunction: Start recording on operator\n\n\nPayload (optional):\n\n\n{\n   \nnumWindows\n: {number of windows to record}  (if not given, the\nrecording goes on forever)\n}\n\n\n\n\nReturns:\n\n\n{\n    \nid\n: \n{recordingId}\n,\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/stop\n\n\nFunction: Stop recording on operator\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/start\n\n\nFunction: Start recording on port\n\n\nPayload (optional):\n\n\n{\n   \nnumWindows\n: {number of windows to record}  (if not given, the\nrecording goes on forever)\n}\n\n\n\n\nReturns:\n\n\n{\n    \nid\n: \n{recordingId}\n,\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/stop\n\n\nFunction: Stop recording on port\n\n\nPayload: none\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers[?states={NEW,ALLOCATED,ACTIVE,KILLED}]\n\n\nFunction: Return the list of containers for this application\n\n\nReturn:\n\n\n{\n    \ncontainers\n: [\n        {\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \njvmName\n: \n{jvmName}\n,\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nmemoryMBAllocated\n: \n{memoryMBAllocated}\n,\n            \nmemoryMBFree\n: \n{memoryMBFree}\n,\n            \nnumOperators\n: \n{numOperators}\n,\n            \ncontainerLogsUrl\n: \n{containerLogsUrl}\n,\n            \nstate\n: \n{state}\n\n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}\n\n\nFunction: Return the information of the specified container\n\n\nReturn:\n\n\n{\n    \nhost\n: \n{host}\n,\n    \nid\n: \n{id}\n,\n    \njvmName\n: \n{jvmName}\n,\n    \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n    \nmemoryMBAllocated\n: \n{memoryMBAllocated}\n,\n    \nmemoryMBFree\n: \n{memoryMBFree}\n,\n    \nnumOperators\n: \n{numOperators}\n,\n    \ncontainerLogsUrl\n: \n{containerLogsUrl}\n,\n    \nstate\n: \n{state}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs\n\n\nFunction: Return the container log list\n\n\nReturn:\n\n\n{\n    \nlogs\n: [\n        {\n            \nlength\n: \n{log length}\n,\n            \nname\n: \n{logName}\n\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs/{logName}[?start={startPos}\nend={endPos}\ngrep={regexp}\nincludeOffset={true/false}]\n\n\nFunction: Return the raw log\n\n\nReturn: if includeOffset=false or not provided, return raw log content\n(Content-Type: text/plain). Otherwise (Content-Type: application/json):\n\n\n{\n    \nlines\n: [\n        { \nbyteOffset\n:\n{byteOffset}\n, \nline\n: \n{line}\n }, \u2026\n     ]\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/kill\n\n\nFunction: Kill this container\n\n\nPayload: none\n\n\nGET /ws/v2/applications/{appid}/logicalPlan\n\n\nFunction: Return the logical plan of this application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n      {\n        \nname\n: \n{name}\n,\n        \nattributes\n: {attributeMap},\n        \nclass\n: \n{class}\n,\n        \nports\n: {\n           [\n            {\n                \nname\n: \n{name}\n,\n                \nattributes\n: {attributeMap},\n                \ntype\n: \ninput/output\n\n            }, ...\n           ]\n         },\n         \nproperties\n: {\n            \nclass\n: \n{class}\n\n         }\n      }, ...\n    ],\n    \nstreams\n: [\n        {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/attributes\n\n\nFunction: Return the application attributes\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators\n\n\nFunction: Return the list of info of the logical operator\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainerIds\n: [ \n{containerid}\n, \u2026 ],\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhosts\n: [ \n{host}\n, \u2026 ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \npartitions\n: [ \n{operatorid}\n, \u2026 ],\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: {\n                \n{state}\n: \n{number}\n, ...\n            },\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nunifiers\n: [ \n{operatorid}\n, \u2026 ],\n            \ncounters\n: {\n                 \n{counterName}: {\n                    \navg\n: \u2026, \nmax\n: \u2026, \nmin\n: \u2026, \nsum\n: ...\n                 }\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n\n\nFunction: Return the info of the logical operator\n\n\nReturn:\n\n\n{\n            \nclassName\n: \n{className}\n,\n            \ncontainerIds\n: [ \n{containerid}\n, \u2026 ],\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhosts\n: [ \n{host}\n, \u2026 ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \npartitions\n: [ \n{operatorid}\n, \u2026 ],\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: {\n                \n{state}\n: \n{number}\n, ...\n            },\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nunifiers\n: [ \n{operatorid}\n, \u2026 ],\n            \ncounters\n: {\n                 \n{counterName}: {\n                    \navg\n: \u2026, \nmax\n: \u2026, \nmin\n: \u2026, \nsum\n: ...\n                 }\n            }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties\n\n\nFunction: Return the properties of the logical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties\n\n\nFunction: Set the properties of the logical operator\nPayload:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties\n\n\nFunction: Return the properties of the physical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties\n\n\nFunction: Set the properties of the physical operator\nPayload:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/attributes\n\n\nFunction: Get the attributes of the logical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/ports/{portName}/attributes\n\n\nFunction:  Get the attributes of the port\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/logicalPlan\n\n\nFunction: Change logical plan of this application\nPayload:\n\n\n{\n    \nrequests\n: [\n        {\n            \nrequestType\n: \nAddStreamSinkRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n            \nsinkOperatorName\n: \n{sinkOperatorName}\n,\n            \nsinkOperatorPortName\n: \n{sinkOperatorPortName}\n\n        },\n        {\n            \nrequestType\n: \nCreateOperatorRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n            \noperatorFQCN\n: \n{operatorFQCN}\n,\n        },\n        {\n            \nrequestType\n: \nCreateStreamRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n            \nsourceOperatorName\n: \n{sourceOperatorName}\n,\n            \nsourceOperatorPortName\n: \n{sourceOperatorPortName}\n\n            \nsinkOperatorName\n: \n{sinkOperatorName}\n,\n            \nsinkOperatorPortName\n: \n{sinkOperatorPortName}\n\n        },\n        {\n            \nrequestType\n: \nRemoveOperatorRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n        },\n        {\n            \nrequestType\n: \nRemoveStreamRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n        },\n        {\n            \nrequestType\n: \nSetOperatorPropertyRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n            \npropertyName\n: \n{propertyName}\n,\n            \npropertyValue\n: \n{propertyValue}\n\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats/meta\n\n\nFunction: Return the meta information about the statistics stored for\nthis operator\n\n\nReturn:\n\n\n{\n    \nappId\n: \n{appId}\n,\n    \noperatorName\n: \n{operatorName}\n,\n    \noperatorIds\n: [ {opid}, \u2026 ],\n    \nstartTime\n: \n{startTime}\n,\n    \nendTime\n: \n{endTime}\n,\n    \ncount\n: \n{count}\n,\n    \nended\n: \n{boolean}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats?startTime={startTime}\nendTime={endTime}\n\n\nFunction: Return the statistics stored for this logical operator\n\n\n{\n    \noperatorStats\n: [\n        {\n            \noperatorId\n: \n{operatorId}\n,\n            \ntimestamp\n: \n{timestamp}\n,\n            \nstats\n: {\n                \ncontainer\n: \ncontainerId\n,\n                \nhost\n: \nhost\n,\n                \ntotalTuplesProcessed\n, \n{totalTuplesProcessed}\n,\n                \ntotalTuplesEmitted\n, \n{totalTuplesEmitted}\n,\n                \ntuplesProcessedPSMA\n, \n{tuplesProcessedPSMA}\n,\n                \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n                \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n                \nlatencyMA\n: \n{latencyMA}\n,\n                \nports\n: [ {\n                    \nname\n: \n{name}\n,\n                    \ntype\n:\n{input/output}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n, \n{tuplesPSMA}\n,\n                    \nbufferServerBytesPSMA\n, \n{bufferServerBytesPSMA}\n\n                }, \u2026 ],\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/stats/meta\n\n\nFunction: Return the meta information about the container statistics\n\n\n{\n    \nappId\n: \n{appId}\n,\n    \ncontainers\n: {\n        \n{containerId}\n: {\n            \nid\n: \n{id}\n,\n            \njvmName\n: \n{jvmName}\n,\n            \nhost\n: \n{host}\n,\n            \nmemoryMBAllocated\n, \n{memoryMBAllocated}\n\n        },\n        \u2026\n    },\n    \nstartTime\n: \n{startTime}\n\n    \nendTime\n: \n{endTime}\n\n    \ncount\n: \n{count}\n\n    \nended\n: {boolean}\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/stats?startTime={startTime}\nendTime={endTime}\n\n\nFunction: Return the container statistics stored for this application\n\n\n{\n    \ncontainerStats\n: [\n        {\n            \ncontainerId\n: \n{containerId}\n\n            \ntimestamp\n: \n{timestamp}\n\n            \nstats\n: {\n                \nnumOperators\n: \n{numOperators}\n,\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/recordings\n\n\nFunction: Get the list of all recordings for this application\n\n\nReturn:\n\n\n{\n    \nrecordings\n: [{\n        \nid\n: \n{id}\n,\n        \nstartTime\n: \n{startTime}\n,\n        \nappId\n: \n{appId}\n,\n        \noperatorId\n: \n{operatorId}\n,\n        \ncontainerId\n: \n{containerId}\n,\n        \ntotalTuples\n: \n{totalTuples}\n,\n        \nports\n: [ {\n            \nname\n: \n{portName}\n,\n            \nstreamName\n: \n{streamName}\n,\n            \ntype\n: \n{type}\n,\n            \nid\n: \n{index}\n,\n            \ntupleCount\n: \n{tupleCount}\n\n        } \u2026 ],\n        \nended\n: {boolean},\n        \nwindowIdRanges\n: [ {\n            \nlow\n: \n{lowId}\n,\n            \nhigh\n: \n{highId}\n\n        } \u2026 ],\n        \nproperties\n: {\n            \nname\n: \nvalue\n, ...\n        }\n    }, ...]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings\n\n\nFunction: Get the list of recordings on this operator\n\n\nReturn:\n\n\n{\n    \nrecordings\n: [ {\n        \nid\n: \n{id}\n,\n        \nstartTime\n: \n{startTime}\n,\n        \nappId\n: \n{appId}\n,\n        \noperatorId\n: \n{operatorId}\n,\n        \ncontainerId\n: \n{containerId}\n,\n        \ntotalTuples\n: \n{totalTuples}\n,\n        \nports\n: [ {\n            \nname\n: \n{portName}\n,\n            \nstreamName\n: \n{streamName}\n,\n            \ntype\n: \n{type}\n,\n            \nid\n: \n{index}\n,\n            \ntupleCount\n: \n{tupleCount}\n\n        } \u2026 ],\n        \nended\n: {boolean},\n        \nwindowIdRanges\n: [ {\n            \nlow\n: \n{lowId}\n,\n            \nhigh\n: \n{highId}\n\n        } \u2026 ],\n        \nproperties\n: {\n            \nname\n: \nvalue\n, ...\n        }\n    }, ...]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}\n\n\nFunction: Get the information about the recording\n\n\nReturn:\n\n\n{\n    \nid\n: \n{id}\n,\n    \nstartTime\n: \n{startTime}\n,\n    \nappId\n: \n{appId}\n,\n    \noperatorId\n: \n{operatorId}\n,\n    \ncontainerId\n: \n{containerId}\n,\n    \ntotalTuples\n: \n{totalTuples}\n,\n    \nports\n: [ {\n       \nname\n: \n{portName}\n,\n       \nstreamName\n: \n{streamName}\n,\n       \ntype\n: \n{type}\n,\n       \nid\n: \n{index}\n,\n       \ntupleCount\n: \n{tupleCount}\n\n     } \u2026 ],\n    \nended\n: {boolean},\n    \nwindowIdRanges\n: [ {\n       \nlow\n: \n{lowId}\n,\n       \nhigh\n: \n{highId}\n\n     } \u2026 ],\n    \nproperties\n: {\n       \nname\n: \nvalue\n, ...\n     }\n}\n\n\n\n\nDELETE /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}\n\n\nFunction: Deletes the specified recording\n\n\nSince: 1.0.4\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}/tuples\n\n\nQuery Parameters:\n\n\noffset\nstartWindow\nlimit\nports\nexecuteEmptyWindow\n\n\n\nFunction: Get the tuples\n\n\nReturn:\n\n\n{\n    \nstartOffset\n: \n{startOffset}\n,\n    \ntuples\n: [ {\n        \nwindowId\n: \n{windowId}\n,\n        \ntuples\n: [ {\n            \nportId\n: \n{portId}\n,\n            \ndata\n: \n{tupleData}\n\n        }, \u2026 ]\n    }, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/events?from={fromTime}\nto={toTime}\noffset={offset}\nlimit={limit}\n\n\nFunction: Get the events\n\n\nReturn:\n\n\n{\n    \nevents\n: [ {\n           \nid\n: \n{id}\n,\n        \ntimestamp\n: \n{timestamp}\n,\n        \ntype\n: \n{type}\n,\n        \ndata\n: {\n            \nname\n: \nvalue\n, \u2026\n        }\n    }, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/profile/user\n\n\nFunction: Get the user profile information, list of roles and list of\npermissions given the user\n\n\nReturn:\n\n\n{\n    \nauthScheme\n: \n{authScheme}\n,\n    \nuserName\n : \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \u2026 ],\n    \npermissions\n: [ \n{permission1}\n, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/profile/settings\n\n\nFunction: Get the current user's settings\n\n\nReturn:\n\n\n{\n    \n{key}\n: {value}, ...\n}\n\n\n\n\nGET /ws/v2/profile/settings/{user}\n\n\nFunction: Get the specified user's settings\n\n\nReturn:\n\n\n{\n    \n{key}\n: {value}, ...\n}\n\n\n\n\nGET /ws/v2/profile/settings/{user}/{key}\n\n\nFunction: Get the specified user's setting key\n\n\nReturn:\n\n\n{\n    \nvalue\n: {value}\n}\n\n\n\n\nPUT /ws/v2/profile/settings/{user}/{key}\n\n\nFunction: Set the specified user's setting key\nPayload:\n\n\n{\n    \nvalue\n: {value}\n}\n\n\n\n\nGET /ws/v2/auth/roles\n\n\nFunction: Get the list of roles the system has\n\n\nReturn:\n\n\n{\n    \nroles\n: [\n       {\n         \nname\n: \n{role1}\n,\n         \npermissions\n: [ \n{permission1}\n, \u2026 ]\n       }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/auth/roles/{role}\n\n\nFunction: Get the list of permissions given the role\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ \n{permissions1}\n, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/auth/roles/{role}\n\n\nFunction: create or edit the list of permissions given the role\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ \n{permissions1}\n, \u2026 ]\n}\n\n\n\n\nPOST /ws/v2/auth/restoreDefaultRoles\n\n\nFunction: Restores default roles\n\n\nDELETE /ws/v2/auth/roles/{role}\n\n\nFunction: delete the given role\n\n\nGET /ws/v2/auth/permissions\n\n\nFunction: Get the list of possible permissions\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ {\n       \nname\n: \n{permissionName}\n,\n       \nadminOnly\n: true/false\n    }, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/applications/{appid}/permissions\n\n\nFunction: Set the permissions details for this application\n\n\nPayload:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/permissions\n\n\nFunction: Get the permissions details for this application\n\n\nReturn:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{name}/permissions\n\n\nFunction: Set the permissions details for this application\n\n\nPayload:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{name}/permissions\n\n\nFunction: Get the permissions details for this application\n\n\nReturn:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nPOST /ws/v2/licenses\n\n\nFunction: Add a license to the registry or generate an eval license\n\n\nPayload: The license file content, if payload is empty, it will try to generate an eval license and return the info\n\n\nReturn:\n\n\n{\n  \nid\n: \n{licenseId}\n,\n  \nexpireTime\n: {unixTimeMillis},\n  \nnodesAllowed\n: {nodesAllowed},\n  \nmemoryMBAllowed\n: {memoryMBAllowed},\n  \ncontextType\n: \n{contextType}\n,\n  \ntype\n: \n{type}\n,\n  \nfeatures\n: [ \n{feature1}\n, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/licenses/current\n\n\nFunction: Get info on the current license\n\n\n{\n      \nid\n: \n{licenseId}\n,\n      \nexpireTime\n: {unixTimeMillis},\n      \nnodesAllowed\n: {nodesAllowed},\n      \nnodesUsed\n: {nodesUsed},\n      \nmemoryMBAllowed\n: {memoryMBAllowed},\n      \nmemoryMBUsed\n: {memoryMBUsed},\n      \ncontextType\n: \n{community|standard|enterprise}\n,\n      \ntype\n: \n{evaluation|non_production|production}\n\n      \nfeatures\n: [ \n{feature1}\n, \u2026 ], // for community, empty array\n      \ncurrent\n: true/false\n}\n\n\n\n\nGET /ws/v2/config/installMode\n\n\nFunction: returns the install mode\n\n\n{\n  \ninstallMode\n: \n{evaluation|community|app}\n,\n  \nappPackageName\n: \n{optionalAppPackageName}\n,\n  \nappPackageVersion\n: \n{optionalAppPackageVersion}\n\n}\n\n\n\n\nGET /ws/v2/config/properties/dt.phoneHome.enable\n\n\nFunction: returns the download type\n\n\n{\n  \nvalue\n: \ntrue/false\n\n}\n\n\n\n\nPUT /ws/v2/config/properties/dt.phoneHome.enable\n\n\nFunction:\n\n\n{\n  \nvalue\n: \ntrue/false\n\n}\n\n\n\n\nFeature List:  \n\n\n\n\nSYSTEM_APPS\n\n\nSYSTEM_ALERTS\n\n\nAPP_DATA_DASHBOARDS\n\n\nRUNTIME_DAG_CHANGE\n\n\nRUNTIME_PROPERTY_CHANGE\n\n\nAPP_CONTAINER_LOGS\n\n\nLOGGING_LEVELS\n\n\nAPP_DATA_TRACKER\n\n\nJAAS_LDAP_AUTH\n\n\nAPP_BUILDER\n\n\n\n\nGET /ws/v2/config/properties\n\n\nFunction: Returns list of properties from dt-site.xml.\n\n\nReturn:\n\n\n{\n    \n{name}\n: {\n        \nvalue\n: \n{PROPERTY_VALUE}\n,\n        \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n    }\n\n}\n\n\n\n\nGET /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Returns single property from dt-site.xml, specify by name\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n}\n\n\n\n\nPOST /ws/v2/config/properties\n\n\nFunction: Overwrites all specified properties in dt-site.xml\n\n\nPayload:\n\n\n{\n    \nproperties\n: [\n        {\n            \nname\n: \n{name}\n\n            \nvalue\n: \n{PROPERTY_VALUE}\n,\n            \nlocal\n: true/false,\n                    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n        }, \u2026\n    ]\n}\n\n\n\n\nPUT /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Overwrites or creates new property in dt-site.xml\nPayload:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n    \nlocal\n: true/false,\n    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n}\n\n\n\n\nDELETE /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Deletes a property from dt-site.xml. This may have to be\nrestricted to custom properties?\n\n\nGET /ws/v2/config/hadoopExecutable\n\n\nFunction: Returns the hadoop executable\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n}\n\n\n\n\nPUT /ws/v2/config/hadoopExecutable\n\n\nFunction: Sets the hadoop executable\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n}\n\n\n\n\nGET /ws/v2/config/issues\n\n\nFunction: Returns list of potential issues with environment\n\n\nReturn:\n\n\n{\n    \nissues\n: [\n        {\n            \nkey\n: \n{issueKey}\n,\n            \npropertyName\n: \n{PROPERTY_NAME}\n,\n            \ndescription\n: \n{ISSUE_DESCRIPTION}\n,\n            \nseverity\n: \nerror\n|\nwarning\n\n        },\n        {...},\n        {...}\n    ]    \n}\n\n\n\n\nGET /ws/v2/config/ipAddresses\n\n\nFunction: Returns list of ip addresses the gateway can listen to\n\n\nReturn:\n\n\n{\n    \nipAddresses\n: [\n      \n1.2.3.4\n, ...\n    ]    \n}\n\n\n\n\nPOST /ws/v2/config/restart\n\n\nFunction: Restarts the gateway\n\n\nPayload: none\n\n\nGET /proxy/rm/v1/\u2026\n\n\nPOST /proxy/rm/v1/\u2026\n\n\nFunction: Proxy calls to resource manager of Hadoop.  Only works for GET and POST calls.\n\n\nGET /proxy/stram/v2/...\n\n\nPOST /proxy/stram/v2/\u2026\n\n\nPUT /proxy/stram/v2/\u2026\n\n\nDELETE /proxy/stram/v2/\u2026\n\n\nFunction: Proxy calls to Stram Web Services.\n\n\nPOST /ws/v2/applications/{appid}/loggers\n\n\nFunction: Set the logger levels of packages/classes.\n\n\nPayload:\n\n\n{\n    \nloggers\n : [\n        {\n            \nlogLevel\n: value,\n            \ntarget\n: value\n        }, \n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/loggers\n\n\nFunction: Gets the logger levels of packages/classes.\n\n\nReturn:\n\n\n{\n    \nloggers\n : [\n        {\n            \nlogLevel\n: value,\n            \ntarget\n: value\n        }, \n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/loggers/search?pattern=\"{pattern}\"\n\n\nFunction: searches for all classes that match the pattern.\n\n\nReturn:\n\n\n{\n    \nloggers\n : [\n        {\n            \nname\n : \n{fully qualified class name}\n,\n            \nlevel\n: \n{logger level}\n\n        }\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of appPackages the user can view in the system\n\n\n{\n    \nappPackages\n: [\n        {\n                 \nappPackageName\n: \n{appPackageName}\n,\n                 \nappPackageVersion\n: \n{appPackageVersion}\n,\n            \nmodificationTime\n: \n{modificationTime}\n,\n            \nowner\n: \n{owner}\n,\n        }, ...\n    ]\n}\n\n\n\n\nPOST /ws/v2/appPackages?merge={replace|fail|ours|theirs}\n\n\nSince: 1.0.4\n\n\nFunction: Uploads an appPackage file, merge with existing app package if exists. Default is replace.\nmerge parameter:\n  replace - replace existing app package with the new app package without merging\n  fail - return error if there is an existing app package already with the same owner and name and version\n  ours - merge, for files existing in both existing and new app packages, use the file in the new package\n  theirs - merge, for files existing in both existing and new app packages, use the file in the existing package\n\n\nPayload: the raw zip file\n\n\nReturn: The information of the app package\n\n\nGET /ws/v2/appPackages/{owner}/{name}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of versions of appPackages with the given name in the system owned by the specified user\n\n\n{\n    \nversions\n: [\n        \n1.0-SNAPSHOT\n\n    ]\n}\n\n\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}\n\n\nSince: 1.0.4\n\n\nFunction: Deletes the appPackage\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/download\n\n\nSince: 1.0.4\n\n\nFunction: Downloads the appPackage zip file\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the meta information of the app package\n\n\nReturns:\n\n\n{\n    \nappPackageName\n: \n{appPackageName}\n,\n    \nappPackageVersion\n: \n{appPackageVersion}\n,\n    \nmodificationTime\n:  \n{modificationTime}\n,\n    \nowner\n: \n{owner}\n,\n    ...\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs\n\n\nSince: 1.0.4\nFunction: Gets the list of configurations of the app package\nReturns:\n\n\n{\n    \nconfigs\n: [\n        \nmy-app-conf1.xml\n\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the properties XML of the specified config\n\n\nReturns:\n\n\nconfiguration\n\n        \nproperty\n\n                \nname\n...\n/name\n\n                \nvalue\n...\n/value\n\n        \n/property\n\n        \u2026\n\n/configuration\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Creates or replaces the specified config with the property parameters specified payload\n\n\nPayload: configuration in XML\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Deletes the specified config\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of applications in the appPackage\n\n\nReturns:\n\n\n{\n    \napplications\n: [\n        {\n            \ndag\n: {dag in json format},\n            \nfile\n: \n{fileName}\n,\n            \nname\n: \n{name}\n,\n            \ntype\n: \n{type}\n,\n            \nerror\n: \n{error}\n,\n            \nfileContent\n: {originalFileContentForJSONTypeApp}\n        }\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the meta data for that application\n\n\nReturns:\n\n\n{\n    \nfile\n: \n{fileName}\n,\n    \nname\n: \n{name}\n,\n    \ntype\n: \n{json/class/properties}\n,\n    \nerror\n: \n{error}\n\n    \ndag\n: {\n        \noperators\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nattributes\n:  {\n                \n{attributeKey}\n: \n{attributeValue}\n, ...\n            },\n            \nclass\n: \n{class}\n,\n            \nports\n: [\n                  {\n                    \nname\n: \n{name}\n,\n                    \nattributes\n:  {\n                       \n{attributeKey}\n: \n{attributeValue}\n, ...\n                     },\n                  }, ...\n            ],\n            \nproperties\n: {\n               \n{propertyName}\n: \n{propertyValue}\n\n            }\n         }, ...\n        ],\n        \nstreams\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n          }, ...\n        ]\n    },\n    \nfileContent\n: {originalFileContentForJSONTypeApp}\n}\n\n\n\n\nPOST /ws/v2/appPackages/{user}/{appPackageName}/{appPackageVersion}/merge\n\n\nFunction: Merge the configuration, json apps, and resources files from the version specified from the payload to the specified app package in the url, without overwriting any existing file in the specified app package\n\n\nPayload:\n\n\n{\n \nversion\n: \n{versionToMergeFrom}\n\n}\n\n\n\n\nPOST /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}/launch[?config={configName}\noriginalAppId={originalAppId}\nqueue={queueName}]\n\n\nSince: 1.0.4\n\n\nFunction: Launches the application with the given configuration specified in the POST payload\n\n\nPayload:\n\n\n{\n    \n{propertyName}\n : \n{propertyValue}\n, ...\n}\n\n\n\n\nReturn:\n\n\n{\n    \nappId\n: \n{appId}\n\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators/{classname}\n\n\nSince: 1.0.4\n\n\nFunction: Get the properties of the operator given the classname in the jar\n\n\n{  \n    \nproperties\n: [  \n        {\n          \nname\n:\n{className}\n,\n          \ncanGet\n: {canGet},\n          \ncanSet\n: {canSet},\n          \ntype\n:\n{type}\n,\n          \ndescription\n:\n{description}\n,\n          \nproperties\n: ...\n        },\n       \u2026\n     ]\n}\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}[?errorIfExists={true/false}]\n\n\nFunction: Creates or Replaces an application using json. Note that \"ports\" are only needed if you need to specify port attributes.  If errorIfExists is true, it returns an error if the application with the same name already exists in the app ackage\n\n\nPayload:\n\n\n{\n        \ndisplayName\n: \n{displayName}\n,\n        \ndescription\n: \n{description}\n,\n        \noperators\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nattributes\n:  {\n                \n{attributeKey}\n: \n{attributeValue}\n, ...\n            },\n            \nclass\n: \n{class}\n,\n            \nports\n: [\n                  {\n                    \nname\n: \n{name}\n,\n                    \nattributes\n:  {\n                       \n{attributeKey}\n: \n{attributeValue}\n, ...\n                     },\n                  }, ...\n            ],\n            \nproperties\n: {\n               \n{propertyName}\n: \n{propertyValue}\n\n            }\n          }, ...\n        ],\n        \nstreams\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n          }, ...\n        ]\n}\n\n\n\n\nReturn:\n\n\n{\n        \nerror\n: \n{error}\n\n}\n\n\n\n\nAvailable port attributes to set: \n\n\n\n\nAUTO_RECORD\n\n\nIS_OUTPUT_UNIFIED\n\n\nPARTITION_PARALLEL\n\n\nQUEUE_CAPACITY\n\n\nSPIN_MILLIS\n\n\nSTREAM_CODEC\n\n\nUNIFIER_LIMIT\n\n\n\n\nAvailable locality options to set: \n\n\n\n\nTHREAD_LOCAL\n\n\nCONTAINER_LOCAL\n\n\nNODE_LOCAL\n\n\nRACK_LOCAL\n\n\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}\n\n\nSince: 1.0.5\n\n\nFunction: Deletes non-jar based application in the app package\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators\n\n\nSince: 1.0.5\n\n\nFunction: Get the classes of operators from specified app package.\n\n\nReturn:\n\n\n{  \n    \noperatorClasses\n: [  \n        {\n            \nname\n:\n{fullyQualifiedClassName}\n, \n            \ntitle\n: \n{title}\n,\n            \nshortDesc\n: \n{description}\n,\n            \nlongDesc\n: \n{description}\n,\n            \ncategory\n: \n{categoryName}\n,\n            \ndoclink\n: \n{doc url}\n,\n            \ntags\n: [ \n{tag}\n, \n{tag}\n, \u2026 ],\n            \ninputPorts\n: [\n                {\n                    \nname\n: \n{portName}\n,\n                    \ntype\n: \n{tupleType}\n,\n                    \noptional\n: true/false  \n                }, \u2026\n            ]\n            \noutputPorts\n: [\n                {\n                    \nname\n: \n{portName}\n,\n                    \ntype\n: \n{tupleType}\n,\n                    \noptional\n: true/false  \n                }, \u2026\n            ],\n            \nproperties\n: [  \n                {\n                    \nname\n:\n{propertyName}\n,\n                    \ncanGet\n: {canGet},\n                    \ncanSet\n: {canSet},\n                    \ntype\n:\n{type}\n,\n                    \ndescription\n:\n{description}\n,\n                    \nproperties\n: ...\n                }, \u2026\n            ],\n            \ndefaultValue\n: {\n                \n{propertyName}\n: [VALUE], // type depends on property\n                ...\n            }\n\n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/import\n\n\nFunction: List the importable app packages on Gateway's local file\nsystem\n\n\nReturn:\n\n\n{\n    \nappPackages: [\n        {\n            \nfile\n: \n{file}\n,\n            \nname\n: \n{name}\n,\n            \ndisplayName\n: \n{displayName}\n,\n            \nversion\n: \n{version}\n,\n            \ndescription\n: \n{description}\n\n        }\n    ]\n}\n\n\n\n\nPOST /ws/v2/appPackages/import\n\n\nFunction: Import app package from Gateway's local file system\n\n\nPayload:\n\n\n{\n        \nfiles\n: [\n{file}\n, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Creates or replaces the specified system alert. The condition has access to an object in its scope called \n_topic\n. An example alert might take the form of the following:\n\n\n_topic[\"applications.application_1400294100000_0001\"].allocatedContainers \n 5\n\n\n\nPayload:\n\n\n{\n        \ncondition\n:\n{condition in javascript}\n,\n        \nemail\n:\n{email}\n,\n        \ntimeThresholdMillis\n:\n{time}\n\n}\n\n\n\n\nDELETE /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Deletes the specified system alert\n\n\nGET /ws/v2/systemAlerts/alerts?inAlert={true/false}\n\n\nFunction: Gets the created alerts\n\n\nReturn:\n\n\n{\n    \nalerts\n: [{\n        \nname\n: \n{alertName}\n,\n        \ncondition\n:\n{condition in javascript}\n,\n        \nemail\n:\n{email}\n,\n        \ntimeThresholdMillis\n:\n{time}\n,\n        \nalertStatus\n: {\n            \nisInAlert\n:{true/false}\n            \ninTime\n: \n{time}\n,\n            \nmessage\n: \n{message}\n,\n            \nemailSent\n: {true/false}\n        }\n    }, \u2026  ]\n}\n\n\n\n\nGET /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Gets the specified system alert\n\n\nReturn:\n\n\n{\n    \nname\n: \n{alertName}\n,\n    \ncondition\n:\n{condition in javascript}\n,        \n    \nemail\n:\n{email}\n,\n    \ntimeThresholdMillis\n:\n{time}\n,\n    \nalertStatus\n: {\n        \nisInAlert\n:{true/false}\n        \ninTime\n: \n{time}\n,\n        \nmessage\n: \n{message}\n,\n        \nemailSent\n: {true/false}\n    }\n}\n\n\n\n\nGET /ws/v2/systemAlerts/history\n\n\nFunction: Gets the history of alerts\n\n\nReturn:\n\n\n{\n    \nhistory\n: [\n        {\n            \nname\n:\n{alertName}\n,\n            \ninTime\n:\n{time}\n,\n            \noutTime\n: \n{time}\n,\n            \nmessage\n: \n{message}\n,\n            \nemailSent\n: {true/false}\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/systemAlerts/topicData\n\n\nFunction: Gets the topic data that is used for evaluating alert\ncondition\n\n\nReturn:\n\n\n{\n     \n{topicName}\n: {json object data}, ...\n}\n\n\n\n\nGET /ws/v2/auth/users/{user}\n\n\nFunction: Gets the info of the given user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nPOST /ws/v2/auth/users/{user}\n\n\nFunction: Changes password and/or roles of the given user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \noldPassword\n: \n{oldPassword}\n,\n    \nnewPassword\n: \n{newPassword}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nPUT /ws/v2/auth/users/{user}\n\n\nFunction: Creates new user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \npassword\n: \n{password}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nDELETE /ws/v2/auth/users/{user}\n\n\nFunction: Deletes the specified user\n\n\nGET /ws/v2/auth/users\n\n\nFunction: Gets the list of users\n\n\nReturn:\n\n\n{\n    \nusers\n: [ {\n       \nuserName\n: \n{username1}\n,\n       \nroles\n: [ \n{role1}\n, \u2026 ],\n       \npermissions\n: [ \n{permission1}\n, \u2026 ]\n    }\n}\n\n\n\n\nPOST /ws/v2/login\n\n\nFunction: Login\nPayload:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \npassword\n: \n{password}\n\n}\n\n\n\n\nReturn:\n\n\n{\n    \nauthScheme\n: \n{authScheme}\n,\n    \nuserName\n : \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \u2026 ],\n    \npermissions\n: [ \n{permission1}\n, \u2026 ]\n}\n\n\n\n\nPOST /ws/v2/logout\n\n\nFunction: Log out the current user\n\n\nReturn:\n\n\n{\n}\n\n\n\n\nPublisher-Subscriber WebSocket Protocol\n\n\nInput\n\n\nPublishing\n\n\n{\"type\":\"publish\", \"topic\":\"{topic}\", \"data\":{data}}\n\n\n\nSubscribing\n\n\n{\"type\":\"subscribe\", \"topic\":\"{topic}\"}\n\n\n\nUnsubscribing\n\n\n{\"type\":\"unsubscribe\", \"topic\":\"{topic}\"}\n\n\n\nSubscribing to the number of subscribers of a topic\n\n\n{\"type\":\"subscribeNumSubscribers\", \"topic\":\"{topic}\"}\n\n\n\nSubscribing to the number of subscribers of a topic\n\n\n{\"type\":\"unsubscribeNumSubscribers\", \"topic\":\"{topic}\"}\n\n\n\nOutput\n\n\nNormal Published Data\n\n\n{\"type\":\"data\", \"topic\":\"{topic}\", \"data\":{data}}\n\n\n\nNumber of Subscribers:\n\n\n{\"type\":\"data\", \"topic\":\"{topic}.numSubscribers\", \"data\":{data}}\n\n\n\nAuto publish topics\n\n\ndata that gets published every one second:\n\n\n\n\napplications\n - list of streaming applications running in the cluster\n\n\napplications.[appid]\n - information about a particular application\n\n\napplications.[appid].containers\n - information about containers of a particular application\n\n\napplications.[appid].physicalOperators\n - information about operators of a particular application\n\n\napplications.[appid].logicalOperators\n - information about logical operators of a particular application\n\n\napplications.[appid].events\n - events from the AM of a particularapplication\n\n\n\n\ndata that gets published every five seconds:\n\n\n\n\ncluster.metrics\n - metrics of the cluster", 
            "title": "REST API"
        }, 
        {
            "location": "/dtgateway_api/#datatorrent-dtgateway-api-v2-specification", 
            "text": "", 
            "title": "DataTorrent dtGateway API v2 Specification"
        }, 
        {
            "location": "/dtgateway_api/#rest-api", 
            "text": "", 
            "title": "REST API"
        }, 
        {
            "location": "/dtgateway_api/#return-codes", 
            "text": "200 : OK  400 : The request is not in the format that the server expects  404 : The resource is not found  500 : Something is wrong on the server side", 
            "title": "Return codes"
        }, 
        {
            "location": "/dtgateway_api/#rest-uri-specification", 
            "text": "", 
            "title": "REST URI Specification"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2about", 
            "text": "Function:  Return:  {\n     buildVersion :  {buildVersion} ,\n     buildDate :  {date and time} ,\n     buildRevision :  {revision} ,\n     buildUser :  {user} ,\n     version :  {version} ,\n     gatewayUser :  {user} ,\n     javaVersion :  {java_version} ,\n     hadoopLocation :  {hadoop_location} ,\n     jvmName :  {pid}@{hostname} ,\n     configDirectory :  {configDir} ,\n     hostname :  {hostname} ,\n     hadoopIsSecurityEnabled :  {true/false} \n}", 
            "title": "GET /ws/v2/about"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2clustermetrics", 
            "text": "Function: List metrics that are relevant to the entire cluster  Return:  {\n     averageAge :  {average running application age in milliseconds} ,\n     cpuPercentage :  {cpuPercentage} ,\n     currentMemoryAllocatedMB :  {currentMemoryAllocatedMB} ,\n     maxMemoryAllocatedMB :  {maxMemoryAllocatedMB} ,\n     numAppsFailed :  {numAppsFailed} ,\n     numAppsFinished :  {numAppsFinished} ,\n     numAppsKilled :  {numAppsKilled} ,\n     numAppsPending :  {numAppsPending} ,\n     numAppsRunning :  {numAppsRunning} ,\n     numAppsSubmitted :  {numAppsSubmitted} ,\n     numContainers :  {numContainers} ,\n     numOperators :  {numOperators} ,\n     tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n     tuplesProcessedPSMA :  {tuplesProcessedPSMA} \n}", 
            "title": "GET /ws/v2/cluster/metrics"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsstatesstate_filternamename_filteruseruser_filter", 
            "text": "Function: List IDs of all streaming applications  Return:  {\n     apps : [\n        {\n             diagnostics :  {diagnostics} ,\n             elapsedTime :  {elapsedTime} ,\n             finalStatus :  {finalStatus} ,\n             finishedTime :  {finishedTime} ,\n             id :  {appId} ,\n             name :  {name} ,\n             queue :  {queue} ,\n             startedTime :  {startedTime} ,\n             state :  {state} ,\n             trackingUrl :  {trackingUrl} ,\n             user :  {user} \n        },  \n        \u2026\n    ]\n}", 
            "title": "GET /ws/v2/applications[?states={STATE_FILTER}&amp;name={NAME_FILTER}&amp;user={USER_FILTER]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappid", 
            "text": "Function: Get the information for the specified application  Return:  {\n     id :  {appid} ,\n     name :  {name} ,\n     state :  {state} ,\n     trackingUrl :  {tracking url} ,\n     finalStatus : {finalStatus},\n     appPath :  {appPath} ,\n     gatewayAddress :  {gatewayAddress} ,\n     elapsedTime :  {elapsedTime} ,\n     startedTime :  {startTime} ,\n     user :  {user} ,\n     version :  {stram version} ,\n     remainingLicensedMB :  {remainingLicensedMB} ,\n     allocatedMB :  {allocatedMB} ,\n     gatewayConnected :  true/false ,\n     connectedToThisGateway :  true/false ,\n     attributes : {\n            {attributeName} :  {attributeValue} , \n            {attributeName-n} :  {attributeValue-n} , \n    },\n     stats : {\n         allocatedContainers :  {allocatedContainer} ,\n         totalMemoryAllocated :  {totalMemoryAllocated} ,\n         latency :  {overall latency} ,\n         criticalPath :  {list of operator id that represents the critical path} ,\n         failedContainers :  {failedContainers} ,\n         numOperators :  {numOperators} ,\n         plannedContainers :  {plannedContainers} ,\n         currentWindowId :  {min of operators:currentWindowId} ,\n         recoveryWindowId :  {min of operators:recoveryWindowId} ,\n         tuplesProcessedPSMA :  {sum of operators:tuplesProcessedPSMA} ,\n         totalTuplesProcessed : {sum of operators:totalTuplesProcessed} ,\n         tuplesEmittedPSMA : {sum of operators:tuplesEmittedPSMA} ,\n         totalTuplesEmitted : {sum of operators:totalTuplesEmitted} ,\n         totalBufferServerReadBytesPSMA :  {totalBufferServerReadBytesPSMA} ,\n         totalBufferServerWriteBytesPSMA :  {totalBufferServerWriteBytesPSMA} \n    }\n}", 
            "title": "GET /ws/v2/applications/{appid}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplan", 
            "text": "Function: Return the physical plan for the given application  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             container :  {containerId} ,\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             host :  {host} ,\n             id :  {id} ,\n             ports : [\n                {\n                     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n                     name :  {name} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA :  {tuplesPSMA} ,\n                     type :  input/output ,\n                     recordingStartTime :  {recordingStartTime} \n                },\n                ...\n            ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,\n             recoveryWindowId :  {recoveryWindowId} ,\n             status :  {status} ,\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             logicalName :  {logicalName} ,\n             isUnifier : true/false\n        },\n         \u2026\n     ],\n      streams : [        \n        {\n             logicalName :  {logicalName} ,\n             sinks : [\n                {\n                     operatorId :  {operatorId} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorId :  {operatorId} ,\n                 portName :  {portName} \n            },\n             locality :  {locality} \n        }, ...\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperators", 
            "text": "Function: Return list of operators for the given application  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             container :  {containerId} ,\n             counters : {\n                 {counterName} :  {counterValue} , \n                ...\n             },\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             host :  {host} ,\n             id :  {id} ,\n             ports : [\n                {\n                     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n                     name :  {name} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA :  {tuplesPSMA} ,\n                     type :  input/output ,\n                     recordingStartTime :  {recordingStartTime} \n                },\n                ...\n            ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,\n             recoveryWindowId :  {recoveryWindowId} ,\n             status :  {status} ,\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             logicalName :  {logicalName} ,\n             unifierClass :  {unifierClass} \n        },\n         \u2026\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanstreams", 
            "text": "Function: Return physical streams  Return:  {\n      streams : [        \n        {\n             logicalName :  {logicalName} ,\n             sinks : [\n                {\n                     operatorId :  {operatorId} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorId :  {operatorId} ,\n                 portName :  {portName} \n            },\n             locality :  {locality} \n        }, ...\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/streams"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopid", 
            "text": "Function: Return information of the given operator for the given application  Return:  {\n     className :  {className} ,\n     container :  {containerId} ,\n     counters : {\n       {counterName}:  {counterValue} , ...            \n    }\n     cpuPercentageMA :  {cpuPercentageMA} ,\n     currentWindowId :  {currentWindowId} ,\n     failureCount :  {failureCount} ,\n     host :  {host} ,\n     id :  {id} ,\n     ports : [\n       {\n           bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n           name :  {name} ,\n           totalTuples :  {totalTuples} ,\n           tuplesPSMA :  {tuplesPSMA} ,\n           type :  input/output \n       }, ...\n    ],\n     lastHeartbeat :  {lastHeartbeat} ,\n     latencyMA :  {latencyMA} ,\n     name :  {name} ,\n     recordingStartTime :  {recordingStartTime} ,\n     recoveryWindowId :  {recoveryWindowId} ,\n     status :  {status} ,\n     totalTuplesEmitted :  {totalTuplesEmitted} ,\n     totalTuplesProcessed :  {totalTuplesProcessed} ,\n     tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n     tuplesProcessedPSMA :  {tuplesProcessedPSMA} \n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopiddeployhistory", 
            "text": "Function: Return container deploy history of this operator\nSince: 1.0.6  Return:  {\n    containers : [  \n        {  \n             container :  {containerId} ,   \n             startTime :  {startTime}   \n        }, ...  \n    ],   \n     name :  {operatorName} \n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/deployHistory"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidports", 
            "text": "Function: Get the information of all ports of the given operator of the\ngiven application  Return:  {  \n     ports : [\n        {  \n             bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,   \n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,  \n             totalTuples :  {totalTuples} ,   \n             tuplesPSMA :  {tuplesPSMA} ,   \n             type :  output   \n        }, \u2026\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidportsportname", 
            "text": "Function: Get the information of a specified port  Return:  {  \n     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,   \n     name :  {name} ,   \n     totalTuples :  {totalTuples} ,   \n     tuplesPSMA :  {tuplesPSMA} ,   \n     type :  {type}   \n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidoperatorclassesparentparentqsearchtermpackageprefixescomma-separated-package-prefixes", 
            "text": "Function: Get the classes of operators, if given the parent parameter,\nall classes that inherits from parent  Return:  {  \n     operatorClasses : [  \n        {  name : {className}  },\n       \u2026\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/operatorClasses[?parent={parent}&amp;q={searchTerm}&amp;packagePrefixes={comma-separated-package-prefixes}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidoperatorclassesoperatorclass", 
            "text": "Function: Get the description of the given operator class  Return:  {\n     inputPorts : [\n        {\n             name :  {name} ,\n             optional : {boolean}\n        },\n          ...\n    ],\n     outputPorts : [\n        {\n             name :  {name} ,\n             optional : {boolean}\n        },\n        \u2026\n    ],\n     properties : [  \n        {\n           name : {className} ,\n           canGet : {canGet},\n           canSet : {canSet},\n           type : {type} ,\n           description : {description} ,\n           properties : ...\n        },\n       \u2026\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/operatorClasses/{operatorClass}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidshutdown", 
            "text": "Function: Shut down the application  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/shutdown"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidkill", 
            "text": "Function: Kill the given application  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/kill"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidrecordingsstart", 
            "text": "Function: Start recording on operator  Payload (optional):  {\n    numWindows : {number of windows to record}  (if not given, the\nrecording goes on forever)\n}  Returns:  {\n     id :  {recordingId} ,\n}", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/start"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidrecordingsstop", 
            "text": "Function: Stop recording on operator  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/stop"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidportsportnamerecordingsstart", 
            "text": "Function: Start recording on port  Payload (optional):  {\n    numWindows : {number of windows to record}  (if not given, the\nrecording goes on forever)\n}  Returns:  {\n     id :  {recordingId} ,\n}", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/start"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidportsportnamerecordingsstop", 
            "text": "Function: Stop recording on port  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/stop"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainersstatesnewallocatedactivekilled", 
            "text": "Function: Return the list of containers for this application  Return:  {\n     containers : [\n        {\n             host :  {host} ,\n             id :  {id} ,\n             jvmName :  {jvmName} ,\n             lastHeartbeat :  {lastHeartbeat} ,\n             memoryMBAllocated :  {memoryMBAllocated} ,\n             memoryMBFree :  {memoryMBFree} ,\n             numOperators :  {numOperators} ,\n             containerLogsUrl :  {containerLogsUrl} ,\n             state :  {state} \n        }, \u2026\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers[?states={NEW,ALLOCATED,ACTIVE,KILLED}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerscontainerid", 
            "text": "Function: Return the information of the specified container  Return:  {\n     host :  {host} ,\n     id :  {id} ,\n     jvmName :  {jvmName} ,\n     lastHeartbeat :  {lastHeartbeat} ,\n     memoryMBAllocated :  {memoryMBAllocated} ,\n     memoryMBFree :  {memoryMBFree} ,\n     numOperators :  {numOperators} ,\n     containerLogsUrl :  {containerLogsUrl} ,\n     state :  {state} \n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerscontaineridlogs", 
            "text": "Function: Return the container log list  Return:  {\n     logs : [\n        {\n             length :  {log length} ,\n             name :  {logName} \n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainerscontaineridlogslognamestartstartposendendposgrepregexpincludeoffsettruefalse", 
            "text": "Function: Return the raw log  Return: if includeOffset=false or not provided, return raw log content\n(Content-Type: text/plain). Otherwise (Content-Type: application/json):  {\n     lines : [\n        {  byteOffset : {byteOffset} ,  line :  {line}  }, \u2026\n     ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs/{logName}[?start={startPos}&amp;end={endPos}&amp;grep={regexp}&amp;includeOffset={true/false}]"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplancontainerscontaineridkill", 
            "text": "Function: Kill this container  Payload: none", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/kill"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplan", 
            "text": "Function: Return the logical plan of this application  Return:  {\n     operators : [\n      {\n         name :  {name} ,\n         attributes : {attributeMap},\n         class :  {class} ,\n         ports : {\n           [\n            {\n                 name :  {name} ,\n                 attributes : {attributeMap},\n                 type :  input/output \n            }, ...\n           ]\n         },\n          properties : {\n             class :  {class} \n         }\n      }, ...\n    ],\n     streams : [\n        {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanattributes", 
            "text": "Function: Return the application attributes  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/attributes"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperators", 
            "text": "Function: Return the list of info of the logical operator  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             containerIds : [  {containerid} , \u2026 ],\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             hosts : [  {host} , \u2026 ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             partitions : [  {operatorid} , \u2026 ],\n             recoveryWindowId :  {recoveryWindowId} ,\n             status : {\n                 {state} :  {number} , ...\n            },\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             unifiers : [  {operatorid} , \u2026 ],\n             counters : {\n                  {counterName}: {\n                     avg : \u2026,  max : \u2026,  min : \u2026,  sum : ...\n                 }\n            }\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopname", 
            "text": "Function: Return the info of the logical operator  Return:  {\n             className :  {className} ,\n             containerIds : [  {containerid} , \u2026 ],\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             hosts : [  {host} , \u2026 ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             partitions : [  {operatorid} , \u2026 ],\n             recoveryWindowId :  {recoveryWindowId} ,\n             status : {\n                 {state} :  {number} , ...\n            },\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             unifiers : [  {operatorid} , \u2026 ],\n             counters : {\n                  {counterName}: {\n                     avg : \u2026,  max : \u2026,  min : \u2026,  sum : ...\n                 }\n            }\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnameproperties", 
            "text": "Function: Return the properties of the logical operator  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidlogicalplanoperatorsopnameproperties", 
            "text": "Function: Set the properties of the logical operator\nPayload:  {\n     {name} : value, ...\n}", 
            "title": "POST /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidproperties", 
            "text": "Function: Return the properties of the physical operator  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidphysicalplanoperatorsopidproperties", 
            "text": "Function: Set the properties of the physical operator\nPayload:  {\n     {name} : value, ...\n}", 
            "title": "POST /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnameattributes", 
            "text": "Function: Get the attributes of the logical operator  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/attributes"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnameportsportnameattributes", 
            "text": "Function:  Get the attributes of the port  Return:  {\n     {name} : value, ...\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/ports/{portName}/attributes"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidlogicalplan", 
            "text": "Function: Change logical plan of this application\nPayload:  {\n     requests : [\n        {\n             requestType :  AddStreamSinkRequest ,\n             streamName :  {streamName} ,\n             sinkOperatorName :  {sinkOperatorName} ,\n             sinkOperatorPortName :  {sinkOperatorPortName} \n        },\n        {\n             requestType :  CreateOperatorRequest ,\n             operatorName :  {operatorName} ,\n             operatorFQCN :  {operatorFQCN} ,\n        },\n        {\n             requestType :  CreateStreamRequest ,\n             streamName :  {streamName} ,\n             sourceOperatorName :  {sourceOperatorName} ,\n             sourceOperatorPortName :  {sourceOperatorPortName} \n             sinkOperatorName :  {sinkOperatorName} ,\n             sinkOperatorPortName :  {sinkOperatorPortName} \n        },\n        {\n             requestType :  RemoveOperatorRequest ,\n             operatorName :  {operatorName} ,\n        },\n        {\n             requestType :  RemoveStreamRequest ,\n             streamName :  {streamName} ,\n        },\n        {\n             requestType :  SetOperatorPropertyRequest ,\n             operatorName :  {operatorName} ,\n             propertyName :  {propertyName} ,\n             propertyValue :  {propertyValue} \n        },\n        ...\n    ]\n}", 
            "title": "POST /ws/v2/applications/{appid}/logicalPlan"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnamestatsmeta", 
            "text": "Function: Return the meta information about the statistics stored for\nthis operator  Return:  {\n     appId :  {appId} ,\n     operatorName :  {operatorName} ,\n     operatorIds : [ {opid}, \u2026 ],\n     startTime :  {startTime} ,\n     endTime :  {endTime} ,\n     count :  {count} ,\n     ended :  {boolean} \n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats/meta"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidlogicalplanoperatorsopnamestatsstarttimestarttimeendtimeendtime", 
            "text": "Function: Return the statistics stored for this logical operator  {\n     operatorStats : [\n        {\n             operatorId :  {operatorId} ,\n             timestamp :  {timestamp} ,\n             stats : {\n                 container :  containerId ,\n                 host :  host ,\n                 totalTuplesProcessed ,  {totalTuplesProcessed} ,\n                 totalTuplesEmitted ,  {totalTuplesEmitted} ,\n                 tuplesProcessedPSMA ,  {tuplesProcessedPSMA} ,\n                 tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n                 cpuPercentageMA :  {cpuPercentageMA} ,\n                 latencyMA :  {latencyMA} ,\n                 ports : [ {\n                     name :  {name} ,\n                     type : {input/output} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA ,  {tuplesPSMA} ,\n                     bufferServerBytesPSMA ,  {bufferServerBytesPSMA} \n                }, \u2026 ],\n            }\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats?startTime={startTime}&amp;endTime={endTime}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainersstatsmeta", 
            "text": "Function: Return the meta information about the container statistics  {\n     appId :  {appId} ,\n     containers : {\n         {containerId} : {\n             id :  {id} ,\n             jvmName :  {jvmName} ,\n             host :  {host} ,\n             memoryMBAllocated ,  {memoryMBAllocated} \n        },\n        \u2026\n    },\n     startTime :  {startTime} \n     endTime :  {endTime} \n     count :  {count} \n     ended : {boolean}\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/stats/meta"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplancontainersstatsstarttimestarttimeendtimeendtime", 
            "text": "Function: Return the container statistics stored for this application  {\n     containerStats : [\n        {\n             containerId :  {containerId} \n             timestamp :  {timestamp} \n             stats : {\n                 numOperators :  {numOperators} ,\n            }\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/containers/stats?startTime={startTime}&amp;endTime={endTime}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidrecordings", 
            "text": "Function: Get the list of all recordings for this application  Return:  {\n     recordings : [{\n         id :  {id} ,\n         startTime :  {startTime} ,\n         appId :  {appId} ,\n         operatorId :  {operatorId} ,\n         containerId :  {containerId} ,\n         totalTuples :  {totalTuples} ,\n         ports : [ {\n             name :  {portName} ,\n             streamName :  {streamName} ,\n             type :  {type} ,\n             id :  {index} ,\n             tupleCount :  {tupleCount} \n        } \u2026 ],\n         ended : {boolean},\n         windowIdRanges : [ {\n             low :  {lowId} ,\n             high :  {highId} \n        } \u2026 ],\n         properties : {\n             name :  value , ...\n        }\n    }, ...]\n}", 
            "title": "GET /ws/v2/applications/{appid}/recordings"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidrecordings", 
            "text": "Function: Get the list of recordings on this operator  Return:  {\n     recordings : [ {\n         id :  {id} ,\n         startTime :  {startTime} ,\n         appId :  {appId} ,\n         operatorId :  {operatorId} ,\n         containerId :  {containerId} ,\n         totalTuples :  {totalTuples} ,\n         ports : [ {\n             name :  {portName} ,\n             streamName :  {streamName} ,\n             type :  {type} ,\n             id :  {index} ,\n             tupleCount :  {tupleCount} \n        } \u2026 ],\n         ended : {boolean},\n         windowIdRanges : [ {\n             low :  {lowId} ,\n             high :  {highId} \n        } \u2026 ],\n         properties : {\n             name :  value , ...\n        }\n    }, ...]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidrecordingsid", 
            "text": "Function: Get the information about the recording  Return:  {\n     id :  {id} ,\n     startTime :  {startTime} ,\n     appId :  {appId} ,\n     operatorId :  {operatorId} ,\n     containerId :  {containerId} ,\n     totalTuples :  {totalTuples} ,\n     ports : [ {\n        name :  {portName} ,\n        streamName :  {streamName} ,\n        type :  {type} ,\n        id :  {index} ,\n        tupleCount :  {tupleCount} \n     } \u2026 ],\n     ended : {boolean},\n     windowIdRanges : [ {\n        low :  {lowId} ,\n        high :  {highId} \n     } \u2026 ],\n     properties : {\n        name :  value , ...\n     }\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2applicationsappidphysicalplanoperatorsopidrecordingsid", 
            "text": "Function: Deletes the specified recording  Since: 1.0.4", 
            "title": "DELETE /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidphysicalplanoperatorsopidrecordingsidtuples", 
            "text": "Query Parameters:  offset\nstartWindow\nlimit\nports\nexecuteEmptyWindow  Function: Get the tuples  Return:  {\n     startOffset :  {startOffset} ,\n     tuples : [ {\n         windowId :  {windowId} ,\n         tuples : [ {\n             portId :  {portId} ,\n             data :  {tupleData} \n        }, \u2026 ]\n    }, \u2026 ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}/tuples"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappideventsfromfromtimetototimeoffsetoffsetlimitlimit", 
            "text": "Function: Get the events  Return:  {\n     events : [ {\n            id :  {id} ,\n         timestamp :  {timestamp} ,\n         type :  {type} ,\n         data : {\n             name :  value , \u2026\n        }\n    }, \u2026 ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/events?from={fromTime}&amp;to={toTime}&amp;offset={offset}&amp;limit={limit}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2profileuser", 
            "text": "Function: Get the user profile information, list of roles and list of\npermissions given the user  Return:  {\n     authScheme :  {authScheme} ,\n     userName  :  {userName} ,\n     roles : [  {role1} , \u2026 ],\n     permissions : [  {permission1} , \u2026 ]\n}", 
            "title": "GET /ws/v2/profile/user"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2profilesettings", 
            "text": "Function: Get the current user's settings  Return:  {\n     {key} : {value}, ...\n}", 
            "title": "GET /ws/v2/profile/settings"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2profilesettingsuser", 
            "text": "Function: Get the specified user's settings  Return:  {\n     {key} : {value}, ...\n}", 
            "title": "GET /ws/v2/profile/settings/{user}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2profilesettingsuserkey", 
            "text": "Function: Get the specified user's setting key  Return:  {\n     value : {value}\n}", 
            "title": "GET /ws/v2/profile/settings/{user}/{key}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2profilesettingsuserkey", 
            "text": "Function: Set the specified user's setting key\nPayload:  {\n     value : {value}\n}", 
            "title": "PUT /ws/v2/profile/settings/{user}/{key}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authroles", 
            "text": "Function: Get the list of roles the system has  Return:  {\n     roles : [\n       {\n          name :  {role1} ,\n          permissions : [  {permission1} , \u2026 ]\n       }, \u2026\n    ]\n}", 
            "title": "GET /ws/v2/auth/roles"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authrolesrole", 
            "text": "Function: Get the list of permissions given the role  Return:  {\n     permissions : [  {permissions1} , \u2026 ]\n}", 
            "title": "GET /ws/v2/auth/roles/{role}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2authrolesrole", 
            "text": "Function: create or edit the list of permissions given the role  Return:  {\n     permissions : [  {permissions1} , \u2026 ]\n}", 
            "title": "PUT /ws/v2/auth/roles/{role}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2authrestoredefaultroles", 
            "text": "Function: Restores default roles", 
            "title": "POST /ws/v2/auth/restoreDefaultRoles"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2authrolesrole", 
            "text": "Function: delete the given role", 
            "title": "DELETE /ws/v2/auth/roles/{role}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authpermissions", 
            "text": "Function: Get the list of possible permissions  Return:  {\n     permissions : [ {\n        name :  {permissionName} ,\n        adminOnly : true/false\n    }, \u2026 ]\n}", 
            "title": "GET /ws/v2/auth/permissions"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2applicationsappidpermissions", 
            "text": "Function: Set the permissions details for this application  Payload:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}", 
            "title": "PUT /ws/v2/applications/{appid}/permissions"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidpermissions", 
            "text": "Function: Get the permissions details for this application  Return:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}", 
            "title": "GET /ws/v2/applications/{appid}/permissions"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2apppackagesownernamepermissions", 
            "text": "Function: Set the permissions details for this application  Payload:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}", 
            "title": "PUT /ws/v2/appPackages/{owner}/{name}/permissions"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownernamepermissions", 
            "text": "Function: Get the permissions details for this application  Return:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{name}/permissions"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2licenses", 
            "text": "Function: Add a license to the registry or generate an eval license  Payload: The license file content, if payload is empty, it will try to generate an eval license and return the info  Return:  {\n   id :  {licenseId} ,\n   expireTime : {unixTimeMillis},\n   nodesAllowed : {nodesAllowed},\n   memoryMBAllowed : {memoryMBAllowed},\n   contextType :  {contextType} ,\n   type :  {type} ,\n   features : [  {feature1} , \u2026 ]\n}", 
            "title": "POST /ws/v2/licenses"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2licensescurrent", 
            "text": "Function: Get info on the current license  {\n       id :  {licenseId} ,\n       expireTime : {unixTimeMillis},\n       nodesAllowed : {nodesAllowed},\n       nodesUsed : {nodesUsed},\n       memoryMBAllowed : {memoryMBAllowed},\n       memoryMBUsed : {memoryMBUsed},\n       contextType :  {community|standard|enterprise} ,\n       type :  {evaluation|non_production|production} \n       features : [  {feature1} , \u2026 ], // for community, empty array\n       current : true/false\n}", 
            "title": "GET /ws/v2/licenses/current"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configinstallmode", 
            "text": "Function: returns the install mode  {\n   installMode :  {evaluation|community|app} ,\n   appPackageName :  {optionalAppPackageName} ,\n   appPackageVersion :  {optionalAppPackageVersion} \n}", 
            "title": "GET /ws/v2/config/installMode"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configpropertiesdtphonehomeenable", 
            "text": "Function: returns the download type  {\n   value :  true/false \n}", 
            "title": "GET /ws/v2/config/properties/dt.phoneHome.enable"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2configpropertiesdtphonehomeenable", 
            "text": "Function:  {\n   value :  true/false \n}  Feature List:     SYSTEM_APPS  SYSTEM_ALERTS  APP_DATA_DASHBOARDS  RUNTIME_DAG_CHANGE  RUNTIME_PROPERTY_CHANGE  APP_CONTAINER_LOGS  LOGGING_LEVELS  APP_DATA_TRACKER  JAAS_LDAP_AUTH  APP_BUILDER", 
            "title": "PUT /ws/v2/config/properties/dt.phoneHome.enable"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configproperties", 
            "text": "Function: Returns list of properties from dt-site.xml.  Return:  {\n     {name} : {\n         value :  {PROPERTY_VALUE} ,\n         description :  {PROPERTY_DESCRIPTION} \n    }\n\n}", 
            "title": "GET /ws/v2/config/properties"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configpropertiesproperty_name", 
            "text": "Function: Returns single property from dt-site.xml, specify by name  Return:  {\n     value :  {PROPERTY_VALUE} ,\n     description :  {PROPERTY_DESCRIPTION} \n}", 
            "title": "GET /ws/v2/config/properties/{PROPERTY_NAME}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2configproperties", 
            "text": "Function: Overwrites all specified properties in dt-site.xml  Payload:  {\n     properties : [\n        {\n             name :  {name} \n             value :  {PROPERTY_VALUE} ,\n             local : true/false,\n                     description :  {PROPERTY_DESCRIPTION} \n        }, \u2026\n    ]\n}", 
            "title": "POST /ws/v2/config/properties"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2configpropertiesproperty_name", 
            "text": "Function: Overwrites or creates new property in dt-site.xml\nPayload:  {\n     value :  {PROPERTY_VALUE} ,\n     local : true/false,\n     description :  {PROPERTY_DESCRIPTION} \n}", 
            "title": "PUT /ws/v2/config/properties/{PROPERTY_NAME}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2configpropertiesproperty_name", 
            "text": "Function: Deletes a property from dt-site.xml. This may have to be\nrestricted to custom properties?", 
            "title": "DELETE /ws/v2/config/properties/{PROPERTY_NAME}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2confighadoopexecutable", 
            "text": "Function: Returns the hadoop executable  Return:  {\n     value :  {PROPERTY_VALUE} ,\n}", 
            "title": "GET /ws/v2/config/hadoopExecutable"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2confighadoopexecutable", 
            "text": "Function: Sets the hadoop executable  Return:  {\n     value :  {PROPERTY_VALUE} ,\n}", 
            "title": "PUT /ws/v2/config/hadoopExecutable"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configissues", 
            "text": "Function: Returns list of potential issues with environment  Return:  {\n     issues : [\n        {\n             key :  {issueKey} ,\n             propertyName :  {PROPERTY_NAME} ,\n             description :  {ISSUE_DESCRIPTION} ,\n             severity :  error | warning \n        },\n        {...},\n        {...}\n    ]    \n}", 
            "title": "GET /ws/v2/config/issues"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2configipaddresses", 
            "text": "Function: Returns list of ip addresses the gateway can listen to  Return:  {\n     ipAddresses : [\n       1.2.3.4 , ...\n    ]    \n}", 
            "title": "GET /ws/v2/config/ipAddresses"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2configrestart", 
            "text": "Function: Restarts the gateway  Payload: none", 
            "title": "POST /ws/v2/config/restart"
        }, 
        {
            "location": "/dtgateway_api/#get-proxyrmv1", 
            "text": "", 
            "title": "GET /proxy/rm/v1/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#post-proxyrmv1", 
            "text": "Function: Proxy calls to resource manager of Hadoop.  Only works for GET and POST calls.", 
            "title": "POST /proxy/rm/v1/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#get-proxystramv2", 
            "text": "", 
            "title": "GET /proxy/stram/v2/..."
        }, 
        {
            "location": "/dtgateway_api/#post-proxystramv2", 
            "text": "", 
            "title": "POST /proxy/stram/v2/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#put-proxystramv2", 
            "text": "", 
            "title": "PUT /proxy/stram/v2/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#delete-proxystramv2", 
            "text": "Function: Proxy calls to Stram Web Services.", 
            "title": "DELETE /proxy/stram/v2/\u2026"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2applicationsappidloggers", 
            "text": "Function: Set the logger levels of packages/classes.  Payload:  {\n     loggers  : [\n        {\n             logLevel : value,\n             target : value\n        }, \n        ...\n    ]\n}", 
            "title": "POST /ws/v2/applications/{appid}/loggers"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidloggers", 
            "text": "Function: Gets the logger levels of packages/classes.  Return:  {\n     loggers  : [\n        {\n             logLevel : value,\n             target : value\n        }, \n        ...\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/loggers"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2applicationsappidloggerssearchpatternpattern", 
            "text": "Function: searches for all classes that match the pattern.  Return:  {\n     loggers  : [\n        {\n             name  :  {fully qualified class name} ,\n             level :  {logger level} \n        }\n    ]\n}", 
            "title": "GET /ws/v2/applications/{appid}/loggers/search?pattern=\"{pattern}\""
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackages", 
            "text": "Since: 1.0.4  Function: Gets the list of appPackages the user can view in the system  {\n     appPackages : [\n        {\n                  appPackageName :  {appPackageName} ,\n                  appPackageVersion :  {appPackageVersion} ,\n             modificationTime :  {modificationTime} ,\n             owner :  {owner} ,\n        }, ...\n    ]\n}", 
            "title": "GET /ws/v2/appPackages"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2apppackagesmergereplacefailourstheirs", 
            "text": "Since: 1.0.4  Function: Uploads an appPackage file, merge with existing app package if exists. Default is replace.\nmerge parameter:\n  replace - replace existing app package with the new app package without merging\n  fail - return error if there is an existing app package already with the same owner and name and version\n  ours - merge, for files existing in both existing and new app packages, use the file in the new package\n  theirs - merge, for files existing in both existing and new app packages, use the file in the existing package  Payload: the raw zip file  Return: The information of the app package", 
            "title": "POST /ws/v2/appPackages?merge={replace|fail|ours|theirs}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownername", 
            "text": "Since: 1.0.4  Function: Gets the list of versions of appPackages with the given name in the system owned by the specified user  {\n     versions : [\n         1.0-SNAPSHOT \n    ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{name}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2apppackagesownerpackagenamepackageversion", 
            "text": "Since: 1.0.4  Function: Deletes the appPackage", 
            "title": "DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversiondownload", 
            "text": "Since: 1.0.4  Function: Downloads the appPackage zip file", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/download"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversion", 
            "text": "Since: 1.0.4  Function: Gets the meta information of the app package  Returns:  {\n     appPackageName :  {appPackageName} ,\n     appPackageVersion :  {appPackageVersion} ,\n     modificationTime :   {modificationTime} ,\n     owner :  {owner} ,\n    ...\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionconfigs", 
            "text": "Since: 1.0.4\nFunction: Gets the list of configurations of the app package\nReturns:  {\n     configs : [\n         my-app-conf1.xml \n    ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionconfigsconfigname", 
            "text": "Since: 1.0.4  Function: Gets the properties XML of the specified config  Returns:  configuration \n         property \n                 name ... /name \n                 value ... /value \n         /property \n        \u2026 /configuration", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2apppackagesownerpackagenamepackageversionconfigsconfigname", 
            "text": "Since: 1.0.4  Function: Creates or replaces the specified config with the property parameters specified payload  Payload: configuration in XML", 
            "title": "PUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2apppackagesownerpackagenamepackageversionconfigsconfigname", 
            "text": "Since: 1.0.4  Function: Deletes the specified config", 
            "title": "DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionapplications", 
            "text": "Since: 1.0.4  Function: Gets the list of applications in the appPackage  Returns:  {\n     applications : [\n        {\n             dag : {dag in json format},\n             file :  {fileName} ,\n             name :  {name} ,\n             type :  {type} ,\n             error :  {error} ,\n             fileContent : {originalFileContentForJSONTypeApp}\n        }\n    ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionapplicationsappname", 
            "text": "Since: 1.0.4  Function: Gets the meta data for that application  Returns:  {\n     file :  {fileName} ,\n     name :  {name} ,\n     type :  {json/class/properties} ,\n     error :  {error} \n     dag : {\n         operators : [\n          {\n             name :  {name} ,\n             attributes :  {\n                 {attributeKey} :  {attributeValue} , ...\n            },\n             class :  {class} ,\n             ports : [\n                  {\n                     name :  {name} ,\n                     attributes :  {\n                        {attributeKey} :  {attributeValue} , ...\n                     },\n                  }, ...\n            ],\n             properties : {\n                {propertyName} :  {propertyValue} \n            }\n         }, ...\n        ],\n         streams : [\n          {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n          }, ...\n        ]\n    },\n     fileContent : {originalFileContentForJSONTypeApp}\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2apppackagesuserapppackagenameapppackageversionmerge", 
            "text": "Function: Merge the configuration, json apps, and resources files from the version specified from the payload to the specified app package in the url, without overwriting any existing file in the specified app package  Payload:  {\n  version :  {versionToMergeFrom} \n}", 
            "title": "POST /ws/v2/appPackages/{user}/{appPackageName}/{appPackageVersion}/merge"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2apppackagesownerpackagenamepackageversionapplicationsappnamelaunchconfigconfignameoriginalappidoriginalappidqueuequeuename", 
            "text": "Since: 1.0.4  Function: Launches the application with the given configuration specified in the POST payload  Payload:  {\n     {propertyName}  :  {propertyValue} , ...\n}  Return:  {\n     appId :  {appId} \n}", 
            "title": "POST /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}/launch[?config={configName}&amp;originalAppId={originalAppId}&amp;queue={queueName}]"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionoperatorsclassname", 
            "text": "Since: 1.0.4  Function: Get the properties of the operator given the classname in the jar  {  \n     properties : [  \n        {\n           name : {className} ,\n           canGet : {canGet},\n           canSet : {canSet},\n           type : {type} ,\n           description : {description} ,\n           properties : ...\n        },\n       \u2026\n     ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators/{classname}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2apppackagesownerpackagenamepackageversionapplicationsapplicationnameerrorifexiststruefalse", 
            "text": "Function: Creates or Replaces an application using json. Note that \"ports\" are only needed if you need to specify port attributes.  If errorIfExists is true, it returns an error if the application with the same name already exists in the app ackage  Payload:  {\n         displayName :  {displayName} ,\n         description :  {description} ,\n         operators : [\n          {\n             name :  {name} ,\n             attributes :  {\n                 {attributeKey} :  {attributeValue} , ...\n            },\n             class :  {class} ,\n             ports : [\n                  {\n                     name :  {name} ,\n                     attributes :  {\n                        {attributeKey} :  {attributeValue} , ...\n                     },\n                  }, ...\n            ],\n             properties : {\n                {propertyName} :  {propertyValue} \n            }\n          }, ...\n        ],\n         streams : [\n          {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n          }, ...\n        ]\n}  Return:  {\n         error :  {error} \n}  Available port attributes to set:    AUTO_RECORD  IS_OUTPUT_UNIFIED  PARTITION_PARALLEL  QUEUE_CAPACITY  SPIN_MILLIS  STREAM_CODEC  UNIFIER_LIMIT   Available locality options to set:    THREAD_LOCAL  CONTAINER_LOCAL  NODE_LOCAL  RACK_LOCAL", 
            "title": "PUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}[?errorIfExists={true/false}]"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2apppackagesownerpackagenamepackageversionapplicationsapplicationname", 
            "text": "Since: 1.0.5  Function: Deletes non-jar based application in the app package", 
            "title": "DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesownerpackagenamepackageversionoperators", 
            "text": "Since: 1.0.5  Function: Get the classes of operators from specified app package.  Return:  {  \n     operatorClasses : [  \n        {\n             name : {fullyQualifiedClassName} , \n             title :  {title} ,\n             shortDesc :  {description} ,\n             longDesc :  {description} ,\n             category :  {categoryName} ,\n             doclink :  {doc url} ,\n             tags : [  {tag} ,  {tag} , \u2026 ],\n             inputPorts : [\n                {\n                     name :  {portName} ,\n                     type :  {tupleType} ,\n                     optional : true/false  \n                }, \u2026\n            ]\n             outputPorts : [\n                {\n                     name :  {portName} ,\n                     type :  {tupleType} ,\n                     optional : true/false  \n                }, \u2026\n            ],\n             properties : [  \n                {\n                     name : {propertyName} ,\n                     canGet : {canGet},\n                     canSet : {canSet},\n                     type : {type} ,\n                     description : {description} ,\n                     properties : ...\n                }, \u2026\n            ],\n             defaultValue : {\n                 {propertyName} : [VALUE], // type depends on property\n                ...\n            }\n\n        }, \u2026\n    ]\n}", 
            "title": "GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2apppackagesimport", 
            "text": "Function: List the importable app packages on Gateway's local file\nsystem  Return:  {\n     appPackages: [\n        {\n             file :  {file} ,\n             name :  {name} ,\n             displayName :  {displayName} ,\n             version :  {version} ,\n             description :  {description} \n        }\n    ]\n}", 
            "title": "GET /ws/v2/appPackages/import"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2apppackagesimport", 
            "text": "Function: Import app package from Gateway's local file system  Payload:  {\n         files : [ {file} , \u2026 ]\n}", 
            "title": "POST /ws/v2/appPackages/import"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2systemalertsalertsname", 
            "text": "Function: Creates or replaces the specified system alert. The condition has access to an object in its scope called  _topic . An example alert might take the form of the following:  _topic[\"applications.application_1400294100000_0001\"].allocatedContainers   5  Payload:  {\n         condition : {condition in javascript} ,\n         email : {email} ,\n         timeThresholdMillis : {time} \n}", 
            "title": "PUT /ws/v2/systemAlerts/alerts/{name}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2systemalertsalertsname", 
            "text": "Function: Deletes the specified system alert", 
            "title": "DELETE /ws/v2/systemAlerts/alerts/{name}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertsalertsinalerttruefalse", 
            "text": "Function: Gets the created alerts  Return:  {\n     alerts : [{\n         name :  {alertName} ,\n         condition : {condition in javascript} ,\n         email : {email} ,\n         timeThresholdMillis : {time} ,\n         alertStatus : {\n             isInAlert :{true/false}\n             inTime :  {time} ,\n             message :  {message} ,\n             emailSent : {true/false}\n        }\n    }, \u2026  ]\n}", 
            "title": "GET /ws/v2/systemAlerts/alerts?inAlert={true/false}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertsalertsname", 
            "text": "Function: Gets the specified system alert  Return:  {\n     name :  {alertName} ,\n     condition : {condition in javascript} ,        \n     email : {email} ,\n     timeThresholdMillis : {time} ,\n     alertStatus : {\n         isInAlert :{true/false}\n         inTime :  {time} ,\n         message :  {message} ,\n         emailSent : {true/false}\n    }\n}", 
            "title": "GET /ws/v2/systemAlerts/alerts/{name}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertshistory", 
            "text": "Function: Gets the history of alerts  Return:  {\n     history : [\n        {\n             name : {alertName} ,\n             inTime : {time} ,\n             outTime :  {time} ,\n             message :  {message} ,\n             emailSent : {true/false}\n        }, ...\n     ]\n}", 
            "title": "GET /ws/v2/systemAlerts/history"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2systemalertstopicdata", 
            "text": "Function: Gets the topic data that is used for evaluating alert\ncondition  Return:  {\n      {topicName} : {json object data}, ...\n}", 
            "title": "GET /ws/v2/systemAlerts/topicData"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authusersuser", 
            "text": "Function: Gets the info of the given user  Return:  {\n     userName :  {userName} ,\n     roles : [  {role1} ,  {role2}  ]\n}", 
            "title": "GET /ws/v2/auth/users/{user}"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2authusersuser", 
            "text": "Function: Changes password and/or roles of the given user  Return:  {\n     userName :  {userName} ,\n     oldPassword :  {oldPassword} ,\n     newPassword :  {newPassword} ,\n     roles : [  {role1} ,  {role2}  ]\n}", 
            "title": "POST /ws/v2/auth/users/{user}"
        }, 
        {
            "location": "/dtgateway_api/#put-wsv2authusersuser", 
            "text": "Function: Creates new user  Return:  {\n     userName :  {userName} ,\n     password :  {password} ,\n     roles : [  {role1} ,  {role2}  ]\n}", 
            "title": "PUT /ws/v2/auth/users/{user}"
        }, 
        {
            "location": "/dtgateway_api/#delete-wsv2authusersuser", 
            "text": "Function: Deletes the specified user", 
            "title": "DELETE /ws/v2/auth/users/{user}"
        }, 
        {
            "location": "/dtgateway_api/#get-wsv2authusers", 
            "text": "Function: Gets the list of users  Return:  {\n     users : [ {\n        userName :  {username1} ,\n        roles : [  {role1} , \u2026 ],\n        permissions : [  {permission1} , \u2026 ]\n    }\n}", 
            "title": "GET /ws/v2/auth/users"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2login", 
            "text": "Function: Login\nPayload:  {\n     userName :  {userName} ,\n     password :  {password} \n}  Return:  {\n     authScheme :  {authScheme} ,\n     userName  :  {userName} ,\n     roles : [  {role1} , \u2026 ],\n     permissions : [  {permission1} , \u2026 ]\n}", 
            "title": "POST /ws/v2/login"
        }, 
        {
            "location": "/dtgateway_api/#post-wsv2logout", 
            "text": "Function: Log out the current user  Return:  {\n}", 
            "title": "POST /ws/v2/logout"
        }, 
        {
            "location": "/dtgateway_api/#publisher-subscriber-websocket-protocol", 
            "text": "", 
            "title": "Publisher-Subscriber WebSocket Protocol"
        }, 
        {
            "location": "/dtgateway_api/#input", 
            "text": "", 
            "title": "Input"
        }, 
        {
            "location": "/dtgateway_api/#publishing", 
            "text": "{\"type\":\"publish\", \"topic\":\"{topic}\", \"data\":{data}}", 
            "title": "Publishing"
        }, 
        {
            "location": "/dtgateway_api/#subscribing", 
            "text": "{\"type\":\"subscribe\", \"topic\":\"{topic}\"}", 
            "title": "Subscribing"
        }, 
        {
            "location": "/dtgateway_api/#unsubscribing", 
            "text": "{\"type\":\"unsubscribe\", \"topic\":\"{topic}\"}", 
            "title": "Unsubscribing"
        }, 
        {
            "location": "/dtgateway_api/#subscribing-to-the-number-of-subscribers-of-a-topic", 
            "text": "{\"type\":\"subscribeNumSubscribers\", \"topic\":\"{topic}\"}", 
            "title": "Subscribing to the number of subscribers of a topic"
        }, 
        {
            "location": "/dtgateway_api/#subscribing-to-the-number-of-subscribers-of-a-topic_1", 
            "text": "{\"type\":\"unsubscribeNumSubscribers\", \"topic\":\"{topic}\"}", 
            "title": "Subscribing to the number of subscribers of a topic"
        }, 
        {
            "location": "/dtgateway_api/#output", 
            "text": "", 
            "title": "Output"
        }, 
        {
            "location": "/dtgateway_api/#normal-published-data", 
            "text": "{\"type\":\"data\", \"topic\":\"{topic}\", \"data\":{data}}", 
            "title": "Normal Published Data"
        }, 
        {
            "location": "/dtgateway_api/#number-of-subscribers", 
            "text": "{\"type\":\"data\", \"topic\":\"{topic}.numSubscribers\", \"data\":{data}}", 
            "title": "Number of Subscribers:"
        }, 
        {
            "location": "/dtgateway_api/#auto-publish-topics", 
            "text": "data that gets published every one second:   applications  - list of streaming applications running in the cluster  applications.[appid]  - information about a particular application  applications.[appid].containers  - information about containers of a particular application  applications.[appid].physicalOperators  - information about operators of a particular application  applications.[appid].logicalOperators  - information about logical operators of a particular application  applications.[appid].events  - events from the AM of a particularapplication   data that gets published every five seconds:   cluster.metrics  - metrics of the cluster", 
            "title": "Auto publish topics"
        }, 
        {
            "location": "/autometrics/", 
            "text": "Apache Apex AutoMetrics\n\n\nIntroduction\n\n\nMetrics collect various statistical information about a process which can be very useful for diagnosis. Auto Metrics in Apex can help monitor operators in a running application.  The goal of \nAutoMetric\n API is to enable operator developer to define relevant metrics for an operator in a simple way which the platform collects and reports automatically.\n\n\nSpecifying AutoMetrics in an Operator\n\n\nAn \nAutoMetric\n can be any object. It can be of a primitive type - int, long, etc. or a complex one. A field or a \nget\n method in an operator can be annotated with \n@AutoMetric\n to specify that its value is a metric. After every application end window, the platform collects the values of these fields/methods in a map and sends it to application master.\n\n\npublic class LineReceiver extends BaseOperator\n{\n @AutoMetric\n long length;\n\n @AutoMetric\n long count;\n\n public final transient DefaultInputPort\nString\n input = new DefaultInputPort\nString\n()\n {\n   @Override\n   public void process(String s)\n   {\n     length += s.length();\n     count++;\n   }\n };\n\n @Override\n public void beginWindow(long windowId)\n {\n   length = 0;\n   count = 0;\n }\n}\n\n\n\n\nThere are 2 auto-metrics declared in the \nLineReceiver\n. At the end of each application window, the platform will send a map with 2 entries - \n[(length, 100), (count, 10)]\n to the application master.\n\n\nAggregating AutoMetrics across Partitions\n\n\nWhen an operator is partitioned, it is useful to aggregate the values of auto-metrics across all its partitions every window to get a logical view of these metrics. The application master performs these aggregations using metrics aggregators.\n\n\nThe AutoMetric API helps to achieve this by providing an interface for writing aggregators- \nAutoMetric.Aggregator\n. Any implementation of \nAutoMetric.Aggregator\n can be set as an operator attribute - \nMETRICS_AGGREGATOR\n for a particular operator which in turn is used for aggregating physical metrics.\n\n\nDefault aggregators\n\n\nMetricsAggregator\n is a simple implementation of \nAutoMetric.Aggregator\n that platform uses as a default for summing up primitive types - int, long, float and double.\n\n\nMetricsAggregator\n is just a collection of \nSingleMetricAggregator\ns. There are multiple implementations of \nSingleMetricAggregator\n that perform sum, min, max, avg which are present in Apex core and Apex malhar.\n\n\nFor the \nLineReceiver\n operator, the application developer need not specify any aggregator. The platform will automatically inject an instance of \nMetricsAggregator\n that contains two \nLongSumAggregator\ns - one for \nlength\n and one for \ncount\n. This aggregator will report sum of length and sum of count across all the partitions of \nLineReceiver\n.\n\n\nBuilding custom aggregators\n\n\nPlatform cannot perform any meaningful aggregations for non-numeric metrics. In such cases, the operator or application developer can write custom aggregators. Let\u2019s say, if the \nLineReceiver\n was modified to have a complex metric as shown below.\n\n\npublic class AnotherLineReceiver extends BaseOperator\n{\n  @AutoMetric\n  final LineMetrics lineMetrics = new LineMetrics();\n\n  public final transient DefaultInputPort\nString\n input = new DefaultInputPort\nString\n()\n  {\n    @Override\n    public void process(String s)\n    {\n      lineMetrics.length += s.length();\n      lineMetrics.count++;\n    }\n  };\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n    lineMetrics.length = 0;\n    lineMetrics.count = 0;\n  }\n\n  public static class LineMetrics implements Serializable\n  {\n    long length;\n    long count;\n\n    private static final long serialVersionUID = 201511041908L;\n  }\n}\n\n\n\n\nBelow is a custom aggregator that can calculate average line length across all partitions of \nAnotherLineReceiver\n.\n\n\npublic class AvgLineLengthAggregator implements AutoMetric.Aggregator\n{\n\n  Map\nString, Object\n result = Maps.newHashMap();\n\n  @Override\n  public Map\nString, Object\n aggregate(long l, Collection\nAutoMetric.PhysicalMetricsContext\n collection)\n  {\n    long totalLength = 0;\n    long totalCount = 0;\n    for (AutoMetric.PhysicalMetricsContext pmc : collection) {\n      AnotherLineReceiver.LineMetrics lm = (AnotherLineReceiver.LineMetrics)pmc.getMetrics().get(\nlineMetrics\n);\n      totalLength += lm.length;\n      totalCount += lm.count;\n    }\n    result.put(\navgLineLength\n, totalLength/totalCount);\n    return result;\n  }\n}\n\n\n\n\nAn instance of above aggregator can be specified as the \nMETRIC_AGGREGATOR\n for \nAnotherLineReceiver\n while creating the DAG as shown below.\n\n\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    AnotherLineReceiver lineReceiver = dag.addOperator(\nLineReceiver\n, new AnotherLineReceiver());\n    dag.setAttribute(lineReceiver, Context.OperatorContext.METRICS_AGGREGATOR, new AvgLineLengthAggregator());\n    ...\n  }\n\n\n\n\nRetrieving AutoMetrics\n\n\nThe Gateway REST API provides a way to retrieve the latest AutoMetrics for each logical operator.  For example:\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n    \nautoMetrics\n: {\n       \ncount\n: \n71314\n,\n       \nlength\n: \n27780706\n\n    },\n    \nclassName\n: \ncom.datatorrent.autometric.LineReceiver\n,\n    ...\n}\n\n\n\n\nSystem Metrics\n\n\nSystem metrics are standard operator metrics provided by the system.  Examples include:\n\n\n\n\nprocessed tuples per second\n\n\nemitted tuples per second\n\n\ntotal tuples processed\n\n\ntotal tuples emitted\n\n\nlatency\n\n\nCPU percentage\n\n\nfailure count\n\n\ncheckpoint elapsed time\n\n\n\n\nThe Gateway REST API provides a way to retrieve the latest values for all of the above for each of the logical operators in the application.\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n    \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n    \nfailureCount\n: \n{failureCount}\n,\n    \nlatencyMA\n: \n{latencyMA}\n,  \n    \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n    \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n    \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n    \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n    ...\n}\n\n\n\n\nHowever, just like AutoMetrics, the Gateway only provides the latest metrics.  For historical metrics, we will need the help of App Data Tracker.\n\n\nApp Data Tracker\n\n\nAs discussed above, STRAM aggregates the AutoMetrics from physical operators (partitions) to something that makes sense in one logical operator.  It pushes the aggregated AutoMetrics values using Websocket to the Gateway at every second along with system metrics for each operator.  Gateway relays the information to an application called App Data Tracker.  It is another Apex application that runs in the background and further aggregates the incoming values by time bucket and stores the values in HDHT.  It also allows the outside to retrieve the aggregated AutoMetrics and system metrics through websocket interface.\n\n\n\n\nApp Data Tracker is enabled by having these properties in dt-site.xml:\n\n\nproperty\n\n  \nname\ndt.appDataTracker.enable\n/name\n\n  \nvalue\ntrue\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.appDataTracker.transport\n/name\n\n  \nvalue\nbuiltin:AppDataTrackerFeed\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.attr.METRICS_TRANSPORT\n/name\n\n  \nvalue\nbuiltin:AppDataTrackerFeed\n/value\n\n\n/property\n\n\n\n\n\nAll the applications launched after the App Data Tracker is enabled will have metrics sent to it.\n\n\nNote\n: The App Data Tracker will be shown running in dtManage as a \u201csystem app\u201d.  It will show up if the \u201cshow system apps\u201d button is pressed.\n\n\nBy default, the time buckets App Data Tracker aggregates upon are one minute, one hour and one day.  It can be overridden by changing the operator attribute \nMETRICS_DIMENSIONS_SCHEME\n.\n\n\nAlso by default, the app data tracker performs all these aggregations: SUM, MIN, MAX, AVG, COUNT, FIRST, LAST on all number metrics.  You can also override by changing the same operator attribute \nMETRICS_DIMENSIONS_SCHEME\n, provided the custom aggregator is known to the App Data Tracker.  (See next section)\n\n\nCustom Aggregator in App Data Tracker\n\n\nCustom aggregators allow you to do your own custom computation on statistics generated by any of your applications. In order to implement a Custom aggregator you have to do two things:\n\n\n\n\nCombining new inputs with the current aggregation\n\n\nCombining two aggregations together into one aggregation\n\n\n\n\nLet\u2019s consider the case where we want to perform the following rolling average:\n\n\nY_n = \u00bd * X_n + \u00bd * X_n-1 + \u00bc * X_n-2 + \u215b * X_n-3 +...\n\n\nThis aggregation could be performed by the following Custom Aggregator:\n\n\n@Name(\nIIRAVG\n)\npublic class AggregatorIIRAVG extends AbstractIncrementalAggregator\n{\n  ...\n\n  private void aggregateHelper(DimensionsEvent dest, DimensionsEvent src)\n  {\n    double[] destVals = dest.getAggregates().getFieldsDouble();\n    double[] srcVals = src.getAggregates().getFieldsDouble();\n\n    for (int index = 0; index \n destLongs.length; index++) {\n      destVals[index] = .5 * destVals[index] + .5 * srcVals[index];\n    }\n  }\n\n  @Override\n  public void aggregate(Aggregate dest, InputEvent src)\n  {\n    //Aggregate a current aggregation with a new input\n    aggregateHelper(dest, src);\n  }\n\n  @Override\n  public void aggregate(Aggregate destAgg, Aggregate srcAgg)\n  {\n    //Combine two existing aggregations together\n    aggregateHelper(destAgg, srcAgg);\n  }\n}\n\n\n\n\nDiscovery of Custom Aggregators\n\n\nAppDataTracker searches for custom aggregator jars under the following directories statically before launching:\n\n\n\n\n{dt_installation_dir}/plugin/aggregators\n\n\n{user_home_dir}/.dt/plugin/aggregators\n\n\n\n\nIt uses reflection to find all the classes that extend from \nIncrementalAggregator\n and \nOTFAggregator\n in these jars and registers them with the name provided by \n@Name\n annotation (or class name when \n@Name\n is absent).\n\n\nUsing \nMETRICS_DIMENSIONS_SCHEME\n\n\nHere is a sample code snippet on how you can make use of \nMETRICS_DIMENSIONS_SCHEME\n to set your own time buckets and your own set of aggregators for certain \nAutoMetric\ns performed by the App Data Tracker in your application.\n\n\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    LineReceiver lineReceiver = dag.addOperator(\nLineReceiver\n, new LineReceiver());\n    ...\n    AutoMetric.DimensionsScheme dimensionsScheme = new AutoMetric.DimensionsScheme()\n    {\n      String[] timeBuckets = new String[] { \n1s\n, \n1m\n, \n1h\n };\n      String[] lengthAggregators = new String[] { \nIIRAVG\n, \nSUM\n };\n      String[] countAggregators = new String[] { \nSUM\n };\n\n      /* Setting the aggregation time bucket to be one second, one minute and one hour */\n      @Override\n      public String[] getTimeBuckets()\n      {\n        return timeBuckets;\n      }\n\n      @Override\n      public String[] getDimensionAggregationsFor(String logicalMetricName)\n      {\n        if (\nlength\n.equals(logicalMetricName)) {\n          return lengthAggregators;\n        } else if (\ncount\n.equals(logicalMetricName)) {\n          return countAggregators;\n        } else {\n          return null; // use default\n        }\n      }\n    };\n\n    dag.setAttribute(lineReceiver, OperatorContext.METRICS_DIMENSIONS_SCHEME, dimensionsScheme);\n    ...\n  }\n\n\n\n\nDashboards\n\n\nWith App Data Tracker enabled, you can visualize the AutoMetrics and system metrics in the Dashboards within dtManage.   Refer back to the diagram in the App Data Tracker section, dtGateway relays queries and query results to and from the App Data Tracker.  In this way, dtManage sends queries and receives results from the App Data Tracker via dtGateway and uses the results to let the user visualize the data.\n\n\nClick on the visualize button in dtManage's application page.\n\n\n\n\nYou will see the dashboard for the AutoMetrics and the system metrics.\n\n\n\n\nThe left widget shows the AutoMetrics of \nline\n and \ncount\n for the LineReceiver operator.  The right widget shows the system metrics.\n\n\nThe Dashboards have some simple builtin widgets to visualize the data.  Line charts and bar charts are some examples.\nUsers will be able to implement their own widgets to visualize their data.", 
            "title": "AutoMetric API"
        }, 
        {
            "location": "/autometrics/#apache-apex-autometrics", 
            "text": "", 
            "title": "Apache Apex AutoMetrics"
        }, 
        {
            "location": "/autometrics/#introduction", 
            "text": "Metrics collect various statistical information about a process which can be very useful for diagnosis. Auto Metrics in Apex can help monitor operators in a running application.  The goal of  AutoMetric  API is to enable operator developer to define relevant metrics for an operator in a simple way which the platform collects and reports automatically.", 
            "title": "Introduction"
        }, 
        {
            "location": "/autometrics/#specifying-autometrics-in-an-operator", 
            "text": "An  AutoMetric  can be any object. It can be of a primitive type - int, long, etc. or a complex one. A field or a  get  method in an operator can be annotated with  @AutoMetric  to specify that its value is a metric. After every application end window, the platform collects the values of these fields/methods in a map and sends it to application master.  public class LineReceiver extends BaseOperator\n{\n @AutoMetric\n long length;\n\n @AutoMetric\n long count;\n\n public final transient DefaultInputPort String  input = new DefaultInputPort String ()\n {\n   @Override\n   public void process(String s)\n   {\n     length += s.length();\n     count++;\n   }\n };\n\n @Override\n public void beginWindow(long windowId)\n {\n   length = 0;\n   count = 0;\n }\n}  There are 2 auto-metrics declared in the  LineReceiver . At the end of each application window, the platform will send a map with 2 entries -  [(length, 100), (count, 10)]  to the application master.", 
            "title": "Specifying AutoMetrics in an Operator"
        }, 
        {
            "location": "/autometrics/#aggregating-autometrics-across-partitions", 
            "text": "When an operator is partitioned, it is useful to aggregate the values of auto-metrics across all its partitions every window to get a logical view of these metrics. The application master performs these aggregations using metrics aggregators.  The AutoMetric API helps to achieve this by providing an interface for writing aggregators-  AutoMetric.Aggregator . Any implementation of  AutoMetric.Aggregator  can be set as an operator attribute -  METRICS_AGGREGATOR  for a particular operator which in turn is used for aggregating physical metrics.", 
            "title": "Aggregating AutoMetrics across Partitions"
        }, 
        {
            "location": "/autometrics/#default-aggregators", 
            "text": "MetricsAggregator  is a simple implementation of  AutoMetric.Aggregator  that platform uses as a default for summing up primitive types - int, long, float and double.  MetricsAggregator  is just a collection of  SingleMetricAggregator s. There are multiple implementations of  SingleMetricAggregator  that perform sum, min, max, avg which are present in Apex core and Apex malhar.  For the  LineReceiver  operator, the application developer need not specify any aggregator. The platform will automatically inject an instance of  MetricsAggregator  that contains two  LongSumAggregator s - one for  length  and one for  count . This aggregator will report sum of length and sum of count across all the partitions of  LineReceiver .", 
            "title": "Default aggregators"
        }, 
        {
            "location": "/autometrics/#building-custom-aggregators", 
            "text": "Platform cannot perform any meaningful aggregations for non-numeric metrics. In such cases, the operator or application developer can write custom aggregators. Let\u2019s say, if the  LineReceiver  was modified to have a complex metric as shown below.  public class AnotherLineReceiver extends BaseOperator\n{\n  @AutoMetric\n  final LineMetrics lineMetrics = new LineMetrics();\n\n  public final transient DefaultInputPort String  input = new DefaultInputPort String ()\n  {\n    @Override\n    public void process(String s)\n    {\n      lineMetrics.length += s.length();\n      lineMetrics.count++;\n    }\n  };\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n    lineMetrics.length = 0;\n    lineMetrics.count = 0;\n  }\n\n  public static class LineMetrics implements Serializable\n  {\n    long length;\n    long count;\n\n    private static final long serialVersionUID = 201511041908L;\n  }\n}  Below is a custom aggregator that can calculate average line length across all partitions of  AnotherLineReceiver .  public class AvgLineLengthAggregator implements AutoMetric.Aggregator\n{\n\n  Map String, Object  result = Maps.newHashMap();\n\n  @Override\n  public Map String, Object  aggregate(long l, Collection AutoMetric.PhysicalMetricsContext  collection)\n  {\n    long totalLength = 0;\n    long totalCount = 0;\n    for (AutoMetric.PhysicalMetricsContext pmc : collection) {\n      AnotherLineReceiver.LineMetrics lm = (AnotherLineReceiver.LineMetrics)pmc.getMetrics().get( lineMetrics );\n      totalLength += lm.length;\n      totalCount += lm.count;\n    }\n    result.put( avgLineLength , totalLength/totalCount);\n    return result;\n  }\n}  An instance of above aggregator can be specified as the  METRIC_AGGREGATOR  for  AnotherLineReceiver  while creating the DAG as shown below.    @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    AnotherLineReceiver lineReceiver = dag.addOperator( LineReceiver , new AnotherLineReceiver());\n    dag.setAttribute(lineReceiver, Context.OperatorContext.METRICS_AGGREGATOR, new AvgLineLengthAggregator());\n    ...\n  }", 
            "title": "Building custom aggregators"
        }, 
        {
            "location": "/autometrics/#retrieving-autometrics", 
            "text": "The Gateway REST API provides a way to retrieve the latest AutoMetrics for each logical operator.  For example:  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n     autoMetrics : {\n        count :  71314 ,\n        length :  27780706 \n    },\n     className :  com.datatorrent.autometric.LineReceiver ,\n    ...\n}", 
            "title": "Retrieving AutoMetrics"
        }, 
        {
            "location": "/autometrics/#system-metrics", 
            "text": "System metrics are standard operator metrics provided by the system.  Examples include:   processed tuples per second  emitted tuples per second  total tuples processed  total tuples emitted  latency  CPU percentage  failure count  checkpoint elapsed time   The Gateway REST API provides a way to retrieve the latest values for all of the above for each of the logical operators in the application.  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n     cpuPercentageMA :  {cpuPercentageMA} ,\n     failureCount :  {failureCount} ,\n     latencyMA :  {latencyMA} ,  \n     totalTuplesEmitted :  {totalTuplesEmitted} ,\n     totalTuplesProcessed :  {totalTuplesProcessed} ,\n     tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n     tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n    ...\n}  However, just like AutoMetrics, the Gateway only provides the latest metrics.  For historical metrics, we will need the help of App Data Tracker.", 
            "title": "System Metrics"
        }, 
        {
            "location": "/autometrics/#app-data-tracker", 
            "text": "As discussed above, STRAM aggregates the AutoMetrics from physical operators (partitions) to something that makes sense in one logical operator.  It pushes the aggregated AutoMetrics values using Websocket to the Gateway at every second along with system metrics for each operator.  Gateway relays the information to an application called App Data Tracker.  It is another Apex application that runs in the background and further aggregates the incoming values by time bucket and stores the values in HDHT.  It also allows the outside to retrieve the aggregated AutoMetrics and system metrics through websocket interface.   App Data Tracker is enabled by having these properties in dt-site.xml:  property \n   name dt.appDataTracker.enable /name \n   value true /value  /property  property \n   name dt.appDataTracker.transport /name \n   value builtin:AppDataTrackerFeed /value  /property  property \n   name dt.attr.METRICS_TRANSPORT /name \n   value builtin:AppDataTrackerFeed /value  /property   All the applications launched after the App Data Tracker is enabled will have metrics sent to it.  Note : The App Data Tracker will be shown running in dtManage as a \u201csystem app\u201d.  It will show up if the \u201cshow system apps\u201d button is pressed.  By default, the time buckets App Data Tracker aggregates upon are one minute, one hour and one day.  It can be overridden by changing the operator attribute  METRICS_DIMENSIONS_SCHEME .  Also by default, the app data tracker performs all these aggregations: SUM, MIN, MAX, AVG, COUNT, FIRST, LAST on all number metrics.  You can also override by changing the same operator attribute  METRICS_DIMENSIONS_SCHEME , provided the custom aggregator is known to the App Data Tracker.  (See next section)", 
            "title": "App Data Tracker"
        }, 
        {
            "location": "/autometrics/#custom-aggregator-in-app-data-tracker", 
            "text": "Custom aggregators allow you to do your own custom computation on statistics generated by any of your applications. In order to implement a Custom aggregator you have to do two things:   Combining new inputs with the current aggregation  Combining two aggregations together into one aggregation   Let\u2019s consider the case where we want to perform the following rolling average:  Y_n = \u00bd * X_n + \u00bd * X_n-1 + \u00bc * X_n-2 + \u215b * X_n-3 +...  This aggregation could be performed by the following Custom Aggregator:  @Name( IIRAVG )\npublic class AggregatorIIRAVG extends AbstractIncrementalAggregator\n{\n  ...\n\n  private void aggregateHelper(DimensionsEvent dest, DimensionsEvent src)\n  {\n    double[] destVals = dest.getAggregates().getFieldsDouble();\n    double[] srcVals = src.getAggregates().getFieldsDouble();\n\n    for (int index = 0; index   destLongs.length; index++) {\n      destVals[index] = .5 * destVals[index] + .5 * srcVals[index];\n    }\n  }\n\n  @Override\n  public void aggregate(Aggregate dest, InputEvent src)\n  {\n    //Aggregate a current aggregation with a new input\n    aggregateHelper(dest, src);\n  }\n\n  @Override\n  public void aggregate(Aggregate destAgg, Aggregate srcAgg)\n  {\n    //Combine two existing aggregations together\n    aggregateHelper(destAgg, srcAgg);\n  }\n}", 
            "title": "Custom Aggregator in App Data Tracker"
        }, 
        {
            "location": "/autometrics/#discovery-of-custom-aggregators", 
            "text": "AppDataTracker searches for custom aggregator jars under the following directories statically before launching:   {dt_installation_dir}/plugin/aggregators  {user_home_dir}/.dt/plugin/aggregators   It uses reflection to find all the classes that extend from  IncrementalAggregator  and  OTFAggregator  in these jars and registers them with the name provided by  @Name  annotation (or class name when  @Name  is absent).", 
            "title": "Discovery of Custom Aggregators"
        }, 
        {
            "location": "/autometrics/#using-metrics_dimensions_scheme", 
            "text": "Here is a sample code snippet on how you can make use of  METRICS_DIMENSIONS_SCHEME  to set your own time buckets and your own set of aggregators for certain  AutoMetric s performed by the App Data Tracker in your application.    @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    LineReceiver lineReceiver = dag.addOperator( LineReceiver , new LineReceiver());\n    ...\n    AutoMetric.DimensionsScheme dimensionsScheme = new AutoMetric.DimensionsScheme()\n    {\n      String[] timeBuckets = new String[] {  1s ,  1m ,  1h  };\n      String[] lengthAggregators = new String[] {  IIRAVG ,  SUM  };\n      String[] countAggregators = new String[] {  SUM  };\n\n      /* Setting the aggregation time bucket to be one second, one minute and one hour */\n      @Override\n      public String[] getTimeBuckets()\n      {\n        return timeBuckets;\n      }\n\n      @Override\n      public String[] getDimensionAggregationsFor(String logicalMetricName)\n      {\n        if ( length .equals(logicalMetricName)) {\n          return lengthAggregators;\n        } else if ( count .equals(logicalMetricName)) {\n          return countAggregators;\n        } else {\n          return null; // use default\n        }\n      }\n    };\n\n    dag.setAttribute(lineReceiver, OperatorContext.METRICS_DIMENSIONS_SCHEME, dimensionsScheme);\n    ...\n  }", 
            "title": "Using METRICS_DIMENSIONS_SCHEME"
        }, 
        {
            "location": "/autometrics/#dashboards", 
            "text": "With App Data Tracker enabled, you can visualize the AutoMetrics and system metrics in the Dashboards within dtManage.   Refer back to the diagram in the App Data Tracker section, dtGateway relays queries and query results to and from the App Data Tracker.  In this way, dtManage sends queries and receives results from the App Data Tracker via dtGateway and uses the results to let the user visualize the data.  Click on the visualize button in dtManage's application page.   You will see the dashboard for the AutoMetrics and the system metrics.   The left widget shows the AutoMetrics of  line  and  count  for the LineReceiver operator.  The right widget shows the system metrics.  The Dashboards have some simple builtin widgets to visualize the data.  Line charts and bar charts are some examples.\nUsers will be able to implement their own widgets to visualize their data.", 
            "title": "Dashboards"
        }, 
        {
            "location": "/dtingest/", 
            "text": "dtIngest Tutorial\n\n\n\"dtIngest\" is a DataTorrent application that ingest data from various\nsources and egress the processed data to various sinks. The data movement\nhappens at scale and in parallel. To know more about dtIngest please\nrefer the \ndtIngest\nblog\n.\n\n\nThis tutorial refers to dtIngest version 1.0.0\n\n\nPre-requisites\n\n\n\n\n\n\nDatatorrent RTS on a Hadoop cluster. Please\n    refer to \nInstallation\n    guide\n\u00a0for\n    details.\n\n\n\n\n\n\nSource and destination file systems must be accessible from all\n    DataTorrent RTS nodes. It can be any of HDFS,\n    NFS, S3, FTP. For sandbox image or single node hadoop cluster you can also use local\n    files as source. But, for multi-node cluster local files cannot be used as source.\n\n\n\n\n\n\nIf source or destination file system is NFS; then NFS\n    should be mounted on all the nodes within the Hadoop cluster at a\n    common mount point and should have read/write permission to the user\n    running dtIngest application.\n\n\n\n\n\n\nLaunching dtIngest\n\n\ndtIngest application can be configured and launched from \nDatatorrent\nManagement\nConsole\n.\n\n\n\n\n\n\nNavigate to 'Develop' tab.\n    \n\n\n\n\n\n\nThe dtIngest application package is already uploaded and available\n    to use under 'Application Packages' section.\u00a0\n\n\n\n\n\n\nSelect 'Ingestion Application' from the list of App\u00a0packages. And click\n    on \u2018launch application\u2019 button.\n    \n\n\n\n\n\n\nConfiguration page for dtingest is displayed after the 'launch'.\n    Enter the configuration values and click\n    'Launch' to ingest your data.\n\n\n\n\n\n\nConfiguring dtIngest Instance Properties\n\n\n\n\n\n\nIn the 'Name this application' textbox; name the application instance. For example, 'Ingestion\n    test'\n    \n\n\n\n\n\n\nLeave 'Specify a queue' unchecked to use default queue.\n\n\nIf you want to specify a queue to launch this application, check 'Specify a queue' checkbox and select queue from the\ndropdown. For more information, go to \nHadoop Capacity Scheduler Docs\n\n\n\n\n\n\n\n\nUnder 'Use config a file' option, check the box to use existing configuration file. Select file from drop down to load the configuration file.\n    \n\n\nOnce it is loaded, you can modify the values. You can save the new configuration as a new file or overwrite the existing one. \n\n\nLeave 'Use a config file' unchecked to create a new one. \n\n\n\n\n\n\nConfigure input source, refer to \nConfiguring input\n    source\n\u00a0section for details.\n\n\n\n\n\n\nConfigure output destination, refer to \nConfiguring output\n    destination\n\u00a0section for details.\n\n\n\n\n\n\nConfigure processing steps, refer to \nConfiguring processing\n    steps\n\u00a0section for details.\n\n\n\n\n\n\nUnder 'Save Configuration file' give name for configuration; if\n    you wish to save this combination of values for future use.\n    You may keep this blank if you do not want to save this for future use.\u00a0\n\n\n\n\n\n\nConfiguring Input Source\n\n\nConfiguring HDFS input\n\n\n\n\n\n\nFor 'Input data source' field; select 'HDFS' option from the\n    drop-down.\n\n\n\n\n\n\n\n\nUnder 'Source directories'; specify complete URL for the file path\n    to be ingested.\n\n    For example, if the namenode is 'namenode1.cluster.company.org' and\n    port is '8020' and file path is ''/user/john/data' then complete URL in\n    this case will be\n    \nhdfs://namenode1.cluster.company.org:8020/user/john/data\n.\n\n    Where,\n\n\n\n\nhdfs://\n indicates HDFS protocol\n\n\nnamenode1.cluster.company.org\n indicates fully qualified domain\n   name for the namenode of source HDFS.\n\n\n8020\n indicates port number for HDFS namenode service\n\n\n/user/john/data\n indicates full path for destination directory  \n\n\n\n\n\n\nIf there are more than one directories/file to be ingested, click on\n'Add directory' button and specify complete URL file path to be\ningested.\n\n\n\n\n\n\nIn the 'Filtering criteria' field, specify regular\u00a0expression for\n    files to be copied. \u00a0For regular expression syntax, please refer to\n    \nJava regular expression  documentation\n.\n     For example, if only \n.log\n files need to be ingested, then use \n.*\\.log\n as regular expression.  \n\n\n\n\nWhere,\n-   \n.*\n indicates any character zero or more times\n-   \n\\.\n indicates dot escaped with backslash\n-   \nlog\n indicates desired extension which is 'log'\n\n\nIn this case, dtingest ingests only '.log' files from the source\ndirectories.\n\n\n\n\n\n\nUnder 'Runs' field, select 'Single run' if you want\n    application to shutdown after completing files ingestion.\n\n\nSelect 'Polling' if you expect application to periodically poll the\ndirectory/file for changes. File change is based on timestamp difference. Entire file will be ingested again in case of\nany change.\n\n\nIf 'Polling' mode is selected, then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.  \n\n\n\n\n\n\n\n\nConfiguring\u00a0NFS input\n\n\n\n\n\n\nFor 'Input data source' field; select 'File/NFS' option from the\n    drop-down. \n\n\n\n\n\n\nUnder 'Source directories'; specify complete URL file path.\n\n\nFor example, if the NFS mount is located at '/disk5/nfsmount' and\n'path/to/data/directory' is the directory under this mount which needs\nto be ingested; then complete URL in this case will be \nfile:///disk5/nfsmount/path/to/data/directory\n.\nWhere,\n-   \nfile://\n indicates that it is some file system mounted on the node.\n-   \n/disk5/nfsmount/\n indicates the mount point. Note that, this has\nto be uniform across all the nodes in the cluster\n-   \npath/to/data/directory\n is the directory to be ingested\n\n\n\n\nNote that, for the above example, there should be \n///\n (triple slash)\nafter \nfile:\n.\n\n\n\n\n\n\nIf there are more than one directories to be ingested, click on\n    'Add directory' button and specify complete URL file\n    path.\n\n\n\n\n\n\nIf there are specific files, as opposed to a directory,specify complete\n    URL file path.\n\n\nFor example, \u00a0if some other NFS mount is located at '/disk6/nfsmount2' and 'path/to/file/to/copy/datafile.txt' is a file under this mount which needs\nto be ingested; then complete URL in this case will be \nfile:///disk6/nfsmount2/path/to/file/to/copy/datafile.txt\n.\n\n\n\n\n\n\n\n\nIn the 'Filtering criteria' field, specify regular expression for\n    files to be copied.\n    For example, if only \n.log\n files need to be ingested; then use \n.*\\.log\n as regular expression.\n\n\n\n\nWhere,\n-   \n.\\*\n indicates any character zero or more times\n-   \n\\\\.\n indicates dot escaped with backslash '\\'\n-   \nlog\n indicates desired extension which is 'log'\n\n\nTherefore, dtingest ingests only \n.log\n files from the source directories.\n\n\n\n\n\n\nUnder 'Runs' field, select 'Single run' if you want application\n    to shutdown after ingesting files currently present in the directory.\n\n\nSelect 'Polling' if you expect application to periodically poll the\ndirectory/file for changes. File change detection is based on timestamp. Entire file will be ingested again in case\nof any change.\n\n\nIf 'Polling' mode is selected; then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.\n\n\n\n\n\n\n\n\nConfiguring FTP input\n\n\nThis section gives details about how to ingest files/directories from FTP using dtIngest. \u00a0\n\n\n\n\n\n\nSelect \u00a0FTP as input type\n    \n\n\n\n\n\n\nAfter selecting the FTP as input type, snapshot of UI as below:     \n\n\n\n\n\n\nThe format for FTP URL input is as follows:  \nftp://username:password@host:port/path\n\n    where,\n\n\n\n\nftp\n : \u00a0protocol name\n\n\nusername\n : \u00a0username for ftp server\n\n\npassword\n : password\n\n\nhost\n : FTP host\n\n\nport\n : port number\n\n\npath\n : path to either file / directory\n\n\n\n\n\n\nTo copy multiple files/directories, see below: \n\n\n\nTo copy multiple directories, see below:\n\n\n\n\n\n\n\nConfiguring Amazon S3 input\n\n\nFor details on Amazon Simple Storage Service (S3), please go to \nAmazon S3\nDocumentation\n. This section gives details about how to\ningest files/directories from S3 using dtIngest. \u00a0\n\n\n\n\n\n\nSelect \u00a0S3 as input type\n\n\n\n\n\n\n\nAfter selecting the S3 as source type then UI looks like as below:\n\n\n\n\n\n\n\nConfigure S3 input url.\n\n\nInput url for S3 needs to be provided in following format,\n\n\ns3n://ukey:upass@bucketName/path\n\n\nwhere,\n- \ns3n\n: \u00a0protocol name\n- \nukey\n: access key\n- \nupass\n: secret access key\n- \nbucketName\n : bucketName\n- \npath\n : path to either file / directory\n\n\n\n\n\n\n\n  If you want to copy multiple directories, then click on (+) button and\nspecify the url\u2019s, UI would be as below:\n  \n\n\nConfiguring Kafka input\n\n\nFor more details on Kafka, please refer to \nApache Kafka\nDocumentation\n.\n\n\nThis section gives details about how to ingest messages from Kafka using dtIngest.\n\n\n\n\n\n\nSelect Kafka as input type\n    \n\n\n\n\n\n\nAfter selecting Kafka as input type then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure topic name and Zookeeper quorum.\n    Zookeeper quorum \u00a0is a string in the form of\n    \nhostname1:port1,hostname2:port2,hostname3:port3\n\n\nwhere,\n\n\n\n\nhostname1,hostname2,hostname3\n are hosts\n\n\nport1,port2,port3\n are ports of zookeeper server\n\n\n\n\ne.g. localhost:2181,localhost:2182\n\n\n\n\n\n\n\nSelect the offset type (default is \u201cLatest\u201d). If\n    you want to consume messages from beginning of Kafka queue, then\n    select \u201cEarliest\u201d offset option.\n\n\n\n\n\n\nIf the topic name is same across the Kafka clusters and want to\n    ingest data from these clusters, then configure the Zookeeper quorum\n    as follows:\n\n\nc1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6\n\n\nwhere,\n- \nc1,c2,c3\n indicates the cluster names,\n- \nhs1,hs2,hs3,hs4,hs5,hs6\n are zookeeper host names\n- \np1,p2,p3,p4,p5,p6\n are corresponding ports.\n\n\nFor\u00a0example, ClusterA and ClusterB are 2 Kafka clusters as below, then\nZookeeper quorum would be as \nClusterA::node3.example.com:2181,node4.example.com:2181;ClusterB::node8.example.com:2181\n\n\n\n\n\n\n\n\nConfiguring JMS input\n\n\nThis section gives details about how to ingest messages from\nJMS using dtIngest. \u00a0\n\n\n\n\n\n\nSelect JMS as input type.\n    \n\n\n\n\n\n\nAfter selecting the JMS as source type then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure Broker URL and topic name as tcp://hostName:port\n    \n\n\n\n\n\n\nConfiguring Output Destination\n\n\nConfiguring HDFS output\n\n\n\n\n\n\nFor 'Output Location' field, select 'HDFS' option from the\n    drop-down.\n\n\n\n\n\n\n\n\nUnder 'Target directory' specify complete HDFS path URL of the destination directory. For example,   \nhdfs://namenode1.cluster.company.org:8020/user/username/path/to/destination/directory\n\n\n\n\n\n\n\n\nWhere,\n    - \nhdfs://\n indicates HDFS protocol\n    - \nnamenode1.cluster.company.org\n indicates fully qualified domain name for\n    the namenode of destination HDFS.\n    - \n:8020\n indicates port number for HDFS namenode service\n    - \n/user/username/path/to/destination/directory\n indicates full path\n    for destination directory.\n\n\n\n\n\n\nUnder 'Recursive copy' option, select 'Yes' if you wish to copy\n    entire directory structure under source directory to the\n    destination. Select 'No' if you want non-recursive copy.\n\n\n\n\n\n\nUnder 'Overwrite conflicting files' option, select 'Yes' if you\n    wish to overwrite the file at the destination if file with the same\n    name is discovered under input source.\n\n\n\n\n\n\nCompact files\n\n\nUse 'Compact files' feature if you want to partition data into fix size. This\ncan be used to combine large number of small files into partitions of\nmanageable size. Vice versa, you can break down a very\nlarge file into partitions of manageable size.\n\n\n\n\n\n\n\n\nSelect 'yes' for radio button under 'Compact files' option. This\n    will display additional options for compaction. If you do not\n    want to compact files but copy them as they are; then select 'no'\n    for 'Compact files' option. If you select 'no' ; additional\n    options for compaction will be hidden.\n\n\n\n\n\n\nSelect delimiter to be used for separating contents of the files.\n    This will be useful if you decide to use some custom logic for\n    parsing partition files. Default value for 'delimiter' option is\n    'none'. You can use new line or any other custom delimiter based\n    on your requirement. Note that, special characters in the custom\n    delimiter should be escaped with \n\\\n. For example, tab character\n    \n\\t\n should be specified as \n\\\\t\n.\n\n\n\n\n\n\nSpecify the size for each partition under 'Max compacted file\n    size'. You can specify partition size in bytes, MB, GB. Data will\n    spill over to the next partition once this size is reached.\n\n\n\n\n\n\nNote that, partition will be of exact sizes in case of continuous\nincoming data. If there is no incoming data for consecutive 600 windows\nthen that partition will be committed to the HDFS. In this case, new\nincoming data will be spilled to the next partition.\n\n\nConfiguring NFS output\n\n\n\n\n\n\nFor \u2018Output Location\u2019 field; select \u2018File/NFS\u2019 option from the\n    drop-down  \n\n\n\n\n\n\n\n\nUnder \u2018Target directory\u2019 specify complete NFS path URL of the destination directory.\n\n    For example, \u00a0if the NFS mount is located at '/disk5/nfsmount' and\n    'path/to/data/directory' is the directory under this mount which\n    needs to be ingested; then complete URL in this case will be\n    \nfile:///disk5/nfsmount/path/to/data/directory\n\n\nWhere,\n\n\n\n\nfile://\n indicates that it is some file system mounted on the node.\n\n\n/disk5/nfsmount/\n indicates the mount point.\nNote that, this has to be uniform across all the nodes in cluster.\n\n\npath/to/data/directory\n is the directory to be ingested\n\n\n\n\n\n\nNote that, for the above example, there should be \n///\n (triple slash)\nafter \nfile:\n.\n\n\n\n\n\n\nConfiguring FTP output\n\n\n\n\n\n\nSelect FTP as output type.\n    \n\n\n\n\n\n\nAfter selecting FTP as output type then UI looks like as below:   \n\n\n\n\n\n\nSpecify the destination URL below the \u201cOutput directory\u201d label.\n    The FTP Output URL is as follows: \nftp://username:password@host:port/path\n\n\nWhere,\n- \nftp\n : \u00a0protocol name\n- \nusername\n : username for ftp server\n- \npassword\n : password\n- \nhost\n : FTP host\n- \nport\n : port number\n- \npath\n : Directory path to ingested\n\n\n\n\n\n\n\n\nConfiguring Amazon S3 output\n\n\n\n\n\n\nSelect S3 as output type.\n    \n\n\n\n\n\n\nAfter selecting S3 as output then UI looks like as below:\n    \n\n\n\n\n\n\nSpecify URL destination below the 'Output directory' label.\nThe S3 output URL is as follows: \ns3n://ukey:upass@bucketName/path\n\n    Where,\n\n\n\n\ns3n\n\u00a0: protocol name\n\n\nukey\n : access key\n\n\nupass\n :\u00a0secret access key\n\n\nbucketName\n : \u00a0bucketName\n\n\npath\n : Directory path\n\n\n\n\n\n\n\n\nConfiguring Kafka output\n\n\n\n\n\n\nSelect Kafka as output type.\n    \n\n\n\n\n\n\nAfter selecting Kafka as output then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure broker list and topic name.\n\n\n\n\n\n\nConfiguring JMS output\n\n\n\n\n\n\nSelect JMS as output type.\n\n\n\n\n\n\n\nAfter selecting JMS as output type then UI looks like as below:\n\n\n\n\n\n\n\nConfigure Broker URL and topic name as tcp://host:port\n\n\n\n\n\n\n\nConfiguring Processing Steps\n\n\nConfiguring compression\n\n\nSelect compression type on configuration page\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\n\n\nSelect LZO radio button to apply LZO compression\n\n\n\n\n\n\nLzo compression is not directly supported. To use lzo compression provide\n  plugin to ingestion app which provides lzo implementation and extends from java FilterOutputStream class. Copy plugin to \\~/.dt/plugins folder\n  (i.e. HOME_DIR/.dt/plugins) of the user who launches ingestion app.\n  We do ship default lzo plugin, and is available to download on maven repository at\nhttps://oss.sonatype.org/content/repositories/releases/com/datatorrent/dtIngest-lzo/1.0.0/\n\n\n\n\nSelect GZIP radio button to apply GZIP compression\n\n\n\n\n\nConfiguring encryption\n\n\nSelect encryption type on configuration page.\n\n\n\n\n\nApply AES encryption:\n\n\n\n\nSelect AES radio button to apply AES encryption\n\n\n\n\n\n\nProvide AES symmetric encryption key in \u201cAES key\u201d text box\n\n \u00a0 \u00a0 Note: AES symmetric key should be of size 128, 192 or 256 bits.\n\n\n\n\n\n\n\nApply PKI encryption:  \n\n\n\n\nSelect PKI encryption button to apply PKI encryption  \n\n\nProvide Asymmetric public key to be used for PKI encryption", 
            "title": "dtIngest"
        }, 
        {
            "location": "/dtingest/#dtingest-tutorial", 
            "text": "\"dtIngest\" is a DataTorrent application that ingest data from various\nsources and egress the processed data to various sinks. The data movement\nhappens at scale and in parallel. To know more about dtIngest please\nrefer the  dtIngest\nblog .  This tutorial refers to dtIngest version 1.0.0", 
            "title": "dtIngest Tutorial"
        }, 
        {
            "location": "/dtingest/#pre-requisites", 
            "text": "Datatorrent RTS on a Hadoop cluster. Please\n    refer to  Installation\n    guide \u00a0for\n    details.    Source and destination file systems must be accessible from all\n    DataTorrent RTS nodes. It can be any of HDFS,\n    NFS, S3, FTP. For sandbox image or single node hadoop cluster you can also use local\n    files as source. But, for multi-node cluster local files cannot be used as source.    If source or destination file system is NFS; then NFS\n    should be mounted on all the nodes within the Hadoop cluster at a\n    common mount point and should have read/write permission to the user\n    running dtIngest application.", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/dtingest/#launching-dtingest", 
            "text": "dtIngest application can be configured and launched from  Datatorrent\nManagement\nConsole .    Navigate to 'Develop' tab.\n        The dtIngest application package is already uploaded and available\n    to use under 'Application Packages' section.\u00a0    Select 'Ingestion Application' from the list of App\u00a0packages. And click\n    on \u2018launch application\u2019 button.\n        Configuration page for dtingest is displayed after the 'launch'.\n    Enter the configuration values and click\n    'Launch' to ingest your data.", 
            "title": "Launching dtIngest"
        }, 
        {
            "location": "/dtingest/#configuring-dtingest-instance-properties", 
            "text": "In the 'Name this application' textbox; name the application instance. For example, 'Ingestion\n    test'\n        Leave 'Specify a queue' unchecked to use default queue.  If you want to specify a queue to launch this application, check 'Specify a queue' checkbox and select queue from the\ndropdown. For more information, go to  Hadoop Capacity Scheduler Docs     Under 'Use config a file' option, check the box to use existing configuration file. Select file from drop down to load the configuration file.\n      Once it is loaded, you can modify the values. You can save the new configuration as a new file or overwrite the existing one.   Leave 'Use a config file' unchecked to create a new one.     Configure input source, refer to  Configuring input\n    source \u00a0section for details.    Configure output destination, refer to  Configuring output\n    destination \u00a0section for details.    Configure processing steps, refer to  Configuring processing\n    steps \u00a0section for details.    Under 'Save Configuration file' give name for configuration; if\n    you wish to save this combination of values for future use.\n    You may keep this blank if you do not want to save this for future use.", 
            "title": "Configuring dtIngest Instance Properties"
        }, 
        {
            "location": "/dtingest/#configuring-input-source", 
            "text": "", 
            "title": "Configuring Input Source"
        }, 
        {
            "location": "/dtingest/#configuring-hdfs-input", 
            "text": "For 'Input data source' field; select 'HDFS' option from the\n    drop-down.     Under 'Source directories'; specify complete URL for the file path\n    to be ingested. \n    For example, if the namenode is 'namenode1.cluster.company.org' and\n    port is '8020' and file path is ''/user/john/data' then complete URL in\n    this case will be\n     hdfs://namenode1.cluster.company.org:8020/user/john/data . \n    Where,   hdfs://  indicates HDFS protocol  namenode1.cluster.company.org  indicates fully qualified domain\n   name for the namenode of source HDFS.  8020  indicates port number for HDFS namenode service  /user/john/data  indicates full path for destination directory      If there are more than one directories/file to be ingested, click on\n'Add directory' button and specify complete URL file path to be\ningested.    In the 'Filtering criteria' field, specify regular\u00a0expression for\n    files to be copied. \u00a0For regular expression syntax, please refer to\n     Java regular expression  documentation .\n     For example, if only  .log  files need to be ingested, then use  .*\\.log  as regular expression.     Where,\n-    .*  indicates any character zero or more times\n-    \\.  indicates dot escaped with backslash\n-    log  indicates desired extension which is 'log'  In this case, dtingest ingests only '.log' files from the source\ndirectories.    Under 'Runs' field, select 'Single run' if you want\n    application to shutdown after completing files ingestion.  Select 'Polling' if you expect application to periodically poll the\ndirectory/file for changes. File change is based on timestamp difference. Entire file will be ingested again in case of\nany change.  If 'Polling' mode is selected, then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.", 
            "title": "Configuring HDFS input"
        }, 
        {
            "location": "/dtingest/#configuring-nfs-input", 
            "text": "For 'Input data source' field; select 'File/NFS' option from the\n    drop-down.     Under 'Source directories'; specify complete URL file path.  For example, if the NFS mount is located at '/disk5/nfsmount' and\n'path/to/data/directory' is the directory under this mount which needs\nto be ingested; then complete URL in this case will be  file:///disk5/nfsmount/path/to/data/directory .\nWhere,\n-    file://  indicates that it is some file system mounted on the node.\n-    /disk5/nfsmount/  indicates the mount point. Note that, this has\nto be uniform across all the nodes in the cluster\n-    path/to/data/directory  is the directory to be ingested   Note that, for the above example, there should be  ///  (triple slash)\nafter  file: .    If there are more than one directories to be ingested, click on\n    'Add directory' button and specify complete URL file\n    path.    If there are specific files, as opposed to a directory,specify complete\n    URL file path.  For example, \u00a0if some other NFS mount is located at '/disk6/nfsmount2' and 'path/to/file/to/copy/datafile.txt' is a file under this mount which needs\nto be ingested; then complete URL in this case will be  file:///disk6/nfsmount2/path/to/file/to/copy/datafile.txt .     In the 'Filtering criteria' field, specify regular expression for\n    files to be copied.\n    For example, if only  .log  files need to be ingested; then use  .*\\.log  as regular expression.   Where,\n-    .\\*  indicates any character zero or more times\n-    \\\\.  indicates dot escaped with backslash '\\'\n-    log  indicates desired extension which is 'log'  Therefore, dtingest ingests only  .log  files from the source directories.    Under 'Runs' field, select 'Single run' if you want application\n    to shutdown after ingesting files currently present in the directory.  Select 'Polling' if you expect application to periodically poll the\ndirectory/file for changes. File change detection is based on timestamp. Entire file will be ingested again in case\nof any change.  If 'Polling' mode is selected; then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.", 
            "title": "Configuring\u00a0NFS input"
        }, 
        {
            "location": "/dtingest/#configuring-ftp-input", 
            "text": "This section gives details about how to ingest files/directories from FTP using dtIngest. \u00a0    Select \u00a0FTP as input type\n        After selecting the FTP as input type, snapshot of UI as below:         The format for FTP URL input is as follows:   ftp://username:password@host:port/path \n    where,   ftp  : \u00a0protocol name  username  : \u00a0username for ftp server  password  : password  host  : FTP host  port  : port number  path  : path to either file / directory    To copy multiple files/directories, see below:   To copy multiple directories, see below:", 
            "title": "Configuring FTP input"
        }, 
        {
            "location": "/dtingest/#configuring-amazon-s3-input", 
            "text": "For details on Amazon Simple Storage Service (S3), please go to  Amazon S3\nDocumentation . This section gives details about how to\ningest files/directories from S3 using dtIngest. \u00a0    Select \u00a0S3 as input type    After selecting the S3 as source type then UI looks like as below:    Configure S3 input url.  Input url for S3 needs to be provided in following format,  s3n://ukey:upass@bucketName/path  where,\n-  s3n : \u00a0protocol name\n-  ukey : access key\n-  upass : secret access key\n-  bucketName  : bucketName\n-  path  : path to either file / directory    \n  If you want to copy multiple directories, then click on (+) button and\nspecify the url\u2019s, UI would be as below:", 
            "title": "Configuring Amazon S3 input"
        }, 
        {
            "location": "/dtingest/#configuring-kafka-input", 
            "text": "For more details on Kafka, please refer to  Apache Kafka\nDocumentation .  This section gives details about how to ingest messages from Kafka using dtIngest.    Select Kafka as input type\n        After selecting Kafka as input type then UI looks like as below:\n        Configure topic name and Zookeeper quorum.\n    Zookeeper quorum \u00a0is a string in the form of\n     hostname1:port1,hostname2:port2,hostname3:port3  where,   hostname1,hostname2,hostname3  are hosts  port1,port2,port3  are ports of zookeeper server   e.g. localhost:2181,localhost:2182    Select the offset type (default is \u201cLatest\u201d). If\n    you want to consume messages from beginning of Kafka queue, then\n    select \u201cEarliest\u201d offset option.    If the topic name is same across the Kafka clusters and want to\n    ingest data from these clusters, then configure the Zookeeper quorum\n    as follows:  c1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6  where,\n-  c1,c2,c3  indicates the cluster names,\n-  hs1,hs2,hs3,hs4,hs5,hs6  are zookeeper host names\n-  p1,p2,p3,p4,p5,p6  are corresponding ports.  For\u00a0example, ClusterA and ClusterB are 2 Kafka clusters as below, then\nZookeeper quorum would be as  ClusterA::node3.example.com:2181,node4.example.com:2181;ClusterB::node8.example.com:2181", 
            "title": "Configuring Kafka input"
        }, 
        {
            "location": "/dtingest/#configuring-jms-input", 
            "text": "This section gives details about how to ingest messages from\nJMS using dtIngest. \u00a0    Select JMS as input type.\n        After selecting the JMS as source type then UI looks like as below:\n        Configure Broker URL and topic name as tcp://hostName:port", 
            "title": "Configuring JMS input"
        }, 
        {
            "location": "/dtingest/#configuring-output-destination", 
            "text": "", 
            "title": "Configuring Output Destination"
        }, 
        {
            "location": "/dtingest/#configuring-hdfs-output", 
            "text": "For 'Output Location' field, select 'HDFS' option from the\n    drop-down.     Under 'Target directory' specify complete HDFS path URL of the destination directory. For example,    hdfs://namenode1.cluster.company.org:8020/user/username/path/to/destination/directory     Where,\n    -  hdfs://  indicates HDFS protocol\n    -  namenode1.cluster.company.org  indicates fully qualified domain name for\n    the namenode of destination HDFS.\n    -  :8020  indicates port number for HDFS namenode service\n    -  /user/username/path/to/destination/directory  indicates full path\n    for destination directory.    Under 'Recursive copy' option, select 'Yes' if you wish to copy\n    entire directory structure under source directory to the\n    destination. Select 'No' if you want non-recursive copy.    Under 'Overwrite conflicting files' option, select 'Yes' if you\n    wish to overwrite the file at the destination if file with the same\n    name is discovered under input source.", 
            "title": "Configuring HDFS output"
        }, 
        {
            "location": "/dtingest/#compact-files", 
            "text": "Use 'Compact files' feature if you want to partition data into fix size. This\ncan be used to combine large number of small files into partitions of\nmanageable size. Vice versa, you can break down a very\nlarge file into partitions of manageable size.     Select 'yes' for radio button under 'Compact files' option. This\n    will display additional options for compaction. If you do not\n    want to compact files but copy them as they are; then select 'no'\n    for 'Compact files' option. If you select 'no' ; additional\n    options for compaction will be hidden.    Select delimiter to be used for separating contents of the files.\n    This will be useful if you decide to use some custom logic for\n    parsing partition files. Default value for 'delimiter' option is\n    'none'. You can use new line or any other custom delimiter based\n    on your requirement. Note that, special characters in the custom\n    delimiter should be escaped with  \\ . For example, tab character\n     \\t  should be specified as  \\\\t .    Specify the size for each partition under 'Max compacted file\n    size'. You can specify partition size in bytes, MB, GB. Data will\n    spill over to the next partition once this size is reached.    Note that, partition will be of exact sizes in case of continuous\nincoming data. If there is no incoming data for consecutive 600 windows\nthen that partition will be committed to the HDFS. In this case, new\nincoming data will be spilled to the next partition.", 
            "title": "Compact files"
        }, 
        {
            "location": "/dtingest/#configuring-nfs-output", 
            "text": "For \u2018Output Location\u2019 field; select \u2018File/NFS\u2019 option from the\n    drop-down       Under \u2018Target directory\u2019 specify complete NFS path URL of the destination directory. \n    For example, \u00a0if the NFS mount is located at '/disk5/nfsmount' and\n    'path/to/data/directory' is the directory under this mount which\n    needs to be ingested; then complete URL in this case will be\n     file:///disk5/nfsmount/path/to/data/directory  Where,   file://  indicates that it is some file system mounted on the node.  /disk5/nfsmount/  indicates the mount point.\nNote that, this has to be uniform across all the nodes in cluster.  path/to/data/directory  is the directory to be ingested    Note that, for the above example, there should be  ///  (triple slash)\nafter  file: .", 
            "title": "Configuring NFS output"
        }, 
        {
            "location": "/dtingest/#configuring-ftp-output", 
            "text": "Select FTP as output type.\n        After selecting FTP as output type then UI looks like as below:       Specify the destination URL below the \u201cOutput directory\u201d label.\n    The FTP Output URL is as follows:  ftp://username:password@host:port/path  Where,\n-  ftp  : \u00a0protocol name\n-  username  : username for ftp server\n-  password  : password\n-  host  : FTP host\n-  port  : port number\n-  path  : Directory path to ingested", 
            "title": "Configuring FTP output"
        }, 
        {
            "location": "/dtingest/#configuring-amazon-s3-output", 
            "text": "Select S3 as output type.\n        After selecting S3 as output then UI looks like as below:\n        Specify URL destination below the 'Output directory' label.\nThe S3 output URL is as follows:  s3n://ukey:upass@bucketName/path \n    Where,   s3n \u00a0: protocol name  ukey  : access key  upass  :\u00a0secret access key  bucketName  : \u00a0bucketName  path  : Directory path", 
            "title": "Configuring Amazon S3 output"
        }, 
        {
            "location": "/dtingest/#configuring-kafka-output", 
            "text": "Select Kafka as output type.\n        After selecting Kafka as output then UI looks like as below:\n        Configure broker list and topic name.", 
            "title": "Configuring Kafka output"
        }, 
        {
            "location": "/dtingest/#configuring-jms-output", 
            "text": "Select JMS as output type.    After selecting JMS as output type then UI looks like as below:    Configure Broker URL and topic name as tcp://host:port", 
            "title": "Configuring JMS output"
        }, 
        {
            "location": "/dtingest/#configuring-processing-steps", 
            "text": "", 
            "title": "Configuring Processing Steps"
        }, 
        {
            "location": "/dtingest/#configuring-compression", 
            "text": "Select compression type on configuration page\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0    Select LZO radio button to apply LZO compression    Lzo compression is not directly supported. To use lzo compression provide\n  plugin to ingestion app which provides lzo implementation and extends from java FilterOutputStream class. Copy plugin to \\~/.dt/plugins folder\n  (i.e. HOME_DIR/.dt/plugins) of the user who launches ingestion app.\n  We do ship default lzo plugin, and is available to download on maven repository at\nhttps://oss.sonatype.org/content/repositories/releases/com/datatorrent/dtIngest-lzo/1.0.0/   Select GZIP radio button to apply GZIP compression", 
            "title": "Configuring compression"
        }, 
        {
            "location": "/dtingest/#configuring-encryption", 
            "text": "Select encryption type on configuration page.   Apply AES encryption:   Select AES radio button to apply AES encryption    Provide AES symmetric encryption key in \u201cAES key\u201d text box \n \u00a0 \u00a0 Note: AES symmetric key should be of size 128, 192 or 256 bits.    Apply PKI encryption:     Select PKI encryption button to apply PKI encryption    Provide Asymmetric public key to be used for PKI encryption", 
            "title": "Configuring encryption"
        }, 
        {
            "location": "/rts/", 
            "text": "DataTorrent RTS Overview\n\n\nDataTorrent RTS is an enterprise product built around Apache Apex, a Hadoop-native unified stream and batch processing platform.  DataTorrent RTS combines Apache Apex engine with a set of enterprise-grade management, monitoring, development, and visualization tools.  \n\n\n\n\nDataTorrent RTS platform enables creation and management of real-time big data applications in a way that is\n\n\n\n\nhighly scalable and performant\n - millions of events per second per node with linear scalability\n\n\nfault tolerant\n - automatic recovery with no data or state loss\n\n\nHadoop native\n - installs in seconds and works with all existing Hadoop distributions\n\n\neasily developed\n - write and re-use generic Java code\n\n\neasily integrated\n - customizable connectors to file, database, and messaging systems\n\n\neasily operable\n - full suite of management, monitoring, development, and visualization tools\n\n\n\n\nThe system is capable of processing billions of events per second, while automatically recovering without any state or data loss when individual nodes fail.  A simple API enables developers to write new and re-use existing generic Java code, lowering the expertise needed to write big data applications.  A library of existing demos and re-usable operators allows applications to be developed quickly.  Native Hadoop support allows DataTorrent RTS to be installed in seconds on any existing Hadoop cluster.  Application adiminstration can be done from a browser with dtManage, a full suite of management, monitoring, and visualization tools.  New applications can be visually built from existing components using \ndtAssemble\n, a graphical application assembly tool.  Application data can be easily visualized with \ndtDashboard\n real-time data visualizations.", 
            "title": "RTS"
        }, 
        {
            "location": "/rts/#datatorrent-rts-overview", 
            "text": "DataTorrent RTS is an enterprise product built around Apache Apex, a Hadoop-native unified stream and batch processing platform.  DataTorrent RTS combines Apache Apex engine with a set of enterprise-grade management, monitoring, development, and visualization tools.     DataTorrent RTS platform enables creation and management of real-time big data applications in a way that is   highly scalable and performant  - millions of events per second per node with linear scalability  fault tolerant  - automatic recovery with no data or state loss  Hadoop native  - installs in seconds and works with all existing Hadoop distributions  easily developed  - write and re-use generic Java code  easily integrated  - customizable connectors to file, database, and messaging systems  easily operable  - full suite of management, monitoring, development, and visualization tools   The system is capable of processing billions of events per second, while automatically recovering without any state or data loss when individual nodes fail.  A simple API enables developers to write new and re-use existing generic Java code, lowering the expertise needed to write big data applications.  A library of existing demos and re-usable operators allows applications to be developed quickly.  Native Hadoop support allows DataTorrent RTS to be installed in seconds on any existing Hadoop cluster.  Application adiminstration can be done from a browser with dtManage, a full suite of management, monitoring, and visualization tools.  New applications can be visually built from existing components using  dtAssemble , a graphical application assembly tool.  Application data can be easily visualized with  dtDashboard  real-time data visualizations.", 
            "title": "DataTorrent RTS Overview"
        }, 
        {
            "location": "/dtmanage/", 
            "text": "dtManage Guide\n\n\nIntroduction\n\n\nThe DataTorrent Console (aka dtManage) is a web-based user interface that allows you to monitor and interact with the DataTorrent platform running on your Hadoop cluster. It is a web-based dashboard that is served by the DataTorrent Gateway, and has five areas: configuration, monitoring, development, and , visualization, and learning.\n\n\nTo download the platform or the VM sandbox, go to \nhttp://www.datatorrent.com/download\n.\n\n\n\n\nConnection Requirements\n\n\nWhen you install DataTorrent RTS on your Hadoop cluster using the installer binary, the DataTorrent Gateway service is started on the node where the installer is executed. By default, the Gateway serves the console from port 9090. The ability to connect to this node and port depends on your environment, and may require special setup such as an ssh tunnel, firewall configuration changes, VPN access, etc.\n\n\n\n\nBrowser Requirements\n\n\nThe Console currently supports Chrome, Firefox, Safari, and IE (version 10 and up).\n\n\nInstallation Wizard\n\n\nThe first time you open the Console, after installing DataTorrent RTS on your cluster, it will take you to the Installation Wizard. This walks you through the initial configuration of your DataTorrent installation, by confirming the following:\n\n\n\n\nLocation of the hadoop executable\n\n\nDFS location where all the DataTorrent files are stored\n\n\nDataTorrent license\n\n\nSummary and review of any remaining configuration items\n\n\n\n\n\n\nWhen Kerberos Security is Enabled\n\n\nWhen your hadoop cluster has security enabled with Kerberos, there will be four additional controls in the installation wizard: \n\n\n\n\nKerberos Principal\n: The Kerberos principal (e.g. primary/instance@REALM) to use on behalf of the management console.\n\n\nKerberos Keytab\n: The location (path) of the Kerberos keytab file to use on the gateway node's local file system.\n\n\nYARN delegation token lifetime\n: If the value of the \nyarn.resourcemanager.delegation.token.max-lifetime\n property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.\n\n\nNamenode delegation token lifetime\n: If the value of the \ndfs.namenode.delegation.token.max-lifetime\n property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.\n\n\n\n\n\n\nNote:\n The token lifetime values you enter will not actually set these values in your hadoop configuration, it is only meant to inform the DataTorrent platform of these values.\n\n\n\n\nConfigure Tab\n\n\nThe configuration page can be found by clicking the \u201cConfigure\u201d link in the main navigation bar at the top. There are links to various tools to help you configure and troubleshoot your DataTorrent installation. \nThe configuration page links may differ depending on your cluster setup. The following is a screenshot with a cluster that has simple authentication/authorization enabled.\n\n\n\n\nSystem Configuration\n\n\nThis page shows diagnostic information regarding the gateway and console, as well as any issues that the gateway may detect.\n\n\n\n\nIn addition, you can perform the following actions from this page:\n\n\nRestart the Gateway\n\n\n\nThis can be useful when Hadoop configuration has changed or some other factor of your cluster environment has changed.\n\n\nToggle Reporting\n\n\n\n\nIf enabled, your DataTorrent installation will send various pieces of information such as bug reporting and usage statistics back to our servers.\n\n\nLicense Information\n\n\nUse the License Information page to view how much of your DataTorrent license capacity your cluster is consuming as well as what capabilities your license permits. You can also upload new license files here.\n\n\n\n\nUser Profile\n\n\nThe User Profile page displays information about the current user, including their username, the authentication scheme being used, and the roles that the current user has. In addition, users can perform the following actions:\n\n\n\n\nchange password \n\n\nchange the default home page\n\n\nchange the theme of the console\n\n\nrestore the default options of the console\n\n\n\n\n\n\nUser Management\n\n\nUse this page to manage users and roles of your DataTorrent cluster:\n\n\n\n\nadd users\n\n\nchange users\u2019 roles\n\n\nchange users\u2019 password\n\n\ndelete users\n\n\nadd roles\n\n\nedit role permissions\n\n\ndelete roles\n\n\n\n\n\n\n\n\nNote:\n With most authentication schemes, the admin role cannot be deleted.\n\n\n\n\nInstallation Wizard\n\n\nAt any time, you can go back to the installation wizard from the Configuration Tab. It can help diagnose issues and reconfigure your cluster and gateway.\n\n\nDevelop Tab\n\n\nThe development area of dtManage is mainly geared towards the creation, upload, configuration, and launch of DataTorrent applications. The development home can be viewed by clicking the \u201cDevelop\u201d tab in the main navigation bar on the top of the screen. A prerequisite to using the development tools of the UI is an understanding of what Apex Application Packages are. For more information, see the \nApplication Packages Guide\n.\n\n\n\n\nApplication Packages\n\n\nTo access the application package listing, click on the \"Apps\" link from the Develop Tab index page. From here, you can perform several operators directly on application packages:\n\n\n\n\nDownload the app package\n\n\nDelete the app package\n\n\nCreate a new application in an application package via dtAssemble (requires enterprise license)\n\n\nLaunch applications in the app package\n\n\nImport default packages (see below)\n\n\n\n\n\n\n\n\nNote:\n If authentication is enabled, you may not be able to see others\u2019 app packages, depending on your permissions.\n\n\n\n\nImporting Default Packages\n\n\nWhen you install the DataTorrent platform, a folder located in the installation directory called \ndemos/app-packages\n will contain various default app packages that can be imported into HDFS for use. Just above the list of Application Packages, there should be a button that says \nImport default packages\n. Clicking this will take you to a page that shows the list of these demo app packages. Select one or more to import and click the \nImport\n button. This will upload the selected app package to HDFS.\n\n\n\n\nApplication Package Page\n\n\nOnce you have uploaded or imported an App Package, clicking on the package name in the list will take you to the Application Package Page, where you can view all the package details.\n\n\n\n\nAside from various pieces of meta information (owner, DataTorrent version, required properties, etc), you will see a list of apps found in this package. \n\n\nLaunching Apps\n\n\nTo launch an app in an App Package, click on the launch button to the far right of the list. A dialog box will appear with several options: \n\n\n\n\nSpecify a name for the running app\n\n  The console will pre-populate this field with an appropriate name, but you can specify your own name. Just be weary that it must be unique compared to the other applications running on your DataTorrent installation.\n\n\nSpecify the \nscheduler queue\n\n  This input allows you to specify which queue you want the application to be launched under. The default behavior depends on your Hadoop installation, but typically will be \nroot.[USER_NAME]\n.\n\n\nUse a config file when launching\n\n  App Package config files are xml files that contain \nproperties\n that get interpreted and used for launching an application. To choose one, enable the check box and choose the config file you want to use for launch.\n\n\nSpecify custom properties\n\n  In addition to choosing a config file, you may also specify properties directly in the launch popup by selecting this option. Note that there are three helpful functions when specifying custom properties:\n\n\nadd required properties\n - App Packages can have required properties (for example, twitter API access keys for the twitter demo). This function adds a new property with the required property name to the form, making it easy to fill in.\n\n\nadd default properties\n - App Packages can also have default properties. This function will add the default properties to the list, making it easy for you to override the defaults\n\n\nsave this configuration as\u2026\n - This function creates a new config file and saves it to the App Package, which can then be used later to launch the app with. That way, you can relaunch the app with the same properties without having to re-enter them.\n\n\n\n\n\n\n\n\n\n\n\n\nNote:\n For more information about config files and custom properties, see the \nApplication Packages Guide\n\n\n\n\nViewing an Application DAG\n\n\nAll DataTorrent applications are made up of operators that connect together via streams to form a Directed Acyclic Graph (DAG). To see a visualization of this DAG, click on the application name in the list of applications.\n\n\n\n\nCreating apps with dtAssemble\n\n\nIf you have an Enterprise license, you will have access to the dtAssemble tool. Using this tool is outside the scope of this guide, but check out the \ndtAssemble guide\n.\n\n\nMonitor Tab\n\n\nThe main operations dashboard can be visited by clicking on the \u201cMonitor\u201d link in the main top navigation bar. This section of the Console can be used to monitor, debug, and kill running DataTorrent applications.\n\n\nOperations Home\n\n\nThe operations home page shows overall cluster statistics as well as a list of running DataTorrent applications.\n\n\n\n\nThe cluster statistics include some performance statistics and memory usage information. As for the application list, there are two options to take note of: \nretrieve ended apps\n and \ninclude system apps\n. The first option will include all ended applications that are still in the resource manager history. The second option will include system apps, which are apps like the App Data Tracker that are developed by DataTorrent and used to add functionality to your DataTorrent cluster.\n\n\nInstance Page\n\n\nTo get to an application instance page, click on either the app name or the app id in the list of running applications.\n\n\n\n\nAll sections and subsections of the instance page currently use a dashboard/widget system. The controls for this system are located near the top of the screen, below the breadcrumbs:\n\n\n\n\nThere are tool tips to help you understand how to work with dashboards and widgets. For most users, the default dashboard configurations (\nlogical\n, \nphysical\n, \nphysical-dag-view\n, \nmetric-view\n) will suffice. The following is a list of widgets available on an app instance page:\n\n\nApplication Overview Widget\n\n\nAll the default dashboard tabs have this widget. It contains basic information regarding the app plus a few controls. To end a running application, use either the \u201cshutdown\u201d or \u201ckill\u201d buttons in this widget:\n\n\n\n\nThe \u201cshutdown\u201d function tries to gracefully stop the application, while \u201ckill\u201d forces the application to end. In either case, you will need to confirm your action.\n\n\nYou can also use the \nset logging level\n button on this widget to specify what logging level gets written to the dt.log files. \n\n\n\n\nYou will then be presented with a dialog where you can specify either fully-qualified class names or package identifiers with wildcards:\n\n\n\n\nStram Events Widget\n\n\nEach application has a stream of notable events that can be viewed with the StrAM Events widget:\n\n\n\n\nSome events have additional information attached to it, which can be viewed by clicking the \u201cdetails\u201d button in the list:\n\n\n\n\nLogical DAG Widget\n\n\nThis widget visualizes the logical plan of the application being viewed:\n\n\n\n\nAdditionally, you can cycle through various metrics aggregated by logical operator. In the screenshot above, processed tuples per second and emitted tuples per second are shown. \n\n\n\n\nPro tip:\n Hold the alt/option key while using your mouse scroll wheel to zoom in and out on the DAG.\n\n\n\n\nPhysical DAG Widget\n\n\nThis is similar to the Logical DAG Widget, except it shows the fully deployed \"physical\" operators. Depending on the partitioning of your application, this could be significantly more complex than the Logical DAG view.\n\n\n\n\nSame-colored physical operators in this widget indicates that these operators are in the same container.\n\n\nLogical Operators List Widget\n\n\nThis widget shows a list of logical operators in the application. This table, like others, has live updates, filtering, column ordering, stacked row sorting, and column resizing. \n\n\nOne nice feature specific to this widget is the ability to set the logging level for the Java class of a logical operator by selecting it in this list and using the provided dropdown, like so:\n\n\n\n\nPhysical Operators List Widget\n\n\nShows the physical operators in the application.\n\n\nContainers List Widget\n\n\nShows the containers in the application. From this widget you can: select a container and go to one of its logs, fetch non-running containers and view information about them, and even kill selected containers.\n\n\nLogical Streams List Widget\n\n\nShows a list of the streams in the application. There are also links to the logical operator pages for the sources and sinks of each stream.\n\n\nMetrics Chart\n\n\nShows various metrics of your application on a real-time line chart. Single-click a metric to toggle its visibility. Double-click a metric to toggle all other keys' visibility.\n\n\nRecording and Viewing Sample Tuples\n\n\nThere is a mechanism called tuple recording that can be used to easily look at the content of tuples flowing through your application. To use this feature, select a physical operator from the Physical Operators List widget and click on the \u201crecord a sample\u201d button. This will bring up a modal window which you can then use to traverse the sample and look at the actual content of the tuple (converted to a JSON structure):\n\n\n\n\nViewing Logs\n\n\nAnother useful feature of the Console is the ability to view container logs of a given application. To do this, select a container from the Containers List widget (default location of this widget is in the \u201cphysical\u201d dashboard). Then click the logs dropdown and select the log you want to look at:\n\n\n\n\nOnce you are viewing a log file in the console, there are few tricks to traversing it. You can scroll to the top to fetch earlier content, scroll to the bottom for later content, grep for strings in the selected range or over the entire log, and click the \u201ceye\u201d icon to the far left of every line to go to that location of the log:\n\n\n\n\nThere are numerous improvements in store for dtManage, and user feedback is highly valued in the planning, so please provide any that will help in your usage of the tool!\n\n\n~The DataTorrent UI Team", 
            "title": "dtManage"
        }, 
        {
            "location": "/dtmanage/#dtmanage-guide", 
            "text": "", 
            "title": "dtManage Guide"
        }, 
        {
            "location": "/dtmanage/#introduction", 
            "text": "The DataTorrent Console (aka dtManage) is a web-based user interface that allows you to monitor and interact with the DataTorrent platform running on your Hadoop cluster. It is a web-based dashboard that is served by the DataTorrent Gateway, and has five areas: configuration, monitoring, development, and , visualization, and learning.  To download the platform or the VM sandbox, go to  http://www.datatorrent.com/download .", 
            "title": "Introduction"
        }, 
        {
            "location": "/dtmanage/#connection-requirements", 
            "text": "When you install DataTorrent RTS on your Hadoop cluster using the installer binary, the DataTorrent Gateway service is started on the node where the installer is executed. By default, the Gateway serves the console from port 9090. The ability to connect to this node and port depends on your environment, and may require special setup such as an ssh tunnel, firewall configuration changes, VPN access, etc.", 
            "title": "Connection Requirements"
        }, 
        {
            "location": "/dtmanage/#browser-requirements", 
            "text": "The Console currently supports Chrome, Firefox, Safari, and IE (version 10 and up).", 
            "title": "Browser Requirements"
        }, 
        {
            "location": "/dtmanage/#installation-wizard", 
            "text": "The first time you open the Console, after installing DataTorrent RTS on your cluster, it will take you to the Installation Wizard. This walks you through the initial configuration of your DataTorrent installation, by confirming the following:   Location of the hadoop executable  DFS location where all the DataTorrent files are stored  DataTorrent license  Summary and review of any remaining configuration items", 
            "title": "Installation Wizard"
        }, 
        {
            "location": "/dtmanage/#when-kerberos-security-is-enabled", 
            "text": "When your hadoop cluster has security enabled with Kerberos, there will be four additional controls in the installation wizard:    Kerberos Principal : The Kerberos principal (e.g. primary/instance@REALM) to use on behalf of the management console.  Kerberos Keytab : The location (path) of the Kerberos keytab file to use on the gateway node's local file system.  YARN delegation token lifetime : If the value of the  yarn.resourcemanager.delegation.token.max-lifetime  property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.  Namenode delegation token lifetime : If the value of the  dfs.namenode.delegation.token.max-lifetime  property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.    Note:  The token lifetime values you enter will not actually set these values in your hadoop configuration, it is only meant to inform the DataTorrent platform of these values.", 
            "title": "When Kerberos Security is Enabled"
        }, 
        {
            "location": "/dtmanage/#configure-tab", 
            "text": "The configuration page can be found by clicking the \u201cConfigure\u201d link in the main navigation bar at the top. There are links to various tools to help you configure and troubleshoot your DataTorrent installation.  The configuration page links may differ depending on your cluster setup. The following is a screenshot with a cluster that has simple authentication/authorization enabled.", 
            "title": "Configure Tab"
        }, 
        {
            "location": "/dtmanage/#system-configuration", 
            "text": "This page shows diagnostic information regarding the gateway and console, as well as any issues that the gateway may detect.   In addition, you can perform the following actions from this page:", 
            "title": "System Configuration"
        }, 
        {
            "location": "/dtmanage/#restart-the-gateway", 
            "text": "This can be useful when Hadoop configuration has changed or some other factor of your cluster environment has changed.", 
            "title": "Restart the Gateway"
        }, 
        {
            "location": "/dtmanage/#toggle-reporting", 
            "text": "If enabled, your DataTorrent installation will send various pieces of information such as bug reporting and usage statistics back to our servers.", 
            "title": "Toggle Reporting"
        }, 
        {
            "location": "/dtmanage/#license-information", 
            "text": "Use the License Information page to view how much of your DataTorrent license capacity your cluster is consuming as well as what capabilities your license permits. You can also upload new license files here.", 
            "title": "License Information"
        }, 
        {
            "location": "/dtmanage/#user-profile", 
            "text": "The User Profile page displays information about the current user, including their username, the authentication scheme being used, and the roles that the current user has. In addition, users can perform the following actions:   change password   change the default home page  change the theme of the console  restore the default options of the console", 
            "title": "User Profile"
        }, 
        {
            "location": "/dtmanage/#user-management", 
            "text": "Use this page to manage users and roles of your DataTorrent cluster:   add users  change users\u2019 roles  change users\u2019 password  delete users  add roles  edit role permissions  delete roles     Note:  With most authentication schemes, the admin role cannot be deleted.", 
            "title": "User Management"
        }, 
        {
            "location": "/dtmanage/#installation-wizard_1", 
            "text": "At any time, you can go back to the installation wizard from the Configuration Tab. It can help diagnose issues and reconfigure your cluster and gateway.", 
            "title": "Installation Wizard"
        }, 
        {
            "location": "/dtmanage/#develop-tab", 
            "text": "The development area of dtManage is mainly geared towards the creation, upload, configuration, and launch of DataTorrent applications. The development home can be viewed by clicking the \u201cDevelop\u201d tab in the main navigation bar on the top of the screen. A prerequisite to using the development tools of the UI is an understanding of what Apex Application Packages are. For more information, see the  Application Packages Guide .", 
            "title": "Develop Tab"
        }, 
        {
            "location": "/dtmanage/#application-packages", 
            "text": "To access the application package listing, click on the \"Apps\" link from the Develop Tab index page. From here, you can perform several operators directly on application packages:   Download the app package  Delete the app package  Create a new application in an application package via dtAssemble (requires enterprise license)  Launch applications in the app package  Import default packages (see below)     Note:  If authentication is enabled, you may not be able to see others\u2019 app packages, depending on your permissions.", 
            "title": "Application Packages"
        }, 
        {
            "location": "/dtmanage/#importing-default-packages", 
            "text": "When you install the DataTorrent platform, a folder located in the installation directory called  demos/app-packages  will contain various default app packages that can be imported into HDFS for use. Just above the list of Application Packages, there should be a button that says  Import default packages . Clicking this will take you to a page that shows the list of these demo app packages. Select one or more to import and click the  Import  button. This will upload the selected app package to HDFS.", 
            "title": "Importing Default Packages"
        }, 
        {
            "location": "/dtmanage/#application-package-page", 
            "text": "Once you have uploaded or imported an App Package, clicking on the package name in the list will take you to the Application Package Page, where you can view all the package details.   Aside from various pieces of meta information (owner, DataTorrent version, required properties, etc), you will see a list of apps found in this package.", 
            "title": "Application Package Page"
        }, 
        {
            "location": "/dtmanage/#launching-apps", 
            "text": "To launch an app in an App Package, click on the launch button to the far right of the list. A dialog box will appear with several options:    Specify a name for the running app \n  The console will pre-populate this field with an appropriate name, but you can specify your own name. Just be weary that it must be unique compared to the other applications running on your DataTorrent installation.  Specify the  scheduler queue \n  This input allows you to specify which queue you want the application to be launched under. The default behavior depends on your Hadoop installation, but typically will be  root.[USER_NAME] .  Use a config file when launching \n  App Package config files are xml files that contain  properties  that get interpreted and used for launching an application. To choose one, enable the check box and choose the config file you want to use for launch.  Specify custom properties \n  In addition to choosing a config file, you may also specify properties directly in the launch popup by selecting this option. Note that there are three helpful functions when specifying custom properties:  add required properties  - App Packages can have required properties (for example, twitter API access keys for the twitter demo). This function adds a new property with the required property name to the form, making it easy to fill in.  add default properties  - App Packages can also have default properties. This function will add the default properties to the list, making it easy for you to override the defaults  save this configuration as\u2026  - This function creates a new config file and saves it to the App Package, which can then be used later to launch the app with. That way, you can relaunch the app with the same properties without having to re-enter them.       Note:  For more information about config files and custom properties, see the  Application Packages Guide", 
            "title": "Launching Apps"
        }, 
        {
            "location": "/dtmanage/#viewing-an-application-dag", 
            "text": "All DataTorrent applications are made up of operators that connect together via streams to form a Directed Acyclic Graph (DAG). To see a visualization of this DAG, click on the application name in the list of applications.", 
            "title": "Viewing an Application DAG"
        }, 
        {
            "location": "/dtmanage/#creating-apps-with-dtassemble", 
            "text": "If you have an Enterprise license, you will have access to the dtAssemble tool. Using this tool is outside the scope of this guide, but check out the  dtAssemble guide .", 
            "title": "Creating apps with dtAssemble"
        }, 
        {
            "location": "/dtmanage/#monitor-tab", 
            "text": "The main operations dashboard can be visited by clicking on the \u201cMonitor\u201d link in the main top navigation bar. This section of the Console can be used to monitor, debug, and kill running DataTorrent applications.", 
            "title": "Monitor Tab"
        }, 
        {
            "location": "/dtmanage/#operations-home", 
            "text": "The operations home page shows overall cluster statistics as well as a list of running DataTorrent applications.   The cluster statistics include some performance statistics and memory usage information. As for the application list, there are two options to take note of:  retrieve ended apps  and  include system apps . The first option will include all ended applications that are still in the resource manager history. The second option will include system apps, which are apps like the App Data Tracker that are developed by DataTorrent and used to add functionality to your DataTorrent cluster.", 
            "title": "Operations Home"
        }, 
        {
            "location": "/dtmanage/#instance-page", 
            "text": "To get to an application instance page, click on either the app name or the app id in the list of running applications.   All sections and subsections of the instance page currently use a dashboard/widget system. The controls for this system are located near the top of the screen, below the breadcrumbs:   There are tool tips to help you understand how to work with dashboards and widgets. For most users, the default dashboard configurations ( logical ,  physical ,  physical-dag-view ,  metric-view ) will suffice. The following is a list of widgets available on an app instance page:", 
            "title": "Instance Page"
        }, 
        {
            "location": "/dtmanage/#application-overview-widget", 
            "text": "All the default dashboard tabs have this widget. It contains basic information regarding the app plus a few controls. To end a running application, use either the \u201cshutdown\u201d or \u201ckill\u201d buttons in this widget:   The \u201cshutdown\u201d function tries to gracefully stop the application, while \u201ckill\u201d forces the application to end. In either case, you will need to confirm your action.  You can also use the  set logging level  button on this widget to specify what logging level gets written to the dt.log files.    You will then be presented with a dialog where you can specify either fully-qualified class names or package identifiers with wildcards:", 
            "title": "Application Overview Widget"
        }, 
        {
            "location": "/dtmanage/#stram-events-widget", 
            "text": "Each application has a stream of notable events that can be viewed with the StrAM Events widget:   Some events have additional information attached to it, which can be viewed by clicking the \u201cdetails\u201d button in the list:", 
            "title": "Stram Events Widget"
        }, 
        {
            "location": "/dtmanage/#logical-dag-widget", 
            "text": "This widget visualizes the logical plan of the application being viewed:   Additionally, you can cycle through various metrics aggregated by logical operator. In the screenshot above, processed tuples per second and emitted tuples per second are shown.    Pro tip:  Hold the alt/option key while using your mouse scroll wheel to zoom in and out on the DAG.", 
            "title": "Logical DAG Widget"
        }, 
        {
            "location": "/dtmanage/#physical-dag-widget", 
            "text": "This is similar to the Logical DAG Widget, except it shows the fully deployed \"physical\" operators. Depending on the partitioning of your application, this could be significantly more complex than the Logical DAG view.   Same-colored physical operators in this widget indicates that these operators are in the same container.", 
            "title": "Physical DAG Widget"
        }, 
        {
            "location": "/dtmanage/#logical-operators-list-widget", 
            "text": "This widget shows a list of logical operators in the application. This table, like others, has live updates, filtering, column ordering, stacked row sorting, and column resizing.   One nice feature specific to this widget is the ability to set the logging level for the Java class of a logical operator by selecting it in this list and using the provided dropdown, like so:", 
            "title": "Logical Operators List Widget"
        }, 
        {
            "location": "/dtmanage/#physical-operators-list-widget", 
            "text": "Shows the physical operators in the application.", 
            "title": "Physical Operators List Widget"
        }, 
        {
            "location": "/dtmanage/#containers-list-widget", 
            "text": "Shows the containers in the application. From this widget you can: select a container and go to one of its logs, fetch non-running containers and view information about them, and even kill selected containers.", 
            "title": "Containers List Widget"
        }, 
        {
            "location": "/dtmanage/#logical-streams-list-widget", 
            "text": "Shows a list of the streams in the application. There are also links to the logical operator pages for the sources and sinks of each stream.", 
            "title": "Logical Streams List Widget"
        }, 
        {
            "location": "/dtmanage/#metrics-chart", 
            "text": "Shows various metrics of your application on a real-time line chart. Single-click a metric to toggle its visibility. Double-click a metric to toggle all other keys' visibility.", 
            "title": "Metrics Chart"
        }, 
        {
            "location": "/dtmanage/#recording-and-viewing-sample-tuples", 
            "text": "There is a mechanism called tuple recording that can be used to easily look at the content of tuples flowing through your application. To use this feature, select a physical operator from the Physical Operators List widget and click on the \u201crecord a sample\u201d button. This will bring up a modal window which you can then use to traverse the sample and look at the actual content of the tuple (converted to a JSON structure):", 
            "title": "Recording and Viewing Sample Tuples"
        }, 
        {
            "location": "/dtmanage/#viewing-logs", 
            "text": "Another useful feature of the Console is the ability to view container logs of a given application. To do this, select a container from the Containers List widget (default location of this widget is in the \u201cphysical\u201d dashboard). Then click the logs dropdown and select the log you want to look at:   Once you are viewing a log file in the console, there are few tricks to traversing it. You can scroll to the top to fetch earlier content, scroll to the bottom for later content, grep for strings in the selected range or over the entire log, and click the \u201ceye\u201d icon to the far left of every line to go to that location of the log:   There are numerous improvements in store for dtManage, and user feedback is highly valued in the planning, so please provide any that will help in your usage of the tool!  ~The DataTorrent UI Team", 
            "title": "Viewing Logs"
        }, 
        {
            "location": "/dtassemble/", 
            "text": "dtAssemble - Graphical Application Builder\n\n\nThe dtAssemble Graphical Application  Builder is a UI tool that allows users to drag-and-drop operators onto a canvas and connect them together to build a DataTorrent application. \n\n\n\n\nAccessing the Builder\n\n\nTo get to the App Builder, you must perform the following steps (illustrated by GIF):\n\n\n\n\n\n\nCreate a DataTorrent Application Package\n  To do this, read through the \nApplication Packages Guide\n, which provides step-by-step instructions on how to do this.\n\n\nUpload it to HDFS using the Console\n  Click on the \nDevelop\n link at the top of the DataTorrent Console, then click the \u201cupload a package\u201d button.\n\n\nAdd a new application to your uploaded package\n  Once your package has been uploaded, click on its name in the list of packages. Then click the \u201cadd new application\u201d button.\n\n\nDrag operators onto the canvas\n  Use the search field of the Operator Library panel on the left to find the operators that were found in your app package, then click and drag them out onto the \u201cApplication Canvas.\u201d\n\n\nConfigure the operators using the Operator Inspector\n  When the operator is selected, the right side of the screen will have the Operator Inspector, where you will see some meta information about the operator as well as the interface to set initial values for operator properties and attributes.\n\n\nConnect the operators\u2019 ports to form streams between operators\n  Select a port by clicking on it. Ports that are compatible with the selected port will pulse green. Click and drag out a stream from one port and connect it with a compatible port of another operator.\n\n\n\n\nUsing Application Builder\n\n\nThe Application Builder contains three main parts: the Operator Library Navigator, the Canvas, and the Inspector:\n\n\n\n\nOperator Library Navigator\n\n\n\n\nUse this to quickly find and add operators that exist in the Application Package Jar. It groups operators by their Prepping Operators for the Application Builder\nsection). You can search for operators using the input field at the top, which will look at the operators\u2019 titles, descriptions, and keywords. Clicking on an operator will expand a window with more information regarding the operator.\n\n\nOnce you have found an operator you would like to add to your application canvas, simply click and drag it onto the canvas.\n\n\n\n\nCanvas\n\n\n\n\nThe Application Canvas is the main area that you use to assemble applications with operators and streams. Specifically, you will be connecting output ports (shown as magenta) of some operators to input ports (shown as blue) of other operators. When you click on a port, other ports that are compatible with it will pulse green, indicating that a stream can connect the two. See the note on tuple types of ports in the Prepping Operators for the Application Builder\nsection.\n\n\n\n\nInspector\n\n\nThe Inspector is visible when an operator, a port, or a stream is selected on the canvas. It will look different depending on what is selected.\n\n\nOperator Inspector\n\n\n\n\nWhen an operator is selected on the canvas, you will see the Operator Inspector on the right side. You will see the operator class name, the java package it is a part of, and a field with the name you have given to it. You will also be able to edit the initial values of the operator\u2019s properties. \n\n\nPort Inspector\n\n\nWhen a port is selected, you will see the Port Inspector on the right side. Here you can see the name of the port, the tuple type that it emits (for an output port) or accepts (for an input port). \n\n\nStream Inspector\n\n\n\n\nThe stream inspector will appear when you have selected a stream in the canvas. You can use this to rename the stream or change the locality of the stream.\n\n\nPrepping Operators for the Application Builder\n\n\nThe way in which an operator shows up in the App Builder depends on how the operator is written in Java. In order to fully prep an operator so that it can be easily used in the App Builder, use the following guidelines:\n\n\n\n\nOperators must be a concrete class and must have a no-arg constructor\n  This is actually a requirement that extends beyond app builder, but is noted here because operators that are abstract or that do not have a no-arg constructor will not appear in the Operator Library panel in the current version.\n\n\n\n\nUse javadoc annotations in the comment block above the class declaration statement:\n  a.  @omitFromUI - Put this annotation if you do not want the operator to show up in the App Builder\n  b.  @category - The high-level category that this operator should reside in. The value you put here is arbitrary; it will be placed in (or create a new) dropdown in the Operator Library Navigator on the left of the App Builder. You can have multiple categories per operator.\n  c.  @tags - Space-separated list of \u201ctags\u201d; arbitrary strings of text that will be searchable via the Operator Library Navigator on the left of the App Builder. Tags work as filters. You can use them to enable search, project identification, etc.\n  d.  @required - This is a future annotation to denote whether a property is required.\n\n\n/**\n * This is an example description of the operator It will be \n * displayed in the app builder under the operator in the Operator \n * Library Navigator.\n * @category Algo\n * @tags math sigma average avg\n */\npublic class MyOperator extends BaseOperator { /* \u2026 */ }\n\n\n\n\n\n\n\nEvery property's getter method should be preceded by a descriptive comment block that indicates what a property does, how to use it, common values, etc. This is not a must have, but very highly recommended\n\n\n/**\n* This is an example description of a property on an operator class.\n* It will be displayed in the app builder under the property name.\n* It is ok to make this long because the UI will only show the first\n* sentence or so and allow the user to expand/collapse the rest.\n*/\npublic String getMyProperty() { /* \u2026 */ }\n\n\n\n\n\n\n\nUtilize the @useSchema doclet annotation above properties\u2019 getter in order to mark a property\u2019s or subproperty\u2019s.  \n\n\n\n\nWhen a property's type is not a primitive, wrapper class for a primitive, or a String, try to be as specific as possible with the type signature.\n  For example, mark a property type as java.util.HashMap instead of java.util.Map, or, more generally, choose ConcreteSubClass over AbstractParentClassOrInterface. This will limit the assignable concrete types that the user of the app builder must choose from. For now, we only support property types that are public and either have a no-arg constructor themselves or their parent class does.\n\n\nMark properties that should be hidden in the app builder with the @omitFromUI javadoc annotation in the javadoc comment block above the getter of the property. This is a critical part of what an operator developer decides to expose for customization.\n/**\n * This is an example description of a property on an operator class\n * WS\n */\npublic String getMyProperty() { /* \u2026 */ }\n\n\n\n\n\nMake the tuple type of output ports strict and that of input ports liberal.\n  The tuple type that an output port emits must be assignable to the tuple type of an input port in order for them to connect. In other words, the input port must either be the exact type or a parent type of the output port tuple type. Because of this, building operators with more specific output and less specific input will make them more reusable.\n\n\n\n\n\n\n\n\nApp Builder Usage Examples\n\n\nPi Demo\n\n\nAs an example, we will rebuild the basic Pi demo. Please note that this example is relatively contrived and is only meant to illustrate how to connect compatible ports of operators to create a new application. \n\n\n\n\n\n\nFirst you will need to clone the malhar repository:\n\n\ngit@github.com:DataTorrent/Malhar.git \n cd Malhar\n\n\n\n\n\n\n\nThen, run maven install, which will create the App Package jar need to upload to your gateway.\n\n\n\n\nFollow steps 2,3, and 4 of the Accessing the App Builder section above, but with the app package jar located in Malhar/demos/pi/target/pi-demo-{VERSION}.apa.\n\n\nAdd the Console Output\noperators.\n\n\nConnect the integer_data port of the Random Event Generator to the input port of the Pi Calculate operator, and connect the output port of the Pi Calculate operator to the input port of the Console Output operator.\n\n\nClick the \u201claunch\u201d button in the top left once it turns purple.\n\n\nIn the resulting modal, click \u201cLaunch\u201d, then the \u201cView it on the Dashboard\u201d link in the subsequent notification box.\n\n\nTo see the output of the Console Output operator, navigate to the stdout log file viewer of the container where the operator is running.", 
            "title": "dtAssemble"
        }, 
        {
            "location": "/dtassemble/#dtassemble-graphical-application-builder", 
            "text": "The dtAssemble Graphical Application  Builder is a UI tool that allows users to drag-and-drop operators onto a canvas and connect them together to build a DataTorrent application.", 
            "title": "dtAssemble - Graphical Application Builder"
        }, 
        {
            "location": "/dtassemble/#accessing-the-builder", 
            "text": "To get to the App Builder, you must perform the following steps (illustrated by GIF):    Create a DataTorrent Application Package\n  To do this, read through the  Application Packages Guide , which provides step-by-step instructions on how to do this.  Upload it to HDFS using the Console\n  Click on the  Develop  link at the top of the DataTorrent Console, then click the \u201cupload a package\u201d button.  Add a new application to your uploaded package\n  Once your package has been uploaded, click on its name in the list of packages. Then click the \u201cadd new application\u201d button.  Drag operators onto the canvas\n  Use the search field of the Operator Library panel on the left to find the operators that were found in your app package, then click and drag them out onto the \u201cApplication Canvas.\u201d  Configure the operators using the Operator Inspector\n  When the operator is selected, the right side of the screen will have the Operator Inspector, where you will see some meta information about the operator as well as the interface to set initial values for operator properties and attributes.  Connect the operators\u2019 ports to form streams between operators\n  Select a port by clicking on it. Ports that are compatible with the selected port will pulse green. Click and drag out a stream from one port and connect it with a compatible port of another operator.", 
            "title": "Accessing the Builder"
        }, 
        {
            "location": "/dtassemble/#using-application-builder", 
            "text": "The Application Builder contains three main parts: the Operator Library Navigator, the Canvas, and the Inspector:", 
            "title": "Using Application Builder"
        }, 
        {
            "location": "/dtassemble/#operator-library-navigator", 
            "text": "Use this to quickly find and add operators that exist in the Application Package Jar. It groups operators by their Prepping Operators for the Application Builder section). You can search for operators using the input field at the top, which will look at the operators\u2019 titles, descriptions, and keywords. Clicking on an operator will expand a window with more information regarding the operator.  Once you have found an operator you would like to add to your application canvas, simply click and drag it onto the canvas.", 
            "title": "Operator Library Navigator"
        }, 
        {
            "location": "/dtassemble/#canvas", 
            "text": "The Application Canvas is the main area that you use to assemble applications with operators and streams. Specifically, you will be connecting output ports (shown as magenta) of some operators to input ports (shown as blue) of other operators. When you click on a port, other ports that are compatible with it will pulse green, indicating that a stream can connect the two. See the note on tuple types of ports in the Prepping Operators for the Application Builder section.", 
            "title": "Canvas"
        }, 
        {
            "location": "/dtassemble/#inspector", 
            "text": "The Inspector is visible when an operator, a port, or a stream is selected on the canvas. It will look different depending on what is selected.", 
            "title": "Inspector"
        }, 
        {
            "location": "/dtassemble/#operator-inspector", 
            "text": "When an operator is selected on the canvas, you will see the Operator Inspector on the right side. You will see the operator class name, the java package it is a part of, and a field with the name you have given to it. You will also be able to edit the initial values of the operator\u2019s properties.", 
            "title": "Operator Inspector"
        }, 
        {
            "location": "/dtassemble/#port-inspector", 
            "text": "When a port is selected, you will see the Port Inspector on the right side. Here you can see the name of the port, the tuple type that it emits (for an output port) or accepts (for an input port).", 
            "title": "Port Inspector"
        }, 
        {
            "location": "/dtassemble/#stream-inspector", 
            "text": "The stream inspector will appear when you have selected a stream in the canvas. You can use this to rename the stream or change the locality of the stream.", 
            "title": "Stream Inspector"
        }, 
        {
            "location": "/dtassemble/#prepping-operators-for-the-application-builder", 
            "text": "The way in which an operator shows up in the App Builder depends on how the operator is written in Java. In order to fully prep an operator so that it can be easily used in the App Builder, use the following guidelines:   Operators must be a concrete class and must have a no-arg constructor\n  This is actually a requirement that extends beyond app builder, but is noted here because operators that are abstract or that do not have a no-arg constructor will not appear in the Operator Library panel in the current version.   Use javadoc annotations in the comment block above the class declaration statement:\n  a.  @omitFromUI - Put this annotation if you do not want the operator to show up in the App Builder\n  b.  @category - The high-level category that this operator should reside in. The value you put here is arbitrary; it will be placed in (or create a new) dropdown in the Operator Library Navigator on the left of the App Builder. You can have multiple categories per operator.\n  c.  @tags - Space-separated list of \u201ctags\u201d; arbitrary strings of text that will be searchable via the Operator Library Navigator on the left of the App Builder. Tags work as filters. You can use them to enable search, project identification, etc.\n  d.  @required - This is a future annotation to denote whether a property is required.  /**\n * This is an example description of the operator It will be \n * displayed in the app builder under the operator in the Operator \n * Library Navigator.\n * @category Algo\n * @tags math sigma average avg\n */\npublic class MyOperator extends BaseOperator { /* \u2026 */ }    Every property's getter method should be preceded by a descriptive comment block that indicates what a property does, how to use it, common values, etc. This is not a must have, but very highly recommended  /**\n* This is an example description of a property on an operator class.\n* It will be displayed in the app builder under the property name.\n* It is ok to make this long because the UI will only show the first\n* sentence or so and allow the user to expand/collapse the rest.\n*/\npublic String getMyProperty() { /* \u2026 */ }    Utilize the @useSchema doclet annotation above properties\u2019 getter in order to mark a property\u2019s or subproperty\u2019s.     When a property's type is not a primitive, wrapper class for a primitive, or a String, try to be as specific as possible with the type signature.\n  For example, mark a property type as java.util.HashMap instead of java.util.Map, or, more generally, choose ConcreteSubClass over AbstractParentClassOrInterface. This will limit the assignable concrete types that the user of the app builder must choose from. For now, we only support property types that are public and either have a no-arg constructor themselves or their parent class does.  Mark properties that should be hidden in the app builder with the @omitFromUI javadoc annotation in the javadoc comment block above the getter of the property. This is a critical part of what an operator developer decides to expose for customization. /**\n * This is an example description of a property on an operator class\n * WS\n */\npublic String getMyProperty() { /* \u2026 */ }   Make the tuple type of output ports strict and that of input ports liberal.\n  The tuple type that an output port emits must be assignable to the tuple type of an input port in order for them to connect. In other words, the input port must either be the exact type or a parent type of the output port tuple type. Because of this, building operators with more specific output and less specific input will make them more reusable.", 
            "title": "Prepping Operators for the Application Builder"
        }, 
        {
            "location": "/dtassemble/#app-builder-usage-examples", 
            "text": "", 
            "title": "App Builder Usage Examples"
        }, 
        {
            "location": "/dtassemble/#pi-demo", 
            "text": "As an example, we will rebuild the basic Pi demo. Please note that this example is relatively contrived and is only meant to illustrate how to connect compatible ports of operators to create a new application.     First you will need to clone the malhar repository:  git@github.com:DataTorrent/Malhar.git   cd Malhar    Then, run maven install, which will create the App Package jar need to upload to your gateway.   Follow steps 2,3, and 4 of the Accessing the App Builder section above, but with the app package jar located in Malhar/demos/pi/target/pi-demo-{VERSION}.apa.  Add the Console Output operators.  Connect the integer_data port of the Random Event Generator to the input port of the Pi Calculate operator, and connect the output port of the Pi Calculate operator to the input port of the Console Output operator.  Click the \u201claunch\u201d button in the top left once it turns purple.  In the resulting modal, click \u201cLaunch\u201d, then the \u201cView it on the Dashboard\u201d link in the subsequent notification box.  To see the output of the Console Output operator, navigate to the stdout log file viewer of the container where the operator is running.", 
            "title": "Pi Demo"
        }, 
        {
            "location": "/dtdashboard/", 
            "text": "dtDashboard - Application Data Visualization\n\n\nThe App Data Framework collection of UI tools and operator APIs that allows DataTorrent developers to visualize the data flowing through their applications.  This guide assumes the reader\u2019s basic knowledge on the DataTorrent RTS platform and the Console, and the concept of operators in streaming applications.\n\n\nExamples\n\n\nTwitter Example\n\n\nThe Twitter Hashtag Count Demo, shipped with DataTorrent RTS distribution, is a streaming application that utilizes the App Data Framework.  To demonstrate how the Application Data Framework works on a very high level, on the Application page in the Console, click on the visualize button next to the application name, and a dashboard for the Twitter Hashtag Count Demo will be created.  In it, you will see visualization of the top 10 hashtags computed in the application:\n\n\n\n\nAds Dimension Example\n\n\nThe Ads Dimension Demo included in the DataTorrent RTS distribution also utilizes the App Data Framework.  The widgets for this application demonstrates more features than the Twitter one because you can issue your own queries to choose what data you want to visualize.  For example, one might want to visualize the running revenue and cost for advertiser \u201cStarbucks\u201d and publisher \u201cGoogle\u201d.\n\n\n\n\nData Sources\n\n\nA Data Source in the application consists of three operators.  The Query Operator, the Data Source Operator and the Result Operator.  The Query Operator takes in queries from a message queue and passes them to the Data Source Operator.  The Data Source Operator processes the queries and sends the results to the Result Operator.  The Result Operator delivers the results to the message queue.  The Data Source Operator generally takes in data from other parts of the DAG.\n\n\n\n\nTo see how this is fit in our previous examples, below is the DAG for the Twitter Hashtag Demo:\n\n\n\n\nThe operators \u201cQuery\u201d, \u201cTabular Server\u201d and \u201cQueryResult\u201d are the three operators that serve the data being visualized in the Console.  The \u201cTabular Server\u201d operator takes in data from the TopCounter operator, processes incoming queries, and generates results.\n\n\nAnd below is the DAG for the Ads Dimension Demo:\n\n\n\n\nIn this DAG, the operators \u201cQuery\u201d, \u201cStore\u201d and \u201cQueryResult\u201d are the three operators that make up the Data Source.  The \u201cStore\u201d operator takes in incoming data, stores them in a persistent storage, and serve them.  In other words, the Data Source serves historical data as well as current data, as opposed to the Twitter Hashtag Demo, which only serves the current data.\n\n\nAll these operators are available in the Malhar (and Megh). When you are familiar with how the built-in operators work, you may want to create your own Data Sources in your own application.  \nApp Data Framework Programming Guide\n\n\nStats and Custom Metrics\n\n\nEach application has statistics such as tuples processed per second, latency, and memory used.  Each operator in an application can contain custom metrics that are part of the application logic.  With the Application Data Framework, each application comes with Data Sources that give out historical and real-time application statistics data and custom metrics data.  You can visualize such data as you would for other Data Sources in the application.\n\n\nData Visualization with Dashboards and Widgets\n\n\nOverview\n\n\nDataTorrent Dashboards and Widgets are UI tools that allow users to quickly and easily visualize historical and real-time application data.  Below is an example of a visualization dashboard with Stacked Area Chart, Pie Chart, Multi Bar Chart, and Table widgets.\n\n\n\n\nDashboards are quick and easy to create, and can include data from a single or multiple applications on the same screen.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget provides a unique way to visualizes the application data, and provides a number of ways to configure the content and the corresponding visualizations.\n\n\nAccessing Dashboards\n\n\nDashboards are accessible from Visualize section in the DataTorrent Console menu.\n\n\n\n\nAfter selecting Visualize menu item, a list of available dashboards is displayed.  The list of available dashboards can be ordered or filtered by dashboard name, description, included applications, creating user, and modified timestamp.  Clicking one of the dashboard name links takes you to the selected dashboard.\n\n\n\n\nAn alternative way to access dashboards is from Monitor section.  Navigate to one of the running applications, and if the application supports data visualization, list of existing dashboards which include selected application will be displayed after clicking on visualize button below the application name.\n\n\n\n\nBelow is an example of accessing the data visualization dashboard from a running application.\n\n\n\n\nCreating Dashboards\n\n\nThere are two ways to create a new visualization dashboard\n\n\n\n\ncreate new button on the Dashboards screen\n\n\ngenerate new dashboard option in the visualization menu of a compatible running DataTorrent application\n\n\n\n\nBelow is an illustrated example and a set of steps for creating a new dashboard from the Dashboards screen using the create new button reatingDashboard.gif](images/dtdashboard/image15.gif)\n\n\n\n\n\n\nProvide a unique dashboard name. Names are required to be unique for a single user.  Two different users can have a dashboard with the same name.\n\n\n\n\n\n\nInclude optional dashboard description.  Descriptions help explain and provide context for visualizations presented in the dashboard to new users, and provide an additional way to search and filter dashboards in the list.\n\n\n\n\n\n\nSelect compatible applications to include in the data visualizations. Only applications with compatible data visualization sources will be listed.  Any number of applications can be included, and selection can be changed after a dashboard is created.\n\n\n\n\n\n\nChoose to automatically generate a new dashboard or create one from scratch. Generating a new dashboard option automatically adds a widget to the new dashboard for every available data source in every selected application.  Creating dashboard from scratch involves manually choosing the widgets to display.\n\n\n\n\n\n\nCustomize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.\n\n\n\n\n\n\nBelow is an illustrated example of creating a new dashboard with generate new dashboard option in the visualization menu of a compatible running DataTorrent application.\n\n\n\n\n\n\n\n\nLocate visualize menu in the Application Summary section of a running application.  Only applications with compatible data visualization sources include visualize menu option.\n\n\n\n\n\n\nChoose to generate new dashboard from the visualize menu drop-down list.  New dashboard will be automatically named, generated, and saved.  The new dashboard name will reflect the selected application name, and widgets will be automatically added one for every available data source.\n\n\n\n\n\n\nCustomize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.\n\n\n\n\n\n\nModifying Dashboards\n\n\nDashboards controls are presented as a row of buttons just below the dashboard title and description.\n\n\n\n\nNew widgets can be added with settings button allows you to change dashboard name, description, and list of associated applications.  Use the save changes button to persist the dashboard state, which includes any changes made to the dashboard settings or widgets.  And finally, display mode enables an alternative visualization mode, which removes widget controls and backgrounds to create a simplified and seamless viewing experience.\n\n\nWidgets Overview\n\n\nDashboard widgets receive and display data in real time from DataTorrent application data sources.  Widgets can be added, removed, rearranged, and resized at any time.  Each widgets has a unique list of configurable properties, which include interactive elements displayed directly on the widget, as well as data query settings available from the widget settings.\n\n\nAdding Widgets\n\n\nWidgets can be added to the dashboard by clicking add widget button, selecting one of the available data sources, selecting one or more widgets, and confirming selection by clicking add widget \n\n\nAlternatively, randomly selected widgets, one for every available data source, can be added to the dashboard by clicking auto generate button.  Widgets will be automatically placed on the dashboard without any further dialogs.  If data sources are responding slowly, auto generate may take longer to add new widgets.  During this time the button will remain disabled to avoid duplicate requests, and will show spinning arrows to indicate that previous action is already in progress.\n\n\n\n\nWhether using auto generate buttons, results are not persisted until save changes is applied.\n\n\nEach data source supports one or more data schema types, such as snapshot, dimensions  Each schema type has a specific list of compatible widgets which can be selected to visualize the data.\n\n\nEditing Widgets\n\n\nEach widget has an dimensions, snapshot) and widget type (table, chart, text For snapshot schema, which represents a single point in time, the primary widget controls include\n\n\n\n\nlabel field selection\n\n\nquantity field selection\n\n\nsort order selection\n\n\n\n\nBelow is an example of changing label field and sort order for a bar chart widget.\n\n\n\n\nFor dimensions schema, which represents a series of points in time, with ability to configure dimensions based on key and value settings, the primary widget controls include\n\n\n\n\nTime ranges selection\n\n\nlive streaming\n\n\nhistorical range\n\n\nDimensions Selections\n\n\nkey combinations and key values selection\n\n\naggregate selection\n\n\n\n\n\n\nFor Notes widget, a text in Markdown format can be entered and should be translated to HTML look.  Below is an example of using Markdown syntax to produce headings, lists, and quoted text.\n\n\n\n\nAfter making the widget settings changes, remember to use save changes button to persist the desired results.  If the resulting changes should not be saved, reloading the dashboard will revert it to the the original state.", 
            "title": "dtDashboard"
        }, 
        {
            "location": "/dtdashboard/#dtdashboard-application-data-visualization", 
            "text": "The App Data Framework collection of UI tools and operator APIs that allows DataTorrent developers to visualize the data flowing through their applications.  This guide assumes the reader\u2019s basic knowledge on the DataTorrent RTS platform and the Console, and the concept of operators in streaming applications.", 
            "title": "dtDashboard - Application Data Visualization"
        }, 
        {
            "location": "/dtdashboard/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/dtdashboard/#twitter-example", 
            "text": "The Twitter Hashtag Count Demo, shipped with DataTorrent RTS distribution, is a streaming application that utilizes the App Data Framework.  To demonstrate how the Application Data Framework works on a very high level, on the Application page in the Console, click on the visualize button next to the application name, and a dashboard for the Twitter Hashtag Count Demo will be created.  In it, you will see visualization of the top 10 hashtags computed in the application:", 
            "title": "Twitter Example"
        }, 
        {
            "location": "/dtdashboard/#ads-dimension-example", 
            "text": "The Ads Dimension Demo included in the DataTorrent RTS distribution also utilizes the App Data Framework.  The widgets for this application demonstrates more features than the Twitter one because you can issue your own queries to choose what data you want to visualize.  For example, one might want to visualize the running revenue and cost for advertiser \u201cStarbucks\u201d and publisher \u201cGoogle\u201d.", 
            "title": "Ads Dimension Example"
        }, 
        {
            "location": "/dtdashboard/#data-sources", 
            "text": "A Data Source in the application consists of three operators.  The Query Operator, the Data Source Operator and the Result Operator.  The Query Operator takes in queries from a message queue and passes them to the Data Source Operator.  The Data Source Operator processes the queries and sends the results to the Result Operator.  The Result Operator delivers the results to the message queue.  The Data Source Operator generally takes in data from other parts of the DAG.   To see how this is fit in our previous examples, below is the DAG for the Twitter Hashtag Demo:   The operators \u201cQuery\u201d, \u201cTabular Server\u201d and \u201cQueryResult\u201d are the three operators that serve the data being visualized in the Console.  The \u201cTabular Server\u201d operator takes in data from the TopCounter operator, processes incoming queries, and generates results.  And below is the DAG for the Ads Dimension Demo:   In this DAG, the operators \u201cQuery\u201d, \u201cStore\u201d and \u201cQueryResult\u201d are the three operators that make up the Data Source.  The \u201cStore\u201d operator takes in incoming data, stores them in a persistent storage, and serve them.  In other words, the Data Source serves historical data as well as current data, as opposed to the Twitter Hashtag Demo, which only serves the current data.  All these operators are available in the Malhar (and Megh). When you are familiar with how the built-in operators work, you may want to create your own Data Sources in your own application.   App Data Framework Programming Guide", 
            "title": "Data Sources"
        }, 
        {
            "location": "/dtdashboard/#stats-and-custom-metrics", 
            "text": "Each application has statistics such as tuples processed per second, latency, and memory used.  Each operator in an application can contain custom metrics that are part of the application logic.  With the Application Data Framework, each application comes with Data Sources that give out historical and real-time application statistics data and custom metrics data.  You can visualize such data as you would for other Data Sources in the application.", 
            "title": "Stats and Custom Metrics"
        }, 
        {
            "location": "/dtdashboard/#data-visualization-with-dashboards-and-widgets", 
            "text": "", 
            "title": "Data Visualization with Dashboards and Widgets"
        }, 
        {
            "location": "/dtdashboard/#overview", 
            "text": "DataTorrent Dashboards and Widgets are UI tools that allow users to quickly and easily visualize historical and real-time application data.  Below is an example of a visualization dashboard with Stacked Area Chart, Pie Chart, Multi Bar Chart, and Table widgets.   Dashboards are quick and easy to create, and can include data from a single or multiple applications on the same screen.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget provides a unique way to visualizes the application data, and provides a number of ways to configure the content and the corresponding visualizations.", 
            "title": "Overview"
        }, 
        {
            "location": "/dtdashboard/#accessing-dashboards", 
            "text": "Dashboards are accessible from Visualize section in the DataTorrent Console menu.   After selecting Visualize menu item, a list of available dashboards is displayed.  The list of available dashboards can be ordered or filtered by dashboard name, description, included applications, creating user, and modified timestamp.  Clicking one of the dashboard name links takes you to the selected dashboard.   An alternative way to access dashboards is from Monitor section.  Navigate to one of the running applications, and if the application supports data visualization, list of existing dashboards which include selected application will be displayed after clicking on visualize button below the application name.   Below is an example of accessing the data visualization dashboard from a running application.", 
            "title": "Accessing Dashboards"
        }, 
        {
            "location": "/dtdashboard/#creating-dashboards", 
            "text": "There are two ways to create a new visualization dashboard   create new button on the Dashboards screen  generate new dashboard option in the visualization menu of a compatible running DataTorrent application   Below is an illustrated example and a set of steps for creating a new dashboard from the Dashboards screen using the create new button reatingDashboard.gif](images/dtdashboard/image15.gif)    Provide a unique dashboard name. Names are required to be unique for a single user.  Two different users can have a dashboard with the same name.    Include optional dashboard description.  Descriptions help explain and provide context for visualizations presented in the dashboard to new users, and provide an additional way to search and filter dashboards in the list.    Select compatible applications to include in the data visualizations. Only applications with compatible data visualization sources will be listed.  Any number of applications can be included, and selection can be changed after a dashboard is created.    Choose to automatically generate a new dashboard or create one from scratch. Generating a new dashboard option automatically adds a widget to the new dashboard for every available data source in every selected application.  Creating dashboard from scratch involves manually choosing the widgets to display.    Customize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.    Below is an illustrated example of creating a new dashboard with generate new dashboard option in the visualization menu of a compatible running DataTorrent application.     Locate visualize menu in the Application Summary section of a running application.  Only applications with compatible data visualization sources include visualize menu option.    Choose to generate new dashboard from the visualize menu drop-down list.  New dashboard will be automatically named, generated, and saved.  The new dashboard name will reflect the selected application name, and widgets will be automatically added one for every available data source.    Customize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.", 
            "title": "Creating Dashboards"
        }, 
        {
            "location": "/dtdashboard/#modifying-dashboards", 
            "text": "Dashboards controls are presented as a row of buttons just below the dashboard title and description.   New widgets can be added with settings button allows you to change dashboard name, description, and list of associated applications.  Use the save changes button to persist the dashboard state, which includes any changes made to the dashboard settings or widgets.  And finally, display mode enables an alternative visualization mode, which removes widget controls and backgrounds to create a simplified and seamless viewing experience.", 
            "title": "Modifying Dashboards"
        }, 
        {
            "location": "/dtdashboard/#widgets-overview", 
            "text": "Dashboard widgets receive and display data in real time from DataTorrent application data sources.  Widgets can be added, removed, rearranged, and resized at any time.  Each widgets has a unique list of configurable properties, which include interactive elements displayed directly on the widget, as well as data query settings available from the widget settings.", 
            "title": "Widgets Overview"
        }, 
        {
            "location": "/dtdashboard/#adding-widgets", 
            "text": "Widgets can be added to the dashboard by clicking add widget button, selecting one of the available data sources, selecting one or more widgets, and confirming selection by clicking add widget   Alternatively, randomly selected widgets, one for every available data source, can be added to the dashboard by clicking auto generate button.  Widgets will be automatically placed on the dashboard without any further dialogs.  If data sources are responding slowly, auto generate may take longer to add new widgets.  During this time the button will remain disabled to avoid duplicate requests, and will show spinning arrows to indicate that previous action is already in progress.   Whether using auto generate buttons, results are not persisted until save changes is applied.  Each data source supports one or more data schema types, such as snapshot, dimensions  Each schema type has a specific list of compatible widgets which can be selected to visualize the data.", 
            "title": "Adding Widgets"
        }, 
        {
            "location": "/dtdashboard/#editing-widgets", 
            "text": "Each widget has an dimensions, snapshot) and widget type (table, chart, text For snapshot schema, which represents a single point in time, the primary widget controls include   label field selection  quantity field selection  sort order selection   Below is an example of changing label field and sort order for a bar chart widget.   For dimensions schema, which represents a series of points in time, with ability to configure dimensions based on key and value settings, the primary widget controls include   Time ranges selection  live streaming  historical range  Dimensions Selections  key combinations and key values selection  aggregate selection    For Notes widget, a text in Markdown format can be entered and should be translated to HTML look.  Below is an example of using Markdown syntax to produce headings, lists, and quoted text.   After making the widget settings changes, remember to use save changes button to persist the desired results.  If the resulting changes should not be saved, reloading the dashboard will revert it to the the original state.", 
            "title": "Editing Widgets"
        }, 
        {
            "location": "/dtgateway/", 
            "text": "dtGateway\n\n\nOne of the main components of DataTorrent RTS is dtGateway.  dtGateway is a window on your DataTorrent RTS platform. It is a Java-based multithreaded web server that allows you to easily access information and perform various operations on DataTorrent RTS, and it is the server behind \ndtManage\n. It can run on any node in your Hadoop cluster or any other node that can access your Hadoop nodes, and is installed as a system service automatically by the RTS \ninstaller\n.\n\n\ndtGateway constantly communicates with all the running Apex App Masters, as well as the Node Managers and the Resource Manager in the Hadoop cluster, in order to gather all the information and to perform all the operations users may need.\n\n\n\n\nThese features are exposed through a \nREST API\n. Here are some of things you can do with the REST API:\n\n\n\n\nGet performance metrics (e.g. CPU, memory usage, tuples per second, latency, etc.) and other details of all Apex application instances\n\n\nGet performance metrics and other details of physical and logical operators of each Apex application instance\n\n\nGet performance metrics and other details of individual containers used by each Apex application instance\n\n\nRetrieve container logs\n\n\nDynamically change operator properties, and add and remove operators from the DAG of a running Apex application\n\n\nRecord and retrieve tuples on the fly\n\n\nShutdown a running container or an entire Apex application\n\n\nDynamically change logging level of a container\n\n\nCreate, manage, and view custom system alerts\n\n\nCreate, manage, and interact with dtDashboard\n\n\nCreate, manage, and launch Apex App Packages\n\n\nBasic health checks of the cluster\n\n\n\n\nSecurity\n\n\nWith all the information dtGateway has and what dtGateway can do, the admin of DataTorrent RTS may want to restrict access to certain information and operations to only certain group of users. This means dtGateway must support authentication and authorization.  For authentication, dtGateway can easily be integrated with existing LDAP, Kerberos, or PAM framework.  You can also choose to have dtGateway manage its own user database.\n\n\nFor authorization, dtGateway provides built-in role-based access control. The admin can decide which roles can view what information and perform what operations in dtGateway. The user-to-role mapping can be managed by dtGateway, or be integrated with LDAP roles.  In addition, we provide access control with granularity to the application instance level as well as to the application package level. For example, you can control which users and which roles have read or write access to which application instances and to which application packages.\n\n\nFor information on configuring security see \ndtGateway security\n guide.\n\n\nRest API\n\n\nHere is an example of using the curl command to access dtGateway\u2019s REST API to get the details of a physical operator with ID=40 of application instance with ID=application_1442448722264_14891, assuming dtGateway is listening at localhost:9090\n\n\n$ curl http://localhost:9090/ws/v2/applications/application_1442448722264_14891/physicalPlan/operators/40\n{\n  \ncheckpointStartTime\n: \n1442512091772\n,\n  \ncheckpointTime\n: \n175\n,\n  \ncheckpointTimeMA\n: \n164\n,\n  \nclassName\n: \ncom.datatorrent.contrib.kafka.KafkaSinglePortOutputOperator\n,\n  \ncontainer\n: \ncontainer_e08_1442448722264_14891_01_000017\n,\n  \ncounters\n: null,\n  \ncpuPercentageMA\n: \n0.2039266316727741\n,\n  \ncurrentWindowId\n: \n6195527785184762469\n,\n  \nfailureCount\n: \n0\n,\n  \nhost\n: \nnode22.morado.com:8041\n,\n  \nid\n: \n40\n,\n  \nlastHeartbeat\n: \n1442512100742\n,\n  \nlatencyMA\n: \n5\n,\n  \nlogicalName\n: \nQueryResult\n,\n  \nmetrics\n: {},\n  \nname\n: \nQueryResult\n,\n  \nports\n: [\n    {\n      \nbufferServerBytesPSMA\n: \n0\n,\n      \nname\n: \ninputPort\n,\n      \nqueueSizeMA\n: \n1\n,\n      \nrecordingId\n: null,\n      \ntotalTuples\n: \n6976\n,\n      \ntuplesPSMA\n: \n0\n,\n      \ntype\n: \ninput\n\n    }\n  ],\n  \nrecordingId\n: null,\n  \nrecoveryWindowId\n: \n6195527785184762451\n,\n  \nstatus\n: \nACTIVE\n,\n  \ntotalTuplesEmitted\n: \n0\n,\n  \ntotalTuplesProcessed\n: \n6976\n,\n  \ntuplesEmittedPSMA\n: \n0\n,\n  \ntuplesProcessedPSMA\n: \n20\n,\n  \nunifierClass\n: null\n}\n\n\n\n\nFor the complete spec of the REST API, please refer to dtGateway \nREST API\n.\n\n\nFor information on configuring dtGateway in general, see \nDataTorrent RTS Configuration", 
            "title": "dtGateway"
        }, 
        {
            "location": "/dtgateway/#dtgateway", 
            "text": "One of the main components of DataTorrent RTS is dtGateway.  dtGateway is a window on your DataTorrent RTS platform. It is a Java-based multithreaded web server that allows you to easily access information and perform various operations on DataTorrent RTS, and it is the server behind  dtManage . It can run on any node in your Hadoop cluster or any other node that can access your Hadoop nodes, and is installed as a system service automatically by the RTS  installer .  dtGateway constantly communicates with all the running Apex App Masters, as well as the Node Managers and the Resource Manager in the Hadoop cluster, in order to gather all the information and to perform all the operations users may need.   These features are exposed through a  REST API . Here are some of things you can do with the REST API:   Get performance metrics (e.g. CPU, memory usage, tuples per second, latency, etc.) and other details of all Apex application instances  Get performance metrics and other details of physical and logical operators of each Apex application instance  Get performance metrics and other details of individual containers used by each Apex application instance  Retrieve container logs  Dynamically change operator properties, and add and remove operators from the DAG of a running Apex application  Record and retrieve tuples on the fly  Shutdown a running container or an entire Apex application  Dynamically change logging level of a container  Create, manage, and view custom system alerts  Create, manage, and interact with dtDashboard  Create, manage, and launch Apex App Packages  Basic health checks of the cluster", 
            "title": "dtGateway"
        }, 
        {
            "location": "/dtgateway/#security", 
            "text": "With all the information dtGateway has and what dtGateway can do, the admin of DataTorrent RTS may want to restrict access to certain information and operations to only certain group of users. This means dtGateway must support authentication and authorization.  For authentication, dtGateway can easily be integrated with existing LDAP, Kerberos, or PAM framework.  You can also choose to have dtGateway manage its own user database.  For authorization, dtGateway provides built-in role-based access control. The admin can decide which roles can view what information and perform what operations in dtGateway. The user-to-role mapping can be managed by dtGateway, or be integrated with LDAP roles.  In addition, we provide access control with granularity to the application instance level as well as to the application package level. For example, you can control which users and which roles have read or write access to which application instances and to which application packages.  For information on configuring security see  dtGateway security  guide.", 
            "title": "Security"
        }, 
        {
            "location": "/dtgateway/#rest-api", 
            "text": "Here is an example of using the curl command to access dtGateway\u2019s REST API to get the details of a physical operator with ID=40 of application instance with ID=application_1442448722264_14891, assuming dtGateway is listening at localhost:9090  $ curl http://localhost:9090/ws/v2/applications/application_1442448722264_14891/physicalPlan/operators/40\n{\n   checkpointStartTime :  1442512091772 ,\n   checkpointTime :  175 ,\n   checkpointTimeMA :  164 ,\n   className :  com.datatorrent.contrib.kafka.KafkaSinglePortOutputOperator ,\n   container :  container_e08_1442448722264_14891_01_000017 ,\n   counters : null,\n   cpuPercentageMA :  0.2039266316727741 ,\n   currentWindowId :  6195527785184762469 ,\n   failureCount :  0 ,\n   host :  node22.morado.com:8041 ,\n   id :  40 ,\n   lastHeartbeat :  1442512100742 ,\n   latencyMA :  5 ,\n   logicalName :  QueryResult ,\n   metrics : {},\n   name :  QueryResult ,\n   ports : [\n    {\n       bufferServerBytesPSMA :  0 ,\n       name :  inputPort ,\n       queueSizeMA :  1 ,\n       recordingId : null,\n       totalTuples :  6976 ,\n       tuplesPSMA :  0 ,\n       type :  input \n    }\n  ],\n   recordingId : null,\n   recoveryWindowId :  6195527785184762451 ,\n   status :  ACTIVE ,\n   totalTuplesEmitted :  0 ,\n   totalTuplesProcessed :  6976 ,\n   tuplesEmittedPSMA :  0 ,\n   tuplesProcessedPSMA :  20 ,\n   unifierClass : null\n}  For the complete spec of the REST API, please refer to dtGateway  REST API .  For information on configuring dtGateway in general, see  DataTorrent RTS Configuration", 
            "title": "Rest API"
        }, 
        {
            "location": "/apex/", 
            "text": "Apache Apex\n\n\nApache Apex (incubating) is the industry\u2019s only open source, enterprise-grade unified stream and batch processing engine.  Apache Apex includes key features requested by open source developer community that are not available in current open source technologies.\n\n\n\n\nEvent processing guarantees\n\n\nIn-memory performance \n scalability\n\n\nFault tolerance and state management\n\n\nNative rolling and tumbling window support\n\n\nHadoop-native YARN \n HDFS implementation\n\n\n\n\nFor additional information visit \nApache Apex\n.", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/apex/#apache-apex", 
            "text": "Apache Apex (incubating) is the industry\u2019s only open source, enterprise-grade unified stream and batch processing engine.  Apache Apex includes key features requested by open source developer community that are not available in current open source technologies.   Event processing guarantees  In-memory performance   scalability  Fault tolerance and state management  Native rolling and tumbling window support  Hadoop-native YARN   HDFS implementation   For additional information visit  Apache Apex .", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/apex_malhar/", 
            "text": "Apache Apex Malhar\n\n\nApache Apex Malhar is an open source operator and codec library that can be used with the Apache Apex platform to build real-time streaming applications.  As part of enabling enterprises extract value quickly, Malhar operators help get data in, analyze it in real-time and get data out of Hadoop in real-time with no paradigm limitations.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.\n\n\n\n\nCapabilities common across Malhar operators\n\n\nFor most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of apex*.md RTS. Hence, they have following core streaming runtime capabilities\n\n\n\n\nFault tolerance\n \u2013 Apache Apex Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.\n\n\nProcessing guarantees\n \u2013 Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once \n at-most once WITHOUT requiring the user to write any additional code.  Some operators like MQTT operator deal with source systems that cant track processed data and hence need the operators to keep track of the data. Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this. Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.\n\n\nDynamic updates\n \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application.\n\n\nEase of extensibility\n \u2013 Malhar operators are based on templates that are easy to extend.\n\n\nPartitioning support\n \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system. E.g. With the Kafka or Flume operator, the operator can automatically scale up or down based on the changes in the number of Kafka partitions or Flume channels\n\n\n\n\nOperator Library Overview\n\n\nInput/output connectors\n\n\nBelow is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator\n\n\n\n\nFile Systems\n \u2013 Most streaming analytics use cases we have seen require the data to be stored in HDFS or perhaps S3 if the application is running in AWS. Also, customers often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share. Hence, it\u2019s not just enough to be able to save data to various file systems. You also have to be able to read data from them. RTS supports input \n output operators for HDFS, S3, NFS \n Local Files\n\n\nFlume\n \u2013 NOTE: Flume operator is not yet part of Malhar\n\n\n\n\nMany customers have existing Flume deployments that are being used to aggregate log data from variety of sources. However Flume does not allow analytics on the log data on the fly. The Flume input/output operator enables RTS to consume data from flume and analyze it in real-time before being persisted.\n\n\n\n\nRelational databases\n \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. RTS supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL etc.\n\n\nNoSQL databases\n \u2013NoSQL key-value pair databases like Cassandra \n HBase are becoming a common part of streaming analytics application architectures to lookup reference data or store results. Malhar has operators for HBase, Cassandra, Accumulo (common with govt. \n healthcare companies) MongoDB \n CouchDB.\n\n\nMessaging systems\n \u2013 JMS brokers have been the workhorses of messaging infrastructure in most enterprises. Also Kafka is fast coming up in almost every customer we talk to. Malhar has operators to read/write to Kafka, any JMS implementation, ZeroMQ \n RabbitMQ.\n\n\nNotification systems\n \u2013 Almost every streaming analytics application has some notification requirements that are tied to a business condition being triggered. Malhar supports sending notifications via SMTP \n SNMP. It also has an alert escalation mechanism built in so users don\u2019t get spammed by notifications (a common drawback in most streaming platforms)\n\n\nIn-memory Databases \n Caching platforms\n - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached \n Redis\n\n\nProtocols\n - Streaming use cases driven by machine-to-machine communication have one thing in common \u2013 there is no standard dominant protocol being used for communication. Malhar currently has support for MQTT. It is one of the more commonly, adopted protocols we see in the IoT space. Malhar also provides connectors that can directly talk to HTTP, RSS, Socket, WebSocket \n FTP sources\n\n\n\n\nCompute\n\n\nOne of the most important promises of a streaming analytics platform like Apache Apex is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant etc. Malhar takes this responsibility away from the application developer by providing a huge variety of out of the box computational operators. The application developer can thus focus on the analysis.\n\n\nBelow is just a snapshot of the compute operators available in Malhar\n\n\n\n\nStatistics \n Math - Provide various mathematical and statistical computations over application defined time windows.\n\n\nFiltering \n pattern matching\n\n\nMachine learning \n Algorithms\n\n\nReal-time model scoring is a very common use case for stream processing platforms. \nMalhar allows users to invoke their R models from streaming applications\n\n\nSorting, Maps, Frequency, TopN, BottomN, Random Generator etc.\n\n\n\n\nQuery \n Script invocation\n\n\nMany streaming use cases are legacy implementations that need to be ported over. This often requires re-use some of the existing investments and code that perhaps would be really hard to re-write. With this in mind, Malhar supports invoking external scripts and queries as part of the streaming application using operators for invoking SQL query, Shell script, Ruby, Jython, and JavaScript etc.\n\n\nParsers\n\n\nThere are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into an Apache Apex application. For example in the Telco space, a Java based CDR parser can be directly plugged into Apache Apex operator. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM \n SAX), JSON (flat map converter), Apache log files, syslog, etc.\n\n\nStream manipulation\n\n\nStreaming data aka \u2018stream\u2019 is raw data that inevitably needs processing to clean, filter, tag, summarize etc. The goal of Malhar is to enable the application developer to focus on \u2018WHAT\u2019 needs to be done to the stream to get it in the right format and not worry about the \u2018HOW\u2019. Hence, Malhar has several operators to perform the common stream manipulation actions like \u2013 DeDupe, GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.\n\n\nSocial Media\n\n\nMalhar includes an operator to connect to the popular Twitter stream fire hose.", 
            "title": "Apache Apex-Malhar"
        }, 
        {
            "location": "/apex_malhar/#apache-apex-malhar", 
            "text": "Apache Apex Malhar is an open source operator and codec library that can be used with the Apache Apex platform to build real-time streaming applications.  As part of enabling enterprises extract value quickly, Malhar operators help get data in, analyze it in real-time and get data out of Hadoop in real-time with no paradigm limitations.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.", 
            "title": "Apache Apex Malhar"
        }, 
        {
            "location": "/apex_malhar/#capabilities-common-across-malhar-operators", 
            "text": "For most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of apex*.md RTS. Hence, they have following core streaming runtime capabilities   Fault tolerance  \u2013 Apache Apex Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.  Processing guarantees  \u2013 Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once   at-most once WITHOUT requiring the user to write any additional code.  Some operators like MQTT operator deal with source systems that cant track processed data and hence need the operators to keep track of the data. Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this. Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.  Dynamic updates  \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application.  Ease of extensibility  \u2013 Malhar operators are based on templates that are easy to extend.  Partitioning support  \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system. E.g. With the Kafka or Flume operator, the operator can automatically scale up or down based on the changes in the number of Kafka partitions or Flume channels", 
            "title": "Capabilities common across Malhar operators"
        }, 
        {
            "location": "/apex_malhar/#operator-library-overview", 
            "text": "", 
            "title": "Operator Library Overview"
        }, 
        {
            "location": "/apex_malhar/#inputoutput-connectors", 
            "text": "Below is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator   File Systems  \u2013 Most streaming analytics use cases we have seen require the data to be stored in HDFS or perhaps S3 if the application is running in AWS. Also, customers often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share. Hence, it\u2019s not just enough to be able to save data to various file systems. You also have to be able to read data from them. RTS supports input   output operators for HDFS, S3, NFS   Local Files  Flume  \u2013 NOTE: Flume operator is not yet part of Malhar   Many customers have existing Flume deployments that are being used to aggregate log data from variety of sources. However Flume does not allow analytics on the log data on the fly. The Flume input/output operator enables RTS to consume data from flume and analyze it in real-time before being persisted.   Relational databases  \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. RTS supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL etc.  NoSQL databases  \u2013NoSQL key-value pair databases like Cassandra   HBase are becoming a common part of streaming analytics application architectures to lookup reference data or store results. Malhar has operators for HBase, Cassandra, Accumulo (common with govt.   healthcare companies) MongoDB   CouchDB.  Messaging systems  \u2013 JMS brokers have been the workhorses of messaging infrastructure in most enterprises. Also Kafka is fast coming up in almost every customer we talk to. Malhar has operators to read/write to Kafka, any JMS implementation, ZeroMQ   RabbitMQ.  Notification systems  \u2013 Almost every streaming analytics application has some notification requirements that are tied to a business condition being triggered. Malhar supports sending notifications via SMTP   SNMP. It also has an alert escalation mechanism built in so users don\u2019t get spammed by notifications (a common drawback in most streaming platforms)  In-memory Databases   Caching platforms  - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached   Redis  Protocols  - Streaming use cases driven by machine-to-machine communication have one thing in common \u2013 there is no standard dominant protocol being used for communication. Malhar currently has support for MQTT. It is one of the more commonly, adopted protocols we see in the IoT space. Malhar also provides connectors that can directly talk to HTTP, RSS, Socket, WebSocket   FTP sources", 
            "title": "Input/output connectors"
        }, 
        {
            "location": "/apex_malhar/#compute", 
            "text": "One of the most important promises of a streaming analytics platform like Apache Apex is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant etc. Malhar takes this responsibility away from the application developer by providing a huge variety of out of the box computational operators. The application developer can thus focus on the analysis.  Below is just a snapshot of the compute operators available in Malhar   Statistics   Math - Provide various mathematical and statistical computations over application defined time windows.  Filtering   pattern matching  Machine learning   Algorithms  Real-time model scoring is a very common use case for stream processing platforms.  Malhar allows users to invoke their R models from streaming applications  Sorting, Maps, Frequency, TopN, BottomN, Random Generator etc.", 
            "title": "Compute"
        }, 
        {
            "location": "/apex_malhar/#query-script-invocation", 
            "text": "Many streaming use cases are legacy implementations that need to be ported over. This often requires re-use some of the existing investments and code that perhaps would be really hard to re-write. With this in mind, Malhar supports invoking external scripts and queries as part of the streaming application using operators for invoking SQL query, Shell script, Ruby, Jython, and JavaScript etc.", 
            "title": "Query &amp; Script invocation"
        }, 
        {
            "location": "/apex_malhar/#parsers", 
            "text": "There are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into an Apache Apex application. For example in the Telco space, a Java based CDR parser can be directly plugged into Apache Apex operator. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM   SAX), JSON (flat map converter), Apache log files, syslog, etc.", 
            "title": "Parsers"
        }, 
        {
            "location": "/apex_malhar/#stream-manipulation", 
            "text": "Streaming data aka \u2018stream\u2019 is raw data that inevitably needs processing to clean, filter, tag, summarize etc. The goal of Malhar is to enable the application developer to focus on \u2018WHAT\u2019 needs to be done to the stream to get it in the right format and not worry about the \u2018HOW\u2019. Hence, Malhar has several operators to perform the common stream manipulation actions like \u2013 DeDupe, GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.", 
            "title": "Stream manipulation"
        }, 
        {
            "location": "/apex_malhar/#social-media", 
            "text": "Malhar includes an operator to connect to the popular Twitter stream fire hose.", 
            "title": "Social Media"
        }, 
        {
            "location": "/installation/", 
            "text": "DataTorrent RTS Installation Guide\n\n\nThis guide covers installation of the DataTorrent RTS platform.\n\n\nPlanning\n\n\nInstallation will extract library files and executables into an installation directory, as\nwell as start a process called dtGateway, which used to configure the\nsystem, communicate with running applications, and serve the dtManage dashboard UI.  Installation\nis typically performed on one of the Hadoop cluster edge nodes, meeting the following criteria:\n\n\n\n\nAccessible by users who will launch and manage applications\n\n\nAccessible by all YARN nodes running Apache Apex applications\n\n\n\n\nNote\n: With \ndtGateway security\n configuration disabled, the applications launched\nthrough dtManage interface will appear as started by dtGateway user.\n\n\nRequirements\n\n\n\n\nLinux operating system (tested on CentOS 6.x and Ubuntu 12.04)\n\n\nJava 7 or 8 (tested with Oracle Java 7, OpenJDK Java 7)\n\n\nHadoop (2.2.0 or above) cluster with YARN, HDFS configured, and hadoop executable available in PATH\n\n\nMinimum of 8G RAM available on the Hadoop cluster\n\n\nGoogle Chrome, Firefox, or Safari to access the DataTorrent Console UI\n\n\nPermissions to create HDFS directory for DataTorrent user\n\n\n\n\nInstallation\n\n\nComplete installation configures DataTorrent to run as a system service, installs globally available binaries, and requires root/sudo privileges to run.  After the installation is complete, you will have the following\n\n\n\n\nDataTorrent platform release ( $DATATORRENT_HOME ) located in /opt/datatorrent/releases/[release_version]\n\n\nBinaries are available in /opt/datatorrent/current/bin and links in /usr/bin\n\n\nConfiguration files located in /opt/datatorrent/current/conf\n\n\nLog files located in /var/log/datatorrent\n\n\nDataTorrent Gateway service dtgateway running as dtadmin, and managed with \"sudo service dtgateway\" command\n\n\n\n\nDownload and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.\n\n\na.  Installing from self-extracting archive (*.bin)\n\n\n    curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\n    sudo sh ./datatorrent-rts.bin\n\n\n\nb.  Installing from RedHat Package Manager archive (*.rpm)\n\n\n  curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.rpm\n  sudo rpm -ivh ./datatorrent-rts.rpm\n\n\n\nLimited Local Installation\n\n\nA limited local installation can be helpful for in environments with no root/sudo privileges, or for testing purposes.  All DataTorrent files will be installed under the user's home directory, and DataTorrent Gateway will be running as a local process.  No services or files outside user's home directory will be created.\n\n\n\n\nDataTorrent platform release ( $DATATORRENT_HOME ) located under $HOME/datatorrent/releases/[release_version]\n\n\nBinaries are available under $HOME/datatorrent/current/bin\n\n\nConfiguration files located under $HOME/datatorrent/conf\n\n\nLog files located under $HOME/.dt/logs\n\n\n\n\nDataTorrent Gateway running as current user, and managed with dtgateway command\n\n\n\n\n\n\nDownload and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.\n\n\ncurl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\nsh ./datatorrent-rts.bin\n\n\n\n\n\n\n\nAdd DataTorrent binaries to user PATH environment variable by pasting following into ~/.bashrc, ~/.profile or equivalent environment settings file.\n\n\nDATATORRENT_HOME=~/datatorrent/current\nexport PATH=$PATH:$DATATORRENT_HOME/bin\n\n\n\n\n\n\n\nUpgrades\n\n\nDataTorrent RTS installer automatically performs an upgrade when the same base installation directory is selected.  Configurations stored in local files such as dt-site.xml and custom-env.sh, as well as contents of DataTorrent DFS root directory will be be used to configure the newer version.\n\n\nAutomatic upgrade behavior can be avoided by providing installer with an options to perform full uninstall prior to running the installation.  See Customizing Installation section below.  \n\n\nFull uninstall prior to installation is strongly recommended when performing major version upgrades.  Not all settings and attributes will be backwards compatible between major releases.  This may result in applications failing to launch when an invalid attribute is loaded from dt-site.xml.  In such cases detailed error messages will be provided during an application launch, helping identify and fix outdated references before successfully launching applications.\n\n\nCustomizing Installation\n\n\nVarious options are available to customize the DataTorrent installation.  List of available options be displayed by running installer with -h flag.\n\n\n./datatorrent-rts*.bin -h\n\nOptions:\n\n-h             Help menu\n-B \npath\n      Use \npath\n as base installation directory.  Must be an absolute path.  Default: /opt/datatorrent\n-U \nuser\n      Use \nuser\n user account for installation.  Default: dtadmin\n-G \ngroup\n     Use \ngroup\n group for installation.  Default: dtadmin ( based on value of \nuser\n )\n-H \npath\n      Use \npath\n for location for hadoop executable.  Overrides defaults of HADOOP_PREFIX and PATH.\n-E \nexpr\n      Adds export \nexpr\n to custom-env.sh file.  Used to set an environment variable.  Examples include:\n                 -E JAVA_HOME=/my/java                 Java used by dtgateway  and dtcli\n                 -E DT_LOG_DIR=/var/log/datatorrent    Directory for dtgateway logs\n                 -E DT_RUN_DIR=/var/run/datatorrent    Directory for dtgateway pid files\n-s \nfile\n      Use \nfile\n DataTorrent dt-site.xml file to configure new installation. Overrides default and previous dt-site.xml\n-e \nfile\n      Use \nfile\n DataTorrent custom-env.sh file to configure new installation. Overrides default and previous custom-env.sh\n-g [ip:]port   DataTorrent Gateway address.  Defaults to 0.0.0.0:9090\n-u             Perform full uninstall prior to running installation. WARNING: All current settings will be removed!\n-x             Skip installation steps.  Useful for debugging installation settings with [-v] or perform uninstall with [-u]\n-r             Uninstall current release only.  Does not remove datatorrent, logs, var, etc directories.  Used with RPM uninstall.\n-v             Run in verbose mode, providing extra details for every step.\n-V             Print DataTorrent version and exit.\n\n\n\nSome Hadoop distributions may rquire changes to default settings.  For example, when running Apache Hadoop, you may encounter that JAVA_HOME is not set for DTGateway user (defaults to dtadmin):\n\n\nsudo -u dtadmin hadoop version\nError: JAVA_HOME is not set and could not be found.\n\n\n\nIf JAVA_HOME is not set, you can export it manually at the end of /opt/datatorrent/current/conf/custom-env.sh or run the installer with following option to set JAVA_HOME during installation time:\n\n\nsudo ./datatorrent-rts*.bin -E JAVA_HOME=/valid/java/home/path\n\n\n\nIn some Hadoop installations, you may already have an existing user, which has administrative privileges to HFDS and YARN, and you prefer to use that user to run DTGateway service.  Assuming this user is named myuser, you can run the following command during installation to ensure DTGateway service runs as myuser.\n\n\nsudo ./datatorrent-rts*.bin -U myuser\n\n\n\nInstallation Wizard\n\n\nAfter the system installation is complete, remaining configuration steps are performed using the Installation Wizard in the DataTorrent UI Console.  DataTorrent UI Console address is typically hostname of the node where DataTorrent platform was installed, listening on port 9090.  The connection details are provided by the system installer, but may need to modified depending on the way Hadoop cluster is being accessed.\n\n\nhttp://\ninstallation_host\n:9090/\n\n\n\nThe Installation Wizard can be accessed and repeated at any time to update key system settings from the Configuration section of the DataTorrent UI Console.\n\n\nDataTorrent installation can be verified by running included demo applications.  See \nLaunching Demo Applications\n for details.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#datatorrent-rts-installation-guide", 
            "text": "This guide covers installation of the DataTorrent RTS platform.", 
            "title": "DataTorrent RTS Installation Guide"
        }, 
        {
            "location": "/installation/#planning", 
            "text": "Installation will extract library files and executables into an installation directory, as\nwell as start a process called dtGateway, which used to configure the\nsystem, communicate with running applications, and serve the dtManage dashboard UI.  Installation\nis typically performed on one of the Hadoop cluster edge nodes, meeting the following criteria:   Accessible by users who will launch and manage applications  Accessible by all YARN nodes running Apache Apex applications   Note : With  dtGateway security  configuration disabled, the applications launched\nthrough dtManage interface will appear as started by dtGateway user.", 
            "title": "Planning"
        }, 
        {
            "location": "/installation/#requirements", 
            "text": "Linux operating system (tested on CentOS 6.x and Ubuntu 12.04)  Java 7 or 8 (tested with Oracle Java 7, OpenJDK Java 7)  Hadoop (2.2.0 or above) cluster with YARN, HDFS configured, and hadoop executable available in PATH  Minimum of 8G RAM available on the Hadoop cluster  Google Chrome, Firefox, or Safari to access the DataTorrent Console UI  Permissions to create HDFS directory for DataTorrent user", 
            "title": "Requirements"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "Complete installation configures DataTorrent to run as a system service, installs globally available binaries, and requires root/sudo privileges to run.  After the installation is complete, you will have the following   DataTorrent platform release ( $DATATORRENT_HOME ) located in /opt/datatorrent/releases/[release_version]  Binaries are available in /opt/datatorrent/current/bin and links in /usr/bin  Configuration files located in /opt/datatorrent/current/conf  Log files located in /var/log/datatorrent  DataTorrent Gateway service dtgateway running as dtadmin, and managed with \"sudo service dtgateway\" command   Download and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.  a.  Installing from self-extracting archive (*.bin)      curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\n    sudo sh ./datatorrent-rts.bin  b.  Installing from RedHat Package Manager archive (*.rpm)    curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.rpm\n  sudo rpm -ivh ./datatorrent-rts.rpm", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#limited-local-installation", 
            "text": "A limited local installation can be helpful for in environments with no root/sudo privileges, or for testing purposes.  All DataTorrent files will be installed under the user's home directory, and DataTorrent Gateway will be running as a local process.  No services or files outside user's home directory will be created.   DataTorrent platform release ( $DATATORRENT_HOME ) located under $HOME/datatorrent/releases/[release_version]  Binaries are available under $HOME/datatorrent/current/bin  Configuration files located under $HOME/datatorrent/conf  Log files located under $HOME/.dt/logs   DataTorrent Gateway running as current user, and managed with dtgateway command    Download and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.  curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\nsh ./datatorrent-rts.bin    Add DataTorrent binaries to user PATH environment variable by pasting following into ~/.bashrc, ~/.profile or equivalent environment settings file.  DATATORRENT_HOME=~/datatorrent/current\nexport PATH=$PATH:$DATATORRENT_HOME/bin", 
            "title": "Limited Local Installation"
        }, 
        {
            "location": "/installation/#upgrades", 
            "text": "DataTorrent RTS installer automatically performs an upgrade when the same base installation directory is selected.  Configurations stored in local files such as dt-site.xml and custom-env.sh, as well as contents of DataTorrent DFS root directory will be be used to configure the newer version.  Automatic upgrade behavior can be avoided by providing installer with an options to perform full uninstall prior to running the installation.  See Customizing Installation section below.    Full uninstall prior to installation is strongly recommended when performing major version upgrades.  Not all settings and attributes will be backwards compatible between major releases.  This may result in applications failing to launch when an invalid attribute is loaded from dt-site.xml.  In such cases detailed error messages will be provided during an application launch, helping identify and fix outdated references before successfully launching applications.", 
            "title": "Upgrades"
        }, 
        {
            "location": "/installation/#customizing-installation", 
            "text": "Various options are available to customize the DataTorrent installation.  List of available options be displayed by running installer with -h flag.  ./datatorrent-rts*.bin -h\n\nOptions:\n\n-h             Help menu\n-B  path       Use  path  as base installation directory.  Must be an absolute path.  Default: /opt/datatorrent\n-U  user       Use  user  user account for installation.  Default: dtadmin\n-G  group      Use  group  group for installation.  Default: dtadmin ( based on value of  user  )\n-H  path       Use  path  for location for hadoop executable.  Overrides defaults of HADOOP_PREFIX and PATH.\n-E  expr       Adds export  expr  to custom-env.sh file.  Used to set an environment variable.  Examples include:\n                 -E JAVA_HOME=/my/java                 Java used by dtgateway  and dtcli\n                 -E DT_LOG_DIR=/var/log/datatorrent    Directory for dtgateway logs\n                 -E DT_RUN_DIR=/var/run/datatorrent    Directory for dtgateway pid files\n-s  file       Use  file  DataTorrent dt-site.xml file to configure new installation. Overrides default and previous dt-site.xml\n-e  file       Use  file  DataTorrent custom-env.sh file to configure new installation. Overrides default and previous custom-env.sh\n-g [ip:]port   DataTorrent Gateway address.  Defaults to 0.0.0.0:9090\n-u             Perform full uninstall prior to running installation. WARNING: All current settings will be removed!\n-x             Skip installation steps.  Useful for debugging installation settings with [-v] or perform uninstall with [-u]\n-r             Uninstall current release only.  Does not remove datatorrent, logs, var, etc directories.  Used with RPM uninstall.\n-v             Run in verbose mode, providing extra details for every step.\n-V             Print DataTorrent version and exit.  Some Hadoop distributions may rquire changes to default settings.  For example, when running Apache Hadoop, you may encounter that JAVA_HOME is not set for DTGateway user (defaults to dtadmin):  sudo -u dtadmin hadoop version\nError: JAVA_HOME is not set and could not be found.  If JAVA_HOME is not set, you can export it manually at the end of /opt/datatorrent/current/conf/custom-env.sh or run the installer with following option to set JAVA_HOME during installation time:  sudo ./datatorrent-rts*.bin -E JAVA_HOME=/valid/java/home/path  In some Hadoop installations, you may already have an existing user, which has administrative privileges to HFDS and YARN, and you prefer to use that user to run DTGateway service.  Assuming this user is named myuser, you can run the following command during installation to ensure DTGateway service runs as myuser.  sudo ./datatorrent-rts*.bin -U myuser", 
            "title": "Customizing Installation"
        }, 
        {
            "location": "/installation/#installation-wizard", 
            "text": "After the system installation is complete, remaining configuration steps are performed using the Installation Wizard in the DataTorrent UI Console.  DataTorrent UI Console address is typically hostname of the node where DataTorrent platform was installed, listening on port 9090.  The connection details are provided by the system installer, but may need to modified depending on the way Hadoop cluster is being accessed.  http:// installation_host :9090/  The Installation Wizard can be accessed and repeated at any time to update key system settings from the Configuration section of the DataTorrent UI Console.  DataTorrent installation can be verified by running included demo applications.  See  Launching Demo Applications  for details.", 
            "title": "Installation Wizard"
        }, 
        {
            "location": "/configuration/", 
            "text": "DataTorrent RTS Configuration\n\n\nThis document covers all the information required to configure DataTorrent RTS\nto run with Hadoop 2.2+. Basic understanding of Hadoop 2.x, including HDFS and YARN\nis required.  To learn more about Hadoop 2.x visit \nhadoop.apache.org\n.\n\n\nInstallation\n\n\nIf you have not installed DataTorrent RTS already, follow the installation instructions in the \ninstallation guide\n.\n\n\n\n\nConfiguration Files\n\n\nSystem configuration is stored in local files on the machine where\nthe DT Gateway was installed, as well as Apache Apex DFS root directory\nselected during the installation. \u00a0The local file \ncustom-env.sh\n can be used\nto configure CLASSPATH, JAVA_HOME, and various runtime settings.\n\n\nDepending on the installation type, these may be located under \n/opt/datatorrent/current/conf\n or \n~/datatorrent/current/conf\n.  See \ninstallation guide\n for details.\n\n\n(install dir)/conf/custom-env.sh\n\n\nThis file can be used to configure behavior of DT Gateway service,\nas well as \ndtcli\n command line utility. \u00a0After adding custom properties\nto this file, dtgateway and dtcli utilities need to be restarted for\nchanges to take effect.\n\n\nExample custom-env.sh configuration:\n\n\n# Increase DT Gateway memory to 2GB\nDT_GATEWAY_HEAP_MEM=2048m\n\n\n\n\nEnvironment variables available for configuration\n\n\n\n\nDT_GATEWAY_HEAP_MEM\n \n Maximum heap size allocated to DT Gateway service.  Default is 1024m.\n\n\nDT_GATEWAY_DEBUG\n \n Set to 1 to enable additional debug information in the dtgateway.log\n\n\nDT_CLASSPATH\n \n Classpath used to load additional jars or properties for dtcli and dtgateway\n\n\nDT_LOG_DIR\n \n Directory for log files\n\n\nDT_RUN_DIR\n \n Directory for process id and other temporary files\ncreated at run time\n\n\n\n\n(user home)/.dt/dt-site.xml\n\n\nThis file is used to customize the DataTorrent platform and the behavior of\napplications. \u00a0It can be particularly useful for changing\nGateway application connection address, or configuring environment specific\nsettings, such as specific machine names, IP addresses, or performance\nsettings which may change from environment to environment.\n\n\nExample of a single property configuration in dt-site.xml:\n\n\nconfiguration\n\n  \nproperty\n\n      \nname\ndt.operator.MyCustomStore.host\n/name\n\n      \nvalue\n192.168.2.35\n/value\n\n  \n/property\n\n   \u2026\n\n/configuration\n\n\n\n\n\nGateway Configuration Properties\n\n\n\n\ndt.gateway.listenAddress\n - The address and port DT Gateway listens to.  Defaults to 0.0.0.0:9090\n\n\ndt.gateway.autoPublishInterval\n - The interval in milliseconds DT Gateway should publish application information on the websocket channel.  Default is 1000.\n\n\ndt.gateway.sslKeystorePath\n - Specifying of the SSL Key store path enables HTTPS on the DT Gateway (See the \ndtGateway Security\n document)\n\n\ndt.gateway.sslKeystorePassword\n - The password of the SSL key store (See the \ndtGateway Security\n document)\n\n\ndt.gateway.allowCrossOrigin\n - Setting it to true allows cross origin HTTP access to the DT Gateway.  Default is false.\n\n\ndt.gateway.authentication.(OPTION)\n - Determines the scheme of Hadoop security authentication (See the \ndtGateway Security\n document).\n\n\ndt.gateway.http.authentication\n - Determines the scheme of DT Gateway HTTP security authentication (See the \ndtGateway Security\n document).\n\n\ndt.gateway.staticResourceDirectory\n - The document root directory where the DT Gateway should serve from for the /static HTTP path.\n\n\n\n\nApplication Configuration Properties\n\n\nFor a complete list of configurable application properties see the Attributes\u00a0section below.\n\n\n\n\nResources Management and Performance Tuning\n\n\nThe platform provides continuous information about CPU, Memory, and Network\nusage for the system as a whole, individual running applications,\noperators, streams, and various internal components.  These statistics\nare available via \nREST API\n, \ndtCli\n, and \ndtManage\n.\n\n\nThe platform is also responsible for\n\n\n\n\nHonoring the resource restrictions enforced by the YARN RM and taking\n    preventive action to ensure they are met. This is done at both launch time\n    (fit the execution plan to the number of containers and their\n    sizes), as well as at run time.\n\n\nHonoring resource constraints an application developer\n    may provide such as the amount of memory allocated to individual operators,\n    associated buffer servers, or the number of partitions.\n\n\n\n\nSTRAM works with the YARN RM on a continual basis to ensure that resource\nconstraints are met. As a multi-tenant application, it is crucial to be able to\nperform within given resource limits. The design of the platform enables\neffective management of all three types of resources (CPU, Memory, I/O).\n\n\nCPU\n\n\nCPU utilization is computed on a per-thread basis within a container by the\nStreamingContainer; this value is also, in effect, the per-operator value\nsince each operator is a single threaded application. CPU utilization is also\ncomputed for the buffer-server as well as other common tasks within a container.\n\n\nNetwork\n\n\nNetwork usage management is needed to ensure that desired latency and\nthroughput levels are achieved and any applicable SLA terms are met.\n\n\nThe platform provides real-time statistics on the number of bytes or tuples\nprocessed by each operator. Application developers can modulate network traffic\nusing a couple of mechanisms:\n- Adjust the locality of streams: Using THREAD_LOCAL or CONTAINER_LOCAL\n  can reduce network load substantially as discussed below.\n- Adjust the number of partitions and unifiers.\n\n\nRAM\n\n\nSTRAM keeps track of resource usage on per container basis. Appropriate\nattributes can be set to limit the amount of RAM on a per-operator or\nper-container basis.\n\n\nSpike Management\n\n\nStreaming applications do not have the same throughput (events/second) for\nall 24 hours of the day; occasional spikes in the incoming data rate are common.\nMost streaming applications resolve this dichotomy\nby providing resources for the peak. So, resource utilization is\nsuboptimal for most of the day because resources, though unused, are locked up\nand therefore unusable by other applications in a multi-tenant environment.\n\n\nThe platform provides mechanisms to manage the spikes by adding partitions\nduring peak, and removing them once the spike subsides.\n\n\nPartitioning\n\n\nPartitioning is a core mechanism to distribute computation (and the associated\nresource utilization) across the cluster. It is discussed, along with the\nrelated concept of unifiers, in greater detail in\n\nApplication development\n and \nOperator Development\n.\n\n\nStream Modes\n\n\nThe platform support 5 stream modes, namely THREAD_LOCAL\n(intra-thread), CONTAINER_LOCAL (intra process/jvm), NODE_LOCAL (intra\nnode), RACK_LOCAL (same rack), and unspecified. While designing an application,\nthe modes should be decided carefully. All\nstream-modes are hints to the STRAM, and hence could be ignored if\nresources are not available, and could be changed on a run-time basis.\nThere are pros and cons of each.\n\n\n\n\nTHREAD_LOCAL\n: All the operators of the ports on this stream\n    share the same thread. Tuples are thus passed via the thread call stack.\n    The performance is massive and go into 100s of millions\n    of tuples/sec. Do note that if the operations are compute intensive,\n    them THREAD_LOCAL may not perform better than CONTAINER_LOCAL.\n    Thread call stack is extremely efficient in terms of I/O (there is\n    no I/O here), but the same thread does both the upstream and\n    downstream computation. A limitation is that all downstream\n    operators on this stream must have only one input port connected. If\n    the JVM process is lost, all the operators are lost, and will be\n    restarted again in another container.\n\n\nCONTAINER_LOCAL\n: All the operators of the ports on this\n    stream are in the same process. Each denotes a separate thread, and\n    tuples are passed in mmemory via a connectionBuffer as the intra-process\n    communication mechanism.\n    This mode has very high throughput and can easily do more than\n    million tuples/sec. However, since there is no bufferserver, features that\n    it provides (spooling, presistence) are not available, so memory needs\n    may grow.\n    This mode relies on the downstream operators consuming tuples, on average,\n    at least as fast as they are emitted by the upstream operator. As with the\n    previous mode, if the JVM process is\n    lost, all the operators are lost, and will be restarted again in\n    another container.\n\n\nNODE_LOCAL\n: All operators on this stream are on the\n    same node. Inter-process communication via the local loopback interface is\n    used. This mode is also very fast, as data does not traverse the NIC\n    but it has a buffer-server and so all the features that the buffer-server\n    provides (spooling, persistence) are available. If one container dies, all\n    the operators in that container will obviously need to be recreated and\n    restarted, but other operators remain unaffected. However if\n    a node dies then all of its containers and the operators hosted by them\n    need to be restarted.\n\n\nRACK_LOCAL\n: All operators on this stream are in the\n    same rack. Communication is not as fast as the previous modes since data\n    needs to pass through the NIC. Like the previous mode, it has a\n    buffer-server and so all the features that buffer-server provides are\n    available. Use of this mode reduces the probability of multi-operator\n    outage since multiple operators are not constrained to run on the same\n    node. This mode however will be affected by outage of a switch.\n\n\nUnspecified: This is the default mode and STRAM makes no special effort\n    to achieve any particular locality. There are thus no guarantees on whether\n    a stream will cross rack, node, or process boundaries.\n\n\n\n\n\n\n\n\nMulti-Tenancy and Security\n\n\nThe platform is a YARN native application, and so all security features\navailable in Hadoop also apply for securing Apache Apex applications.\n\nThe default security for the streaming application is Kerberos based.\n\n\nKerberos Authentication\n\n\nKerberos is a ticket based authentication system that provides\nauthentication in a distributed environment where authentication between\nmultiple users, hosts and services is needed. It is the de-facto\nauthentication mechanism supported in Hadoop. To use Kerberos\nauthentication, the Hadoop installation must first be configured for\nsecure mode with Kerberos. Please refer to the administration guide of\nyour Hadoop distribution on how to do that. Once Hadoop is running with\nKerberos security enabled, DataTorrent platform also needs to be\nconfigured for Kerberos. There are two parts of the platform that need\nto be configured, CLI (Command Line Interface) and DT Gateway.\n\n\nHadoop Configuration\n\n\nThe DataTorrent application uses delegation tokens to communicate\nwith the ResourceManager (YARN) and NameNode (HDFS) and these tokens are\nissued by those servers respectively. Since the application is long-running,\nthe tokens should be valid for the lifetime of the application.\nHadoop has a configuration setting for the maximum lifetime of the\ntokens and they should be set to cover the lifetime of the application.\nThere are separate settings for ResourceManager and NameNode delegation\ntokens.\n\n\nThe ResourceManager delegation token max lifetime is specified in\n\nyarn-site.xml\n and can be specified as follows for example for a lifetime\nof 1 year\n\n\nproperty\n\n  \nname\nyarn.resourcemanager.delegation.token.max-lifetime\n/name\n\n  \nvalue\n31536000000\n/value\n\n\n/property\n\n\n\n\n\nThe NameNode delegation token max lifetime is specified in\nhdfs-site.xml and can be specified as follows for example for a lifetime\nof 1 year\n\n\nproperty\n\n   \nname\ndfs.namenode.delegation.token.max-lifetime\n/name\n\n   \nvalue\n31536000000\n/value\n\n \n/property\n\n\n\n\n\nAlso, DataTorrent RTS needs the DT Gateway user (default \ndtadmin\n) to be a\nproxy user in the Hadoop configuration when running in\nsecure mode. \u00a0This will enable the DT Gateway to launch applications on\nbehalf of the logged-in user. \u00a0Please add the following in \ncore-site.xml\n\nand replace [username] with the user running the DT Gateway:\n\n\nproperty\n\n  \nname\nhadoop.proxyuser.[username].groups\n/name\n\n  \nvalue\n*\n/value\n\n\n/property\n\n\n\nproperty\n\n  \nname\nhadoop.proxyuser.[username].hosts\n/name\n\n  \nvalue\n*\n/value\n\n\n/property\n\n\n\n\n\nCLI Configuration\n\n\nThe DataTorrent command line interface is used to launch\napplications along with performing various other operations on\napplications. \u00a0When Kerberos security is enabled in Hadoop, a Kerberos\nticket granting ticket or the Kerberos credentials of the user is needed\nby the CLI program \ndtcli\n to authenticate with Hadoop for any\noperation. Kerberos credentials are composed of a principal and either a\n\nkeytab\n or a password. For security and operational reasons only keytabs\nare supported in Hadoop and by extension in DataTorrent platform. When\nuser credentials are specified, all operations including launching\napplication are performed as that user.\n\n\nUsing kinit\n\n\nA Keberos ticket granting ticket (TGT) can be obtained by using the\nKerberos command \nkinit\n. Detailed documentation for the command can be\nfound online or in man pages. An sample usage of this command is\n\n\nkinit -k -t path-tokeytab-file kerberos-principal\n\n\n\nIf this command is successful, the TGT is obtained, cached and available for\nother programs. The CLI program \ndtcli\n can then be started to launch\napplications and perform other operations.\n\n\nUsing Kerberos credentials\n\n\nThe CLI program \ndtcli\n can also use the Kerberos credentials directly without\nrequiring a TGT to be obtained separately. This can be useful in batch mode\nwhere \ndtcli\n is not launched manually and also in scenarios where running a\nseparate program like \nkinit\n is not feasible.\n\n\nThe credentials can be specified in the \ndt-site.xml\n configuration\nfile. If only a single user is launching applications, the global\n\ndt-site.xml\n configuration file in the installation folder can be used.\nIn a multi-user environment the user can use the \ndt-site.xml\n file in his\nhome directory. The location of this file will be \n$HOME/.dt/dt-site.xml\n\nIf this file does not exist, the user can create a new one.\n\n\nThe snippet below shows the how the credentials are specified in\nthe configuration file as properties.\n\n\nproperty\n\n        \nname\ndt.authentication.principal\n/name\n\n        \nvalue\nkerberos-principal-of-user\n/value\n\n\n/property\n\n\nproperty\n\n        \nname\ndt.authentication.keytab\n/name\n\n        \nvalue\nabsolute-path-to-keytab-file\n/value\n\n\n/property\n\n\n\n\n\nThe property \ndt.authentication.principal\n specifies the Kerberos\nuser principal and \ndt.authentication.keytab\n specifies the absolute path\nto the keytab file for the user.\n\n\nDT Gateway Configuration\n\n\nDT Gateway is a service that provides the backend functionality\nfor the DataTorrent UI console. Refer to \ndtManage\n\u00a0for\ndetails on the UI console. The DT Gateway provides real-time information\nabout running applications, allows changes to applications, launching\nnew applications among various other operations. In Kerberos secure\nmode, Kerberos credentials are required for DT Gateway to operate. The\ncredentials should match the user that the DT Gateway service is running\nas.\n\n\nIn a multi-user installation DT Gateway is typically running as\nuser \ndtadmin\n and the Kerberos credentials specified should be for this\nuser. They are specified in the \ndt-site.xml\n configuration file in the\ninstallation folder. For a single user installation where gateway is\nrunning as the user, the Kerberos credentials will be the user\u2019s and\nthey will be specified in the \ndt-site.xml\n in the home directory location\n\n$HOME/.dt/dt-site.xml\n.\n\n\nThe snippet below shows how the credentials are specified in the\nconfiguration file as properties.\n\n\nproperty\n\n        \nname\ndt.gateway.authentication.principal\n/name\n\n        \nvalue\nkerberos-principal-of-gateway-user\n/value\n\n\n/property\n\n\nproperty\n\n        \nname\ndt.gateway.authentication.keytab\n/name\n\n        \nvalue\nabsolute-path-to-keytab-file\n/value\n\n\n/property\n\n\n\n\n\ndtManage Authentication\n\n\nAccess to the UI console can be protected by having users authenticate before they can access the contents of the console.\nDifferent authentication mechanisms are supported including local password,\nKerberos and LDAP. Please refer to \nGateway Security\n\nfor details of how to configure this.\n\n\nRun-Time Management\n\n\nUnlike Map-Reduce, streaming applications never end. They are designed\nto run 24x7, processing a continuous stream of input data. This makes run-time\nmanagement of the applications very critical. The platform provides strong\nsupport for various operations. These include\n\n\n\n\nRuntime metrics and stats on various components of the\n    application including aggregated metrics\n\n\nAbility to change the logical plan, physical plan, and\n    execution plan of the application\n\n\nAbility to dump the current state of the application to enable\n    re-launch (in case of Hadoop grid outage)\n\n\nIntegration of STRAM state with Zookeeper (in later versions)\n\n\nDebugger, charting, and other tools triggered at run time\n\n\n\n\nDynamic Functional Modifications\n\n\nPlatform supports changes to an application at multiple stages.\nApplication design parameters (attributes and properties) can be\nchanged at launch time via the job configuration file and during runtime via\nthe \ndtcli\n tool or using the REST webservice calls. Support for runtime\nchanges is critical for operability as it enables changes to a running\napplication without being forced to kill it. This is a critical need for\nstreaming applications and a significant difference from map-reduce/batch\napplications.\n\n\nFrom an operational perspective, the platform will allow changes\nin both the logical plan (query modification, or properties\nmodifications) and the physical plan (attribute modification generally and\npartitioning changes specifically).\n\n\nExamples of dynamic changes to logical plan include\n-   Changing properties of an operator\n-   Adding or deleting a sub-dag. Some examples are\n  -   Change in persistence\n  -   Insertion of charts, debugger etc.\n  -   Query insertion on a particular stream\n\n\nAny change to a logical plan will change the physical and the\nexecution plan. Examples of dynamic changes only to physical plan\ninclude\n\n\n\n\nChange in attributes that triggers STRAM to change the number\n    of physical operators.\n\n\nRuntime changes in load or grid resources (via RM, or\n    outages), that triggers STRAM to change the physical plan to meet\n    SLA, latency, resources usage goals\n\n\nDirect request to change the number of partitions of\n    an operator. For example new input adapters to handle expected\n    uptick in ingestion rate\n\n\n\n\nAny change to a physical plan will change the execution plan.\nExamples of dynamic changes only to the execution plan include\n\n\n\n\nChanges in attributes that need a new execution plan\n\n\nChanges to stream modes\n\n\nNode recovery from an outage\n\n\n\n\nRuntime Code\n\n\nSTRAM is able to reuse and move any code that is supplied with the\ninitial launch of the application. The default behavior is for the jar\nto include all the library templates in addition to the application\ncode. This enables STRAM to make changes to the application as the code\nis already available.\n\n\nLoad\n\n\nSTRAM manages runtime changes to ensure that the application\nscales up and down. This includes changes in load, changes in skew,\nchanges in resource behavior (network goes slow) etc. STRAM proactively\nmonitors the application and will make run time changes as\nneeded.\n\n\nUptime\n\n\nNode recovery is a change in the execution plan caused by external\nevents (node outage) or RM taking resources back (pre-emption). The\nplatform enables node recovery in three modes, namely, at least once, at\nmost once, and exactly once. SLA enforcement in terms of latency,\nuptime, etc. is done via runtime changes.\n\n\n\n\nAttributes\n\n\nAttribute specification is the process by which operational\ncustomization is achieved. Currently, modification of attributes on an\napplication that is already running is not supported. We intend to add\nthis in future versions on an attribute by attribute basis. Attributes\nare parameters that the platform recognizes and acts on and are not part\nof user code. Attributes are the mechanism by which platform features\ncan be customized. They are specified with a key and a value. In this\nsection we list the attributes, their default values, and briefly\nexplain what they do.\n\n\nThere are three kinds of attributes\n\n\n\n\nApplication attributes\n\n\nOperator attributes\n\n\nPort attributes\n\n\n\n\nFor implementation details look at the \njavadocs\n. Some very common attributes are\n\n\n\n\nApplication: Application window, Application name, Checkpoint\n    window, Container JVM options, container memory, containers, max\n    count, heartbeat interval, STRAM memory, launch mode, tuple\n    recording, etc. \u00a0See \nContext.DAGContext\n\n\nOperators: Initial partitions, checkpoint window, locality\n    host, locality rack, memory, recovery attempts, stateless, storage\n    agent, etc. See \nContext.OperatorContext\n\n\nPorts: Queue capacity, auto_record, partition parallel, etc.\n    See \nContext.PortContext\n\n\n\n\nThese attributes are available via the \nContext\n class and can be\naccessed in the \nsetup\n call of an operator.\n\n\nApplication Configuration\n\n\nStarting from RTS release 2.0.0, applications are configured through Application Packages. \u00a0Please refer to the\n\nApplication Packages\n\u00a0for details.\n\n\nAdjusting Logging Levels\n\n\nApplication Logging\n\n\nLogging levels for specific classes or groups of classes can be raised or\nlowered at runtime from \ndtManage\n application view with the\n\nSet Log Levels\n button. \u00a0Explicit class paths or patterns like\n\norg.apache.hadoop.*\n\u00a0or \ncom.datatorrent.*\n\u00a0can be used to adjust logging to\nvalid \nlog4j\n levels such as DEBUG or INFO.\u00a0This produces immediate change in\nthe logs, but does not persist across application restarts.\n\n\nFor permanent changes to logging levels, lines similar to these can be inserted\ninto \ndt-site.xml\n. To specify multiple patterns, use a comma-separated list.\n\n\nproperty\n\n  \nname\ndt.loggers.level\n/name\n\n  \nvalue\ncom.datatorrent.*:DEBUG,org.apache.*:INFO\n/value\n\n\n/property\n\n\n\n\n\nFull DEBUG logging can be enabled by adding these lines:\n\n\nproperty\n\n  \nname\ndt.attr.DEBUG\n/name\n\n  \nvalue\ntrue\n/value\n\n\n/property\n\n\n\n\n\nCustom log4j Properties for Application Packages\n\n\nThere are two ways of setting custom \nlog4j\n properties in an Apex application\npackage\n\n\n\n\n\n\nAt the Application level. This will ensure that the custom log4j properties\n   are used for all containers including Application Master. An example:\n\n\nproperty\n\n  \nname\ndt.attr.CONTAINER_JVM_OPTIONS\n/name\n\n  \nvalue\n-Dlog4j.configuration=custom_log4j.properties\n/value\n\n\n/property\n\n\n\n\n\n\n\n\nAt an individual operator level. This sets the custom log4j properties only\n   on the container that is hosting the operator.  An example:\n\n\nproperty\n\n  \nname\ndt.operator.\nOPERATOR_NAME\n.attr.JVM_OPTIONS\n/name\n\n  \nvalue\n -Dlog4j.configuration=custom_log4j.properties\n/value\n\n\n/property\n\n\n\n\n\n\n\n\nMake sure that the file \ncustom_log4j.properties\n is part of your application\njar and is located under \nsrc/main/resources\n.  Some examples of custom log4j\nproperties files follow.\n\n\n\n\n\n\nWriting to a file\n\n\nlog4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}\n\n\n\n\n\n\n\nWriting to Console\n\n\nlog4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.ConsoleAppender\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\n\n\n\n\n\n\n\ndtGateway Logging\n\n\nDT Gateway log level can be changed to DEBUG by settings following\nenvironment variable before launching DT Gateway (as of version\n2.0).\n\n\nexport DT_GATEWAY_DEBUG=1\n\n\n\nHadoop Tuning\n\n\nYARN vmem-pmem ratio tuning\n\n\nAfter performing a new installation, sometimes the following\nmessage is displayed while launching an application:\n\n\nApplication application_1408120377110_0002 failed 2\ntimes due to AM Container for appattempt_1408120377110_0002_000002\nexited with exitCode: 143 due to:\nContainer\\[pid=27163,containerID=container_1408120377110_0002_02_000001\\]\nis running beyond virtual memory limits. Current usage: 308.1 MB of 1 GB\nphysical memory used; 2.5 GB of 2.1 GB virtual memory used. Killing\ncontainer.\n\nDump of the process-tree for container_1408120377110_0002_02_000001 :\n\n\nPID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n\n27208 27163 27163 27163 (java) 604 19 2557546496 78566\n/usr/java/default/bin/java\n-agentlib:jdwp=transport=dt_socket,server=y,suspend=n -Xmx768m\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/dt-heap-2.bin\n-Dhadoop.root.logger=DEBUG,RFA\n-Dhadoop.log.dir=/disk2/phd/dn/yarn/userlogs/application_1408120377110_0002/container_1408120377110_0002_02_000001\n\n\n\n\nTo fix this \nyarn.nodemanager.vmem-pmem-ratio\n in \nyarn-site.xml\n should be\nincreased from 2 to 5 or higher. \u00a0Here is an example setting:\n\n\nproperty\n\n   \ndescription\nRatio between virtual memory to physical memory when\n     setting memory limits for containers. Container allocations are\n     expressed in terms of physical memory, and virtual memory usage\n     is allowed to exceed this allocation by this ratio.\n   \n/description\n\n   \nname\nyarn.nodemanager.vmem-pmem-ratio\n/name\n\n   \nvalue\n10\n/value\n\n \n/property", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuration/#datatorrent-rts-configuration", 
            "text": "This document covers all the information required to configure DataTorrent RTS\nto run with Hadoop 2.2+. Basic understanding of Hadoop 2.x, including HDFS and YARN\nis required.  To learn more about Hadoop 2.x visit  hadoop.apache.org .", 
            "title": "DataTorrent RTS Configuration"
        }, 
        {
            "location": "/configuration/#installation", 
            "text": "If you have not installed DataTorrent RTS already, follow the installation instructions in the  installation guide .", 
            "title": "Installation"
        }, 
        {
            "location": "/configuration/#configuration-files", 
            "text": "System configuration is stored in local files on the machine where\nthe DT Gateway was installed, as well as Apache Apex DFS root directory\nselected during the installation. \u00a0The local file  custom-env.sh  can be used\nto configure CLASSPATH, JAVA_HOME, and various runtime settings.  Depending on the installation type, these may be located under  /opt/datatorrent/current/conf  or  ~/datatorrent/current/conf .  See  installation guide  for details.", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/configuration/#install-dirconfcustom-envsh", 
            "text": "This file can be used to configure behavior of DT Gateway service,\nas well as  dtcli  command line utility. \u00a0After adding custom properties\nto this file, dtgateway and dtcli utilities need to be restarted for\nchanges to take effect.  Example custom-env.sh configuration:  # Increase DT Gateway memory to 2GB\nDT_GATEWAY_HEAP_MEM=2048m  Environment variables available for configuration   DT_GATEWAY_HEAP_MEM    Maximum heap size allocated to DT Gateway service.  Default is 1024m.  DT_GATEWAY_DEBUG    Set to 1 to enable additional debug information in the dtgateway.log  DT_CLASSPATH    Classpath used to load additional jars or properties for dtcli and dtgateway  DT_LOG_DIR    Directory for log files  DT_RUN_DIR    Directory for process id and other temporary files\ncreated at run time", 
            "title": "(install dir)/conf/custom-env.sh"
        }, 
        {
            "location": "/configuration/#user-homedtdt-sitexml", 
            "text": "This file is used to customize the DataTorrent platform and the behavior of\napplications. \u00a0It can be particularly useful for changing\nGateway application connection address, or configuring environment specific\nsettings, such as specific machine names, IP addresses, or performance\nsettings which may change from environment to environment.  Example of a single property configuration in dt-site.xml:  configuration \n   property \n       name dt.operator.MyCustomStore.host /name \n       value 192.168.2.35 /value \n   /property \n   \u2026 /configuration", 
            "title": "(user home)/.dt/dt-site.xml"
        }, 
        {
            "location": "/configuration/#gateway-configuration-properties", 
            "text": "dt.gateway.listenAddress  - The address and port DT Gateway listens to.  Defaults to 0.0.0.0:9090  dt.gateway.autoPublishInterval  - The interval in milliseconds DT Gateway should publish application information on the websocket channel.  Default is 1000.  dt.gateway.sslKeystorePath  - Specifying of the SSL Key store path enables HTTPS on the DT Gateway (See the  dtGateway Security  document)  dt.gateway.sslKeystorePassword  - The password of the SSL key store (See the  dtGateway Security  document)  dt.gateway.allowCrossOrigin  - Setting it to true allows cross origin HTTP access to the DT Gateway.  Default is false.  dt.gateway.authentication.(OPTION)  - Determines the scheme of Hadoop security authentication (See the  dtGateway Security  document).  dt.gateway.http.authentication  - Determines the scheme of DT Gateway HTTP security authentication (See the  dtGateway Security  document).  dt.gateway.staticResourceDirectory  - The document root directory where the DT Gateway should serve from for the /static HTTP path.", 
            "title": "Gateway Configuration Properties"
        }, 
        {
            "location": "/configuration/#application-configuration-properties", 
            "text": "For a complete list of configurable application properties see the Attributes\u00a0section below.", 
            "title": "Application Configuration Properties"
        }, 
        {
            "location": "/configuration/#resources-management-and-performance-tuning", 
            "text": "The platform provides continuous information about CPU, Memory, and Network\nusage for the system as a whole, individual running applications,\noperators, streams, and various internal components.  These statistics\nare available via  REST API ,  dtCli , and  dtManage .  The platform is also responsible for   Honoring the resource restrictions enforced by the YARN RM and taking\n    preventive action to ensure they are met. This is done at both launch time\n    (fit the execution plan to the number of containers and their\n    sizes), as well as at run time.  Honoring resource constraints an application developer\n    may provide such as the amount of memory allocated to individual operators,\n    associated buffer servers, or the number of partitions.   STRAM works with the YARN RM on a continual basis to ensure that resource\nconstraints are met. As a multi-tenant application, it is crucial to be able to\nperform within given resource limits. The design of the platform enables\neffective management of all three types of resources (CPU, Memory, I/O).", 
            "title": "Resources Management and Performance Tuning"
        }, 
        {
            "location": "/configuration/#cpu", 
            "text": "CPU utilization is computed on a per-thread basis within a container by the\nStreamingContainer; this value is also, in effect, the per-operator value\nsince each operator is a single threaded application. CPU utilization is also\ncomputed for the buffer-server as well as other common tasks within a container.", 
            "title": "CPU"
        }, 
        {
            "location": "/configuration/#network", 
            "text": "Network usage management is needed to ensure that desired latency and\nthroughput levels are achieved and any applicable SLA terms are met.  The platform provides real-time statistics on the number of bytes or tuples\nprocessed by each operator. Application developers can modulate network traffic\nusing a couple of mechanisms:\n- Adjust the locality of streams: Using THREAD_LOCAL or CONTAINER_LOCAL\n  can reduce network load substantially as discussed below.\n- Adjust the number of partitions and unifiers.", 
            "title": "Network"
        }, 
        {
            "location": "/configuration/#ram", 
            "text": "STRAM keeps track of resource usage on per container basis. Appropriate\nattributes can be set to limit the amount of RAM on a per-operator or\nper-container basis.", 
            "title": "RAM"
        }, 
        {
            "location": "/configuration/#spike-management", 
            "text": "Streaming applications do not have the same throughput (events/second) for\nall 24 hours of the day; occasional spikes in the incoming data rate are common.\nMost streaming applications resolve this dichotomy\nby providing resources for the peak. So, resource utilization is\nsuboptimal for most of the day because resources, though unused, are locked up\nand therefore unusable by other applications in a multi-tenant environment.  The platform provides mechanisms to manage the spikes by adding partitions\nduring peak, and removing them once the spike subsides.", 
            "title": "Spike Management"
        }, 
        {
            "location": "/configuration/#partitioning", 
            "text": "Partitioning is a core mechanism to distribute computation (and the associated\nresource utilization) across the cluster. It is discussed, along with the\nrelated concept of unifiers, in greater detail in Application development  and  Operator Development .", 
            "title": "Partitioning"
        }, 
        {
            "location": "/configuration/#stream-modes", 
            "text": "The platform support 5 stream modes, namely THREAD_LOCAL\n(intra-thread), CONTAINER_LOCAL (intra process/jvm), NODE_LOCAL (intra\nnode), RACK_LOCAL (same rack), and unspecified. While designing an application,\nthe modes should be decided carefully. All\nstream-modes are hints to the STRAM, and hence could be ignored if\nresources are not available, and could be changed on a run-time basis.\nThere are pros and cons of each.   THREAD_LOCAL : All the operators of the ports on this stream\n    share the same thread. Tuples are thus passed via the thread call stack.\n    The performance is massive and go into 100s of millions\n    of tuples/sec. Do note that if the operations are compute intensive,\n    them THREAD_LOCAL may not perform better than CONTAINER_LOCAL.\n    Thread call stack is extremely efficient in terms of I/O (there is\n    no I/O here), but the same thread does both the upstream and\n    downstream computation. A limitation is that all downstream\n    operators on this stream must have only one input port connected. If\n    the JVM process is lost, all the operators are lost, and will be\n    restarted again in another container.  CONTAINER_LOCAL : All the operators of the ports on this\n    stream are in the same process. Each denotes a separate thread, and\n    tuples are passed in mmemory via a connectionBuffer as the intra-process\n    communication mechanism.\n    This mode has very high throughput and can easily do more than\n    million tuples/sec. However, since there is no bufferserver, features that\n    it provides (spooling, presistence) are not available, so memory needs\n    may grow.\n    This mode relies on the downstream operators consuming tuples, on average,\n    at least as fast as they are emitted by the upstream operator. As with the\n    previous mode, if the JVM process is\n    lost, all the operators are lost, and will be restarted again in\n    another container.  NODE_LOCAL : All operators on this stream are on the\n    same node. Inter-process communication via the local loopback interface is\n    used. This mode is also very fast, as data does not traverse the NIC\n    but it has a buffer-server and so all the features that the buffer-server\n    provides (spooling, persistence) are available. If one container dies, all\n    the operators in that container will obviously need to be recreated and\n    restarted, but other operators remain unaffected. However if\n    a node dies then all of its containers and the operators hosted by them\n    need to be restarted.  RACK_LOCAL : All operators on this stream are in the\n    same rack. Communication is not as fast as the previous modes since data\n    needs to pass through the NIC. Like the previous mode, it has a\n    buffer-server and so all the features that buffer-server provides are\n    available. Use of this mode reduces the probability of multi-operator\n    outage since multiple operators are not constrained to run on the same\n    node. This mode however will be affected by outage of a switch.  Unspecified: This is the default mode and STRAM makes no special effort\n    to achieve any particular locality. There are thus no guarantees on whether\n    a stream will cross rack, node, or process boundaries.", 
            "title": "Stream Modes"
        }, 
        {
            "location": "/configuration/#multi-tenancy-and-security", 
            "text": "The platform is a YARN native application, and so all security features\navailable in Hadoop also apply for securing Apache Apex applications. \nThe default security for the streaming application is Kerberos based.", 
            "title": "Multi-Tenancy and Security"
        }, 
        {
            "location": "/configuration/#kerberos-authentication", 
            "text": "Kerberos is a ticket based authentication system that provides\nauthentication in a distributed environment where authentication between\nmultiple users, hosts and services is needed. It is the de-facto\nauthentication mechanism supported in Hadoop. To use Kerberos\nauthentication, the Hadoop installation must first be configured for\nsecure mode with Kerberos. Please refer to the administration guide of\nyour Hadoop distribution on how to do that. Once Hadoop is running with\nKerberos security enabled, DataTorrent platform also needs to be\nconfigured for Kerberos. There are two parts of the platform that need\nto be configured, CLI (Command Line Interface) and DT Gateway.", 
            "title": "Kerberos Authentication"
        }, 
        {
            "location": "/configuration/#hadoop-configuration", 
            "text": "The DataTorrent application uses delegation tokens to communicate\nwith the ResourceManager (YARN) and NameNode (HDFS) and these tokens are\nissued by those servers respectively. Since the application is long-running,\nthe tokens should be valid for the lifetime of the application.\nHadoop has a configuration setting for the maximum lifetime of the\ntokens and they should be set to cover the lifetime of the application.\nThere are separate settings for ResourceManager and NameNode delegation\ntokens.  The ResourceManager delegation token max lifetime is specified in yarn-site.xml  and can be specified as follows for example for a lifetime\nof 1 year  property \n   name yarn.resourcemanager.delegation.token.max-lifetime /name \n   value 31536000000 /value  /property   The NameNode delegation token max lifetime is specified in\nhdfs-site.xml and can be specified as follows for example for a lifetime\nof 1 year  property \n    name dfs.namenode.delegation.token.max-lifetime /name \n    value 31536000000 /value \n  /property   Also, DataTorrent RTS needs the DT Gateway user (default  dtadmin ) to be a\nproxy user in the Hadoop configuration when running in\nsecure mode. \u00a0This will enable the DT Gateway to launch applications on\nbehalf of the logged-in user. \u00a0Please add the following in  core-site.xml \nand replace [username] with the user running the DT Gateway:  property \n   name hadoop.proxyuser.[username].groups /name \n   value * /value  /property  property \n   name hadoop.proxyuser.[username].hosts /name \n   value * /value  /property", 
            "title": "Hadoop Configuration"
        }, 
        {
            "location": "/configuration/#cli-configuration", 
            "text": "The DataTorrent command line interface is used to launch\napplications along with performing various other operations on\napplications. \u00a0When Kerberos security is enabled in Hadoop, a Kerberos\nticket granting ticket or the Kerberos credentials of the user is needed\nby the CLI program  dtcli  to authenticate with Hadoop for any\noperation. Kerberos credentials are composed of a principal and either a keytab  or a password. For security and operational reasons only keytabs\nare supported in Hadoop and by extension in DataTorrent platform. When\nuser credentials are specified, all operations including launching\napplication are performed as that user.", 
            "title": "CLI Configuration"
        }, 
        {
            "location": "/configuration/#using-kinit", 
            "text": "A Keberos ticket granting ticket (TGT) can be obtained by using the\nKerberos command  kinit . Detailed documentation for the command can be\nfound online or in man pages. An sample usage of this command is  kinit -k -t path-tokeytab-file kerberos-principal  If this command is successful, the TGT is obtained, cached and available for\nother programs. The CLI program  dtcli  can then be started to launch\napplications and perform other operations.", 
            "title": "Using kinit"
        }, 
        {
            "location": "/configuration/#using-kerberos-credentials", 
            "text": "The CLI program  dtcli  can also use the Kerberos credentials directly without\nrequiring a TGT to be obtained separately. This can be useful in batch mode\nwhere  dtcli  is not launched manually and also in scenarios where running a\nseparate program like  kinit  is not feasible.  The credentials can be specified in the  dt-site.xml  configuration\nfile. If only a single user is launching applications, the global dt-site.xml  configuration file in the installation folder can be used.\nIn a multi-user environment the user can use the  dt-site.xml  file in his\nhome directory. The location of this file will be  $HOME/.dt/dt-site.xml \nIf this file does not exist, the user can create a new one.  The snippet below shows the how the credentials are specified in\nthe configuration file as properties.  property \n         name dt.authentication.principal /name \n         value kerberos-principal-of-user /value  /property  property \n         name dt.authentication.keytab /name \n         value absolute-path-to-keytab-file /value  /property   The property  dt.authentication.principal  specifies the Kerberos\nuser principal and  dt.authentication.keytab  specifies the absolute path\nto the keytab file for the user.", 
            "title": "Using Kerberos credentials"
        }, 
        {
            "location": "/configuration/#dt-gateway-configuration", 
            "text": "DT Gateway is a service that provides the backend functionality\nfor the DataTorrent UI console. Refer to  dtManage \u00a0for\ndetails on the UI console. The DT Gateway provides real-time information\nabout running applications, allows changes to applications, launching\nnew applications among various other operations. In Kerberos secure\nmode, Kerberos credentials are required for DT Gateway to operate. The\ncredentials should match the user that the DT Gateway service is running\nas.  In a multi-user installation DT Gateway is typically running as\nuser  dtadmin  and the Kerberos credentials specified should be for this\nuser. They are specified in the  dt-site.xml  configuration file in the\ninstallation folder. For a single user installation where gateway is\nrunning as the user, the Kerberos credentials will be the user\u2019s and\nthey will be specified in the  dt-site.xml  in the home directory location $HOME/.dt/dt-site.xml .  The snippet below shows how the credentials are specified in the\nconfiguration file as properties.  property \n         name dt.gateway.authentication.principal /name \n         value kerberos-principal-of-gateway-user /value  /property  property \n         name dt.gateway.authentication.keytab /name \n         value absolute-path-to-keytab-file /value  /property", 
            "title": "DT Gateway Configuration"
        }, 
        {
            "location": "/configuration/#dtmanage-authentication", 
            "text": "Access to the UI console can be protected by having users authenticate before they can access the contents of the console.\nDifferent authentication mechanisms are supported including local password,\nKerberos and LDAP. Please refer to  Gateway Security \nfor details of how to configure this.", 
            "title": "dtManage Authentication"
        }, 
        {
            "location": "/configuration/#run-time-management", 
            "text": "Unlike Map-Reduce, streaming applications never end. They are designed\nto run 24x7, processing a continuous stream of input data. This makes run-time\nmanagement of the applications very critical. The platform provides strong\nsupport for various operations. These include   Runtime metrics and stats on various components of the\n    application including aggregated metrics  Ability to change the logical plan, physical plan, and\n    execution plan of the application  Ability to dump the current state of the application to enable\n    re-launch (in case of Hadoop grid outage)  Integration of STRAM state with Zookeeper (in later versions)  Debugger, charting, and other tools triggered at run time", 
            "title": "Run-Time Management"
        }, 
        {
            "location": "/configuration/#dynamic-functional-modifications", 
            "text": "Platform supports changes to an application at multiple stages.\nApplication design parameters (attributes and properties) can be\nchanged at launch time via the job configuration file and during runtime via\nthe  dtcli  tool or using the REST webservice calls. Support for runtime\nchanges is critical for operability as it enables changes to a running\napplication without being forced to kill it. This is a critical need for\nstreaming applications and a significant difference from map-reduce/batch\napplications.  From an operational perspective, the platform will allow changes\nin both the logical plan (query modification, or properties\nmodifications) and the physical plan (attribute modification generally and\npartitioning changes specifically).  Examples of dynamic changes to logical plan include\n-   Changing properties of an operator\n-   Adding or deleting a sub-dag. Some examples are\n  -   Change in persistence\n  -   Insertion of charts, debugger etc.\n  -   Query insertion on a particular stream  Any change to a logical plan will change the physical and the\nexecution plan. Examples of dynamic changes only to physical plan\ninclude   Change in attributes that triggers STRAM to change the number\n    of physical operators.  Runtime changes in load or grid resources (via RM, or\n    outages), that triggers STRAM to change the physical plan to meet\n    SLA, latency, resources usage goals  Direct request to change the number of partitions of\n    an operator. For example new input adapters to handle expected\n    uptick in ingestion rate   Any change to a physical plan will change the execution plan.\nExamples of dynamic changes only to the execution plan include   Changes in attributes that need a new execution plan  Changes to stream modes  Node recovery from an outage", 
            "title": "Dynamic Functional Modifications"
        }, 
        {
            "location": "/configuration/#runtime-code", 
            "text": "STRAM is able to reuse and move any code that is supplied with the\ninitial launch of the application. The default behavior is for the jar\nto include all the library templates in addition to the application\ncode. This enables STRAM to make changes to the application as the code\nis already available.", 
            "title": "Runtime Code"
        }, 
        {
            "location": "/configuration/#load", 
            "text": "STRAM manages runtime changes to ensure that the application\nscales up and down. This includes changes in load, changes in skew,\nchanges in resource behavior (network goes slow) etc. STRAM proactively\nmonitors the application and will make run time changes as\nneeded.", 
            "title": "Load"
        }, 
        {
            "location": "/configuration/#uptime", 
            "text": "Node recovery is a change in the execution plan caused by external\nevents (node outage) or RM taking resources back (pre-emption). The\nplatform enables node recovery in three modes, namely, at least once, at\nmost once, and exactly once. SLA enforcement in terms of latency,\nuptime, etc. is done via runtime changes.", 
            "title": "Uptime"
        }, 
        {
            "location": "/configuration/#attributes", 
            "text": "Attribute specification is the process by which operational\ncustomization is achieved. Currently, modification of attributes on an\napplication that is already running is not supported. We intend to add\nthis in future versions on an attribute by attribute basis. Attributes\nare parameters that the platform recognizes and acts on and are not part\nof user code. Attributes are the mechanism by which platform features\ncan be customized. They are specified with a key and a value. In this\nsection we list the attributes, their default values, and briefly\nexplain what they do.  There are three kinds of attributes   Application attributes  Operator attributes  Port attributes   For implementation details look at the  javadocs . Some very common attributes are   Application: Application window, Application name, Checkpoint\n    window, Container JVM options, container memory, containers, max\n    count, heartbeat interval, STRAM memory, launch mode, tuple\n    recording, etc. \u00a0See  Context.DAGContext  Operators: Initial partitions, checkpoint window, locality\n    host, locality rack, memory, recovery attempts, stateless, storage\n    agent, etc. See  Context.OperatorContext  Ports: Queue capacity, auto_record, partition parallel, etc.\n    See  Context.PortContext   These attributes are available via the  Context  class and can be\naccessed in the  setup  call of an operator.", 
            "title": "Attributes"
        }, 
        {
            "location": "/configuration/#application-configuration", 
            "text": "Starting from RTS release 2.0.0, applications are configured through Application Packages. \u00a0Please refer to the Application Packages \u00a0for details.", 
            "title": "Application Configuration"
        }, 
        {
            "location": "/configuration/#adjusting-logging-levels", 
            "text": "", 
            "title": "Adjusting Logging Levels"
        }, 
        {
            "location": "/configuration/#application-logging", 
            "text": "Logging levels for specific classes or groups of classes can be raised or\nlowered at runtime from  dtManage  application view with the Set Log Levels  button. \u00a0Explicit class paths or patterns like org.apache.hadoop.* \u00a0or  com.datatorrent.* \u00a0can be used to adjust logging to\nvalid  log4j  levels such as DEBUG or INFO.\u00a0This produces immediate change in\nthe logs, but does not persist across application restarts.  For permanent changes to logging levels, lines similar to these can be inserted\ninto  dt-site.xml . To specify multiple patterns, use a comma-separated list.  property \n   name dt.loggers.level /name \n   value com.datatorrent.*:DEBUG,org.apache.*:INFO /value  /property   Full DEBUG logging can be enabled by adding these lines:  property \n   name dt.attr.DEBUG /name \n   value true /value  /property", 
            "title": "Application Logging"
        }, 
        {
            "location": "/configuration/#custom-log4j-properties-for-application-packages", 
            "text": "There are two ways of setting custom  log4j  properties in an Apex application\npackage    At the Application level. This will ensure that the custom log4j properties\n   are used for all containers including Application Master. An example:  property \n   name dt.attr.CONTAINER_JVM_OPTIONS /name \n   value -Dlog4j.configuration=custom_log4j.properties /value  /property     At an individual operator level. This sets the custom log4j properties only\n   on the container that is hosting the operator.  An example:  property \n   name dt.operator. OPERATOR_NAME .attr.JVM_OPTIONS /name \n   value  -Dlog4j.configuration=custom_log4j.properties /value  /property     Make sure that the file  custom_log4j.properties  is part of your application\njar and is located under  src/main/resources .  Some examples of custom log4j\nproperties files follow.    Writing to a file  log4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}    Writing to Console  log4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.ConsoleAppender\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout", 
            "title": "Custom log4j Properties for Application Packages"
        }, 
        {
            "location": "/configuration/#dtgateway-logging", 
            "text": "DT Gateway log level can be changed to DEBUG by settings following\nenvironment variable before launching DT Gateway (as of version\n2.0).  export DT_GATEWAY_DEBUG=1", 
            "title": "dtGateway Logging"
        }, 
        {
            "location": "/configuration/#hadoop-tuning", 
            "text": "", 
            "title": "Hadoop Tuning"
        }, 
        {
            "location": "/configuration/#yarn-vmem-pmem-ratio-tuning", 
            "text": "After performing a new installation, sometimes the following\nmessage is displayed while launching an application:  Application application_1408120377110_0002 failed 2\ntimes due to AM Container for appattempt_1408120377110_0002_000002\nexited with exitCode: 143 due to:\nContainer\\[pid=27163,containerID=container_1408120377110_0002_02_000001\\]\nis running beyond virtual memory limits. Current usage: 308.1 MB of 1 GB\nphysical memory used; 2.5 GB of 2.1 GB virtual memory used. Killing\ncontainer.\n\nDump of the process-tree for container_1408120377110_0002_02_000001 :\n\n\nPID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n\n27208 27163 27163 27163 (java) 604 19 2557546496 78566\n/usr/java/default/bin/java\n-agentlib:jdwp=transport=dt_socket,server=y,suspend=n -Xmx768m\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/dt-heap-2.bin\n-Dhadoop.root.logger=DEBUG,RFA\n-Dhadoop.log.dir=/disk2/phd/dn/yarn/userlogs/application_1408120377110_0002/container_1408120377110_0002_02_000001  To fix this  yarn.nodemanager.vmem-pmem-ratio  in  yarn-site.xml  should be\nincreased from 2 to 5 or higher. \u00a0Here is an example setting:  property \n    description Ratio between virtual memory to physical memory when\n     setting memory limits for containers. Container allocations are\n     expressed in terms of physical memory, and virtual memory usage\n     is allowed to exceed this allocation by this ratio.\n    /description \n    name yarn.nodemanager.vmem-pmem-ratio /name \n    value 10 /value \n  /property", 
            "title": "YARN vmem-pmem ratio tuning"
        }, 
        {
            "location": "/dtgateway_security/", 
            "text": "DataTorrent Gateway Security\n\n\nDataTorrent Gateway supports different authentication mechanisms to\nsecure access to the Console. The supported types are local password\nauthentication, kerberos authentication and J\n\n\nConfiguring Authentication\n\n\nAfter DataTorrent RTS installation, you can turn on authentication and\nauthorization support to secure the DataTorrent Gateway. Gateway\nsupports three types of authentication.\n\n\n\n\nPassword\n\n\nKerberos\n\n\nJAAS\n\n\n\n\nEnabling Password Auth\n\n\n\u201cPassword\u201d is the only authentication mechanism presented here that does\nnot depend on any external systems.  When enabled, all users will be\npresented with the login prompt before being able to use the DT Console.\nPassword authentication can be enabled by performing following two\nsteps.\n\n\n\n\n\n\nAdd a property to \ndt-site.xml\n configuration file, typically located\n    under \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n...\n    \nproperty\n\n    \nname\ndt.gateway.http.authentication.type\n/name\n\n    \nvalue\npassword\n/value\n\n    \n/property\n\n...\n\n/configuration\n\n\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n\n\n( when running Gateway in local mode use  dtgateway restart command)\n\n\n\n\n\n\nOpen the Gateway URL in your browser, and you should be prompted for\nuser name and password.  Starting with DataTorrent RTS 2.0.0, the\ndefault username and password is \ndtadmin\n and \ndtadmin\n.\n\n\n\n\nOnce authenticated, active user name and an option to log out is\npresented in the top right corner of the DT Console screen.\n\n\n\n\nEnabling Kerberos Auth\n\n\nKerberos authentication can optionally be enabled for Hadoop web access.\nIf this is configured then all web browser access to the Hadoop\nmanagement consoles is Kerberos authenticated. Access to all the web\nservices is also Kerberos authenticated. This Kerberos authentication is\nperformed using a protocol called SPNEGO which is Kerberos over HTTP.\nPlease refer to the administration guide of your Hadoop distribution on\nhow to enable this. The web browsers must also support SPNEGO, most\nmodern browsers do and should be configured to use SPNEGO for the\nspecific URLs being accessed. Please refer to the documentation of the\nindividual browsers on how to configure this. Gateway handles the SPNEGO\nauthentication needed when communicating with the Hadoop web-services if\nKerberos authentication is enabled for Hadoop web access.\n\n\nKerberos authentication can be enabled for UI Console as well. When it\nis enabled access to the Gateway web-services is Kerberos authenticated.\nThe browser should be configured for SPNEGO authentication when\naccessing the Console. A user typically obtains a master ticket first by\nlogging in to kerberos system in a terminal emulator using kinit. Then\nthe user launches a browser and accesses the Console URL. The browser\nwill use the master ticket obtained by kinit earlier to obtain the\nnecessary security tokens to authenticate with the Gateway.\n\n\nWhen this authentication is enabled the first authenticated user that\naccesses the system is assigned the admin role as there are no other\nusers in the system at this point. Any subsequent authenticated user\nthat access the system starts with no roles. The admin user can then\nassign roles to these users. This behavior can be changed by configuring\nan external role mapping. Please refer to the \nExternal Role Mapping\n in the \nAuthorization using external roles\n section below for that.\n\n\nAdditional configuration is needed to enable Kerberos authentication for\nthe Console. A separate set of kerberos credentials are needed. These\ncan be same as the Hadoop web-service Kerberos credentials which are\ntypically identified with the principal HTTP/HOST@DOMAIN. A few other\nconfiguration properties are also needed. These can be specified in the\nsame \u201cdt-site.xml\u201d configuration file as the DT Gateway authentication\nconfiguration described in the Operation and Installation Guide. This\nauthentication can be set up using the following steps.\n\n\n\n\n\n\nAdd the following properties to \ndt-site.xml\n configuration file, typically located under \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install)\n\n\nconfiguration\n\n...\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.type\n/name\n\n    \nvalue\nkerberos\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.kerberos.principal\n/name\n\n    \nvalue\n{kerberos-principal-of-web-service}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.kerberos.keytab\n/name\n\n    \nvalue\n{absolute-path-to-keytab-file}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.token.validity\n/name\n\n    \nvalue\n{authentication-token-validity-in-seconds}\n/value\n\n  \n/property\n\n  \nproperty\n\n  \nname\\\ndt.gateway.http.authentication.cookie.domain\n/name\n\n  \nvalue\\\n{http-cookie-domain-for-authentication-token}\n/value\n\n  \nproperty\n\n    \nname\\\ndt.gateway.http.authentication.cookie.path\n/name\n\n    \nvalue\n{http-cookie-path}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\\\ndt.gateway.http.authentication.signature.secret\n/name\n\n    \nvalue\n{absolute-path-of-secret-file-for-signing-authentication-tokens} \n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\nNote that the kerberos principal for web service should begin with\nHTTP/\u2026  All the values for the properties above except for the property\n\ndt.gateway.http.authentication.type\n should be replaced with the\nappropriate values for your setup.\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n( when running Gateway in local mode use  \ndtgateway restart\n command)\n\n\n\n\n\n\nEnabling JAAS Auth\n\n\nJAAS is Java Authentication and Authorization Service. It is a pluggable\nand extensible mechanism for authentication. It is a generic framework\nand the actual authentication is performed by a JAAS plugin module which\ncan be configured using a configuration file. Among others LDAP and PAM\nauthentication can be performed via JAAS.\n\n\nSimilar to Kerberos when this authentication is enabled the first\nauthenticated user that accesses the system is assigned the admin role\nas there are no other users in the system at this point. Any subsequent\nauthenticated user that access the system starts with no roles. The\nadmin user can then assign roles to these users. This behavior can be\nchanged by configuring an external role mapping. Please refer to the\n\nExternal Role Mapping\n in the \nAuthorization using external roles\n section below for that.\n\n\nThis authentication can be enabled by performing the following general\nsteps. The specific steps for a couple of authentication mechanisms LDAP\nand PAM are shown in the next sections but other authentication\nmechanisms that have a JAAS plugin module can also be used.\n\n\n\n\n\n\nAdd a property to \ndt-site.xml\n configuration file, typically located\n    under \n/opt/datatorrent/current/conf\n ( or\n    \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n...\n  \nproperty\n\n      \nname\ndt.gateway.http.authentication.type\n/name\n\n      \nvalue\njaas\n/value\n\n  \n/property\n\n  \nproperty\\\n\n      \nname\ndt.gateway.http.authentication.jaas.name\n/name\n\n      \nvalue\nname-of-jaas-module\n/value\n\n  \n/property\n\n...\n\n/configuration\n\n\n\n\nThe \ndt.gateway.http.authentication.jaas.name\n property specifies the\nplugin module to use with JAAS and the value should be the name of the\nplugin module.\n\n\n\n\n\n\nThe name of the plugin module specified above should be configured\n    with the appropriate settings for the plugin. This is module\n    specific configuration. This can be done in a file named\n    .java.login.config in the home directory of the user Gateway is\n    running under. If DataTorrent RTS was installed as a superuser,\n    Gateway would typically run as dtadmin and this file path would\n    typically be \n/home/dtadmin/.java.login.config\n, if running as a\n    normal user it would be \n~/.java.login.config\n. The sample\n    configurations for LDAP and PAM are shown in the next sections.\n\n\n\n\n\n\nThe classes for the plugin module may need to be made available to\n    Gateway if they are not available in the default classpath. This can\n    be done by specifying the jars containing these classes in a\n    classpath variable that is used by Gateway.\n\n\nThe following step shows how to do this\n\n\na.  Edit the \ncustom-env.sh\n configuration file, typically located under\n    \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for\n    local install) and append the list of jars obtained above to the\n    DT_CLASSPATH variable. This needs to be added at the end of the\n    file in the section for specifying local overrides to environment\n    variables. The line would look like\n\n\nexport DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/jar1:path/to/jar2:..\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n( when running Gateway in local mode use  \ndtgateway restart\n command)\n\n\n\n\n\n\nLDAP\n\n\nLDAP authentication is a directory based authentication mechanism used\nin many enterprises. To enable LDAP authentication following are the\nspecifics for the configuration steps described above.\n\n\n\n\n\n\nFor step 1 above specify LDAP as the authentication module to use\n    with JAAS by first specifying the value of the\n    dt.gateway.http.authentication.jaas.name property above to be \u201cldap\u201d\n    (without the quotes). This name should now be configured with the\n    appropriate settings as described in the next step.\n\n\n\n\n\n\nFor step 2, the JAAS name specified above should be configured with\n    the appropriate LDAP settings in the .java.login.config file. A\n    sample configuration is shown below\n\n\nldap {\n  com.sun.security.auth.module.LdapLoginModule required\n  userProvider=\"ldap://ldap-server-hostname\"\n  authIdentity=\"uid={USERNAME},ou=users,dc=domain,dc=com\";\n};\n\n\n\n\n\n\n\nNote that the first string before the open brace, in this case\n\u201cldap\u201d must match the jaas name specified in step 1. The first property\nwithin the braces \ncom.sun.security.auth.module.LdapLoginModule\n specifies\nthe actual JAAS plugin implementation class providing the LDAP\nauthentication. The next settings are LDAP settings specific to your\norganization and could be different from ones shown above. The above\nsettings are only provided as a reference example.\n\n\nPAM\n\n\nPAM is Pluggable Authentication Module. It is a Linux system equivalent\nof JAAS where applications call the generic PAM interface and the actual\nauthentication implementations called PAM modules are specified using\nconfiguration files. PAM is the login authentication mechanism in Linux\nso it can be used for example to authenticate and use local Linux user\naccounts in Gateway. If organizations have configured other PAM modules\nthey can be used in Gateway as well.\n\n\nPAM is implemented in C language and has C API. JPam is Java PAM bridge\nthat uses JNI to make PAM calls. It is available here\n\nhttp://jpam.sourceforge.net/\n and has detailed documentation on how to install and set it up. JPam also has a JAAS plugin module and hence can be used in Gateway via JAAS. Note that any other PAM implementation can be used as long as a JAAS plugin module is available.\n\n\nTo enable JPAM following are the specifics for the configuration steps\nto enable JAAS authentication described above.\n\n\n\n\n\n\nJPAM has to be first installed on the system. Please following the\n    installation instructions from the JPAM website.\n\n\n\n\n\n\nFor step 1 above Specify JPAM as the authentication module to use\n    with JAAS by first specifying the value of the\n    dt.gateway.http.authentication.jaas.name property above to be\n    \u201cnet-sf-jpam\u201d (without the quotes). This name should now be\n    configured with the appropriate settings as described in the next\n    step.\n\n\n\n\n\n\nFor step 2 the JAAS name specified above should be configured with\n    the appropriate JPAM settings in the .java.login.config file. A\n    sample configuration is shown below\n\n\nnet-sf-jpam {\n   net.sf.jpam.jaas.JpamLoginModule required serviceName=\"net-sf-jpam\";\n};\n\n\n\n\n\n\n\nNote that the first string before the open brace, in this case\n\u201cnet-sf-jpam\u201d must match the jaas name specified in step 1. The first\nproperty within the braces net.sf.jpam.jaas.JpamLoginModule specifies\nthe actual JAAS plugin implementation class providing the JPAM\nauthentication. The next settings are JPAM specific settings. The\nserviceName setting for example specifies the PAM service which would\nneed to be further configured in /etc/pam.d/net-sf-jpam to specify the\nPAM modules to use. Refer to PAM documentation on how to configure a PAM\nservice with PAM modules. If using Linux local accounts system-auth\ncould be specified as the PAM module in this file. The above settings\nare only provided as a reference example and a different serviceName for\nexample can be chosen.\n\n\n\n\nFor step 3 add the JPam jar to the DT_CLASSPATH variable. The JPam\n    jar should be available in the JPam installation package and\n    typically has the filename format \nJPam-\nversion\n.jar\n where\n   \nversion\n denotes the version, version 1.1 has been tested.\nexport DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/JPam-\\\nversion\\\n.jar\n\n\n\n\n\n\n\nGroups\n\n\nFor group support such as using LDAP groups for authorization refer to\nthe \nAuthorization using external roles\n section below.\n\n\nConfiguring Authorization\n\n\nWhen any authentication method is enabled, authorization will also be\nenabled.  DT Gateway uses roles and permissions for authorization.\n Permissions are possession of the authority to perform certain actions,\ne.g. to launch apps, or to add users.  Roles have a collection of\npermissions and individual users are assigned to one or more roles.\n What roles the user is in dictates what permissions the user has.\n\n\nPermissions\n\n\nThe list of all possible permissions in the DT Gateway is as follow:\n\n\n ACCESS_RM_PROXY\n\n\nAllow HTTP proxying requests to YARN\u2019s Resource Manager REST API\n\n\n EDIT_GLOBAL_CONFIG\n\n\nEdit global settings\n\n\n EDIT_OTHER_USERS_CONFIG\n\n\nEdit other users\u2019 settings\n\n\n LAUNCH_APPS\n\n\nLaunch Apps\n\n\nMANAGE_LICENSES\n\n\nManage DataTorrent RTS licenses\n\n\n MANAGE_OTHER_USERS_APPS\n\n\nManage (e.g. edit, kill, etc) applications launched by other users\n\n\n MANAGE_OTHER_USERS_APP_PACKAGES\n\n\nManage App Packages uploaded by other users  \n\n\n MANAGE_ROLES\n\n\nManage roles (create/delete roles, or assign permissions to roles)\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nManage system alerts\n\n\n MANAGE_USERS\n\n\nManage users (create/delete users, change password)\n\n\n UPLOAD_APP_PACKAGES\n\n\nUpload App Packages and use the app builder\n\n\n VIEW_GLOBAL_CONFIG\n\n\nView global settings   \n\n\n VIEW_LICENSES\n\n\nView DataTorrent RTS licenses\n\n\n VIEW_OTHER_USERS_APPS\n\n\nView applications launched by others\n\n\n VIEW_OTHER_USERS_APP_PACKAGES\n\n\nView App Packages uploaded by other users\n\n\n VIEW_OTHER_USERS_CONFIG\n\n\nEdit other users\u2019 settings\n\n\n VIEW_SYSTEM_ALERTS\n\n\nView system alerts\n\n\nDefault Roles\n\n\nDataTorrent RTS ships with three roles by default. The permissions for\nthese roles have been set accordingly but can be customized if needed.\n\n\nAdmin\n\n\nAn administrator of DataTorrent RTS is intended to be able to install,\nmanage \n modify DataTorrent RTS as well as all the applications running\non it. They have ALL the permissions.\n\n\nOperator\n\n\nOperators are intended to ensure DataTorrent RTS and the applications\nrunning on top of it are always up and running optimally. The default\npermissions assigned to Operators, enable them to effectively\ntroubleshoot the entire RTS system and the applications running on it.\nOperators are not allowed however to develop or launch new applications.\nOperators are also not allowed to make system configuration changes or\nmanage users.\n\n\nHere is the list of default permissions given to operators\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nVIEW_GLOBAL_CONFIG\n\n\nVIEW_LICENSES\n\n\nVIEW_OTHER_USERS_APPS\n\n\nVIEW_OTHER_USERS_APP_PACKAGES\n\n\nVIEW_SYSTEM_ALERTS\n\n\nNote that VIEW_OTHER_USERS_APPS and VIEW_OTHER_USERS_APP_PACKAGES\nare in the list.  This means all users in the \u201coperator\u201d role will have\nread access to all apps and all app packages in the system.  You can\nremove those permissions from the \u201coperator\u201d role using the Console if\nthis is not desirable.  \n\n\nDeveloper\n\n\nDevelopers need to have to ability to develop, manage and run\napplications on DataTorrent RTS. They are not allowed to change any\nsystem settings or manage licenses.\n\n\nHere is the list of default permissions given to developers\n\n\nLAUNCH_APPS\n\n\nUPLOAD_APP_PACKAGES\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nVIEW_GLOBAL_CONFIG\n\n\nVIEW_LICENSES\n\n\nVIEW_SYSTEM_ALERTS\n\n\nApp Permissions and App Package Permissions\n\n\nUsers can share their running application instances and their\napplication packages with certain roles or certain users on a\nper-instance or per-package basis.  Users can specify which users or\nwhile roles have read-only or read-write access.  In addition, users can\nset their own defaults so they does not have to make permission change\nevery time they launch an app or uploads an app package.\n\n\nThe default for app and app package sharing is that the \u201coperator\u201d role\nhas read-only permission.\n\n\nAs of RTS 2.0.0, the Console does not yet support managing App\nPermissions or App Package Permissions.  But one can manage App\nPermissions and App Package Permissions using the Gateway REST API with\nURI\u2019s /ws/v2/apps/{appid}/permissions and\n/ws/v2/appPackages/{user}/{name}/permissions respectively.  Please refer\nto the \nDT Gateway REST API document\n and \nhere\n for examples on how to use the REST API.\n\n\nViewing and Managing Auth in the Console\n\n\nViewing User Profile\n\n\nAfter you are logged in on the Console, click on the Configuration tab\non the left, and select \u201cUser Profile\u201d.  This gives you the information\nof the logged in user, including what roles the user is in, and what\npermissions the user has.\n\n\nAdministering Auth\n\n\n\n\nFrom the Configuration tab, click on \u201cAuth Management\u201d.  On this page,\nyou can perform the following tasks:\n\n\n\n\nCreate new users\n\n\nDelete users\n\n\nChange existing users\u2019 passwords\n\n\nAssign roles to users\n\n\nCreate roles\n\n\nAssign permissions to roles\n\n\n\n\n\n\nDataTorrent RTS installation comes with three preset roles (admin,\noperator, and developer).  You can edit the permissions for those roles\nexcept for admin.\n\n\n Authorization using external roles\n\n\nWhen using an external authentication mechanism such as Kerberos or\nJAAS, roles defined in these external systems can be used to control\nauthorization in DataTorrent RTS. There are two steps involved. First\nsupport for external roles has to be configured in Gateway. This is\ndescribed below in the sections \nKerberos roles\n and\n\nJAAS roles\n. Then a mapping should be specified between\nthe external roles and DataTorrent roles to specify which role should be\nused for a user when the user logs in. How to setup this mapping is\ndescribed in the \nExternal Role Mapping\n section below.\nWhen this mapping is setup only users with roles that have a mapping are\nallowed to login the rest are not. The next sections describe how to\nconfigure the system for handling external roles.\n\n\nKerberos roles \n\n\nWhen Kerberos authentication is used the role for the user is derived\nfrom the principal. If the principal is of the form \nuser/group@DOMAIN\n\nthe group portion is used as the external role and no additional\nconfiguration is necessary.\n\n\nJAAS roles \n\n\nTo use JAAS roles the system should be configured first to recognize\nthese roles. When a user is authenticated with JAAS a list of principals\nis returned for the user by the implementing JAAS authentication module.\nSome of these principals can be for roles and these role principals need\nto be identified from the list. Additional configuration is needed to do\nthis and this configuration is specific to the JAAS authentication\nmodule being used. Specifically the JAVA class name identifying the role\nprincipal needs to be specified. This can be specified using the\nproperty \n\u201cdt.gateway.http.authentication.jaas.role.class.name\u201d\n in the\nconfiguration file as shown below\n\n\nconfiguration\n\n...\n  \nproperty\n\n       \nname\ndt.gateway.http.authentication.jaas.role.class.name\n/name\n\n          \nvalue\nfull-class-name-of-role\n/value\n\n \n/property\n\n...\n\n/configuration\n\n\n\n\n\nCallback Handlers\n\n\nIn JAAS authentication, a login module may need a custom callback to be\nhandled by the caller. The callbacks are typically used to provide\nauthentication credentials to the login module. DataTorrent RTS supports\nspecification of a custom callback handler to handle these callbacks.\nThe custom callback handler can be specified using the property\n\u201cdt.gateway.http.authentication.jaas.callback.class.name\u201d. When this\nproperty is not specified a default callback handler is used but it may\nnot be sufficient for all login modules like in the LDAP case described\nbelow. The property can be specified as follows\n\n\nconfiguration\n\n...\n\nproperty\n                                                                            \nname\ndt.gateway.http.authentication.jaas.callback.class.name\n/name\n\n\nvalue\nfull-class-name-of-callback\n/value\n\n\n/property\n\n...\n\n/configuration\n\n\n\n\n\nCustom callback handlers can be implemented by extending the default\ncallback handler so they can build upon it or they can be built from\nscratch. The LDAP callback handler described below also extends the\ndefault callback handler. The source for the default callback handler\ncan be found here \nDefaultCallbackHandler\n can be used as a reference when\nimplementing new callback handlers.\n\n\nLDAP\n\n\nWhen using LDAP with JAAS, to use LDAP roles, a  LDAP login module\nsupporting roles should be used. Any LDAP module that supports roles can\nbe used. Jetty implements one such login module. The steps to configure\nthis module are as follows.\n\n\n\n\n\n\nThe Gateway service in DataTorrent RTS 2.0 is compatible with Jetty 8. The class name identifying the \n    role principal is \n\u201corg.eclipse.jetty.plus.jaas.JAASRole\u201d\n. When using the Jetty LDAP\n    login module a custom JAAS callback has to be handled by the caller.\n    This has been implemented by DataTorrent in a callback handler. The\n    class name for the callback handler implementation should be\n    specified as a property. The property name is\n    \n\u201cdt.gateway.http.authentication.jaas.callback.class.name\u201d\n. The class\n    name of the callback handler is\n    \n\u201ccom.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\u201d\n.\n    This property should be specified along with the class name\n    identifying the role principal for the Jetty login module as\n    described in the section above. The configuration file with all the\n    JAAS properties including these looks as follows\n\n\nconfiguration\n\n...\n  \nproperty\n\n          \nname\ndt.gateway.http.authentication.type\n/name\n\n          \nvalue\njaas\n/value\n\n  \n/property\n\n  \nproperty\n\n         \nname\ndt.gateway.http.authentication.jaas.name\n/name\n\n         \nvalue\nldap\n/value\n\n  \n/property\n\n  \nproperty\n  \n        \nname\ndt.gateway.http.authentication.jaas.role.class.name\n/name\n\n         \nvalue\norg.eclipse.jetty.plus.jaas.JAASRole\n/value\n\n  \n/property\n\n  \n/property\n\n\nname\n\n    dt.gateway.http.authentication.jaas.callback.class.name \n/name\n            \n\nvalue\n\n    com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\n\n/value\n\n  \n/property\n\n...\n\n/configuration\n\n\n\n\nNote that the JAAS callback handler property can be used to specify a\ncustom callback handler. The source for the Jetty custom callback\nhandler used above can be found here JettyJAASCallbackHandler\n\n\n\n\n\n\nAn issue was discovered with the Jetty login module supplied with\n    Jetty 8 that prevented LDAP authentication to be successful even\n    when the user credentials were correct. DataTorrent has a fix for\n    this and is providing the login module with the fix in a separate\n    package called dt-auth. The classname for the module is\n    \u201ccom.datatorrent.auth.jetty.JettyLdapLoginModule\u201d. The dt-auth\n    project along with the source can be found here \nAuth\n. DataTorrent\n    is working on submitting this fix back to Jetty project so that it\n    gets back into the main source.\n\n\nThe JAAS configuration file as described in\n\nLDAP\n section under \nEnabling JAAS Auth\n should be configured to specify the ldap settings\nfor roles. A sample configuration  roles based parameters to the\nconfiguration shown before\n\n\nldap {\n        com.datatorrent.auth.jetty.JettyLdapLoginModule required\n    hostname=\"ldap-server-hostname\" authenticationMethod=\"simple\"\n    userBaseDn=\"ou=users,dc=domain,dc=com\" userIdAttribute=\"uid\"\n    userRdnAttribute=\"uid\" roleBaseDn=\"ou=groups,dc=domain,dc=com\"\n    roleNameAttribute=\u201dcn\u201d\n    contextFactory=\u201dcom.sun.jndi.ldap.LdapCtxFactory\u201d;\n};\n\n\n\nFor more ldap settings refer to the java documentation of the login\n\n\n\n\n\n\nAfter the above configuration changes are made the gateway service\n    needs to be restarted. Before restarting however the different\n    classes specified in the configuration files above namely the\n    DataTorrent Jetty callback handler, the Jetty login module with the\n    DataTorrent fix and the Jetty dependencies containing the role class\n    should all be available for Gateway. This can be done by specifying\n    the jars containing these classes in a classpath variable that is\n    used by Gateway.\n\n\nThe jars can be obtained from the \nDataTorrent Auth\n project.\n\n\nPlease follow the instructions in the above url to obtain the project\njar files. After obtaining the jar files perform the following step to\nmake them available to Gateway\n\n\nEdit the custom-env.sh configuration file, typically located under\n\n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for\nlocal install) and append the list of jars obtained above to the\n\nDT_CLASSPATH\n variable. This needs to be added at the end of the\nfile in the section for specifying local overrides to environment\nvariables. The line would look like\n\n\nexport DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/jar1:path/to/jar2:..\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n\n\nwhen running Gateway in local mode use dtgateway restart command.\n\n\n\n\n\n\nExternal Role Mapping \n\n\nExternal role mapping is specified to map the external roles to the\nDataTorrent roles. For example users from an LDAP group called admins\nshould have the admin role when using the Console. This can be specified\nby doing the following steps\n\n\n\n\n\n\nIn the configuration folder typically located under\n    \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for\n    local install) edit the file called external-roles or create the\n    file if it not already present. In this file each line contains a\n    mapping from an external role to a datatorrent role separated by a\n    delimiter \u2018:\u2019 An example listing is\n\n\nadmins:admin\nstaff: developer\n\n\n\nThis maps the external role admins to the DataTorrent role admin and\nexternal role staff to the DataTorrent role developer.\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n\n\nwhen running Gateway in local mode use  dtgateway restart command\n\n\n\n\n\n\nAdministering Using Command Line \n\n\nYou can also utilize the \ndtGateway REST API\n (under /ws/v2/auth) to add or remove users and to change roles and passwords.\n Here are the examples with password authentication.\n\n\nLog in as admin:\n\n\n% curl -c ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"userName\":\"admin\",\"password\":\"admin\"}' http://localhost:9090/ws/v2/login\n\n\n\nThis curl command logs in as user \u201cadmin\u201d (with the default password\n\u201cadmin\u201d) and stores the cookie in ~/cookie-jar\n\n\nChanging the admin password:\n\n\n% curl -b ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"newPassword\":\"xyz\"}' http://localhost:9090/ws/v2/auth/users/admin\n\n\n\nThis uses the \u201cadmin\u201d credential from the cookie jar to change the password to \u201cxyz\u201d for user \u201cadmin\u201d.\n\n\nAdding a second admin user:\n\n\n% curl -b ~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [ \"admin\" ] }' http://localhost:9090/ws/v2/auth/users/john\n\n\n\nThis command adds a user \u201cjohn\u201d with password \u201cabc\u201d with admin access.\n\n\nAdding a user in the developer role:\n\n\n% curl -b \\~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [\"developer\"] (http://localhost:9090/ws/v1/login) }' http://localhost:9090/ws/v2/auth/users/jane\n\n\n\nThis command adds a user \u201cjane\u201d with password \u201cabc\u201d with the developer role.\n\n\nListing all users:\n\n\n% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users\n\n\n\nGetting info for a specific user:\n\n\n% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users/john\n\n\n\nThis command returns the information about the user \u201cjohn\u201d.\n\n\nRemoving a user:\n\n\n% curl -b ~/cookie-jar -XDELETE http://localhost:9090/ws/v2/auth/users/jane\n\n\n\nThis command removes the user \u201cjane\u201d.\n\n\nEnabling HTTPS in Gateway\n\n\nHTTPS in the Gateway can be enabled by performing following two steps.\n\n\n\n\n\n\nGenerate an SSL keystore if you don\u2019t have one.  Instruction on how to generate an SSL keystore is here: \nhttp://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html\n\n\n\n\n\n\nAdd a property to dt-site.xml configuration file, typically located under \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n...\n  \nproperty\n\n           \nname\ndt.gateway.sslKeystorePath\n/name\n\n           \nvalue\n{/path/to/keystore}\n/value\n\n  \n/property\n\n  \nproperty\n\n            \nname\ndt.gateway.sslKeystorePassword\n/name\n\n             \nvalue\n{keystore-password}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.attr.GATEWAY_USE_SSL\n/name\n\n          \nvalue\ntrue\n/value\n\n  \n/property\n\n...\n\n/configuration\n\n\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n\n\n( when running Gateway in local mode use \ndtgateway restart\n command)", 
            "title": "Security"
        }, 
        {
            "location": "/dtgateway_security/#datatorrent-gateway-security", 
            "text": "DataTorrent Gateway supports different authentication mechanisms to\nsecure access to the Console. The supported types are local password\nauthentication, kerberos authentication and J", 
            "title": "DataTorrent Gateway Security"
        }, 
        {
            "location": "/dtgateway_security/#configuring-authentication", 
            "text": "After DataTorrent RTS installation, you can turn on authentication and\nauthorization support to secure the DataTorrent Gateway. Gateway\nsupports three types of authentication.   Password  Kerberos  JAAS", 
            "title": "Configuring Authentication"
        }, 
        {
            "location": "/dtgateway_security/#enabling-password-auth", 
            "text": "\u201cPassword\u201d is the only authentication mechanism presented here that does\nnot depend on any external systems.  When enabled, all users will be\npresented with the login prompt before being able to use the DT Console.\nPassword authentication can be enabled by performing following two\nsteps.    Add a property to  dt-site.xml  configuration file, typically located\n    under  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install).  configuration \n...\n     property \n     name dt.gateway.http.authentication.type /name \n     value password /value \n     /property \n... /configuration     Restart the Gateway by running  sudo service dtgateway restart  ( when running Gateway in local mode use  dtgateway restart command)    Open the Gateway URL in your browser, and you should be prompted for\nuser name and password.  Starting with DataTorrent RTS 2.0.0, the\ndefault username and password is  dtadmin  and  dtadmin .   Once authenticated, active user name and an option to log out is\npresented in the top right corner of the DT Console screen.", 
            "title": "Enabling Password Auth"
        }, 
        {
            "location": "/dtgateway_security/#enabling-kerberos-auth", 
            "text": "Kerberos authentication can optionally be enabled for Hadoop web access.\nIf this is configured then all web browser access to the Hadoop\nmanagement consoles is Kerberos authenticated. Access to all the web\nservices is also Kerberos authenticated. This Kerberos authentication is\nperformed using a protocol called SPNEGO which is Kerberos over HTTP.\nPlease refer to the administration guide of your Hadoop distribution on\nhow to enable this. The web browsers must also support SPNEGO, most\nmodern browsers do and should be configured to use SPNEGO for the\nspecific URLs being accessed. Please refer to the documentation of the\nindividual browsers on how to configure this. Gateway handles the SPNEGO\nauthentication needed when communicating with the Hadoop web-services if\nKerberos authentication is enabled for Hadoop web access.  Kerberos authentication can be enabled for UI Console as well. When it\nis enabled access to the Gateway web-services is Kerberos authenticated.\nThe browser should be configured for SPNEGO authentication when\naccessing the Console. A user typically obtains a master ticket first by\nlogging in to kerberos system in a terminal emulator using kinit. Then\nthe user launches a browser and accesses the Console URL. The browser\nwill use the master ticket obtained by kinit earlier to obtain the\nnecessary security tokens to authenticate with the Gateway.  When this authentication is enabled the first authenticated user that\naccesses the system is assigned the admin role as there are no other\nusers in the system at this point. Any subsequent authenticated user\nthat access the system starts with no roles. The admin user can then\nassign roles to these users. This behavior can be changed by configuring\nan external role mapping. Please refer to the  External Role Mapping  in the  Authorization using external roles  section below for that.  Additional configuration is needed to enable Kerberos authentication for\nthe Console. A separate set of kerberos credentials are needed. These\ncan be same as the Hadoop web-service Kerberos credentials which are\ntypically identified with the principal HTTP/HOST@DOMAIN. A few other\nconfiguration properties are also needed. These can be specified in the\nsame \u201cdt-site.xml\u201d configuration file as the DT Gateway authentication\nconfiguration described in the Operation and Installation Guide. This\nauthentication can be set up using the following steps.    Add the following properties to  dt-site.xml  configuration file, typically located under  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install)  configuration \n...\n   property \n     name dt.gateway.http.authentication.type /name \n     value kerberos /value \n   /property \n   property \n     name dt.gateway.http.authentication.kerberos.principal /name \n     value {kerberos-principal-of-web-service} /value \n   /property \n   property \n     name dt.gateway.http.authentication.kerberos.keytab /name \n     value {absolute-path-to-keytab-file} /value \n   /property \n   property \n     name dt.gateway.http.authentication.token.validity /name \n     value {authentication-token-validity-in-seconds} /value \n   /property \n   property \n   name\\ dt.gateway.http.authentication.cookie.domain /name \n   value\\ {http-cookie-domain-for-authentication-token} /value \n   property \n     name\\ dt.gateway.http.authentication.cookie.path /name \n     value {http-cookie-path} /value \n   /property \n   property \n     name\\ dt.gateway.http.authentication.signature.secret /name \n     value {absolute-path-of-secret-file-for-signing-authentication-tokens}  /value \n   /property  /configuration   Note that the kerberos principal for web service should begin with\nHTTP/\u2026  All the values for the properties above except for the property dt.gateway.http.authentication.type  should be replaced with the\nappropriate values for your setup.    Restart the Gateway by running  sudo service dtgateway restart \n( when running Gateway in local mode use   dtgateway restart  command)", 
            "title": "Enabling Kerberos Auth"
        }, 
        {
            "location": "/dtgateway_security/#enabling-jaas-auth", 
            "text": "JAAS is Java Authentication and Authorization Service. It is a pluggable\nand extensible mechanism for authentication. It is a generic framework\nand the actual authentication is performed by a JAAS plugin module which\ncan be configured using a configuration file. Among others LDAP and PAM\nauthentication can be performed via JAAS.  Similar to Kerberos when this authentication is enabled the first\nauthenticated user that accesses the system is assigned the admin role\nas there are no other users in the system at this point. Any subsequent\nauthenticated user that access the system starts with no roles. The\nadmin user can then assign roles to these users. This behavior can be\nchanged by configuring an external role mapping. Please refer to the External Role Mapping  in the  Authorization using external roles  section below for that.  This authentication can be enabled by performing the following general\nsteps. The specific steps for a couple of authentication mechanisms LDAP\nand PAM are shown in the next sections but other authentication\nmechanisms that have a JAAS plugin module can also be used.    Add a property to  dt-site.xml  configuration file, typically located\n    under  /opt/datatorrent/current/conf  ( or\n     ~/datatorrent/current/conf  for local install).  configuration \n...\n   property \n       name dt.gateway.http.authentication.type /name \n       value jaas /value \n   /property \n   property\\ \n       name dt.gateway.http.authentication.jaas.name /name \n       value name-of-jaas-module /value \n   /property \n... /configuration   The  dt.gateway.http.authentication.jaas.name  property specifies the\nplugin module to use with JAAS and the value should be the name of the\nplugin module.    The name of the plugin module specified above should be configured\n    with the appropriate settings for the plugin. This is module\n    specific configuration. This can be done in a file named\n    .java.login.config in the home directory of the user Gateway is\n    running under. If DataTorrent RTS was installed as a superuser,\n    Gateway would typically run as dtadmin and this file path would\n    typically be  /home/dtadmin/.java.login.config , if running as a\n    normal user it would be  ~/.java.login.config . The sample\n    configurations for LDAP and PAM are shown in the next sections.    The classes for the plugin module may need to be made available to\n    Gateway if they are not available in the default classpath. This can\n    be done by specifying the jars containing these classes in a\n    classpath variable that is used by Gateway.  The following step shows how to do this  a.  Edit the  custom-env.sh  configuration file, typically located under\n     /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for\n    local install) and append the list of jars obtained above to the\n    DT_CLASSPATH variable. This needs to be added at the end of the\n    file in the section for specifying local overrides to environment\n    variables. The line would look like  export DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/jar1:path/to/jar2:..    Restart the Gateway by running  sudo service dtgateway restart \n( when running Gateway in local mode use   dtgateway restart  command)", 
            "title": "Enabling JAAS Auth"
        }, 
        {
            "location": "/dtgateway_security/#groups", 
            "text": "For group support such as using LDAP groups for authorization refer to\nthe  Authorization using external roles  section below.", 
            "title": "Groups"
        }, 
        {
            "location": "/dtgateway_security/#configuring-authorization", 
            "text": "When any authentication method is enabled, authorization will also be\nenabled.  DT Gateway uses roles and permissions for authorization.\n Permissions are possession of the authority to perform certain actions,\ne.g. to launch apps, or to add users.  Roles have a collection of\npermissions and individual users are assigned to one or more roles.\n What roles the user is in dictates what permissions the user has.", 
            "title": "Configuring Authorization"
        }, 
        {
            "location": "/dtgateway_security/#permissions", 
            "text": "The list of all possible permissions in the DT Gateway is as follow:", 
            "title": "Permissions"
        }, 
        {
            "location": "/dtgateway_security/#default-roles", 
            "text": "DataTorrent RTS ships with three roles by default. The permissions for\nthese roles have been set accordingly but can be customized if needed.", 
            "title": "Default Roles"
        }, 
        {
            "location": "/dtgateway_security/#admin", 
            "text": "An administrator of DataTorrent RTS is intended to be able to install,\nmanage   modify DataTorrent RTS as well as all the applications running\non it. They have ALL the permissions.", 
            "title": "Admin"
        }, 
        {
            "location": "/dtgateway_security/#operator", 
            "text": "Operators are intended to ensure DataTorrent RTS and the applications\nrunning on top of it are always up and running optimally. The default\npermissions assigned to Operators, enable them to effectively\ntroubleshoot the entire RTS system and the applications running on it.\nOperators are not allowed however to develop or launch new applications.\nOperators are also not allowed to make system configuration changes or\nmanage users.  Here is the list of default permissions given to operators  MANAGE_SYSTEM_ALERTS  VIEW_GLOBAL_CONFIG  VIEW_LICENSES  VIEW_OTHER_USERS_APPS  VIEW_OTHER_USERS_APP_PACKAGES  VIEW_SYSTEM_ALERTS  Note that VIEW_OTHER_USERS_APPS and VIEW_OTHER_USERS_APP_PACKAGES\nare in the list.  This means all users in the \u201coperator\u201d role will have\nread access to all apps and all app packages in the system.  You can\nremove those permissions from the \u201coperator\u201d role using the Console if\nthis is not desirable.", 
            "title": "Operator"
        }, 
        {
            "location": "/dtgateway_security/#developer", 
            "text": "Developers need to have to ability to develop, manage and run\napplications on DataTorrent RTS. They are not allowed to change any\nsystem settings or manage licenses.  Here is the list of default permissions given to developers  LAUNCH_APPS  UPLOAD_APP_PACKAGES  MANAGE_SYSTEM_ALERTS  VIEW_GLOBAL_CONFIG  VIEW_LICENSES  VIEW_SYSTEM_ALERTS", 
            "title": "Developer"
        }, 
        {
            "location": "/dtgateway_security/#app-permissions-and-app-package-permissions", 
            "text": "Users can share their running application instances and their\napplication packages with certain roles or certain users on a\nper-instance or per-package basis.  Users can specify which users or\nwhile roles have read-only or read-write access.  In addition, users can\nset their own defaults so they does not have to make permission change\nevery time they launch an app or uploads an app package.  The default for app and app package sharing is that the \u201coperator\u201d role\nhas read-only permission.  As of RTS 2.0.0, the Console does not yet support managing App\nPermissions or App Package Permissions.  But one can manage App\nPermissions and App Package Permissions using the Gateway REST API with\nURI\u2019s /ws/v2/apps/{appid}/permissions and\n/ws/v2/appPackages/{user}/{name}/permissions respectively.  Please refer\nto the  DT Gateway REST API document  and  here  for examples on how to use the REST API.", 
            "title": "App Permissions and App Package Permissions"
        }, 
        {
            "location": "/dtgateway_security/#viewing-and-managing-auth-in-the-console", 
            "text": "", 
            "title": "Viewing and Managing Auth in the Console"
        }, 
        {
            "location": "/dtgateway_security/#viewing-user-profile", 
            "text": "After you are logged in on the Console, click on the Configuration tab\non the left, and select \u201cUser Profile\u201d.  This gives you the information\nof the logged in user, including what roles the user is in, and what\npermissions the user has. \nAdministering Auth   From the Configuration tab, click on \u201cAuth Management\u201d.  On this page,\nyou can perform the following tasks:   Create new users  Delete users  Change existing users\u2019 passwords  Assign roles to users  Create roles  Assign permissions to roles    DataTorrent RTS installation comes with three preset roles (admin,\noperator, and developer).  You can edit the permissions for those roles\nexcept for admin.", 
            "title": "Viewing User Profile"
        }, 
        {
            "location": "/dtgateway_security/#kerberos-roles", 
            "text": "When Kerberos authentication is used the role for the user is derived\nfrom the principal. If the principal is of the form  user/group@DOMAIN \nthe group portion is used as the external role and no additional\nconfiguration is necessary.", 
            "title": "Kerberos roles "
        }, 
        {
            "location": "/dtgateway_security/#jaas-roles", 
            "text": "To use JAAS roles the system should be configured first to recognize\nthese roles. When a user is authenticated with JAAS a list of principals\nis returned for the user by the implementing JAAS authentication module.\nSome of these principals can be for roles and these role principals need\nto be identified from the list. Additional configuration is needed to do\nthis and this configuration is specific to the JAAS authentication\nmodule being used. Specifically the JAVA class name identifying the role\nprincipal needs to be specified. This can be specified using the\nproperty  \u201cdt.gateway.http.authentication.jaas.role.class.name\u201d  in the\nconfiguration file as shown below  configuration \n...\n   property \n        name dt.gateway.http.authentication.jaas.role.class.name /name \n           value full-class-name-of-role /value \n  /property \n... /configuration", 
            "title": "JAAS roles "
        }, 
        {
            "location": "/dtgateway_security/#callback-handlers", 
            "text": "In JAAS authentication, a login module may need a custom callback to be\nhandled by the caller. The callbacks are typically used to provide\nauthentication credentials to the login module. DataTorrent RTS supports\nspecification of a custom callback handler to handle these callbacks.\nThe custom callback handler can be specified using the property\n\u201cdt.gateway.http.authentication.jaas.callback.class.name\u201d. When this\nproperty is not specified a default callback handler is used but it may\nnot be sufficient for all login modules like in the LDAP case described\nbelow. The property can be specified as follows  configuration \n... property                                                                              name dt.gateway.http.authentication.jaas.callback.class.name /name  value full-class-name-of-callback /value  /property \n... /configuration   Custom callback handlers can be implemented by extending the default\ncallback handler so they can build upon it or they can be built from\nscratch. The LDAP callback handler described below also extends the\ndefault callback handler. The source for the default callback handler\ncan be found here  DefaultCallbackHandler  can be used as a reference when\nimplementing new callback handlers.", 
            "title": "Callback Handlers"
        }, 
        {
            "location": "/dtgateway_security/#ldap_1", 
            "text": "When using LDAP with JAAS, to use LDAP roles, a  LDAP login module\nsupporting roles should be used. Any LDAP module that supports roles can\nbe used. Jetty implements one such login module. The steps to configure\nthis module are as follows.    The Gateway service in DataTorrent RTS 2.0 is compatible with Jetty 8. The class name identifying the \n    role principal is  \u201corg.eclipse.jetty.plus.jaas.JAASRole\u201d . When using the Jetty LDAP\n    login module a custom JAAS callback has to be handled by the caller.\n    This has been implemented by DataTorrent in a callback handler. The\n    class name for the callback handler implementation should be\n    specified as a property. The property name is\n     \u201cdt.gateway.http.authentication.jaas.callback.class.name\u201d . The class\n    name of the callback handler is\n     \u201ccom.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\u201d .\n    This property should be specified along with the class name\n    identifying the role principal for the Jetty login module as\n    described in the section above. The configuration file with all the\n    JAAS properties including these looks as follows  configuration \n...\n   property \n           name dt.gateway.http.authentication.type /name \n           value jaas /value \n   /property \n   property \n          name dt.gateway.http.authentication.jaas.name /name \n          value ldap /value \n   /property \n   property   \n         name dt.gateway.http.authentication.jaas.role.class.name /name \n          value org.eclipse.jetty.plus.jaas.JAASRole /value \n   /property \n   /property  name \n    dt.gateway.http.authentication.jaas.callback.class.name  /name              value \n    com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler /value \n   /property \n... /configuration   Note that the JAAS callback handler property can be used to specify a\ncustom callback handler. The source for the Jetty custom callback\nhandler used above can be found here JettyJAASCallbackHandler    An issue was discovered with the Jetty login module supplied with\n    Jetty 8 that prevented LDAP authentication to be successful even\n    when the user credentials were correct. DataTorrent has a fix for\n    this and is providing the login module with the fix in a separate\n    package called dt-auth. The classname for the module is\n    \u201ccom.datatorrent.auth.jetty.JettyLdapLoginModule\u201d. The dt-auth\n    project along with the source can be found here  Auth . DataTorrent\n    is working on submitting this fix back to Jetty project so that it\n    gets back into the main source.  The JAAS configuration file as described in LDAP  section under  Enabling JAAS Auth  should be configured to specify the ldap settings\nfor roles. A sample configuration  roles based parameters to the\nconfiguration shown before  ldap {\n        com.datatorrent.auth.jetty.JettyLdapLoginModule required\n    hostname=\"ldap-server-hostname\" authenticationMethod=\"simple\"\n    userBaseDn=\"ou=users,dc=domain,dc=com\" userIdAttribute=\"uid\"\n    userRdnAttribute=\"uid\" roleBaseDn=\"ou=groups,dc=domain,dc=com\"\n    roleNameAttribute=\u201dcn\u201d\n    contextFactory=\u201dcom.sun.jndi.ldap.LdapCtxFactory\u201d;\n};  For more ldap settings refer to the java documentation of the login    After the above configuration changes are made the gateway service\n    needs to be restarted. Before restarting however the different\n    classes specified in the configuration files above namely the\n    DataTorrent Jetty callback handler, the Jetty login module with the\n    DataTorrent fix and the Jetty dependencies containing the role class\n    should all be available for Gateway. This can be done by specifying\n    the jars containing these classes in a classpath variable that is\n    used by Gateway.  The jars can be obtained from the  DataTorrent Auth  project.  Please follow the instructions in the above url to obtain the project\njar files. After obtaining the jar files perform the following step to\nmake them available to Gateway  Edit the custom-env.sh configuration file, typically located under /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for\nlocal install) and append the list of jars obtained above to the DT_CLASSPATH  variable. This needs to be added at the end of the\nfile in the section for specifying local overrides to environment\nvariables. The line would look like  export DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/jar1:path/to/jar2:..    Restart the Gateway by running  sudo service dtgateway restart  when running Gateway in local mode use dtgateway restart command.", 
            "title": "LDAP"
        }, 
        {
            "location": "/dtgateway_security/#external-role-mapping", 
            "text": "External role mapping is specified to map the external roles to the\nDataTorrent roles. For example users from an LDAP group called admins\nshould have the admin role when using the Console. This can be specified\nby doing the following steps    In the configuration folder typically located under\n     /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for\n    local install) edit the file called external-roles or create the\n    file if it not already present. In this file each line contains a\n    mapping from an external role to a datatorrent role separated by a\n    delimiter \u2018:\u2019 An example listing is  admins:admin\nstaff: developer  This maps the external role admins to the DataTorrent role admin and\nexternal role staff to the DataTorrent role developer.    Restart the Gateway by running  sudo service dtgateway restart  when running Gateway in local mode use  dtgateway restart command", 
            "title": "External Role Mapping "
        }, 
        {
            "location": "/dtgateway_security/#administering-using-command-line", 
            "text": "You can also utilize the  dtGateway REST API  (under /ws/v2/auth) to add or remove users and to change roles and passwords.\n Here are the examples with password authentication.", 
            "title": "Administering Using Command Line "
        }, 
        {
            "location": "/dtgateway_security/#log-in-as-admin", 
            "text": "% curl -c ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"userName\":\"admin\",\"password\":\"admin\"}' http://localhost:9090/ws/v2/login  This curl command logs in as user \u201cadmin\u201d (with the default password\n\u201cadmin\u201d) and stores the cookie in ~/cookie-jar", 
            "title": "Log in as admin:"
        }, 
        {
            "location": "/dtgateway_security/#changing-the-admin-password", 
            "text": "% curl -b ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"newPassword\":\"xyz\"}' http://localhost:9090/ws/v2/auth/users/admin  This uses the \u201cadmin\u201d credential from the cookie jar to change the password to \u201cxyz\u201d for user \u201cadmin\u201d.", 
            "title": "Changing the admin password:"
        }, 
        {
            "location": "/dtgateway_security/#adding-a-second-admin-user", 
            "text": "% curl -b ~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [ \"admin\" ] }' http://localhost:9090/ws/v2/auth/users/john  This command adds a user \u201cjohn\u201d with password \u201cabc\u201d with admin access.", 
            "title": "Adding a second admin user:"
        }, 
        {
            "location": "/dtgateway_security/#adding-a-user-in-the-developer-role", 
            "text": "% curl -b \\~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [\"developer\"] (http://localhost:9090/ws/v1/login) }' http://localhost:9090/ws/v2/auth/users/jane  This command adds a user \u201cjane\u201d with password \u201cabc\u201d with the developer role.", 
            "title": "Adding a user in the developer role:"
        }, 
        {
            "location": "/dtgateway_security/#listing-all-users", 
            "text": "% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users", 
            "title": "Listing all users:"
        }, 
        {
            "location": "/dtgateway_security/#getting-info-for-a-specific-user", 
            "text": "% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users/john  This command returns the information about the user \u201cjohn\u201d.", 
            "title": "Getting info for a specific user:"
        }, 
        {
            "location": "/dtgateway_security/#removing-a-user", 
            "text": "% curl -b ~/cookie-jar -XDELETE http://localhost:9090/ws/v2/auth/users/jane  This command removes the user \u201cjane\u201d.", 
            "title": "Removing a user:"
        }, 
        {
            "location": "/dtgateway_security/#enabling-https-in-gateway", 
            "text": "HTTPS in the Gateway can be enabled by performing following two steps.    Generate an SSL keystore if you don\u2019t have one.  Instruction on how to generate an SSL keystore is here:  http://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html    Add a property to dt-site.xml configuration file, typically located under  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install).  configuration \n...\n   property \n            name dt.gateway.sslKeystorePath /name \n            value {/path/to/keystore} /value \n   /property \n   property \n             name dt.gateway.sslKeystorePassword /name \n              value {keystore-password} /value \n   /property \n   property \n     name dt.attr.GATEWAY_USE_SSL /name \n           value true /value \n   /property \n... /configuration     Restart the Gateway by running  sudo service dtgateway restart  ( when running Gateway in local mode use  dtgateway restart  command)", 
            "title": "Enabling HTTPS in Gateway"
        }, 
        {
            "location": "/dtcli/", 
            "text": "Apache Apex Command Line Interface\n\n\ndtCli, the Apache Apex command line interface, can be used to launch, monitor, and manage\nApache Apex applications.\n\nIt provides a developer friendly way of interacting with Apache Apex platform.\nAnother advantage of dtCli is to provide scope, by connecting and executing commands in a context\nof specific application.  dtCli enables easy integration with existing enterprise toolset for automated application monitoring\nand management.  Currently the following high level tasks are supported.\n\n\n\n\nLaunch or kill applications\n\n\nView system metrics including load, throughput, latency, etc.\n\n\nStart or stop tuple recording\n\n\nRead operator, stream, port properties and attributes\n\n\nWrite to operator properties\n\n\nDynamically change the application logical plan\n\n\nCreate custom macros\n\n\n\n\ndtcli Commands\n\n\ndtCli can be launched by running following command:\n\n\ndtcli\n\n\n\nHelp on all commands is available via \u201chelp\u201d command in the CLI\n\n\nGlobal Commands\n\n\nGLOBAL COMMANDS EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nalias alias-name command\n    Create a command alias\n\nbegin-macro name\n    Begin Macro Definition ($1...$9 to access parameters and type 'end' to end the definition)\n\nconnect app-id\n    Connect to an app\n\ndump-properties-file out-file jar-file class-name\n    Dump the properties file of an app class\n\necho [arg ...]\n    Echo the arguments\n\nexit\n    Exit the CLI\n\nget-app-info app-id\n    Get the information of an app\n\nget-app-package-info app-package-file\n    Get info on the app package file\n\nget-app-package-operator-properties app-package-file operator-class\n    Get operator properties within the given app package\n\nget-app-package-operators [options] app-package-file [search-term]\n    Get operators within the given app package\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-config-parameter [parameter-name]\n    Get the configuration parameter\n\nget-jar-operator-classes [options] jar-files-comma-separated [search-term]\n    List operators in a jar list\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-jar-operator-properties jar-files-comma-separated operator-class-name\n    List properties in specified operator\n\nhelp [command]\n    Show help\n\nkill-app app-id [app-id ...]\n    Kill an app\n\n  launch [options] jar-file/json-file/properties-file/app-package-file [matching-app-name]\n    Launch an app\n    Options:\n            -apconf \napp package configuration file\n        Specify an application\n                                                            configuration file\n                                                            within the app\n                                                            package if launching\n                                                            an app package.\n            -archives \ncomma separated list of archives\n    Specify comma\n                                                            separated archives\n                                                            to be unarchived on\n                                                            the compute machines.\n            -conf \nconfiguration file\n                      Specify an\n                                                            application\n                                                            configuration file.\n            -D \nproperty=value\n                             Use value for given\n                                                            property.\n            -exactMatch                                     Only consider\n                                                            applications with\n                                                            exact app name\n            -files \ncomma separated list of files\n          Specify comma\n                                                            separated files to\n                                                            be copied on the\n                                                            compute machines.\n            -ignorepom                                      Do not run maven to\n                                                            find the dependency\n            -libjars \ncomma separated list of libjars\n      Specify comma\n                                                            separated jar files\n                                                            or other resource\n                                                            files to include in\n                                                            the classpath.\n            -local                                          Run application in\n                                                            local mode.\n            -originalAppId \napplication id\n                 Specify original\n                                                            application\n                                                            identifier for restart.\n            -queue \nqueue name\n                             Specify the queue to\n                                                            launch the application\n\nlist-application-attributes\n    Lists the application attributes\nlist-apps [pattern]\n    List applications\nlist-operator-attributes\n    Lists the operator attributes\nlist-port-attributes\n    Lists the port attributes\nset-pager on/off\n    Set the pager program for output\nshow-logical-plan [options] jar-file/app-package-file [class-name]\n    List apps in a jar or show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars \ncomma separated list of jars\n    Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshutdown-app app-id [app-id ...]\n    Shutdown an app\nsource file\n    Execute the commands in a file\n\n\n\n\nCommands after connecting to an application\n\n\nCOMMANDS WHEN CONNECTED TO AN APP (via connect \nappid\n) EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nbegin-logical-plan-change\n    Begin Logical Plan Change\ndump-properties-file out-file [jar-file] [class-name]\n    Dump the properties file of an app class\nget-app-attributes [attribute-name]\n    Get attributes of the connected app\nget-app-info [app-id]\n    Get the information of an app\nget-operator-attributes operator-name [attribute-name]\n    Get attributes of an operator\nget-operator-properties operator-name [property-name]\n    Get properties of a logical operator\nget-physical-operator-properties [options] operator-id\n    Get properties of a physical operator\n    Options:\n            -propertyName \nproperty name\n    The name of the property whose\n                                             value needs to be retrieved\n            -waitTime \nwait time\n            How long to wait to get the result\nget-port-attributes operator-name port-name [attribute-name]\n    Get attributes of a port\nget-recording-info [operator-id] [start-time]\n    Get tuple recording info\nkill-app [app-id ...]\n    Kill an app\nkill-container container-id [container-id ...]\n    Kill a container\nlist-containers\n    List containers\nlist-operators [pattern]\n    List operators\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-physical-operator-property operator-id property-name property-value\n    Set a property of an operator\nshow-logical-plan [options] [jar-file/app-package-file] [class-name]\n    Show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars \ncomma separated list of jars\n    Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshow-physical-plan\n    Show physical plan\nshutdown-app [app-id ...]\n    Shutdown an app\nstart-recording operator-id [port-name] [num-windows]\n    Start recording\nstop-recording operator-id [port-name]\n    Stop recording\nwait timeout\n    Wait for completion of current application\n\n\n\n\nCommands when changing the logical plan\n\n\nCOMMANDS WHEN CHANGING LOGICAL PLAN (via begin-logical-plan-change):\n\nabort\n    Abort the plan change\nadd-stream-sink stream-name to-operator-name to-port-name\n    Add a sink to an existing stream\ncreate-operator operator-name class-name\n    Create an operator\ncreate-stream stream-name from-operator-name from-port-name to-operator-name to-port-name\n    Create a stream\nhelp [command]\n    Show help\nremove-operator operator-name\n    Remove an operator\nremove-stream stream-name\n    Remove a stream\nset-operator-attribute operator-name attr-name attr-value\n    Set an attribute of an operator\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-port-attribute operator-name port-name attr-name attr-value\n    Set an attribute of a port\nset-stream-attribute stream-name attr-name attr-value\n    Set an attribute of a stream\nshow-queue\n    Show the queue of the plan change\nsubmit\n    Submit the plan change\n\n\n\n\nExamples\n\n\nAn example of defining a custom macro.  The macro updates a running application by inserting a new operator.  It takes three parameters and executes a logical plan changes.\n\n\ndt\n begin-macro add-console-output\nmacro\n begin-logical-plan-change\nmacro\n create-operator $1 com.datatorrent.lib.io.ConsoleOutputOperator\nmacro\n create-stream stream_$1 $2 $3 $1 in\nmacro\n submit\n\n\n\n\nThen execute the \nadd-console-output\n macro like this\n\n\ndt\n add-console-output xyz opername portname\n\n\n\n\nThis macro then expands to run the following command\n\n\nbegin-logical-plan-change\ncreate-operator xyz com.datatorrent.lib.io.ConsoleOutputOperator\ncreate-stream stream_xyz opername portname xyz in\nsubmit\n\n\n\n\nNote\n:  To perform runtime logical plan changes, like ability to add new operators,\nthey must be part of the jar files that were deployed at application launch time.", 
            "title": "dtCli"
        }, 
        {
            "location": "/dtcli/#apache-apex-command-line-interface", 
            "text": "dtCli, the Apache Apex command line interface, can be used to launch, monitor, and manage\nApache Apex applications. \nIt provides a developer friendly way of interacting with Apache Apex platform.\nAnother advantage of dtCli is to provide scope, by connecting and executing commands in a context\nof specific application.  dtCli enables easy integration with existing enterprise toolset for automated application monitoring\nand management.  Currently the following high level tasks are supported.   Launch or kill applications  View system metrics including load, throughput, latency, etc.  Start or stop tuple recording  Read operator, stream, port properties and attributes  Write to operator properties  Dynamically change the application logical plan  Create custom macros", 
            "title": "Apache Apex Command Line Interface"
        }, 
        {
            "location": "/dtcli/#dtcli-commands", 
            "text": "dtCli can be launched by running following command:  dtcli  Help on all commands is available via \u201chelp\u201d command in the CLI", 
            "title": "dtcli Commands"
        }, 
        {
            "location": "/dtcli/#global-commands", 
            "text": "GLOBAL COMMANDS EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nalias alias-name command\n    Create a command alias\n\nbegin-macro name\n    Begin Macro Definition ($1...$9 to access parameters and type 'end' to end the definition)\n\nconnect app-id\n    Connect to an app\n\ndump-properties-file out-file jar-file class-name\n    Dump the properties file of an app class\n\necho [arg ...]\n    Echo the arguments\n\nexit\n    Exit the CLI\n\nget-app-info app-id\n    Get the information of an app\n\nget-app-package-info app-package-file\n    Get info on the app package file\n\nget-app-package-operator-properties app-package-file operator-class\n    Get operator properties within the given app package\n\nget-app-package-operators [options] app-package-file [search-term]\n    Get operators within the given app package\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-config-parameter [parameter-name]\n    Get the configuration parameter\n\nget-jar-operator-classes [options] jar-files-comma-separated [search-term]\n    List operators in a jar list\n    Options:\n            -parent    Specify the parent class for the operators\n\nget-jar-operator-properties jar-files-comma-separated operator-class-name\n    List properties in specified operator\n\nhelp [command]\n    Show help\n\nkill-app app-id [app-id ...]\n    Kill an app\n\n  launch [options] jar-file/json-file/properties-file/app-package-file [matching-app-name]\n    Launch an app\n    Options:\n            -apconf  app package configuration file         Specify an application\n                                                            configuration file\n                                                            within the app\n                                                            package if launching\n                                                            an app package.\n            -archives  comma separated list of archives     Specify comma\n                                                            separated archives\n                                                            to be unarchived on\n                                                            the compute machines.\n            -conf  configuration file                       Specify an\n                                                            application\n                                                            configuration file.\n            -D  property=value                              Use value for given\n                                                            property.\n            -exactMatch                                     Only consider\n                                                            applications with\n                                                            exact app name\n            -files  comma separated list of files           Specify comma\n                                                            separated files to\n                                                            be copied on the\n                                                            compute machines.\n            -ignorepom                                      Do not run maven to\n                                                            find the dependency\n            -libjars  comma separated list of libjars       Specify comma\n                                                            separated jar files\n                                                            or other resource\n                                                            files to include in\n                                                            the classpath.\n            -local                                          Run application in\n                                                            local mode.\n            -originalAppId  application id                  Specify original\n                                                            application\n                                                            identifier for restart.\n            -queue  queue name                              Specify the queue to\n                                                            launch the application\n\nlist-application-attributes\n    Lists the application attributes\nlist-apps [pattern]\n    List applications\nlist-operator-attributes\n    Lists the operator attributes\nlist-port-attributes\n    Lists the port attributes\nset-pager on/off\n    Set the pager program for output\nshow-logical-plan [options] jar-file/app-package-file [class-name]\n    List apps in a jar or show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars  comma separated list of jars     Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshutdown-app app-id [app-id ...]\n    Shutdown an app\nsource file\n    Execute the commands in a file", 
            "title": "Global Commands"
        }, 
        {
            "location": "/dtcli/#commands-after-connecting-to-an-application", 
            "text": "COMMANDS WHEN CONNECTED TO AN APP (via connect  appid ) EXCEPT WHEN CHANGING LOGICAL PLAN:\n\nbegin-logical-plan-change\n    Begin Logical Plan Change\ndump-properties-file out-file [jar-file] [class-name]\n    Dump the properties file of an app class\nget-app-attributes [attribute-name]\n    Get attributes of the connected app\nget-app-info [app-id]\n    Get the information of an app\nget-operator-attributes operator-name [attribute-name]\n    Get attributes of an operator\nget-operator-properties operator-name [property-name]\n    Get properties of a logical operator\nget-physical-operator-properties [options] operator-id\n    Get properties of a physical operator\n    Options:\n            -propertyName  property name     The name of the property whose\n                                             value needs to be retrieved\n            -waitTime  wait time             How long to wait to get the result\nget-port-attributes operator-name port-name [attribute-name]\n    Get attributes of a port\nget-recording-info [operator-id] [start-time]\n    Get tuple recording info\nkill-app [app-id ...]\n    Kill an app\nkill-container container-id [container-id ...]\n    Kill a container\nlist-containers\n    List containers\nlist-operators [pattern]\n    List operators\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-physical-operator-property operator-id property-name property-value\n    Set a property of an operator\nshow-logical-plan [options] [jar-file/app-package-file] [class-name]\n    Show logical plan of an app class\n    Options:\n            -exactMatch                                Only consider exact match\n                                                       for app name\n            -ignorepom                                 Do not run maven to find\n                                                       the dependency\n            -libjars  comma separated list of jars     Specify comma separated\n                                                       jar/resource files to\n                                                       include in the classpath.\nshow-physical-plan\n    Show physical plan\nshutdown-app [app-id ...]\n    Shutdown an app\nstart-recording operator-id [port-name] [num-windows]\n    Start recording\nstop-recording operator-id [port-name]\n    Stop recording\nwait timeout\n    Wait for completion of current application", 
            "title": "Commands after connecting to an application"
        }, 
        {
            "location": "/dtcli/#commands-when-changing-the-logical-plan", 
            "text": "COMMANDS WHEN CHANGING LOGICAL PLAN (via begin-logical-plan-change):\n\nabort\n    Abort the plan change\nadd-stream-sink stream-name to-operator-name to-port-name\n    Add a sink to an existing stream\ncreate-operator operator-name class-name\n    Create an operator\ncreate-stream stream-name from-operator-name from-port-name to-operator-name to-port-name\n    Create a stream\nhelp [command]\n    Show help\nremove-operator operator-name\n    Remove an operator\nremove-stream stream-name\n    Remove a stream\nset-operator-attribute operator-name attr-name attr-value\n    Set an attribute of an operator\nset-operator-property operator-name property-name property-value\n    Set a property of an operator\nset-port-attribute operator-name port-name attr-name attr-value\n    Set an attribute of a port\nset-stream-attribute stream-name attr-name attr-value\n    Set an attribute of a stream\nshow-queue\n    Show the queue of the plan change\nsubmit\n    Submit the plan change", 
            "title": "Commands when changing the logical plan"
        }, 
        {
            "location": "/dtcli/#examples", 
            "text": "An example of defining a custom macro.  The macro updates a running application by inserting a new operator.  It takes three parameters and executes a logical plan changes.  dt  begin-macro add-console-output\nmacro  begin-logical-plan-change\nmacro  create-operator $1 com.datatorrent.lib.io.ConsoleOutputOperator\nmacro  create-stream stream_$1 $2 $3 $1 in\nmacro  submit  Then execute the  add-console-output  macro like this  dt  add-console-output xyz opername portname  This macro then expands to run the following command  begin-logical-plan-change\ncreate-operator xyz com.datatorrent.lib.io.ConsoleOutputOperator\ncreate-stream stream_xyz opername portname xyz in\nsubmit  Note :  To perform runtime logical plan changes, like ability to add new operators,\nthey must be part of the jar files that were deployed at application launch time.", 
            "title": "Examples"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "Troubleshooting DataTorrent RTS\n\n\nDownload\n\n\nWhere can I get DataTorrent RTS software?\n\n\nDataTorrent products are available for download from \nhttps://www.datatorrent.com/download/\n\n\n\n\nCommunity Edition\n:  It is a packaged version of Apache Apex and enables developers to quickly develop their big data streaming and batch projects.\n\n\nEnterprise Edition\n:  Designed for enterprise production deployment and includes security, advanced monitoring and troubleshooting, graphical application assembly, and application data visualization.\n\n\nSandbox Edition\n:  Enterprise Edition and demo applications pre-installed and configured with a single-node Hadoop cluster running in a virtual machine.  Optimized for evaluation and training purposes.\n\n\ndtIngest Application\n: simplifies the collection, aggregation and movement of large amounts of data to and from Hadoop and is available for production use at no cost.\n\n\n\n\nWhat is the difference between DataTorrent RTS editions?\n\n\nPlease refer to \nDataTorrent RTS editions overview\n\n\nWhere can I find the Standard edition installer?\n\n\nYou can use the download link for Enterprise edition as the package is\nsame for both editions. But, you have to apply the license to enable the\nStandard edition. You can upgrade the license by using dtManage.\nLicenses are available in 2 types : evaluation and production.\n\n\nWhat are DataTorrent RTS package contents of Community vs Enterprise edition?\n\n\nPackage contents for Community edition:\n\n\n\n\nApache Apex (incubating)\n\n\nDataTorrent Demo Applications\n\n\nDataTorrent dtManage\n\n\nDataTorrent dtIngest\n\n\n\n\nPackage contents for Enterprise edition:\n\n\n\n\nApache Apex (incubating)\n\n\nDataTorrent Demo Applications\n\n\nDataTorrent Operator Library\n\n\nDataTorrent Enterprise Security\n\n\nDataTorrent dtManage\n\n\nDataTorrent dtIngest\n\n\nDataTorrent dtAssemble\n\n\nDataTorrent dtDashboard\n\n\n\n\nHow do I confirm the package downloaded correctly?\n\n\nYou can verify the downloaded DataTorrent RTS package by comparing with\nMD5 sum. The command to get md5 sum of downloaded package:\n\n\n# md5sum \nDT_RTS_Package\n\n\n\n\nCompare the result with MD5 sum posted on the product download page.\n\n\nHow do I download the DataTorrent RTS package using CLI?\n\n\nUse following curl command to download DataTorrent RTS package:\n\n\ncurl -LSO\u00a0\nDT_RTS_download_link\n\n\n\n\nWe recommend using \u2018curl\u2019 instead of \u2018wget\u2019, which lacks chunked transfer encoding support, potentially resulting in corrupted downloads.\n\n\nWhat are the prerequisites of DataTorrent RTS ?\n\n\nDataTorrent RTS platform has following Hadoop cluster requirements:\n\n\n\n\nOperating system supported by Hadoop distribution\n\n\nHadoop (2.2 or above) cluster with HDFS, YARN configured. Make sure\n    hadoop executable available in PATH variable\n\n\nJava 7 or 8 as supported by Hadoop distribution\n\n\nMinimum of 8G RAM available on the Hadoop cluster\n\n\nPermissions to create HDFS directory for DataTorrent user\n\n\nGoogle Chrome, Firefox, or Safari to access dtManage (DataTorrent UI\n    console)\n\n\n\n\nWhere can I start from after downloading DataTorrent RTS?\n\n\n\n\nAfter successful download of DataTorrent RTS, make sure all prerequisites are satisfied.\n\n\nYou will need to install DataTorrent RTS on Hadoop cluster using the downloaded installer. Refer to \ninstallation guide\n\n\nOnce installed, you will be prompted to proceed to dtManage, the DataTorrent management console, where you will be able to launch and manage applications.\n\n\n\n\nWhat are the supported Hadoop distribution by DataTorrent RTS?\n\n\nDataTorrent RTS is a Hadoop 2.x native application and is fully\nintegrated with YARN and HDFS providing tight integration with any\nApache Hadoop 2.x based distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nHadoop Distribution\n\n\nSupported Version\n\n\n\n\n\n\nAmazon EMR\n\n\nHadoop 2.4 and higher\n\n\n\n\n\n\nApache Hadoop\n\n\nHadoop 2.2 and higher\n\n\n\n\n\n\nCloudera\n\n\nCDH 5.0 and higher\n\n\n\n\n\n\nHortonworks\n\n\nHDP 2.0 and higher\n\n\n\n\n\n\nMapR\n\n\n4.0 and higher\n\n\n\n\n\n\nMicrosoft\n\n\nHDInsight\n\n\n\n\n\n\nPivotal\n\n\n2.1 and higher\n\n\n\n\n\n\n\n\n\nWhat is the Datatorrent Sandbox?\n\n\nThe Sandbox provides a quick and simple way to experience DataTorrent RTS without setting up and managing a complete Hadoop cluster. The Sandbox contains pre-installed DataTorrent RTS Enterprise Edition along with all the Hadoop services required to launch and run the included demo applications.\n\n\nWhere do I get DataTorrent Sandbox download link?\n\n\nSandbox can be downloaded by visiting \ndatatorrent.com/download\n\n\nWhat are the system requirements for sandbox deployment?\n\n\nThe DataTorrent RTS Sandbox is a complete, stand-alone, instance of the\nEnterprise Edition as a single-node Hadoop cluster on your local\nmachine. Following are prerequisites for DataTorrent RTS:\n\n\n\n\nVirtualBox\n 4.3 or greater installed.\n\n\n6GB RAM or greater available for Sandbox VM.\n\n\n\n\nWhat are the DataTorrent RTS package content details in sandbox?\n\n\n\n\nUbuntu 12.04 LTS + Apache Hadoop 2.2 (DataTorrent RTS 3.1 or earlier)\n\n\nLubuntu 14.04 LTS + Apache Hadoop 2.7 (DataTorrent RTS 3.2 or later)\n\n\nApache Apex (incubating), Apache Apex-Malhar (incubating)\n\n\nDataTorrent Operator Library\n\n\nDataTorrent Enterprise Security\n\n\nDataTorrent dtManage\n\n\nDataTorrent dtIngest\n\n\nDataTorrent dtAssemble\n\n\nDataTorrent dtDashboard\n\n\nDemo Applications\n\n\n\n\nWhat is dtIngest applicaiton?\n\n\nDataTorrent dtIngest simplifies the collection, aggregation and movement of large amounts of data to and from Hadoop and is available for production use at no cost.\n\n\nWhere do I get dtingest application?\n\n\nApplication can be downloaded by visiting \ndatatorrent.com/download\n\n\nWhat are the dtingest package contents?\n\n\nPackage comprises of DataTorrent RTS and dtIngest application.\n\n\nWhat are the prerequisites of dtIngest?\n\n\nDataTorrent RTS 3.x and above. Please refer \ndtIngest tutorial\n\u00a0for more details.\n\n\nWhere can I start from after downloading dtingest?\n\n\n\n\nMake sure all DataTorrent RTS prerequisites are satisfied before dtIngest installation.\n\n\nRun the downloaded installer installer. Refer to DataTorrent RTS \ninstallation guide\n.\n\n\nAfter DataTorrent RTS installation and configuration, you can configure and launch the\n    dtIngest application from dtManage, the DataTorrent console. Refer to \ndtIngest tutorial\n\u00a0for more details.\n\n\n\n\nHow do I get specific DT version ?\n\n\nYou can find archive list of various DataTorrent RTS versions at the bottom of each product download page.\n\n\nWhere can I request new / upgrade current license?\n\n\nPlease follow the instructions at \nLicense Upgrade\n\n\nWhere do I find product documentation?\n\n\nPlease refer to: \nDataTorrent Documentation\n\n\nWhere can I learn more about Apache Apex?\n\n\nYou can refer Apex page for more details: \nApache Apex\n\n\nDo you need help?\n\n\nYou can contact us at \nhttps://www.datatorrent.com/contact\n\n\nInstallation\n\n\nThere are multiple installations available e.g. Sandbox Edition, Community Edition, Enterprise Edition, dtIngest. Supported operating systems are which support Hadoop platform (tested on CentOS 6.x and Ubuntu 12.04).\n\n\nMinimum hardware requirements, what happens if certain minimum configuration requirement has not been met?\n\n\nMinimum of 8G RAM is required on the Hadoop cluster.\n\n\nWhat happens if java is not installed?\n\n\nFollowing message can be seen when Java is not abilable on the system\n\n\nError: java executable not found. Please ensure java\nor hadoop are installed and available in PATH environment variable\nbefore proceeding with this installation.\n\n\n\nInstall Java 7 from package manager of Linux Distribution and try running installer again.\n\n\nWhat happens if Hadoop is not installed?\n\n\nInstallation will be successful, however Hadoop Configuration page in dtManage (e.g. http://localhost:9090) will expect hadoop binary (/usr/bin/hadoop) \n DFS location.\n\n\n\n\nInstall Hadoop > 2.2.0 and update the configuration parameters above.\n\n\nHow do I check if Hadoop is installed and running correctly?\n\n\nFollowing commands can be used to confirm installed Hadoop version and if Hadoop services are running.\n\n\n$ hadoop version\n\nHadoop 2.4.0\nSubversion [http://svn.apache.org/repos/asf/hadoop/common](http://svn.apache.org/repos/asf/hadoop/common)\u00a0-r\n1583262\nCompiled by jenkins on 2014-03-31T08:29Z\nCompiled with protoc 2.5.0\nFrom source with checksum 375b2832a6641759c6eaf6e3e998147\nThis command was run using\n/usr/local/hadoop/share/hadoop/common/hadoop-common-2.4.0.jar\n\n$ jps\n\n10211 NameNode\n10772 ResourceManager\n10427 DataNode\n14691 Jps\n10995 NodeManager\n\n\n\nWhat happens if the downloaded file is corrupted?\n\n\nMD5 checksum will result in the following error:\n\n\n\u201cVerifying archive integrity...Error in MD5 checksums: \nMD5 checksum\n is different from \nMD5 checksum\n\u201d.\n\n\n\nDownloaded installer could be corrupted.  Try downloading the installer again.  If using command line, use \ncurl\n instead of \nwget\n.\n\n\nWhy do I see the following permissions errors?\n\n\nDuring installation following error message will be seen on screen\n\n\n\n\nThese typically happen when specified directory does not exist, and the installation user does not have permissions to create it.  Following commands can be executed by user with sufficient privileges to resolve this issue:\n\n\n$ hadoop dfs -ls /user/\nUSER\n/datatorrent\n$ hadoop dfs -mkdir /user/\nUSER\n/datatorrent  \n$ hadoop dfs -chown \nUSER\n /user/\nUSER\n/datatorrent\n\n\n\nUpgrade\n\n\nLicense agent errors cause problems during upgrade from DataTorrent RTS 2.0 to 3.0.\n\n\nIf your applications are being launched continuously, or you are unable to launch apps due to licensing errors, try running complete uninstall and re-installation of DataTorrent RTS.  See \ninstallation guide\n for details.\n\n\nConfiguration\n\n\nConfiguring Memory\n\n\nConfiguring Operator Memory:\n\n\nOperator memory for an operator can be configured in one of the following two ways:\n\n\n1 Using the same default values for all the operators: \n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.operator.*.attr.MEMORY_MB\n/name\n\n  \nvalue\n2048\n/value\n\n\n/property\n\n\n\n\nThis would set 2GB as size of all the operators in the given application.\n\n\n2 Setting specific value for a particular operator: Following example will set 8GB as the operator memory for operator \nOp\n.\n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.operator.Op.attr.MEMORY_MB\n/name\n\n  \nvalue\n8192\n/value\n\n\n/property\n\n\n\n\nThe amount of memory required by an operator should be based on maximum amount of data that operator will be storing in-memory for all the fields -- both transient and non-transient. Default value for this attribute is 1024 MB.\n\n\nConfiguring Buffer Server Memory:\n\n\nThere is a buffer server in each container hosting an operator with an output port connected to an input port outside the container. The buffer server memory of a container can be controlled by BUFFER_MEMORY_MB. This can be configured in one of the following ways:\n\n\n1 Using the same default values for all the output ports of all the operators\n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.operator.*.port.*.attr.BUFFER_MEMORY_MB\n/name\n\n  \nvalue\n128\n/value\n\n\n/property\n\n\n\n\nThis sets 128Mb as buffer memory for all the output ports of all the operators.\n\n\n2 Setting specific value for a particular output port of particular operator: Following example sets 1GB as buffer memory for output port \np\n of an operator \nOp\n:\n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.operator.Op.port.p.attr.BUFFER_MEMORY_MB\n/name\n\n  \nvalue\n1024\n/value\n\n\n/property\n\n\n\n\nDefault value for this attribute is 512 MB\n\n\nCalculating Container memory:\n\n\nFollowing formula is used to calculate the container memory.\n\n\nContainer Memory = Sum of MEMORY_MB of All the operators in the container+ Sum of BUFFER_MEMORY_MB of all the output ports that have a sink in a different container.\n\n\n\nSometimes the memory allocated to the container is not same as calculated by the above formula, it is because actual container memory allocated by RM has to lie between\n\n\n[yarn.scheduler.minimum-allocation-mb, yarn.scheduler.maximum-allocation-mb]\n\n\n\nThese values can be found in yarn configuration\n\n\nConfiguring Application Master Memory:\n\n\nApplication Master memory can be configured using MASTER_MEMORY_MB attribute. Following example sets 4GB as the memory for Application Master:\n\n\nproperty\n\n  \nname\ndt.application.\nAPPLICATION_NAME\n.attr.MASTER_MEMORY_MB\n/name\n\n  \nvalue\n4096\n/value\n\n\n/property\n\n\n\n\nDefault value for this attribute is 1024 MB. You may need to increase this value if you are running a big application that has large number of containers\n\n\nDevelopment\n\n\nHadoop dependencies conflicts\n\n\nYou have to make sure that the hadoop jars are not bundled with the application package o/w they may conflict with the versions available in hadoop classpath. Here are some of the ways to exclude hadoop dependencies from the application package\n\n\n\n\n\n\nIf your application is directly dependent on the hadoop jars, make sure that the scope of the dependency is \nprovided\n. For eg if your application is dependent on hadoop-common, this is how you should add the dependency in pom.xml\n\n\ndependency\n\n  \ngroupId\norg.apache.hadoop\n/groupId\n\n  \nartifactId\nhadoop-common\n/artifactId\n\n  \nversion\n2.2.0\n/version\n\n  \nscope\nprovided\n/scope\n\n\n/dependency\n\n\n\n\n\n\n\n\nIf your application has trasitive dependency on hadoop jars, make sure that hadoop jars are excluded from the transitive dependency and added back as application depedency with provided scope as mentioned above. Exclusions in pom.xml can be set as follows\n\n\ndependency\n\n  \ngroupId\n/groupId\n\n  \nartifactId\n/artifactId\n\n  \nversion\n/version\n\n  \nexclusions\n\n    \nexclusion\n\n      \ngroupId\norg.apache.hadoop\n/groupId\n\n      \nartifactId\n*\n/artifactId\n\n    \n/exclusion\n\n  \n/exclusions\n\n\n/dependency\n\n\n\n\n\n\n\n\nGetting this message in STRAM logs. Is anything wrong in my code?\n\n\n2015-10-09 04:31:06,749 INFO com.datatorrent.stram.StreamingContainerManager: Heartbeat for unknown operator 3 (container container_1443694550865_0150_01_000007)\n\n\n\nComing soon.\n\n\nDebugging\n\n\nHow to remote debug gateway service?\n\n\nUpdate hadoop OPTS variable by running,\n\n\nexport HADOOP_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5432 $HADOOP_OPTS\n\n\n\nHow to setup DEBUG level while running an application?\n\n\nAdd the property :\n\n\nproperty\n\n  \nname\ndt.application.\nAPP-NAME\n.attr.DEBUG\n/name\n\n  \nvalue\ntrue\n/value\n\n\n/property\n\n\n\n\nMy gateway is throwing the following exception.\n\n\n  ERROR YARN Resource Manager has problem: java.net.ConnectException: Call From myexample.com/192.168.3.21 to 0.0.0.0:8032\u00a0failed on connection\n  exception: java.net.ConnectException: Connection refused; For more  details see:[http://wiki.apache.org/hadoop/ConnectionRefused](http://wiki.apache.org/hadoop/ConnectionRefused)\u00a0at\n  sun.reflect.GeneratedConstructorAccessor27.newInstance(Unknown Source)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  ...\n\n\n\nCheck if the host where gateway is running has yarn-site.xml file. You need to have all Hadoop configuration files accessible to dtGateway for it to run successfully.\n\n\nApplication throwing following Kryo exception.\n\n\n  com.esotericsoftware.kryo.KryoException: Class cannot be created (missing no-arg constructor):\n\n\n\nThis means that Kryo is not able to deserialize the object because the type is missing default constructor. There are couple of ways to address this exception\n\n\n\n\nAdd default constructor to the type in question. \n\n\n\n\nUsing \ncustom serializer\n for the type in question. Some existing alternative serializers can be found at \nhttps://github.com/magro/kryo-serializers\n. A custom serializer can be used as follows:\n\n\n2.1 Using Kryo's @FieldSerializer.Bind annotation for the field causing the exception. Here is how to bind custom serializer.\n\n\n@FieldSerializer.Bind(CustomSerializer.class)\nSomeType someType\n\n\n\nKryo will use this CustomSerializer to serialize and deserialize type SomeType.\n\n\n2.2 Using custom serializer with stream codec. You need to define custom stream codec and attach this custome codec to the input port that is expecting the type in question. Following is an example of creating custom stream codec:\n\n\nimport java.io.IOException;\nimport java.io.ObjectInputStream;\nimport java.util.UUID;\nimport com.esotericsoftware.kryo.Kryo;\n\npublic class CustomSerializableStreamCodec\nT\n extends com.datatorrent.lib.codec.KryoSerializableStreamCodec\nT\n\n{\n    private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException\n    {\n        in.defaultReadObject();\n        this.kryo = new Kryo();\n        this.kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n        this.kryo.register(SomeType.class, new CustomSerializer()); // Register the types along with custom serializers\n    }\n\n    private static final long serialVersionUID = 201411031405L;\n}\n\n\n\nLet's say there is an Operator \nCustomOperator\n with an input port \ninput\n that expects type SomeType. Following is how to use above defined custom stream codec\n\n\nCustomOperator op = dag.addOperator(\"CustomOperator\", new CustomOperator());\nCustomSerializableStreamCodec\nSomeType\n codec = new CustomSerializableStreamCodec\nSomeType\n();\ndag.setInputPortAttribute(op.input, Context.PortContext.STREAM_CODEC, codec);\n\n\n\nThis works only when the type is passed between different operators. If the type is part of the operator state, please use one of the above two ways. \n\n\n\n\n\n\nLog analysis\n\n\nThere are multiple ways to adjust logging levels.  For details see \nconfiguraiton guide\n.\n\n\nHow to check STRAM logs\n\n\nYou can get STRAM logs by retrieving YARN logs from command line, or by using dtManage web interface.  \n\n\nIn dtManage console, select first container from the Containers List in Physical application view.  The first container is numbered 000001. Then click the logs dropdown and select the log you want to look at.  \n\n\nAlternatively, the following command can retrieve all application logs, where the first container includes the STRAM log.\n\n\nyarn logs -applicationId \napplicationId\n\n\n\n\nHow to check application logs\n\n\nOn dt console, select a container from the Containers List widget\n(default location of this widget is in the \u201cphysical\u201d dashboard). Then\nclick the logs dropdown and select the log you want to look at.\n\n\n\n\nHow to check killed operator\u2019s state\n\n\nOn dt console, click on \u201cretrieve killed\u201d button of container List.\nContainers List widget\u2019s default location is on the \u201cphysical\u201d\ndashboard. Then select the appropriate container of killed operator and\ncheck the state.\n\n\n\n\nHow to search for particular any application or container?\n\n\nIn applications or containers table there is search text box. You can\ntype in application name or container number to locate particular\napplication or container.\n\n\nHow do I search within logs?\n\n\nOnce you navigate to the logs page,  \n\n\n\n\nDownload log file to search using your preferred editor  \n\n\nuse \u201cgrep\u201d option and provide the search range \u201cwithin specified range\u201d or \u201cover entire log\u201d\n\n\n\n\nLaunching Applications\n\n\nApplication goes from accepted state to Finished(FAILED) state\n\n\nCheck if your application name conflicts with any of the already running\napplications in your cluster. Apex do not allow two application with\nsame names run simultaneously.\n\nYour STRAM logs will have following error:\n\n\u201cForced shutdown due to Application master failed due to application\n\\\nappId> with duplicate application name \\\nappName> by the same user\n\\\nuser name> is already started.\u201d  \n\n\nConstraintViolationException while application launch\n\n\nCheck if all @NotNull properties of application are set. Apex operator\nproperties are meant to configure parameter to operators. Some of the\nproperties are must have, marked as @NotNull, to use an operator. If you\ndon\u2019t set any of such @NotNull properties application launch will fail\nand stram will throw ConstraintViolationException. \u00a0  \n\n\nEvents\n\n\nHow to check container failures\n\n\nIn StramEvents list (default location of this widget is in the \u201clogical\u201d\ndashboard), look for event \u201cStopContainer\u201d. Click on \u201cdetails\u201d button in\nfront of event to get details of container failure.\n\n\nHow to search within events\n\n\nYou can search events in specified time range. Select \u201crange\u201d mode in\nStramEvents widget. Then select from and to timestamp and hit the search\nbutton.\n\n\ntail vs range mode\n\n\ntail mode allows you to see events as they come in while range mode\nallows you to search for events by time range.\n\n\nWhat is \u201cfollowing\u201d button in events pane\n\n\nWhen we enable \u201cfollowing\u201d button the stram events list will\nautomatically scroll to bottom when new events come in.\n\n\nHow do I get a heap dump when a container gets an OutOfMemoryError ?\n\n\nThe JVM has a special option for triggering a heap dump when an Out Of Memory error\noccurs, as well an associated option for specifying the name of the file to contain\nthe dump namely \n-XX:+HeapDumpOnOutOfMemoryError\n and \n-XX:HeapDumpPath=/tmp/op.heapdump\n.\nTo add them to a specific operator, use this stanza in your configuration file\nwith \nOPERATOR_NAME\n replaced by the actual name of an operator:\n\n\n    \nproperty\n\n      \nname\ndt.operator.\nOPERATOR_NAME\n.attr.JVM_OPTIONS\n/name\n\n      \nvalue\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/op.heapdump\n/value\n\n    \n/property\n\n\n\n\nTo add them to all your containers, add this stanza to your configuration file:\n\n\n    \nproperty\n\n      \nname\ndt.attr.CONTAINER_JVM_OPTIONS\n/name\n\n      \nvalue\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/op.heapdump\n/value\n\n    \n/property\n\n\n\n\nWith these options, when an \nOutOfMemoryError\n occurs, the JVM writes the heap dump to the\nfile \n/tmp/op.heapdump\n; you'll then need to retrieve the file from the node on which the\noperator was running.\n\n\nYou can use the tool \njmap\n (bundled with the JDK) to get a heap dump from a running\ncontainer. Depending on the environment, you might need to run it as root and/or use\nthe \n-F\n option; here is a sample invocation on the sandbox:\n\n\ndtadmin@dtbox:~$ sudo jmap -dump:format=b,file=dump.bin -F 15557\nAttaching to process ID 15557, please wait...\nDebugger attached successfully.\nServer compiler detected.\nJVM version is 24.79-b02\nDumping heap to dump.bin ...\nHeap dump file created\n\n\n\nThe heap dump shows the content of the entire heap in binary form and, as such, is\nnot human readable and needs special tools such as\n\njhat\n or\n\nMAT\n to analyze it.\n\n\nThe former (\njhat\n) is bundled as part of the JDK distribution, so it is very convenient\nto use. When run on a file containing a heap dump, it parses the file and makes the data\nviewable via a browser on port 7000 of the local host. Here is a typical run:\n\n\ntmp: jhat op.heapdump \nReading from op.heapdump...\nDump file created Fri Feb 26 14:06:48 PST 2016\nSnapshot read, resolving...\nResolving 70966 objects...\nChasing references, expect 14 dots..............\nEliminating duplicate references..............\nSnapshot resolved.\nStarted HTTP server on port 7000\nServer is ready.\n\n\n\nIt is important to remember that a heap dump is different from a thread dump. The\nlatter shows the stack trace of every thread running in the container and is useful\nwhen threads are deadlocked.\nAdditional information on tools related to both types of dumps is available\n\nhere\n.\n\n\nComing Soon\n\n\n\n\nConnection Refused Exception\n\n\nClassNotFound Exception\n\n\nLaunching apa vs jar\n\n\nDAG validation failed\n\n\nMultiple gateways running simultaneously, app not launched.\n\n\nHDFS in safe mode\n\n\nApplication stays in accepted state\n\n\nSome containers do not get resources (specially in case of repartition)\n\n\nInsufficient memory set to operator causes operator kill continuously.\n\n\n\n\nWhy is the number of events same/different at input and output port of each operator?\n\n\n\n\n\n\nShutdown vs kill option\n\n\n\n\nWhy shutdown doesn\u2019t work? (if some containers are not running)\n\n\nCan I kill multiple applications at same time?\n\n\nKilling containers vs killing application\n\n\nSTRAM failures (during define partitions)\n\n\nThread local + partition parallel configuration\n\n\nWhat to do when downstream operators are slow than the input  operators.\n\n\nI am seeing high latency, what to do?\n\n\nappConf in ADT (inside apa file) vs conf option in dtcli\n\n\nApplication keeps restarting (has happened once due to license agent during upgrade)\n\n\nOperator getting killed after every 60 secs (Timeout issue)\n\n\nHow to change commit frequency\n\n\nDifference between exactly once, at least once and at most once\n\n\nThread local vs container local vs node local\n\n\nCluster nodes not able to access edge node where Gateway is running\n\n\n\n\nDevelopers not sure when to process incoming tuples in end window or when to do it in process function of operator\n\n\n\n\n\n\nHow partitioning works\n\n\n\n\nHow the data is partitioned between different partitions.\n\n\nHow to use stream-codec\n\n\nData on which ports is partitioned? By default default partitioner partitions data on first port.\n\n\nHow to enable stream-codec on multiple ports. (Join operator?? where both input-ports needs to receive same set of keys).\n\n\n\n\n\n\n\n\npom dependency management, exclusions etc. eg: Malhar library and\n    contrib, Hive (includes hadoop dependencies, we need to explicitly\n    exclude), Jersey(we work only with 1.9 version) etc\n\n\n\n\n\n\nAll non-transient members of the operator object need to be\n    serializable. All members that are not serializable cannot be saved\n    during checkpoint and must be declared transient (e.g. connection\n    objects). This is such a common problem that we need to dedicate a\n    section to it.\n\n\n\n\n\n\nExactly once processing mode. Commit operation is supposed to be\n    done at endWindow. This is only best-effort exactly once and not\n    100% guaranteed exactly once because operators may go down after\n    endWindow and before checkpointing finishes.\n\n\n\n\n\n\nHow to check checkpoint size. (large checkpoint size cause instability in the DAG).\n\n\n\n\nHow to add custom metrics and metric aggregator.\n\n\nExample of how to implement dynamic partitioning.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#troubleshooting-datatorrent-rts", 
            "text": "", 
            "title": "Troubleshooting DataTorrent RTS"
        }, 
        {
            "location": "/troubleshooting/#download", 
            "text": "", 
            "title": "Download"
        }, 
        {
            "location": "/troubleshooting/#where-can-i-get-datatorrent-rts-software", 
            "text": "DataTorrent products are available for download from  https://www.datatorrent.com/download/   Community Edition :  It is a packaged version of Apache Apex and enables developers to quickly develop their big data streaming and batch projects.  Enterprise Edition :  Designed for enterprise production deployment and includes security, advanced monitoring and troubleshooting, graphical application assembly, and application data visualization.  Sandbox Edition :  Enterprise Edition and demo applications pre-installed and configured with a single-node Hadoop cluster running in a virtual machine.  Optimized for evaluation and training purposes.  dtIngest Application : simplifies the collection, aggregation and movement of large amounts of data to and from Hadoop and is available for production use at no cost.", 
            "title": "Where can I get DataTorrent RTS software?"
        }, 
        {
            "location": "/troubleshooting/#what-is-the-difference-between-datatorrent-rts-editions", 
            "text": "Please refer to  DataTorrent RTS editions overview", 
            "title": "What is the difference between DataTorrent RTS editions?"
        }, 
        {
            "location": "/troubleshooting/#where-can-i-find-the-standard-edition-installer", 
            "text": "You can use the download link for Enterprise edition as the package is\nsame for both editions. But, you have to apply the license to enable the\nStandard edition. You can upgrade the license by using dtManage.\nLicenses are available in 2 types : evaluation and production.", 
            "title": "Where can I find the Standard edition installer?"
        }, 
        {
            "location": "/troubleshooting/#what-are-datatorrent-rts-package-contents-of-community-vs-enterprise-edition", 
            "text": "Package contents for Community edition:   Apache Apex (incubating)  DataTorrent Demo Applications  DataTorrent dtManage  DataTorrent dtIngest   Package contents for Enterprise edition:   Apache Apex (incubating)  DataTorrent Demo Applications  DataTorrent Operator Library  DataTorrent Enterprise Security  DataTorrent dtManage  DataTorrent dtIngest  DataTorrent dtAssemble  DataTorrent dtDashboard", 
            "title": "What are DataTorrent RTS package contents of Community vs Enterprise edition?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-confirm-the-package-downloaded-correctly", 
            "text": "You can verify the downloaded DataTorrent RTS package by comparing with\nMD5 sum. The command to get md5 sum of downloaded package:  # md5sum  DT_RTS_Package   Compare the result with MD5 sum posted on the product download page.", 
            "title": "How do I confirm the package downloaded correctly?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-download-the-datatorrent-rts-package-using-cli", 
            "text": "Use following curl command to download DataTorrent RTS package:  curl -LSO\u00a0 DT_RTS_download_link   We recommend using \u2018curl\u2019 instead of \u2018wget\u2019, which lacks chunked transfer encoding support, potentially resulting in corrupted downloads.", 
            "title": "How do I download the DataTorrent RTS package using CLI?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-prerequisites-of-datatorrent-rts", 
            "text": "DataTorrent RTS platform has following Hadoop cluster requirements:   Operating system supported by Hadoop distribution  Hadoop (2.2 or above) cluster with HDFS, YARN configured. Make sure\n    hadoop executable available in PATH variable  Java 7 or 8 as supported by Hadoop distribution  Minimum of 8G RAM available on the Hadoop cluster  Permissions to create HDFS directory for DataTorrent user  Google Chrome, Firefox, or Safari to access dtManage (DataTorrent UI\n    console)", 
            "title": "What are the prerequisites of DataTorrent RTS ?"
        }, 
        {
            "location": "/troubleshooting/#where-can-i-start-from-after-downloading-datatorrent-rts", 
            "text": "After successful download of DataTorrent RTS, make sure all prerequisites are satisfied.  You will need to install DataTorrent RTS on Hadoop cluster using the downloaded installer. Refer to  installation guide  Once installed, you will be prompted to proceed to dtManage, the DataTorrent management console, where you will be able to launch and manage applications.", 
            "title": "Where can I start from after downloading DataTorrent RTS?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-supported-hadoop-distribution-by-datatorrent-rts", 
            "text": "DataTorrent RTS is a Hadoop 2.x native application and is fully\nintegrated with YARN and HDFS providing tight integration with any\nApache Hadoop 2.x based distribution.       Hadoop Distribution  Supported Version    Amazon EMR  Hadoop 2.4 and higher    Apache Hadoop  Hadoop 2.2 and higher    Cloudera  CDH 5.0 and higher    Hortonworks  HDP 2.0 and higher    MapR  4.0 and higher    Microsoft  HDInsight    Pivotal  2.1 and higher", 
            "title": "What are the supported Hadoop distribution by DataTorrent RTS?"
        }, 
        {
            "location": "/troubleshooting/#what-is-the-datatorrent-sandbox", 
            "text": "The Sandbox provides a quick and simple way to experience DataTorrent RTS without setting up and managing a complete Hadoop cluster. The Sandbox contains pre-installed DataTorrent RTS Enterprise Edition along with all the Hadoop services required to launch and run the included demo applications.", 
            "title": "What is the Datatorrent Sandbox?"
        }, 
        {
            "location": "/troubleshooting/#where-do-i-get-datatorrent-sandbox-download-link", 
            "text": "Sandbox can be downloaded by visiting  datatorrent.com/download", 
            "title": "Where do I get DataTorrent Sandbox download link?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-system-requirements-for-sandbox-deployment", 
            "text": "The DataTorrent RTS Sandbox is a complete, stand-alone, instance of the\nEnterprise Edition as a single-node Hadoop cluster on your local\nmachine. Following are prerequisites for DataTorrent RTS:   VirtualBox  4.3 or greater installed.  6GB RAM or greater available for Sandbox VM.", 
            "title": "What are the system requirements for sandbox deployment?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-datatorrent-rts-package-content-details-in-sandbox", 
            "text": "Ubuntu 12.04 LTS + Apache Hadoop 2.2 (DataTorrent RTS 3.1 or earlier)  Lubuntu 14.04 LTS + Apache Hadoop 2.7 (DataTorrent RTS 3.2 or later)  Apache Apex (incubating), Apache Apex-Malhar (incubating)  DataTorrent Operator Library  DataTorrent Enterprise Security  DataTorrent dtManage  DataTorrent dtIngest  DataTorrent dtAssemble  DataTorrent dtDashboard  Demo Applications", 
            "title": "What are the DataTorrent RTS package content details in sandbox?"
        }, 
        {
            "location": "/troubleshooting/#what-is-dtingest-applicaiton", 
            "text": "DataTorrent dtIngest simplifies the collection, aggregation and movement of large amounts of data to and from Hadoop and is available for production use at no cost.", 
            "title": "What is dtIngest applicaiton?"
        }, 
        {
            "location": "/troubleshooting/#where-do-i-get-dtingest-application", 
            "text": "Application can be downloaded by visiting  datatorrent.com/download", 
            "title": "Where do I get dtingest application?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-dtingest-package-contents", 
            "text": "Package comprises of DataTorrent RTS and dtIngest application.", 
            "title": "What are the dtingest package contents?"
        }, 
        {
            "location": "/troubleshooting/#what-are-the-prerequisites-of-dtingest", 
            "text": "DataTorrent RTS 3.x and above. Please refer  dtIngest tutorial \u00a0for more details.", 
            "title": "What are the prerequisites of dtIngest?"
        }, 
        {
            "location": "/troubleshooting/#where-can-i-start-from-after-downloading-dtingest", 
            "text": "Make sure all DataTorrent RTS prerequisites are satisfied before dtIngest installation.  Run the downloaded installer installer. Refer to DataTorrent RTS  installation guide .  After DataTorrent RTS installation and configuration, you can configure and launch the\n    dtIngest application from dtManage, the DataTorrent console. Refer to  dtIngest tutorial \u00a0for more details.", 
            "title": "Where can I start from after downloading dtingest?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-get-specific-dt-version", 
            "text": "You can find archive list of various DataTorrent RTS versions at the bottom of each product download page.", 
            "title": "How do I get specific DT version ?"
        }, 
        {
            "location": "/troubleshooting/#where-can-i-request-new-upgrade-current-license", 
            "text": "Please follow the instructions at  License Upgrade", 
            "title": "Where can I request new / upgrade current license?"
        }, 
        {
            "location": "/troubleshooting/#where-do-i-find-product-documentation", 
            "text": "Please refer to:  DataTorrent Documentation", 
            "title": "Where do I find product documentation?"
        }, 
        {
            "location": "/troubleshooting/#where-can-i-learn-more-about-apache-apex", 
            "text": "You can refer Apex page for more details:  Apache Apex", 
            "title": "Where can I learn more about Apache Apex?"
        }, 
        {
            "location": "/troubleshooting/#do-you-need-help", 
            "text": "You can contact us at  https://www.datatorrent.com/contact", 
            "title": "Do you need help?"
        }, 
        {
            "location": "/troubleshooting/#installation", 
            "text": "There are multiple installations available e.g. Sandbox Edition, Community Edition, Enterprise Edition, dtIngest. Supported operating systems are which support Hadoop platform (tested on CentOS 6.x and Ubuntu 12.04).", 
            "title": "Installation"
        }, 
        {
            "location": "/troubleshooting/#minimum-hardware-requirements-what-happens-if-certain-minimum-configuration-requirement-has-not-been-met", 
            "text": "Minimum of 8G RAM is required on the Hadoop cluster.", 
            "title": "Minimum hardware requirements, what happens if certain minimum configuration requirement has not been met?"
        }, 
        {
            "location": "/troubleshooting/#what-happens-if-java-is-not-installed", 
            "text": "Following message can be seen when Java is not abilable on the system  Error: java executable not found. Please ensure java\nor hadoop are installed and available in PATH environment variable\nbefore proceeding with this installation.  Install Java 7 from package manager of Linux Distribution and try running installer again.", 
            "title": "What happens if java is not installed?"
        }, 
        {
            "location": "/troubleshooting/#what-happens-if-hadoop-is-not-installed", 
            "text": "Installation will be successful, however Hadoop Configuration page in dtManage (e.g. http://localhost:9090) will expect hadoop binary (/usr/bin/hadoop)   DFS location.   Install Hadoop > 2.2.0 and update the configuration parameters above.", 
            "title": "What happens if Hadoop is not installed?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-check-if-hadoop-is-installed-and-running-correctly", 
            "text": "Following commands can be used to confirm installed Hadoop version and if Hadoop services are running.  $ hadoop version\n\nHadoop 2.4.0\nSubversion [http://svn.apache.org/repos/asf/hadoop/common](http://svn.apache.org/repos/asf/hadoop/common)\u00a0-r\n1583262\nCompiled by jenkins on 2014-03-31T08:29Z\nCompiled with protoc 2.5.0\nFrom source with checksum 375b2832a6641759c6eaf6e3e998147\nThis command was run using\n/usr/local/hadoop/share/hadoop/common/hadoop-common-2.4.0.jar\n\n$ jps\n\n10211 NameNode\n10772 ResourceManager\n10427 DataNode\n14691 Jps\n10995 NodeManager", 
            "title": "How do I check if Hadoop is installed and running correctly?"
        }, 
        {
            "location": "/troubleshooting/#what-happens-if-the-downloaded-file-is-corrupted", 
            "text": "MD5 checksum will result in the following error:  \u201cVerifying archive integrity...Error in MD5 checksums:  MD5 checksum  is different from  MD5 checksum \u201d.  Downloaded installer could be corrupted.  Try downloading the installer again.  If using command line, use  curl  instead of  wget .", 
            "title": "What happens if the downloaded file is corrupted?"
        }, 
        {
            "location": "/troubleshooting/#why-do-i-see-the-following-permissions-errors", 
            "text": "During installation following error message will be seen on screen   These typically happen when specified directory does not exist, and the installation user does not have permissions to create it.  Following commands can be executed by user with sufficient privileges to resolve this issue:  $ hadoop dfs -ls /user/ USER /datatorrent\n$ hadoop dfs -mkdir /user/ USER /datatorrent  \n$ hadoop dfs -chown  USER  /user/ USER /datatorrent", 
            "title": "Why do I see the following permissions errors?"
        }, 
        {
            "location": "/troubleshooting/#upgrade", 
            "text": "", 
            "title": "Upgrade"
        }, 
        {
            "location": "/troubleshooting/#license-agent-errors-cause-problems-during-upgrade-from-datatorrent-rts-20-to-30", 
            "text": "If your applications are being launched continuously, or you are unable to launch apps due to licensing errors, try running complete uninstall and re-installation of DataTorrent RTS.  See  installation guide  for details.", 
            "title": "License agent errors cause problems during upgrade from DataTorrent RTS 2.0 to 3.0."
        }, 
        {
            "location": "/troubleshooting/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/troubleshooting/#configuring-memory", 
            "text": "", 
            "title": "Configuring Memory"
        }, 
        {
            "location": "/troubleshooting/#configuring-operator-memory", 
            "text": "Operator memory for an operator can be configured in one of the following two ways:  1 Using the same default values for all the operators:   property \n   name dt.application. APPLICATION_NAME .operator.*.attr.MEMORY_MB /name \n   value 2048 /value  /property   This would set 2GB as size of all the operators in the given application.  2 Setting specific value for a particular operator: Following example will set 8GB as the operator memory for operator  Op .  property \n   name dt.application. APPLICATION_NAME .operator.Op.attr.MEMORY_MB /name \n   value 8192 /value  /property   The amount of memory required by an operator should be based on maximum amount of data that operator will be storing in-memory for all the fields -- both transient and non-transient. Default value for this attribute is 1024 MB.", 
            "title": "Configuring Operator Memory:"
        }, 
        {
            "location": "/troubleshooting/#configuring-buffer-server-memory", 
            "text": "There is a buffer server in each container hosting an operator with an output port connected to an input port outside the container. The buffer server memory of a container can be controlled by BUFFER_MEMORY_MB. This can be configured in one of the following ways:  1 Using the same default values for all the output ports of all the operators  property \n   name dt.application. APPLICATION_NAME .operator.*.port.*.attr.BUFFER_MEMORY_MB /name \n   value 128 /value  /property   This sets 128Mb as buffer memory for all the output ports of all the operators.  2 Setting specific value for a particular output port of particular operator: Following example sets 1GB as buffer memory for output port  p  of an operator  Op :  property \n   name dt.application. APPLICATION_NAME .operator.Op.port.p.attr.BUFFER_MEMORY_MB /name \n   value 1024 /value  /property   Default value for this attribute is 512 MB", 
            "title": "Configuring Buffer Server Memory:"
        }, 
        {
            "location": "/troubleshooting/#calculating-container-memory", 
            "text": "Following formula is used to calculate the container memory.  Container Memory = Sum of MEMORY_MB of All the operators in the container+ Sum of BUFFER_MEMORY_MB of all the output ports that have a sink in a different container.  Sometimes the memory allocated to the container is not same as calculated by the above formula, it is because actual container memory allocated by RM has to lie between  [yarn.scheduler.minimum-allocation-mb, yarn.scheduler.maximum-allocation-mb]  These values can be found in yarn configuration", 
            "title": "Calculating Container memory:"
        }, 
        {
            "location": "/troubleshooting/#configuring-application-master-memory", 
            "text": "Application Master memory can be configured using MASTER_MEMORY_MB attribute. Following example sets 4GB as the memory for Application Master:  property \n   name dt.application. APPLICATION_NAME .attr.MASTER_MEMORY_MB /name \n   value 4096 /value  /property   Default value for this attribute is 1024 MB. You may need to increase this value if you are running a big application that has large number of containers", 
            "title": "Configuring Application Master Memory:"
        }, 
        {
            "location": "/troubleshooting/#development", 
            "text": "", 
            "title": "Development"
        }, 
        {
            "location": "/troubleshooting/#hadoop-dependencies-conflicts", 
            "text": "You have to make sure that the hadoop jars are not bundled with the application package o/w they may conflict with the versions available in hadoop classpath. Here are some of the ways to exclude hadoop dependencies from the application package    If your application is directly dependent on the hadoop jars, make sure that the scope of the dependency is  provided . For eg if your application is dependent on hadoop-common, this is how you should add the dependency in pom.xml  dependency \n   groupId org.apache.hadoop /groupId \n   artifactId hadoop-common /artifactId \n   version 2.2.0 /version \n   scope provided /scope  /dependency     If your application has trasitive dependency on hadoop jars, make sure that hadoop jars are excluded from the transitive dependency and added back as application depedency with provided scope as mentioned above. Exclusions in pom.xml can be set as follows  dependency \n   groupId /groupId \n   artifactId /artifactId \n   version /version \n   exclusions \n     exclusion \n       groupId org.apache.hadoop /groupId \n       artifactId * /artifactId \n     /exclusion \n   /exclusions  /dependency", 
            "title": "Hadoop dependencies conflicts"
        }, 
        {
            "location": "/troubleshooting/#getting-this-message-in-stram-logs-is-anything-wrong-in-my-code", 
            "text": "2015-10-09 04:31:06,749 INFO com.datatorrent.stram.StreamingContainerManager: Heartbeat for unknown operator 3 (container container_1443694550865_0150_01_000007)  Coming soon.", 
            "title": "Getting this message in STRAM logs. Is anything wrong in my code?"
        }, 
        {
            "location": "/troubleshooting/#debugging", 
            "text": "", 
            "title": "Debugging"
        }, 
        {
            "location": "/troubleshooting/#how-to-remote-debug-gateway-service", 
            "text": "Update hadoop OPTS variable by running,  export HADOOP_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5432 $HADOOP_OPTS", 
            "title": "How to remote debug gateway service?"
        }, 
        {
            "location": "/troubleshooting/#how-to-setup-debug-level-while-running-an-application", 
            "text": "Add the property :  property \n   name dt.application. APP-NAME .attr.DEBUG /name \n   value true /value  /property", 
            "title": "How to setup DEBUG level while running an application?"
        }, 
        {
            "location": "/troubleshooting/#my-gateway-is-throwing-the-following-exception", 
            "text": "ERROR YARN Resource Manager has problem: java.net.ConnectException: Call From myexample.com/192.168.3.21 to 0.0.0.0:8032\u00a0failed on connection\n  exception: java.net.ConnectException: Connection refused; For more  details see:[http://wiki.apache.org/hadoop/ConnectionRefused](http://wiki.apache.org/hadoop/ConnectionRefused)\u00a0at\n  sun.reflect.GeneratedConstructorAccessor27.newInstance(Unknown Source)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  ...  Check if the host where gateway is running has yarn-site.xml file. You need to have all Hadoop configuration files accessible to dtGateway for it to run successfully.", 
            "title": "My gateway is throwing the following exception."
        }, 
        {
            "location": "/troubleshooting/#application-throwing-following-kryo-exception", 
            "text": "com.esotericsoftware.kryo.KryoException: Class cannot be created (missing no-arg constructor):  This means that Kryo is not able to deserialize the object because the type is missing default constructor. There are couple of ways to address this exception   Add default constructor to the type in question.    Using  custom serializer  for the type in question. Some existing alternative serializers can be found at  https://github.com/magro/kryo-serializers . A custom serializer can be used as follows:  2.1 Using Kryo's @FieldSerializer.Bind annotation for the field causing the exception. Here is how to bind custom serializer.  @FieldSerializer.Bind(CustomSerializer.class)\nSomeType someType  Kryo will use this CustomSerializer to serialize and deserialize type SomeType.  2.2 Using custom serializer with stream codec. You need to define custom stream codec and attach this custome codec to the input port that is expecting the type in question. Following is an example of creating custom stream codec:  import java.io.IOException;\nimport java.io.ObjectInputStream;\nimport java.util.UUID;\nimport com.esotericsoftware.kryo.Kryo;\n\npublic class CustomSerializableStreamCodec T  extends com.datatorrent.lib.codec.KryoSerializableStreamCodec T \n{\n    private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException\n    {\n        in.defaultReadObject();\n        this.kryo = new Kryo();\n        this.kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n        this.kryo.register(SomeType.class, new CustomSerializer()); // Register the types along with custom serializers\n    }\n\n    private static final long serialVersionUID = 201411031405L;\n}  Let's say there is an Operator  CustomOperator  with an input port  input  that expects type SomeType. Following is how to use above defined custom stream codec  CustomOperator op = dag.addOperator(\"CustomOperator\", new CustomOperator());\nCustomSerializableStreamCodec SomeType  codec = new CustomSerializableStreamCodec SomeType ();\ndag.setInputPortAttribute(op.input, Context.PortContext.STREAM_CODEC, codec);  This works only when the type is passed between different operators. If the type is part of the operator state, please use one of the above two ways.", 
            "title": "Application throwing following Kryo exception."
        }, 
        {
            "location": "/troubleshooting/#log-analysis", 
            "text": "There are multiple ways to adjust logging levels.  For details see  configuraiton guide .", 
            "title": "Log analysis"
        }, 
        {
            "location": "/troubleshooting/#how-to-check-stram-logs", 
            "text": "You can get STRAM logs by retrieving YARN logs from command line, or by using dtManage web interface.    In dtManage console, select first container from the Containers List in Physical application view.  The first container is numbered 000001. Then click the logs dropdown and select the log you want to look at.    Alternatively, the following command can retrieve all application logs, where the first container includes the STRAM log.  yarn logs -applicationId  applicationId", 
            "title": "How to check STRAM logs"
        }, 
        {
            "location": "/troubleshooting/#how-to-check-application-logs", 
            "text": "On dt console, select a container from the Containers List widget\n(default location of this widget is in the \u201cphysical\u201d dashboard). Then\nclick the logs dropdown and select the log you want to look at.", 
            "title": "How to check application logs"
        }, 
        {
            "location": "/troubleshooting/#how-to-check-killed-operators-state", 
            "text": "On dt console, click on \u201cretrieve killed\u201d button of container List.\nContainers List widget\u2019s default location is on the \u201cphysical\u201d\ndashboard. Then select the appropriate container of killed operator and\ncheck the state.", 
            "title": "How to check killed operator\u2019s state"
        }, 
        {
            "location": "/troubleshooting/#how-to-search-for-particular-any-application-or-container", 
            "text": "In applications or containers table there is search text box. You can\ntype in application name or container number to locate particular\napplication or container.", 
            "title": "How to search for particular any application or container?"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-search-within-logs", 
            "text": "Once you navigate to the logs page,     Download log file to search using your preferred editor    use \u201cgrep\u201d option and provide the search range \u201cwithin specified range\u201d or \u201cover entire log\u201d", 
            "title": "How do I search within logs?"
        }, 
        {
            "location": "/troubleshooting/#launching-applications", 
            "text": "", 
            "title": "Launching Applications"
        }, 
        {
            "location": "/troubleshooting/#application-goes-from-accepted-state-to-finishedfailed-state", 
            "text": "Check if your application name conflicts with any of the already running\napplications in your cluster. Apex do not allow two application with\nsame names run simultaneously. \nYour STRAM logs will have following error: \n\u201cForced shutdown due to Application master failed due to application\n\\ appId> with duplicate application name \\ appName> by the same user\n\\ user name> is already started.\u201d", 
            "title": "Application goes from accepted state to Finished(FAILED) state"
        }, 
        {
            "location": "/troubleshooting/#constraintviolationexception-while-application-launch", 
            "text": "Check if all @NotNull properties of application are set. Apex operator\nproperties are meant to configure parameter to operators. Some of the\nproperties are must have, marked as @NotNull, to use an operator. If you\ndon\u2019t set any of such @NotNull properties application launch will fail\nand stram will throw ConstraintViolationException.", 
            "title": "ConstraintViolationException while application launch"
        }, 
        {
            "location": "/troubleshooting/#events", 
            "text": "", 
            "title": "Events"
        }, 
        {
            "location": "/troubleshooting/#how-to-check-container-failures", 
            "text": "In StramEvents list (default location of this widget is in the \u201clogical\u201d\ndashboard), look for event \u201cStopContainer\u201d. Click on \u201cdetails\u201d button in\nfront of event to get details of container failure.", 
            "title": "How to check container failures"
        }, 
        {
            "location": "/troubleshooting/#how-to-search-within-events", 
            "text": "You can search events in specified time range. Select \u201crange\u201d mode in\nStramEvents widget. Then select from and to timestamp and hit the search\nbutton.", 
            "title": "How to search within events"
        }, 
        {
            "location": "/troubleshooting/#tail-vs-range-mode", 
            "text": "tail mode allows you to see events as they come in while range mode\nallows you to search for events by time range.", 
            "title": "tail vs range mode"
        }, 
        {
            "location": "/troubleshooting/#what-is-following-button-in-events-pane", 
            "text": "When we enable \u201cfollowing\u201d button the stram events list will\nautomatically scroll to bottom when new events come in.", 
            "title": "What is \u201cfollowing\u201d button in events pane"
        }, 
        {
            "location": "/troubleshooting/#how-do-i-get-a-heap-dump-when-a-container-gets-an-outofmemoryerror", 
            "text": "The JVM has a special option for triggering a heap dump when an Out Of Memory error\noccurs, as well an associated option for specifying the name of the file to contain\nthe dump namely  -XX:+HeapDumpOnOutOfMemoryError  and  -XX:HeapDumpPath=/tmp/op.heapdump .\nTo add them to a specific operator, use this stanza in your configuration file\nwith  OPERATOR_NAME  replaced by the actual name of an operator:       property \n       name dt.operator. OPERATOR_NAME .attr.JVM_OPTIONS /name \n       value -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/op.heapdump /value \n     /property   To add them to all your containers, add this stanza to your configuration file:       property \n       name dt.attr.CONTAINER_JVM_OPTIONS /name \n       value -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/op.heapdump /value \n     /property   With these options, when an  OutOfMemoryError  occurs, the JVM writes the heap dump to the\nfile  /tmp/op.heapdump ; you'll then need to retrieve the file from the node on which the\noperator was running.  You can use the tool  jmap  (bundled with the JDK) to get a heap dump from a running\ncontainer. Depending on the environment, you might need to run it as root and/or use\nthe  -F  option; here is a sample invocation on the sandbox:  dtadmin@dtbox:~$ sudo jmap -dump:format=b,file=dump.bin -F 15557\nAttaching to process ID 15557, please wait...\nDebugger attached successfully.\nServer compiler detected.\nJVM version is 24.79-b02\nDumping heap to dump.bin ...\nHeap dump file created  The heap dump shows the content of the entire heap in binary form and, as such, is\nnot human readable and needs special tools such as jhat  or MAT  to analyze it.  The former ( jhat ) is bundled as part of the JDK distribution, so it is very convenient\nto use. When run on a file containing a heap dump, it parses the file and makes the data\nviewable via a browser on port 7000 of the local host. Here is a typical run:  tmp: jhat op.heapdump \nReading from op.heapdump...\nDump file created Fri Feb 26 14:06:48 PST 2016\nSnapshot read, resolving...\nResolving 70966 objects...\nChasing references, expect 14 dots..............\nEliminating duplicate references..............\nSnapshot resolved.\nStarted HTTP server on port 7000\nServer is ready.  It is important to remember that a heap dump is different from a thread dump. The\nlatter shows the stack trace of every thread running in the container and is useful\nwhen threads are deadlocked.\nAdditional information on tools related to both types of dumps is available here .", 
            "title": "How do I get a heap dump when a container gets an OutOfMemoryError ?"
        }, 
        {
            "location": "/troubleshooting/#coming-soon", 
            "text": "Connection Refused Exception  ClassNotFound Exception  Launching apa vs jar  DAG validation failed  Multiple gateways running simultaneously, app not launched.  HDFS in safe mode  Application stays in accepted state  Some containers do not get resources (specially in case of repartition)  Insufficient memory set to operator causes operator kill continuously.   Why is the number of events same/different at input and output port of each operator?    Shutdown vs kill option   Why shutdown doesn\u2019t work? (if some containers are not running)  Can I kill multiple applications at same time?  Killing containers vs killing application  STRAM failures (during define partitions)  Thread local + partition parallel configuration  What to do when downstream operators are slow than the input  operators.  I am seeing high latency, what to do?  appConf in ADT (inside apa file) vs conf option in dtcli  Application keeps restarting (has happened once due to license agent during upgrade)  Operator getting killed after every 60 secs (Timeout issue)  How to change commit frequency  Difference between exactly once, at least once and at most once  Thread local vs container local vs node local  Cluster nodes not able to access edge node where Gateway is running   Developers not sure when to process incoming tuples in end window or when to do it in process function of operator    How partitioning works   How the data is partitioned between different partitions.  How to use stream-codec  Data on which ports is partitioned? By default default partitioner partitions data on first port.  How to enable stream-codec on multiple ports. (Join operator?? where both input-ports needs to receive same set of keys).     pom dependency management, exclusions etc. eg: Malhar library and\n    contrib, Hive (includes hadoop dependencies, we need to explicitly\n    exclude), Jersey(we work only with 1.9 version) etc    All non-transient members of the operator object need to be\n    serializable. All members that are not serializable cannot be saved\n    during checkpoint and must be declared transient (e.g. connection\n    objects). This is such a common problem that we need to dedicate a\n    section to it.    Exactly once processing mode. Commit operation is supposed to be\n    done at endWindow. This is only best-effort exactly once and not\n    100% guaranteed exactly once because operators may go down after\n    endWindow and before checkpointing finishes.    How to check checkpoint size. (large checkpoint size cause instability in the DAG).   How to add custom metrics and metric aggregator.  Example of how to implement dynamic partitioning.", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/release_notes/", 
            "text": "DataTorrent RTS Release Notes\n\n\nVersion 3.2.0\n\n\nNew Feature\n\n\n\n\n[SPOI-6351] - Add feature to REST API to get queue information from cluster\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-5777] - Kafka start offset should have user option to read from latest or earliest\n\n\n[SPOI-5828] - Stackstraces should not be shown on errors\n\n\n[SPOI-6641] - Implement \"forever\" bucket in DimensionComputation\n\n\n[DTIN-40] - Observed unused variables in SplunkBytesInputOperator\n\n\n[DTIN-69] - Move Query Operators implementation to newer one\n\n\n[DTIN-50] - [dtIngest] add parameter \"parallel readers\"\n\n\n\n\nBug\n\n\n\n\n[SPOI-5104] - Ingestion: Failed to copy data when inputs include a directory and a subdirectory\n\n\n[SPOI-5216] - FileSplitter fails with ConcurrentModificationException\n\n\n[SPOI-5571] - S3 : Copying data failed with RuntimeException saying 'Unable to move file'\n\n\n[SPOI-5809] - Kryo Exception In Stateful Stream Codec When Operator Is Killed From UI and Comes Back Up\n\n\n[SPOI-5823] - Downstream container falling behind when buffer spooling is enabled\n\n\n[SPOI-5899] - Ability to retrieve schemas from an app package\n\n\n[SPOI-6053] - Schema Generator - bool getter should be isBool and not getBool\n\n\n[SPOI-6073] - AbstractFileOutputOperator not finalizing the file after the recovery\n\n\n[SPOI-6079] - Installation wizard: 'continue' button is misplaced in last step of installation\n\n\n[SPOI-6098] - 'single run' doesnt work\n\n\n[SPOI-6202] - Sandbox 3.1 has community license instead of enterprise\n\n\n[SPOI-6204] - Sandbox 3.1 loses uploaded license after reboot\n\n\n[SPOI-6222] - HDFS recovery in sandbox causes license to degrade to community\n\n\n[SPOI-6236] - Add 1s aggregations to App Data Tracker\n\n\n[SPOI-6298] - Dimensions Store Can Become Blocked\n\n\n[SPOI-6383] - Sometimes expired query is still executed.\n\n\n[SPOI-6393] - Markdown code blocks render with invalid syntax highlights in console\n\n\n[SPOI-6446] - Merge PojoEnrichment and TupleEnrichment\n\n\n[SPOI-6505] - Exceptions from asm when uploading apps\n\n\n[SPOI-6603] - Warning messages shown on console are not completely visible\n\n\n[SPOI-6709] - Temporarily Remove Methods Which Override Their Return Type In MEGH From Semver Checks\n\n\n[SPOI-6846] - DT dashboard guide link is not working\n\n\n[SPOI-6855] - Broken links observed on summary page while DT RTS configuration \n\n\n[SPOI-6856] - Correct docs index.html links and title\n\n\n[DTIN-51] - bandwidth option was removed during merge\n\n\n[DTIN-101] - For message-based-input to message-based-output, compression \n encryption options should not be visible on config UI\n\n\n[DTIN-102] - [Ingestion UI] If kafka as output type, then Brokerlist is the configuration parameter, not zookeeper quorum\n\n\n[DTIN-112] - Message based input to FTP output fails with IOException while appending the data\n\n\n[DTIN-113] - Kafka input to kafka output fails in MessageWriter with EOFException while fetching output topic metadata\n\n\n[DTIN-123] - All files are not getting copied when bandwidth option is specified\n\n\n[DTIN-124] - For kafka to hdfs, if offset is set to 'Earliest', messages are not consumed from the beginning\n\n\n[DTIN-126] - 'Compact files' option should be disabled for message based Input type\n\n\n[DTIN-129] - StreamCorruptedException while decrypting AES/PKI encrypted file\n\n\n[DTIN-130] - Text box for 'Bandwidth to use' should accept integer values only\n\n\n[DTIN-155] - Messages are dropped when bandwidth option is enabled\n\n\n[DTIN-160] - Copying data from S3 to HDFS fails with NoSuchMethodError\n\n\n[DTIN-161] - Message based input to S3N output fails with IOException while appending the data\n\n\n[DTIN-162] - JMS messages are not fully consumed in case of JMS to kafka\n\n\n[DTIN-164] - Data values in 'Table' widget flicker when encryption is enabled\n\n\n[DTIN-165] - Data source names for dtingest should not contain 'null'\n\n\n[DTIN-174] - Append doesn't work for S3, FTP filesystems and ObjectOutputStream\n\n\n[DTIN-176] - Parallel read is not working in case of FTP and S3 as input\n\n\n[DTIN-186] - FileMerger failed with unable to merge file exception\n\n\n\n\nTask\n\n\n\n\n[SPOI-5173] - DAG validation: Attribute values Serializable\n\n\n[SPOI-5403] - Ingestion Splunk integration\n\n\n[SPOI-5801] - Make a MapR partner (datatorrent) sandbox\n\n\n[SPOI-5958] - dtView Integration for ingestion metric visualization\n\n\n[SPOI-6241] - Change in enterprise license\n\n\n[SPOI-6439] - Run benchmark for 3.2.0 release\n\n\n[SPOI-6691] - Decouple malhar version from dt version in dtingest pom dependancies\n\n\n[DTIN-20] - Accept bandwidth limit in units other than byes/s in dtingest script\n\n\n[DTIN-38] - Needs to check the Query frequency option is available for Splunk Input Operator\n\n\n[DTIN-47] - Merge code from release-1.0.1 branch to ingegration-1.1.0 branch\n\n\n[DTIN-54] - Disabling the splunk from dtIngest script\n\n\n[DTIN-71] - Integrate release 1.0.1 branch with integration-1.1.0\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-5369] - Integrate schema support in Cassandra Input/Output\n\n\n[SPOI-5437] - Create Jdbc Pojo input operator and integrate schema support in Jdbc POJO input/output\n\n\n[SPOI-5819] - Bandwidth control for file based sources\n\n\n[SPOI-5820] - Bandwidth control for message based sources\n\n\n[SPOI-5959] - Metrics for compression, encryption\n\n\n[SPOI-6337] - Expose bandwidth metrics\n\n\n[SPOI-6441] - Change console home page to be welcome screen instead of operations summary\n\n\n\n\nVersion 3.1.1\n\n\nImprovement\n\n\n\n\n[SPOI-5828] - Stackstraces should not be shown on errors\n\n\n\n\nBug\n\n\n\n\n[SPOI-5786] - AppDataTracker Custom Metric Store Deadlock\n\n\n[SPOI-6032] - App Builder should not show property from super class\n\n\n[SPOI-6049] - DimensionStoreHDHT Should always set meta data on aggregates in processEvent, even if the aggregate is received in a committed window.\n\n\n[SPOI-6073] - AbstractFileOutputOperator not finalizing the file after the recovery\n\n\n[SPOI-6090] - Ingestion: Error decrypting files for message based sources\n\n\n[SPOI-6147] - Launch issue with \"Starter Application Pack\"\n\n\n[SPOI-6202] - Sandbox 3.1 has community license instead of enterprise\n\n\n[SPOI-6203] - -ve Memory reported for Application Master\n\n\n[SPOI-6204] - Sandbox 3.1 loses uploaded license after reboot\n\n\n[SPOI-6222] - HDFS recovery in sandbox causes license to degrade to community\n\n\n[SPOI-6286] - App Data Tracker Number Format Exception In Idempotent Storage Manager\n\n\n[SPOI-6304] - Fix netlet dependency\n\n\n[SPOI-6313] - Work Around APEX-129\n\n\n[SPOI-6333] - RandomNumberGenerator in apex-app-archetype does not use numTuples property\n\n\n\n\nTask\n\n\n\n\n[SPOI-5755] - Gateway should show \"HDFS is not up yet\"\n\n\n[SPOI-6148] - Update website with release 3.1\n\n\n[SPOI-6241] - Change in enterprise license\n\n\n\n\nVersion 3.1.0\n\n\nNew Feature\n\n\n\n\n[SPOI-4670] - Enable message schema management in App Builder\n\n\n[SPOI-5844] - Retrieve older versions of schemas\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-5611] - Simplify PubSub operators to supply Gateway connect address URI by default\n\n\n\n\nBug\n\n\n\n\n[SPOI-4380] - MxN unifier not removed when partition is removed from physical plan.\n\n\n[SPOI-5338] - Cleanup OperatorDiscoverer class to remove reflection and use ASM\n\n\n[SPOI-5582] - Ingestion: Fails with \"Unable to merge file\" with FTP as destination\n\n\n[SPOI-5697] - Unable to launch ingestion app through Ingestion wizard\n\n\n[SPOI-5743] - App package import also needs to do the typegraph stuff as upload\n\n\n[SPOI-5831] - Add getter for properties for Twitter Demo\n\n\n[SPOI-5833] - Schema class not loaded during validation\n\n\n[SPOI-5841] - e2e files being added to app/index.html with gulp inject:scripts\n\n\n[SPOI-5857] - Gateway has trouble getting to STRAM until after restart\n\n\n[SPOI-5943] - chicken and egg problem for entering kerberos credentials data to dt-site.xml\n\n\n[SPOI-5946] - Port compatibility when port require schemas\n\n\n[SPOI-6002] - AppBuilder fails to load operator due to NullPointerException\n\n\n\n\nTask\n\n\n\n\n[SPOI-5307] - Mocha based tests for Gateway API calls\n\n\n[SPOI-5749] - Certify MapR sandbox\n\n\n[SPOI-5750] - Create Splunk forwarder operators\n\n\n[SPOI-5751] - Create splunk forwarder input operator\n\n\n[SPOI-5752] - Create splunk forwarder output operator\n\n\n[SPOI-5753] - Create splunk forwarder configuration specification\n\n\n[SPOI-5754] - Change node1 twitter demos settings to 5 mins in node1 conf demo conf file\n\n\n[SPOI-5755] - Gateway should show \"HDFS is not up yet\"\n\n\n[SPOI-5756] - Test license expiry on sandbox as part of sandbox testing\n\n\n[SPOI-5764] - Gateway to show better error messages on all 500 errors\n\n\n[SPOI-5803] - Support of custom aggregation for ADT\n\n\n[SPOI-5893] - Changes for retrieving container and operator history information\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-4946] - Handle new @useSchema and @description property metadata in UI\n\n\n[SPOI-4980] - Schema Management in the UI\n\n\n[SPOI-5505] - handle Object-to-Object port compatibility with and without schema\n\n\n[SPOI-5506] - handle Object-to-Pojo port compat with schema\n\n\n[SPOI-5513] - handle port compatibility with generic type ports\n\n\n[SPOI-5747] - Automatically generate new eval license when sandbox starts up\n\n\n\n\nVersion 3.0.0\n\n\nSub-task\n\n\n\n\n[SPOI-1901] - Dynamic property changes lost on AM restart \n\n\n[SPOI-4820] - Add an api call to retrieve all the application, operator and port attributes available for the app-package\n\n\n[SPOI-4891] - Example for schema meta data and property doclet tag\n\n\n[SPOI-4968] - Capture @useSchema and @description doclet tags from docblocks\n\n\n[SPOI-4972] - Design Schema API for managing schemas\n\n\n[SPOI-4973] - Create a Schema resource which will contain schema calls\n\n\n[SPOI-4987] - Generate pojo class using schema provided in json\n\n\n[SPOI-4999] - Implement the rest call to save schema on the backend\n\n\n[SPOI-5211] - Support custom visualization link\n\n\n[SPOI-5237] - Type of attributes are wrong when it is an inner class or enum\n\n\n[SPOI-5357] - LogicalNode may swallow END_STREAM message in catchUp\n\n\n[SPOI-5361] - Button for dt.phoneHome.enable property on system config page\n\n\n[SPOI-5527] - Implement Queue Option\n\n\n\n\nBug\n\n\n\n\n[SPOI-4321] - Datatorrent Core Trigger Jenkings Job hangs\n\n\n[SPOI-4633] - Resolve type variable across the type hierarchy \n\n\n[SPOI-4789] - Time calculated from window Id using WindowGenerator.getMillis is incorrect at times \n\n\n[SPOI-4884] - Intermittent failure for CustomMetricTest\n\n\n[SPOI-4896] - Kafka operator stop consuming from kafka cluster after it is restarted.\n\n\n[SPOI-4941] - For kafka operator in app builder certain properties need to be set twice\n\n\n[SPOI-5006] - If incompatible change made in platform, we need to recompute the typegraph automatically\n\n\n[SPOI-5074] - Code/fix code that leads to classloader leaks in Gateway\n\n\n[SPOI-5087] - Bucket ID Tagger In app data tracker has very high latency\n\n\n[SPOI-5323] - Malhar operators are not packaged with distribution for 3.0\n\n\n[SPOI-5359] - License Type, License ID, and Features should not just be empty on LicenseInfo page\n\n\n[SPOI-5360] - When license upload fails with no message, install wizard should not have an unaddressed colon\n\n\n[SPOI-5379] - NoClassDefFoundError due to indirect reference to dt-common classes\n\n\n[SPOI-5385] - No error message is returned when DTCli gets an NPE\n\n\n[SPOI-5389] - Support for RM and HDFS delegation token renewal in secure HA environments\n\n\n[SPOI-5433] - Asm code not working for jdk 1.8\n\n\n[SPOI-5469] - Datatorrent DTX trigger Jenkins Job fails on timeout\n\n\n[SPOI-5472] - Undeploy heartbeat requests are not processes if container is idle\n\n\n[SPOI-5484] - Ingestion FTP as output does not work with vsftpd\n\n\n[SPOI-5497] - Could not open pi demo in 3.0.0 RC2\n\n\n[SPOI-5498] - Typegraph exception with 3.0.0 RC2\n\n\n[SPOI-5500] - Saving an app whose streams have an assigned schema (not handwritten java class) fails with 404\n\n\n[SPOI-5504] - Make temporary file names unique to avoid lease expiry\n\n\n[SPOI-5530] - Failed to edit pidemo JSON based application\n\n\n[SPOI-5536] - App jar is missing from typegraph\n\n\n[SPOI-5552] - Non-concrete classes being returned with assignableClasses call\n\n\n[SPOI-5562] - Boolean Operator Property Values Are Blank In the Operator Properties Table\n\n\n[SPOI-5569] - Launching AdsDimensionsDemoGeneric fails with ClassNotFoundException \n\n\n[SPOI-5577] - Sometimes CustomMetrics Store Returns No Data When There is Data\n\n\n[SPOI-5578] - Sometimes Custom Metrics Data Source Is Not Accessible From Widgets\n\n\n[SPOI-5582] - Ingestion: Fails with \"Unable to merge file\" with FTP as destination\n\n\n[SPOI-5583] - DFS root directory check is failing on MapR cluster\n\n\n[SPOI-5593] - App Builder search tooltip\n\n\n[SPOI-5597] - Bad log level WARNING in UI\n\n\n[SPOI-5604] - Pressing \"Enter\" in newDashboardModal closes the modal\n\n\n[SPOI-5607] - Sales Enrichment operator needs to be recovered and added to Sales Dimensions demo\n\n\n[SPOI-5612] - AppBuilder can not deserialize instance of java.net.URI with PubSubWebSocketAppData operators\n\n\n[SPOI-5627] - error message from install script from 3.0.0-RC4 \n\n\n[SPOI-5628] - Update dt-site of sandbox for new Sales demo enrichment operator\n\n\n[SPOI-5629] - Data visualization links broken with APPLICATION_DATA_LINK \n\n\n[SPOI-5632] - If there are no uncategorized operators, dont show an uncategorized group\n\n\n[SPOI-5636] - dtcp is not getting packaged in installation (RC4)\n\n\n[SPOI-5637] - Allatori Configuration errors in ingestion pom.xml\n\n\n[SPOI-5640] - dtcp: When invalid value is provided for scan interval no error is thrown by dtcp, just exits out\n\n\n[SPOI-5643] - When An Application Is Restarted Under A Different User Name Custom Metrics Data is Not Saved.\n\n\n[SPOI-5644] - isChar validation should strip \\u000 character\n\n\n[SPOI-5646] - Megh demos is failing release build\n\n\n[SPOI-5647] - launchPkgApplicationDropdown tries to push an alert to scope.alerts, which is undefined\n\n\n[SPOI-5653] - When Gateway Restarts App Data Tracker It Should Do It Using HA Mode\n\n\n[SPOI-5655] - If gateway fails to launch (e.g., port 9090 is in use), installer should inform user\n\n\n[SPOI-5656] - Update Bucket ID Tagger To use only user name for determining bucket Ids\n\n\n[SPOI-5661] - Set default store in HDHTReader\n\n\n[SPOI-5662] - Improve defaults of Enrichment Operator In Sales Demo\n\n\n[SPOI-5664] - Omit Aggregator Registry From UI\n\n\n[SPOI-5672] - App Data Tracker Compliation is failing\n\n\n[SPOI-5673] - Unable to launch ingestion app with JMS as input source \n\n\n[SPOI-5675] - Kafka keys population error\n\n\n[SPOI-5676] - Fix API doc generation\n\n\n[SPOI-5685] - Complete App Builder category and property name fixes\n\n\n[SPOI-5688] - Correct interactive demo tutorials available in the Learn section\n\n\n[SPOI-5689] - Installer produces error message about removing docs directory\n\n\n[SPOI-5690] - Images missing from lean section tutorials after installing release build\n\n\n[SPOI-5693] - Console does not upload default DT application packages\n\n\n[SPOI-5697] - Unable to launch ingestion app through Ingestion wizard\n\n\n[SPOI-5704] - App package references in docs\n\n\n[SPOI-5705] - Build version incorrect\n\n\n[SPOI-5706] - api and user docs not viewable through gateway\n\n\n[SPOI-5707] - Packaging utilities jar in Ingestion\n\n\n[SPOI-5708] - Update netlet dependency to release for 3.0\n\n\n[SPOI-5712] - Unable to launch dtingest app using dtingest command line utility but able to launch via UI\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-4513] - Expose app data tracker stats as \"pseudo-datasource\" for each application\n\n\n[SPOI-4889] - Make BucketIdTagger fault tolerant\n\n\n[SPOI-5320] - License URL should open in new window \n\n\n[SPOI-5494] - Support \"queue\" query parameters from launch app modal\n\n\n[SPOI-5535] - Console should validate duplicate app name in the same app package\n\n\n[SPOI-5548] - Check compaction keys from UI\n\n\n[SPOI-5603] - Add syntax highlighting to Markdown code blocks\n\n\n[SPOI-5611] - Simplify PubSub operators to supply Gateway connect address URI by default\n\n\n[SPOI-5623] - License upgrade link should open in separate tab\n\n\n[SPOI-5633] - Improve markdown content display styles\n\n\n[SPOI-5657] - Redirect to welcome screen of the Learn section after install\n\n\n[SPOI-5694] - Ingestion default app-config should populate meaningful defaults\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-4672] - Support for secure HA environments\n\n\n[SPOI-5288] - Add \"launch\" button to each item in the list of app packages\n\n\n[SPOI-5304] - Support ingestion install mode\n\n\n[SPOI-5313] - Obfuscate ADT in the distribution build\n\n\n[SPOI-5561] - Markdown documentation support in the Console\n\n\n\n\nTask\n\n\n\n\n[SPOI-4228] - The location of App Data Tracker from installation and from devel mode\n\n\n[SPOI-4658] - App Data Tracker Technical Doc\n\n\n[SPOI-4879] - Hide operators that have no input ports from App builder\n\n\n[SPOI-4983] - Installer for Ingestion app\n\n\n[SPOI-4984] - Obfuscate ingestion jar\n\n\n[SPOI-5029] - Move new x-java dependencies in appDataFramework branch to core\n\n\n[SPOI-5032] - Review comparison for appDataFramework pull request to the master\n\n\n[SPOI-5050] - Licensing-related changes to the UI for 3.0\n\n\n[SPOI-5255] - Support enhancements to uiTypes\n\n\n[SPOI-5314] - Include obfuscated App Data Tracker in the install bundle\n\n\n[SPOI-5370] - Megh obfuscation\n\n\n[SPOI-5502] - Allatori obfuscates operator class names even config has keep-name on class * implements com.datatorrent.api.Operator\n\n\n[SPOI-5555] - [dtcp] set default app package for ingestion\n\n\n[SPOI-5574] - Update installer README\n\n\n[SPOI-5608] - Docs for backward compatibility in 3.0 \n\n\n[SPOI-5610] - Add testing to Markdown support models, pages, directives\n\n\n[SPOI-5621] - Include Ingestion utilities in DT installer\n\n\n[SPOI-5624] - [Ingestion] Hide message output destination when input is file source\n\n\n[SPOI-5635] - Changing product name to dtIngest\n\n\n[SPOI-5638] - Post ingestion packge to central DT maven repository\n\n\n[SPOI-5639] - App Data Tracker Release Job\n\n\n[SPOI-5645] - support \"short\" primitive type in app builder\n\n\n[SPOI-5658] - Dummy ticket for Apex-17 - Change CustomMetric annotation to AutoMetric and enhancements\n\n\n\n\nBug\n\n\n\n\n[MLHR-1726] - Bug in PojoUtils\n\n\n[MLHR-1742] - Create a wrapper method in POJOUtils that returns the appropriate getter for primitives\n\n\n[MLHR-1752] - PojoUtils create/constructSetter does not handle boxing/unboxing\n\n\n[MLHR-1756] - Exclude hadoop-common and other hadoop libraries from application\n\n\n[MLHR-1789] - Complete Category and property name fixes\n\n\n\n\nImprovement\n\n\n\n\n[MLHR-1725] - Add Support for Setters to PojoUtils\n\n\n[MLHR-1757] - change scope of dt-engine in maven build\n\n\n\n\nVersion 2.1.0\n\n\nBug\n\n\n\n\n[SPOI-3732] - Get User Info APIs creates users\n\n\n[SPOI-3816] - Ingestion UI: Node server should store pipelines on hdfs rather than local filesystem\n\n\n[SPOI-3820] - Ingestion UI: Rest calls to gateway from Node server fails\n\n\n[SPOI-3845] - BlockSynchronizer  sometimes misbehaves when BlockReader is killed. \n\n\n[SPOI-3862] - Ingestion UI - left/right margins have no width\n\n\n[SPOI-3876] - Review the changes done to DimensionsComputation for HadoopWorld Demo\n\n\n[SPOI-3940] - Demo app package version stuck at v1.0-SNAPSHOT\n\n\n[SPOI-4029] - PiJavaScript demo pi calc operator fails to re deploy\n\n\n[SPOI-4067] - Launch of ingestionApp using DTCP is failing\n\n\n[SPOI-4070] - Stray directory /opt/datatorrent/current/datatorrent  is created after installation.  \n\n\n[SPOI-4077] - Ingestion UI: Create option to scan the directory recursively\n\n\n[SPOI-4079] - Gateway does not retain listen address specified during installation\n\n\n[SPOI-4080] - Gateway guard hides errors with restarts\n\n\n[SPOI-4081] - Ingestion UI: pipeline table doesn't auto refreshes\n\n\n[SPOI-4085] - UI: wrong application selection after sorting in pipeline instances table\n\n\n[SPOI-4087] - Relaunch functionality keeps relaunching multiple apps\n\n\n[SPOI-4094] - Ingestion app tests failing and empty directory getting created under target that are not removed\n\n\n[SPOI-4128] - Need to explain difference between properties \n state holding data structure\n\n\n[SPOI-4175] - Remove directory prop from base block reader and ftp block reader from ingestion \n\n\n[SPOI-4229] - committed function not getting invoked in local mode\n\n\n[SPOI-4352] - App Builder uiType not present for several classes\n\n\n[SPOI-4362] - Remove dependencies on DirectoryScanner from OperatorDiscoverer when used in stram\n\n\n[SPOI-4691] - DT console shows negative memory size for killed application\n\n\n[SPOI-4702] - IndexOutOfBoundsException in Stram due to counters in AbstractBlockReader\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-3592] - Clean up appInstance widget definitions\n\n\n[SPOI-3783] - Remove gateway port automatic re-selection on startup feature\n\n\n[SPOI-3932] - demos ui references web services v1\n\n\n[SPOI-3944] - Ingestion UI : Display pipelines progress\n\n\n[SPOI-3945] - UI: Add multiple input support\n\n\n[SPOI-3946] - Create an auto scaling scheme for the Block Reader and Writer \n\n\n[SPOI-4003] - Ingestion app: add a feature to re-try failed blocks\n\n\n[SPOI-4026] - Aggregated counters are not published via web-socket in the logical operators topic\n\n\n[SPOI-4048] - Block application launch during critical system issues\n\n\n[SPOI-4078] - Gateway address argument validation during installation and launch\n\n\n[SPOI-4551] - Enhance Gateway API to return AppIDs for a given application name\n\n\n[SPOI-4578] - Allow supporting archive jars in configuration definition for app packages\n\n\n[SPOI-4728] - Support -originalAppId when launching apps through the DT Gateway API\n\n\n[SPOI-4751] - Upgrade sniffer maven plugin to the latest version\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-3028] - [HDHT] Hive Interoperability\n\n\n[SPOI-3029] - [HDHT] Export to ORC Format\n\n\n[SPOI-4050] - AppData UI Dashboard page\n\n\n[SPOI-4093] - Ingestion UI: Control for triggering scan of files\n\n\n[SPOI-4636] - Added $? feature in dtcli and use it in CLIProxy\n\n\n[SPOI-4718] - CLI commands to support variable arguments\n\n\n\n\nTask\n\n\n\n\n[SPOI-3242] - Rethrow the exceptions being caught in catch block of operators.\n\n\n[SPOI-3502] - Create the landing page for the UI of ingestion application\n\n\n[SPOI-3504] - [Ingestion UI] Create views for creating a pipeline and its description\n\n\n[SPOI-3762] - Provide installation version option for installer\n\n\n[SPOI-3763] - Update document export tokens\n\n\n[SPOI-3870] - Implement transformation functions for rainier poc1\n\n\n[SPOI-3871] - Filter records based field values for rainier poc1\n\n\n[SPOI-3872] - Generate audit and bad record logs for rainier poc1\n\n\n[SPOI-3970] - Ingestion UI : Use the console package \n\n\n[SPOI-3981] - Remove unused imports\n\n\n[SPOI-4040] - Add an operator that keeps track of failed files\n\n\n[SPOI-4122] - [UI] : Display list of skipped files on UI when overwrite flag is false\n\n\n[SPOI-4172] - After effects of removing threshold property from BlockReader in ReaderWriter partitioner/stats-listener  \n\n\n[SPOI-4369] - [AppData][AppDataTracker] Deserializer operator \n\n\n[SPOI-4385] - Create metrics aggregators - sum, min, max, count\n\n\n[SPOI-4544] - Custom metrics can be cumulative or per window which can be statically declared by the operator developer\n\n\n[SPOI-4653] - Create Aggregators registry in AppDataTracker\n\n\n[SPOI-3767] - Auto scaling of BlockReader using partitioning\n\n\n[SPOI-4748] - Add data query to custom metrics store\n\n\n\n\nBug\n\n\n\n\n[MLHR-1614] - AbstractFSWriter in append mode is not fault-tolerant\n\n\n[MLHR-1620] - Remove close file from AbstractFSWriter\n\n\n[MLHR-1637] - Cleanup skipping of records from setup of FSDirectoryInputOperator \n\n\n[MLHR-1643] - FileSplitter recovery fails\n\n\n[MLHR-1644] - Add Mock Server Libraries in unit tests of database/key value store operators\n\n\n[MLHR-1653] - Remove JavaScriptOperatorBenchMark form library\n\n\n[MLHR-1656] - AbstractFileOutputOperator LeaseExpired exception when cache reaches its threshold\n\n\n[MLHR-1668] - Stateless Partitioner in case of parallel partition ignores parallel partition count (except the first time define partitions is called)\n\n\n[MLHR-1687] - AbstractBlockReader threshold breaks the idempotency\n\n\n[MLHR-1708] - Duplicate data read from kafka if kafka partitions are less than DT partitions\n\n\n[MLHR-1712] - The directory under which the idempotent state is stored should be relative to the app directory so that the state is copied on relaunch\n\n\n[MLHR-1723] - FTPStringInputOperatorTest fails on Windows OS\n\n\n\n\nImprovement\n\n\n\n\n[MLHR-1547] - Integration of Idempotent storage manager to Directory scanner\n\n\n[MLHR-1621] - Parititon Couchbase Output Operator\n\n\n[MLHR-1632] - Add couchbase mock to couchbase tests.\n\n\n[MLHR-1634] - Improve the BlockReader partitioning scheme to accommodate ingestion rate\n\n\n[MLHR-1641] - Improve the BlockReader\n\n\n[MLHR-1661] - Ability to override the stream-codec of input port in AbstractFileOutputOperator\n\n\n[MLHR-1684] - Improve file splitter not to emit all files  in one shot and hold scanned file names in memory\n\n\n[MLHR-1694] - BasicCounters improvements\n\n\n\n\nNew Feature\n\n\n\n\n[MLHR-1355] - Supports secure hadoop cluster in the installer\n\n\n[MLHR-1497] - Operators for ElasticSearch\n\n\n[MLHR-1578] - UI Auth: UI to show different tabs and/or buttons for different user permissions\n\n\n\n\nVersion 2.0.1\n\n\nBug\n\n\n\n\n[SPOI-4379] - Operator removed from physical plan due to invalid SHUTDOWN status\n\n\n[SPOI-4381] - Queue size missing on physical operator page \n\n\n[SPOI-4382] - queueSize port metric reports bogus values\n\n\n[SPOI-4384] - Recovery fails due to corrupted checkpoints.\n\n\n[SPOI-4437] - DTCli not recognizing $HOME variable set\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-4728] - Support -originalAppId when launching apps through the DT Gateway API\n\n\n[SPOI-4390] - Replace 'start time' on console to 'Up time'\n\n\n\n\nVersion 2.0.0\n\n\nBug\n\n\n\n\n[SPOI-4046] - Gateway /containers?states={state} call returns erroneous state information\n\n\n[SPOI-4057] - License can not be upgraded to Evaluation / Production License\n\n\n[SPOI-4037] - CONTAINERS_MAX_COUNT in sandbox prevents demos launch from dtcli\n\n\n[SPOI-4008] - HDHT Error while flushing write cache\n\n\n[SPOI-3837] - Partitioner interface semantics broken in 2.0.0 physical plan implementation]\n\n\n[SPOI-3901] - twitter demo app package launch throws NoClassDefFoundError\n\n\n[SPOI-3900] - Gateway gets 400 error when phone home\n\n\n[SPOI-3922] - dtcli does not show output port attributes.\n\n\n[SPOI-3934] - DT CLI Needs to validate user inputs\n\n\n[SPOI-3054] - # Of Tuples Produced != Number Of Tuples Output By Unifier When Repartitioning\n\n\n[SPOI-3210] - HDHT DTFile reader bug\n\n\n[SPOI-3340] - Settings in ~/.dt/dt-site.xml don't override app package defaults\n\n\n[SPOI-3349] - Can we document how to enable password security in gateway\n\n\n[SPOI-3365] - Launch App Package does not honor -local option\n\n\n[SPOI-3369] - Implement the pam authentication as a hadoop authentication handler\n\n\n[SPOI-3396] - launch command should be able to specify local conf file when launching app package\n\n\n[SPOI-3397] - DT Gateway doesn't start on Mapr 4.0.1\n\n\n[SPOI-3402] - Checking the UID for Dtadmin\n\n\n[SPOI-3419] - KafkaAdsDimensionsDemo demo available with installer fails\n\n\n[SPOI-3420] - Installation issues on HDP 2.2\n\n\n[SPOI-3469] - gateway, dtcli fail to start in development mode\n\n\n[SPOI-3503] - AppPackage tests produce 100+GB files\n\n\n[SPOI-3522] - MaxEventsPerSecond for Flume Ingestor should adjust for partitioned instances\n\n\n[SPOI-3528] - Installer does not show relevant error if nonexistent env file passed as argument and finishes silently with defaults\n\n\n[SPOI-3548] - browser inconsistencies with table\n\n\n[SPOI-3550] - Confusing Breadcrumb navigation on Application Page\n\n\n[SPOI-3555] - Unifier names should not be links to nonexistent pages\n\n\n[SPOI-3594] - Systems Diagnostic produces conflicting results for Hadoop installation\n\n\n[SPOI-3595] - Correctly Implement the clone method in platform.\n\n\n[SPOI-3608] - User Profile and User Management should not show when auth is disabled\n\n\n[SPOI-3613] - RBAC Configuration screen is completely missing after recent changes\n\n\n[SPOI-3628] - Pubsub websocket auth not working with kerberos frontend authentication\n\n\n[SPOI-3638] - Cant assign roles to user in console\n\n\n[SPOI-3644] - Develop Tab Can not be seen on naviagation bar \n\n\n[SPOI-3646] - Error creating new config xml file in package\n\n\n[SPOI-3647] - MR Operator demo throws null pointer exception\n\n\n[SPOI-3648] - Cant add admin permissions to existing roles\n\n\n[SPOI-3654] - In passwd policy user can not change own password using ui console\n\n\n[SPOI-3657] - Can't Launch Apps Using ui console if user is not the same as dtgateway user\n\n\n[SPOI-3663] - modify certification scripts and gateway proxy to upload demo jars from repackaged malhar demos\n\n\n[SPOI-3665] - DataTorrent logo link broken in console\n\n\n[SPOI-3670] - Update DTCliTest to create app package like AppPackageTest.java\n\n\n[SPOI-3672] - Uploaded license not used when launching app\n\n\n[SPOI-3674] - License info not updated after license upload until page reload\n\n\n[SPOI-3675] - Display Error in Sandbox. License Manager Error:null\n\n\n[SPOI-3682] - doubleclick select active kills containers\n\n\n[SPOI-3684] - Gateway deletes JSON app silently (no error) on PUT\n\n\n[SPOI-3685] - Error message in launch app modal is not red\n\n\n[SPOI-3686] - Launch properties do not get sent with the launch request\n\n\n[SPOI-3689] - AppPackages APIs give 404 not found messages\n\n\n[SPOI-3690] - APIs falsely report role assignments\n\n\n[SPOI-3691] - API /ws/v1/config/hadoopInstallDirectory does not work\n\n\n[SPOI-3708] - Gateway shows License error after license agent gets back to normal\n\n\n[SPOI-3710] - Sandbox packaging invalid due to HDFS dependency\n\n\n[SPOI-3712] - Exclamation point in DT cli tries to expand \"event\"\n\n\n[SPOI-3713] - When CLI has a problem running a CLIProxy command, there is out of memory error from the gateway\n\n\n[SPOI-3714] - Launch button from other screens should bring up launch modal\n\n\n[SPOI-3716] - locality performance benchmarks operators failing with OOM exception\n\n\n[SPOI-3717] - Insufficient license memory for Node Local Performance benckmark application\n\n\n[SPOI-3718] - change high availability certification benchmarking to compare number of active containers instead of total number of containers\n\n\n[SPOI-3720] - License manager not found returned from Gateway even though it returns valid license including agent info\n\n\n[SPOI-3721] - Changes to XMLConfig to incorporate changed properties\n\n\n[SPOI-3725] - NullPointerException in Certification-initDemo for PerformanceBenchmarkForFixedNumberOfTuples\n\n\n[SPOI-3729] - Thread local validation failure for multiple ports between two operators\n\n\n[SPOI-3732] - Get User Info APIs creates users\n\n\n[SPOI-3733] - Can't Launch Apps Using UI or gateway APIs in kerberos environment\n\n\n[SPOI-3734] - Remove macro from cli  for application or add relevant application\n\n\n[SPOI-3754] - Gateway can only restore roles once\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-2036] - Ensure that definePartitions and partitioned are called for parallel partitioned operators\n\n\n[SPOI-2955] - Provide tool-tip capability to RTS UI\n\n\n[SPOI-3194] - Support PAM authentication mechanism as an agent for LDAP\n\n\n[SPOI-3304] - Reorganize code to ensure easy consumability\n\n\n[SPOI-3345] - Delete OperatorContext.PartitionTPSMax and .PartitionTPSMin \n\n\n[SPOI-3346] - Delete OperatorContext.InitialPartitionCount\n\n\n[SPOI-3366] - Update Eval license for RTS 2.0\n\n\n[SPOI-3401] - Switch installer, gateway, console from using HADOOP_PREFIX to hadoop executable\n\n\n[SPOI-3404] - Provide support for viewing/collecting YARN logs across the cluster\n\n\n[SPOI-3415] - Remove defaults from dt-site.xml and add version\n\n\n[SPOI-3418] - Remove dependency on /etc/datatorrent\n\n\n[SPOI-3447] - Partitioning of couchbase input operator\n\n\n[SPOI-3525] - Messages shown in installer and uninstaller are longer than 80 characters\n\n\n[SPOI-3554] - Log-Level Setter for angular\n\n\n[SPOI-3626] - appPackageDisplayName in /ws/v1/appPackages\n\n\n[SPOI-3667] - Confirmation box to shutdown or kill apps should have \"warm\" colors\n\n\n[SPOI-3668] - DTGateway logs are flooded with permission WARN logs\n\n\n[SPOI-3694] - Populate launch properties with required properties of app\n\n\n[SPOI-3697] - Add content-disposition so that app package download has a reasonable file name when downloaded from browser\n\n\n[SPOI-3698] - Support URI codec out the box so that UI can set URI operator properties\n\n\n[SPOI-4066] - Portable environment settings across releases\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-3393] - Track recordings by a generated ID rather than startTime\n\n\n[SPOI-3422] - Restore default roles with backend\n\n\n[SPOI-3683] - Add download button to app package\n\n\n[SPOI-3753] - \"Reset to Defaults\" for user roles\n\n\n\n\nTask\n\n\n\n\n[SPOI-3927] - Support per application configuration defaults and required properties\n\n\n[SPOI-2809] - Implement password authentication as a hadoop authentication type\n\n\n[SPOI-3115] - Update document with password authentication once UI supports user mgmt\n\n\n[SPOI-3239] - HDHT store removal support\n\n\n[SPOI-3351] - Abstract the reconciler used in Guava and Rainier to malhar\n\n\n[SPOI-3368] - Configuration handling on major version upgrade\n\n\n[SPOI-3371] - CLI to doAs the user specified by HADOOP_USER_NAME if security is enabled\n\n\n[SPOI-3372] - Gateway REST service to only return rows the user is authorized to view\n\n\n[SPOI-3373] - Gateway Websocket service to only publish to subscribers with info individual subscribers are authorized to view\n\n\n[SPOI-3374] - Gateway to return 403 Forbidden for resources the user is not authorized to view or operations the user is not authorized to perform\n\n\n[SPOI-3375] - Gateway support for permission based authorization\n\n\n[SPOI-3376] - Gateway REST call to show what permissions the current user has\n\n\n[SPOI-3377] - Gateway REST call to support role and permission management\n\n\n[SPOI-3378] - HTTP Header auth support in Gateway \n\n\n[SPOI-3381] - Implement Kerberos group to role mapping\n\n\n[SPOI-3385] - Gateway ACL support at the application level\n\n\n[SPOI-3390] - LDAP/AD support in Gateway\n\n\n[SPOI-3405] - Implement Kerberos security context and security context filter in Gateway\n\n\n[SPOI-3443] - Ingestion Application repo and package\n\n\n[SPOI-3472] - Installer testing\n\n\n[SPOI-3485] - Console repository relocation\n\n\n[SPOI-3486] - Create HDHT section in Application Developer Guide\n\n\n[SPOI-3517] - Document JVM_OPTIONS and QUEUE_NAME features in user docs\n\n\n[SPOI-3519] - Add PAM auth user docs\n\n\n[SPOI-3520] - Update RBAC user docs\n\n\n[SPOI-3521] - Console user docs updates\n\n\n[SPOI-3536] - Create hdfs output operator to write table files\n\n\n[SPOI-3537] - write hdfs writer for small files to copy to vertica\n\n\n[SPOI-3538] - Combine small file writer with vertica writer\n\n\n[SPOI-3540] - Sandbox updates to support app package launches from console\n\n\n[SPOI-3593] - Clean up import functionality for demo apps\n\n\n[SPOI-3610] - Implement RBAC features for App Packages\n\n\n[SPOI-3619] - Update console for the new app package REST API\n\n\n[SPOI-3627] - Convert our REST API from v1 to v2 as we have made some backward incompatible changes in the REST API\n\n\n[SPOI-3631] - Test HDHT recovery\n\n\n[SPOI-3632] - Upgrade and test demos on 2.0.0\n\n\n[SPOI-3633] - Set certain sensitive permissions \"admin\" role only\n\n\n[SPOI-3634] - Allow default app instance and app package permissions for each user\n\n\n[SPOI-3635] - Remove INITITAL_PARTITION_COUNT from demo app pkg properties\n\n\n[SPOI-3636] - Upgrade and test demos on 2.0.0\n\n\n[SPOI-3650] - Test demos on sandbox on dt 2.0.0\n\n\n[SPOI-3651] - benchmarks for dt 2.0.0\n\n\n[SPOI-3652] - list of current benchmarks\n\n\n[SPOI-3653] - Document property changes needed run demos on 2.0.0\n\n\n[SPOI-3661] - Update security section in operations guide\n\n\n[SPOI-3662] - Operations Guide: Update application configuration section\n\n\n[SPOI-3669] - Remove gateway jars api usage from benchmarking and use app pkg api instead\n\n\n[SPOI-3695] - Documentation on System Alerts in the Gateway\n\n\n[SPOI-3700] - run app memory usage benchmarks for 2.0\n\n\n[SPOI-3701] - run performance benchmarks for 2.0\n\n\n[SPOI-3702] - run high availability benchmarks for 2.0\n\n\n[SPOI-3703] - Run Fs output operator benchmark\n\n\n[SPOI-3704] - run performance across tuple size benchmarks for 2.0\n\n\n[SPOI-3705] - Change jars api usage to apps package api usage in certification\n\n\n[SPOI-3706] - Convert Malhar benchmark module to app package\n\n\n[SPOI-3730] - Getting Started Guide updates\n\n\n\n\nBug\n\n\n\n\n[MLHR-1237] - Scrolling log viewer with mouse does not trigger getting more log content\n\n\n[MLHR-1242] - \"last heartbeat\" in killed container list shows date in 1969\n\n\n[MLHR-1443] - Attempt reconnect if web socket connection is dropped\n\n\n[MLHR-1523] - Physical operator ids sorted lexicographically\n\n\n[MLHR-1552] - Port JMS Input operator changes to library.\n\n\n[MLHR-1553] - shutdown and kill cmds still visible when ended apps are selected\n\n\n[MLHR-1588] - New installation does not walk through welcome steps\n\n\n[MLHR-1590] - App Instance dashboard cannot find Logical DAG widget for starting app\n\n\n[MLHR-1591] - pending undeploy does not have an icon or color associated\n\n\n[MLHR-1592] - webSocket does not reconnect if user logs out then logs back in\n\n\n[MLHR-1593] - Installer should be ok with 404 errors when fetching HadoopLocation and dfsRootDirectory\n\n\n[MLHR-1594] - DFS permissions error during install does not provide resolution steps\n\n\n[MLHR-1595] - Installer asks for login even if auth is disabled\n\n\n[MLHR-1606] - Change EDIT_AND_KILL_OTHERS_APPS to MANAGE_OTHERS_APPS\n\n\n[MLHR-1609] - Physical Operators have an invalid heartbeat when PENDING_DEPLOY\n\n\n[MLHR-1611] - Do not try to put or edit admin role in auth management page\n\n\n[MLHR-1612] - New containers from websocket do not get jvm name\n\n\n[MLHR-1615] - Feedback for retrieving ended apps\n\n\n[MLHR-1619] - When launching app package, allow launch time parameters to be specified\n\n\n[MLHR-1622] - YahooFinanceApplication throws null pointer exception on 2.0\n\n\n[MLHR-1627] - Make twitter credentials into requiredProperties in app package\n\n\n[MLHR-1631] - NullPointerException in launching HDFSBenchmarking App:TupleSize property missing in appResponse\n\n\n\n\nImprovement\n\n\n\n\n[MLHR-1190] - Infinite scroll mechanism in stram events widget\n\n\n[MLHR-1234] - Add version information to system diagnostics screen\n\n\n[MLHR-1555] - Remove dashboard component from main operations page\n\n\n[MLHR-1571] - Height of app list should be determined by available space\n\n\n[MLHR-1596] - Extract two-way infinite scroll behavior into directive\n\n\n[MLHR-1597] - Fix FTP Input operator\n\n\n[MLHR-1602] - Partitioning of couchbase input operator\n\n\n\n\nNew Feature\n\n\n\n\n[MLHR-1228] - create profile storage object\n\n\n[MLHR-1261] - AngularJS Migration - Tuple Viewer\n\n\n[MLHR-1262] - AngularJS Migration - Tuple Recording\n\n\n[MLHR-1354] - Auth management support in the UI\n\n\n\n\nStory\n\n\n\n\n[MLHR-975] - Migration from Backbone to Angular\n\n\n\n\nTask\n\n\n\n\n[MLHR-1292] - Recording View\n\n\n[MLHR-1331] - Update the tuple recording topic\n\n\n[MLHR-1586] - Remove DirectoryScanInputOperator from library\n\n\n[MLHR-1601] - Re-arrange logstream app in dt application package format\n\n\n[MLHR-1608] - Create a Block reader which emits Slice and doesn't read ahead of a block boundary\n\n\n[MLHR-1617] - Sensitive permission assignment need dialog box warning with consequences\n\n\n[MLHR-1618] - Update console for the new app package REST API\n\n\n[MLHR-1623] - Create synchronizer for asynchronously processing streaming data for committed windows\n\n\n[MLHR-1626] - Write unit tests for AbstractSynchronizer\n\n\n\n\nBug\n\n\n\n\n[SPOI-2946] - Provide a tool that generates a license report based on license audit logs\n\n\n[SPOI-2974] - Better error handling when license is expired\n\n\n[SPOI-3147] - Issues with dynamic partitioning of Block readers \n\n\n[SPOI-3192] - Send proxy user when making web service calls from gateway\n\n\n[SPOI-3267] - HDHT Operator crash when launching application against existing store\n\n\n[SPOI-3271] - Change port stats propogation test to check for cumulative buffer server bytes\n\n\n[SPOI-3286] - HDHT WAL recovery crashing with NegativeArraySizeException\n\n\n[SPOI-3287] - Research the types of data that can be stored in hive orc files\n\n\n[SPOI-3288] - NullPointerException in StreamingContainerManager-\n fillLogicalOperatorInfo\n\n\n[SPOI-3289] - App Packages launch not reading required properties from dt-site.xml\n\n\n[SPOI-3293] - Incorrect application name when launching JSON app.\n\n\n[SPOI-3294] - Configuration setting from property.xml not effective for JSON app\n\n\n[SPOI-3296] - npm install fails for malhar-ui-console\n\n\n[SPOI-3297] - Remove keys from metric list in app dashboard widget.\n\n\n[SPOI-3300] - Two operators named \"Dimension Computation\"\n\n\n[SPOI-3305] - Create Sales Demo JSON Input Generator\n\n\n[SPOI-3306] - Change Event Schema to default to Sales Schema\n\n\n[SPOI-3309] - DT flume sink not draining under some circumstances\n\n\n[SPOI-3313] - Setting attributes with JSON and properties app needs cleanup\n\n\n[SPOI-3319] - NPE in FSStatsRecorder\n\n\n[SPOI-3320] - The platform should use user folder in hdfs for storage\n\n\n[SPOI-3323] - Gateway resource leak when RM is not running and/or during network problem \n\n\n[SPOI-3325] - HDHT DTFile clean cache when reader close\n\n\n[SPOI-3338] - Application level operator properties do not override \"global\" operator properties\n\n\n[SPOI-3343] - Gateway was stuck\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-2918] - Document Counters\n\n\n[SPOI-3188] - Ability to interpret an input operator as a regular operator \n\n\n[SPOI-3312] - OperatorAnnotation that enforces checkpointing to happen at application window boundary\n\n\n[SPOI-3317] - Add sales generator tuples per window randomization controls\n\n\n[SPOI-3318] - Add more dimensions and aggregates to Sales demo\n\n\n[SPOI-3321] - Improve data variation between categories, regions, and discounts\n\n\n[SPOI-3329] - Implement simple variable substitution in properties files\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-2014] - Support for doc link for an operator \n\n\n[SPOI-2931] - Make sure app packages work in sandbox\n\n\n[SPOI-2989] - HDHT - Recovery\n\n\n\n\nTask\n\n\n\n\n[SPOI-2663] - Certify datatorrent on a CDH5 secure cluster\n\n\n[SPOI-2702] - HDS - Generic time series query\n\n\n[SPOI-2843] - Tracker ticket for work related to app package in Malhar\n\n\n[SPOI-2945] - Allow application to start even if license manager is not running for production license file\n\n\n[SPOI-2950] - Generate monthly alerts when the memory used crosses the licensed memory\n\n\n[SPOI-3022] - Run jpa and jdbc tests on vertica installation in cluster\n\n\n[SPOI-3098] - Scrub all components that make it to the release list of jiras\n\n\n[SPOI-3119] - Generate audit logs in the applications as well\n\n\n[SPOI-3153] - Certify datatorrent on HDP secure cluster\n\n\n[SPOI-3156] - Tool to generate license report from a single Hadoop grid\n\n\n[SPOI-3158] - Webservice for license report\n\n\n[SPOI-3160] - Uptime debugging for license mgr\n\n\n[SPOI-3161] - license mgr should write memory reported by each app in the logs\n\n\n[SPOI-3184] - Test case to reproduce locality issue in CDH\n\n\n[SPOI-3229] - New app will launch for production license even if no memory is available in license\n\n\n[SPOI-3261] - Kafka one to many dynamic partitioning pilot\n\n\n[SPOI-3269] - Support contact update\n\n\n[SPOI-3278] - Research elastic search functionality to build operators\n\n\n[SPOI-3280] - Create a model parser that loads the model mapping file and generates sql mapping\n\n\n[SPOI-3282] - Create a vertica output operator\n\n\n[SPOI-3283] - Start License Manager as part of Installation\n\n\n[SPOI-3284] - Watch dog process for License Manager in Gateway\n\n\n[SPOI-3299] - O15 demo: Dictionary for keys\n\n\n[SPOI-3307] - Add Enrichment operator data file\n\n\n[SPOI-3315] - O15 demo: Update widget configuration for sales schema\n\n\n[SPOI-3335] - Kafka offset manager\n\n\n[SPOI-3339] - Move FSStorageAgent to Malhar\n\n\n[SPOI-3352] - Process OperatorCommand returned by stats listener\n\n\n[SPOI-3411] - Create a feed processor operator that converts input event to table rows\n\n\n[SPOI-3412] - Create gzip input operator\n\n\n[SPOI-3413] - Create pluggable functions for each column based given function name and parameters\n\n\n[SPOI-3414] - Create Table controller to handle value generation for each column including user defined functions\n\n\n[SPOI-3561] - Recording View: links to start and stop recording\n\n\n[SPOI-3562] - Recording View: page module\n\n\n[SPOI-3565] - Container Page (angular)\n\n\n\n\nBug\n\n\n\n\n[MLHR-1267] - AngularJS Migration - Dashboard Layout Lock\n\n\n[MLHR-1352] - Custom directive for breadcrumbs\n\n\n[MLHR-1489] - Create a hive output operator that can write to hive orc files\n\n\n[MLHR-1500] - Cleanup of the junit.framework.Assert \n\n\n[MLHR-1505] - Stram event error closes when clicking on stack trace\n\n\n[MLHR-1506] - Stram event collection removes events\n\n\n[MLHR-1512] - UI Console - Dashboard Reset\n\n\n[MLHR-1532] - Table says \"loading\" when active filter result is 0 rows\n\n\n[MLHR-1541] - Add orc file output to adsdimension demo\n\n\n[MLHR-1542] - App Data Framework - WebSocket Support\n\n\n[MLHR-1585] - Installer fails with auth enabled\n\n\n\n\nImprovement\n\n\n\n\n[MLHR-1256] - Create a new landing dashboard\n\n\n[MLHR-1432] - Add confirm for deleting an app package\n\n\n[MLHR-1451] - Loading feedback for malhar-angular-table\n\n\n[MLHR-1452] - Loading feedback for app packages and package apps\n\n\n[MLHR-1454] - Performance Tuning for malhar-angular-table\n\n\n[MLHR-1499] - Idempotent State Manager\n\n\n[MLHR-1531] - App overview does not show started time\n\n\n[MLHR-1540] - Remove stram events from physical view\n\n\n[MLHR-1543] - Switch to gulp in malhar-angular-table\n\n\n[MLHR-1549] - Short operator properties should be shown in line\n\n\n\n\nNew Feature\n\n\n\n\n[MLHR-1282] - AngularJS Migration - App Instance Page - Alerts Recordings View\n\n\n[MLHR-1301] - AngularJS Migration - Physical Operator Page - Recordings\n\n\n[MLHR-1317] - AngularJS Migration - App Instance Page - Logical View - Log Levels\n\n\n[MLHR-1362] - AngularJS Migration - App Instance Page - Stram Events Widget Resize\n\n\n[MLHR-1370] - AngularJS Migration - Container View - Chart\n\n\n[MLHR-1428] - Dashboard Component - Option to Disable Vertical Resize\n\n\n[MLHR-1462] - Socket.IO Kafka Communication Protocol\n\n\n[MLHR-1463] - UI Console - Client Settings\n\n\n[MLHR-1464] - Kafka App Data - Server Socket.IO Node.js\n\n\n[MLHR-1465] - Kafka Socket.IO Service - Latency\n\n\n[MLHR-1466] - Kafka Server - Query Cache\n\n\n[MLHR-1470] - Dashboard Builder Integration\n\n\n[MLHR-1486] - UI Console - Deploy Scripts\n\n\n[MLHR-1488] - App Package Dag Viewer - Handle Case when DAG is empty\n\n\n[MLHR-1491] - App Data UI - Runtime Configuration for Default Dashboard/Widgets/Queries\n\n\n[MLHR-1495] - Kafka Debug Widget - Kafka Producer/Consumer Topics\n\n\n[MLHR-1498] - Provide Ability to Reset Dashboard Configuration \n\n\n[MLHR-1502] - App Data Server - Fetch Latest Kafka Offset\n\n\n[MLHR-1514] - App Data Server - EventEmitter\n\n\n[MLHR-1515] - App Data Server - UML Diagrams\n\n\n[MLHR-1520] - App Data UI - Table Widget\n\n\n[MLHR-1521] - App Data UI - WebSocket Support\n\n\n[MLHR-1530] - App Data UI - Table Widget\n\n\n\n\nStory\n\n\n\n\n[MLHR-1295] - AngularJS Migration - Logical Operator Page\n\n\n[MLHR-1296] - AngularJS Migration - Physical Operator Page\n\n\n\n\nTask\n\n\n\n\n[MLHR-1270] - Container Page (angular)\n\n\n[MLHR-1272] - Container Page: memory gauge\n\n\n[MLHR-1274] - Container Page: metric chart\n\n\n[MLHR-1281] - Stream View\n\n\n[MLHR-1286] - Stream View: sources table\n\n\n[MLHR-1287] - Stream View: sinks\n\n\n[MLHR-1288] - Port View\n\n\n[MLHR-1290] - Port View: overview\n\n\n[MLHR-1291] - Port View: chart\n\n\n[MLHR-1293] - Recording View: page module\n\n\n[MLHR-1294] - Recording View: links to start and stop recording\n\n\n[MLHR-1307] - Installation Wizard\n\n\n[MLHR-1310] - Info Menu Links\n\n\n[MLHR-1312] - Lock Layout (malhar-angular-dashboard)\n\n\n[MLHR-1453] - System Diagnostics Page\n\n\n[MLHR-1460] - Installation Wizard: welcome page\n\n\n[MLHR-1461] - Installation Wizard: hadoop config page\n\n\n[MLHR-1467] - Installation Wizard: license screen\n\n\n[MLHR-1482] - Installation Wizard: summary screen\n\n\n[MLHR-1483] - Installation Wizard: license upload\n\n\n[MLHR-1496] - Create an operator for Solr\n\n\n[MLHR-1516] - Annotate new FS output operators\n\n\n[MLHR-1518] - WebSocket support for dimension demo\n\n\n[MLHR-1574] - UI Auth: support for current password auth\n\n\n[MLHR-1575] - UI Auth: Managing role and permissions\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-3123] - Add troubleshooting section to Operations and Installation Guide\n\n\n[SPOI-3124] - Check for vmem/pmem ratio during System Diagnostics test\n\n\n[SPOI-3129] - Integrate Kafka query protocol for AdsDimension demo.\n\n\n[SPOI-3154] - Setup AdsDimension demo on cluster\n\n\n[SPOI-3226] - Create JSON AdInfo Generator for App Builder Demo\n\n\n[SPOI-3227] - Crate JSON to Map converter for App Builder Demo\n\n\n[SPOI-3228] - Create shared dimensions computation schema for App Builder Demo\n\n\n\n\nBug\n\n\n\n\n[SPOI-1667] - Drop of Performance for Machine demo\n\n\n[SPOI-2742] - Core dumps on EMR\n\n\n[SPOI-2942] - Disable physical plan locking when there is no available license memory\n\n\n[SPOI-2947] - Provide a tool that distributes memory and generates licenses that can be deployed on multiple clusters\n\n\n[SPOI-3018] - Investigate raw performance of vertica jdbc\n\n\n[SPOI-3024] - Adding Virtual Mem to Physical Memory setting in Readme file\n\n\n[SPOI-3025] - Can't see the container logs from DT Console in Pivotal installation\n\n\n[SPOI-3053] - Continual Repartitioning Of Operator Causes Out Of Memory Exception\n\n\n[SPOI-3081] - Core StatsTest.testPortStatsPropagation Failing\n\n\n[SPOI-3107] - Failed to load Logical Plan \n\n\n[SPOI-3132] - User Interface Guide URL  specified in current Sandbox gives 404\n\n\n[SPOI-3181] - dtgateway script: $PATH is being appended to repeatedly when gateway has trouble starting\n\n\n[SPOI-3187] - HDHT query results partition duplication\n\n\n[SPOI-3190] - HDHT AdsDimension demo last bar should grow\n\n\n[SPOI-3195] - Operator class exception when loading app package\n\n\n[SPOI-3196] - CLI returns Perm Gen Space errors when there are too many classes to inspect in app package\n\n\n[SPOI-3197] - Provide meta information for operators that can be used by DAG builder\n\n\n[SPOI-3198] - App Package archetype is erroneously including app package manifest in the low level jar file\n\n\n[SPOI-3213] - Newer maven versions (\n3.1) do not work with copy-maven-plugin in app package archetype\n\n\n[SPOI-3215] - Duplicate streams in OperatorDeploymentInfo with default partition/parallel partitioning \n\n\n[SPOI-3232] - Mobile demo not dynamically partitioning\n\n\n[SPOI-3262] - Issue with serialization of xml cartesian product operator\n\n\n[SPOI-3319] - NPE in FSStatsRecorder\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-1967] - Accept Password for Private Key\n\n\n[SPOI-2997] - License file content updates\n\n\n[SPOI-3083] - Ability to save DAG as a DT App Package\n\n\n[SPOI-3084] - Operator Library Browser\n\n\n[SPOI-3087] - Properties Section\n\n\n[SPOI-3088] - Attributes Section\n\n\n[SPOI-3089] - Operators needed for demo\n\n\n[SPOI-3102] - FileSplitter needs to be idempotent\n\n\n[SPOI-3136] - Add a check to see if stram is connected to THIS gateway\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-3142] - Ability to take Logical Plan Serialization JSON to construct a DAG and launch it\n\n\n[SPOI-3148] - App Package REST API needs to allow PUT of json logical plan for adding/replacing application in app package\n\n\n[SPOI-3149] - Ability to launch a json-specified app within app package\n\n\n[SPOI-3185] - Import demo app packages from dtgateway fs\n\n\n[SPOI-3186] - Group app packages under demos folder in release\n\n\n[SPOI-3189] - Add display name and description to app package's manifest\n\n\n[SPOI-3199] - XML Javadoc\n\n\n[SPOI-3266] - Provide Ability to Allow Cross Origin Access\n\n\n\n\nTask\n\n\n\n\n[SPOI-3039] - Print data upon license mgr app start\n\n\n[SPOI-3061] - Add port tuple type in logical plan serialization\n\n\n[SPOI-3067] - Remove time-bombed public/private key pair from the code\n\n\n[SPOI-3068] - Make sure the release process (sandbox, installer binary) is okay with the time bound key pair removed from the code\n\n\n[SPOI-3069] - Remove the entire request blob process and generate license straight from customer info and license info\n\n\n[SPOI-3070] - Device ID change in license\n\n\n[SPOI-3071] - Generate License web tool to prompt for password for the private key\n\n\n[SPOI-3126] - O15 - Generic Dimension Computation/Store Operator\n\n\n[SPOI-3128] - HDHT - AdsDimension app integration for internal demo \n\n\n[SPOI-3130] - HDHT - Test files with varying block sizes\n\n\n[SPOI-3134] - Add a organization field in license file\n\n\n[SPOI-3162] - Kafka ingestion demo\n\n\n[SPOI-3164] - Integrate Kafka Query frontend into demos\n\n\n[SPOI-3165] - Ability to return JSON app as-is from app package through REST\n\n\n[SPOI-3166] - Ability to delete JSON apps from app package\n\n\n[SPOI-3167] - Ability to save incomplete json-based app to an app package\n\n\n[SPOI-3179] - Investigate how IDEs parse javadoc\n\n\n[SPOI-3182] - Complete dag annotations spec\n\n\n[SPOI-3193] - Investigate pluggable authorization mechanisms supported by Hadoop\n\n\n[SPOI-3200] - Generate resource file containing javadoc comments and custom tags\n\n\n[SPOI-3201] - Configure default javadoc doclet in pom to include custom tags\n\n\n[SPOI-3202] - Transform xml javadoc resources file to contain only comments and tags\n\n\n[SPOI-3203] - Add javadoc resource file to the class jar at maven build\n\n\n[SPOI-3206] - Add field and method comments to transformed xml javadoc\n\n\n[SPOI-3217] - Limit events DTFlumeSink pumps into the dag\n\n\n[SPOI-3219] - Help diagnose launch issue\n\n\n[SPOI-3225] - Check and fix machine and twitter hash tag demos\n\n\n[SPOI-3236] - displayName should be available from appPackage/applications call\n\n\n\n\nBug\n\n\n\n\n[MLHR-1347] - Create a block reader in library which is capable of dynamic partitioning itself\n\n\n[MLHR-1366] - HdfsBucketStore is not completely covered by unit test cases\n\n\n[MLHR-1367] - Add Counter Aggregators For Monitoring AbstractThroughputFSDirectoryInputOperator\n\n\n[MLHR-1373] - Container log download link\n\n\n[MLHR-1404] - App Package File Upload\n\n\n[MLHR-1407] - App List Kill App Selection Issue\n\n\n[MLHR-1410] - Add a \"remove\" option to the \"set/subscribe\" method of BaseCollection\n\n\n[MLHR-1412] - App Package archetype is erroneously including app package manifest in the low level jar file\n\n\n[MLHR-1419] - Xml cartesian product operator has kryo serialization errors in some cases\n\n\n[MLHR-1422] - UI Physical tab-\n sorting by operator id is by lexical order not numerical\n\n\n[MLHR-1423] - app instance page does not update when state goes ACCEPTED =\n RUNNING\n\n\n[MLHR-1426] - favicon not being handled in gulp build\n\n\n[MLHR-1436] - Uptime shows -1 day even when the app is runing for some time\n\n\n[MLHR-1438] - AdsDimensionsWithHDSDemo - Expose Aggregation in Operator Properties\n\n\n[MLHR-1442] - Move the recipient property for the machine data to config\n\n\n[MLHR-1557] - subscribe handler on app instance alters original ws message\n\n\n[MLHR-1558] - Palette in the Apps List does not update after one app gets killed\n\n\n\n\nImprovement\n\n\n\n\n[MLHR-1427] - Make app name link to instance page as well in AppsList\n\n\n\n\nNew Feature\n\n\n\n\n[MLHR-1388] - Base DAG Viewer\n\n\n[MLHR-1389] - App Package List Page\n\n\n[MLHR-1390] - Applications/Package Info Page\n\n\n[MLHR-1391] - App Package REST API Integration\n\n\n[MLHR-1392] - App Package CRUD\n\n\n[MLHR-1406] - UI Support of app package import\n\n\n[MLHR-1408] - Allow Shutting Down/Killing Multiple Applications\n\n\n[MLHR-1424] - UI Grid 3 Integration - Fonts\n\n\n[MLHR-1429] - Dashboard Component - Configurable Widget\n\n\n[MLHR-1430] - Kafka UI Demo\n\n\n[MLHR-1431] - Visual Data Demo Update\n\n\n\n\nStory\n\n\n\n\n[MLHR-1241] - App Package support in UI\n\n\n[MLHR-1357] - UI Dashboard Front Page Design\n\n\n\n\nTask\n\n\n\n\n[MLHR-1079] - Normalize timestamps for info pages\n\n\n[MLHR-1104] - Design App Package and Upload/Launch Feature\n\n\n[MLHR-1105] - Polish simplified (current) app data alert feature\n\n\n[MLHR-1207] - Convert demo apps to app packages\n\n\n[MLHR-1309] - License Info\n\n\n[MLHR-1363] - Create file splitter that breaks file into blocks and emits block metadata and a block reader \n\n\n[MLHR-1415] - Clean-up Hdfs Output operators in library and incorporate the features of fault-tolerant Writer\n\n\n[MLHR-1503] - Remove deprecated jdbc package from contrib\n\n\n\n\nVersion 1.0.4\n\n\nBug\n\n\n\n\n[SPOI-2346] - ExactlyOnceTest#testLinearInputOperatorRecovery hangs\n\n\n[SPOI-2511] - ResourceManager HA support\n\n\n[SPOI-2625] - uninstall.sh should print message that reminds users of running DT applications\n\n\n[SPOI-2922] - Counters Aggregator gets lost when an operator is parallel partitioned - test fix\n\n\n[SPOI-2939] - AppBundles test resource refers to fixed version\n\n\n[SPOI-2967] - Mobile demo dies after couple of days\n\n\n[SPOI-2979] - App Bundle unit test should load properties.xml\n\n\n[SPOI-2991] - [kafka-yarn] Archive resource to use less space in HDFS\n\n\n[SPOI-2993] - appBundle with conflicting names in config and ApplicationAnnotation can't launch in CLI with either name\n\n\n[SPOI-2995] - Generate MANIFEST.MF automatically instead of requiring user to change it\n\n\n[SPOI-2999] - Investigate application integration to Vertica\n\n\n[SPOI-3012] - launch application throws NPE when yarn.application.classpath is not defined\n\n\n[SPOI-3015] - DTGateway log grows indefinitely\n\n\n[SPOI-3019] - Investigate how many objects per second (with a single field) using JPA can be written to vertica\n\n\n[SPOI-3027] - Sandbox GATEWAY_CONNECT_ADDRESS change to support VMWare\n\n\n[SPOI-3055] - NullPointerException When Repartitioning Too Frequently\n\n\n[SPOI-3074] - NullPointerException in AppMaster\n\n\n[SPOI-3079] - NPE while launching application\n\n\n[SPOI-3080] - Content-disposition: attachment for container logs\n\n\n[SPOI-3091] - Null Pointer Exception/Internal Server Error When Getting Operator Stats\n\n\n[SPOI-3092] - DefaultUnifier of a port in an operator does not function correctly when the port has more than one sinks\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-2821] - Add bucket processed time\n\n\n[SPOI-2891] - DTGateway default log setting too verbose\n\n\n[SPOI-2938] - Document AppBundles development and deployment\n\n\n[SPOI-3044] - gateway should show the full stack trace of the origin upon error when proxying\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-2720] - API Call for determining if an application's stram can connect to the gateway\n\n\n[SPOI-2981] - HDS - File Format\n\n\n[SPOI-2982] - HDS - Writing Data Files\n\n\n[SPOI-2983] - HDS - Bucket Meta Data\n\n\n[SPOI-2984] - HDS - WAL\n\n\n[SPOI-2986] - HDS - Bucket Management\n\n\n[SPOI-3031] - Add ability to specify number of recording windows\n\n\n[SPOI-3032] - Delete request for recording\n\n\n[SPOI-3033] - Provide byte offset for each line when grepping container log\n\n\n[SPOI-3049] - Ingestion streamlet design for first cut\n\n\n\n\nTask\n\n\n\n\n[SPOI-1734] - Licensing Agent Logging for Audit\n\n\n[SPOI-2810] - Update document with details on password authentication\n\n\n[SPOI-2906] - Write documentation on App Bundle\n\n\n[SPOI-2908] - Prototype kafka-on-yarn\n\n\n[SPOI-2934] - Add application id to the tuple record topic\n\n\n[SPOI-2962] - Create operator to write events in bucket file and write index files for keys\n\n\n[SPOI-2963] - Change query processor operator to fetch events based on indices\n\n\n[SPOI-2996] - Support launch-app-bundle command without the appname argument\n\n\n[SPOI-3006] - Create a LATLON to MGRS converter utility\n\n\n[SPOI-3009] - Create a utility to calculate MD5 hash\n\n\n[SPOI-3010] - Create a utility for Blowfish\n\n\n[SPOI-3014] - Update sandbox release docs\n\n\n[SPOI-3026] - Change the name app bundle to app package and the extension zip to jar\n\n\n[SPOI-3036] - Review issue with ingestion\n\n\n[SPOI-3050] - A first cut at design for ingestion streamlet\n\n\n[SPOI-3051] - License work\n\n\n[SPOI-3108] - Add REST API to get the entire hadoop configuration\n\n\n[SPOI-3121] - HDS - File Format - Performance testing TFile / DTFile\n\n\n[SPOI-2985] - HDS - File Format - Performance testing - HFile\n\n\n[SPOI-2987] - HDS - File Format - Performance testing - MapFile\n\n\n[SPOI-3122] - HDS - File Format - HFile implementation\n\n\n[SPOI-3017] - HDS - File Format - Common Interface\n\n\n[SPOI-3100] - Create the SinglePointCalculator \n\n\n[SPOI-3103] - Create Synchronizer Operator\n\n\n[SPOI-3104] - Create Persister to store the data\n\n\n\n\nBug\n\n\n\n\n[MLHR-1223] - JDBC Store doesn't support connection properties\n\n\n[MLHR-1253] - documenting keyhashvalpair\n\n\n[MLHR-1255] - db api in malhar lib has KeyValueStore interface which is similar to db/cache/Store api \n\n\n[MLHR-1260] - AngularJS Migrations - Metrics Grouping\n\n\n[MLHR-1268] - AbstractBatchTransactionableStoreOutputOperator add to library db is same as public abstract class AbstractAggregateTransactionableStoreOutputOperator\n\n\n[MLHR-1269] - Mobile demo dies after couple of days\n\n\n[MLHR-1319] - AngularJS Migration - WebSocket Service\n\n\n[MLHR-1320] - AngularJS Migration - Page Visibility API\n\n\n[MLHR-1323] - stram events links broken (angular)\n\n\n[MLHR-1325] - Wrong \"track by\" in physical operators list\n\n\n[MLHR-1328] - \"inspect\" button in appslist broken\n\n\n[MLHR-1335] - dtPageHref directive does not update href of element on changes\n\n\n[MLHR-1336] - Container shorthand directive not watching value changes\n\n\n[MLHR-1342] - Allow DAG widgets to be vertically resized (using jquery resizable)\n\n\n[MLHR-1344] - AngularJS Migration - Metrics Chart\n\n\n[MLHR-1345] - AngularJS Migration - Physical Operator Page - Chart\n\n\n[MLHR-1346] - Test for AbstractFSDirectoryInputOperator fails sporadically\n\n\n[MLHR-1359] - AngularJS Migration - Dashboard Widgets Vertical Resize\n\n\n[MLHR-1364] - BucketManager while reading values doesn't ensure that latest value for a key is read\n\n\n[MLHR-1368] - AngularJS Migration - App Instance Page - Overview Grouping\n\n\n[MLHR-1374] - Remove all occurrences of dag.setAttribute(DAG.APPLICATION_NAME, \"xxx\") in demos code\n\n\n\n\nImprovement\n\n\n\n\n[MLHR-1263] - Migrate JDBC non-transaction output operator to new db api and move it to lib\n\n\n[MLHR-1322] - Changing Machine Data Operator and Stream Names\n\n\n[MLHR-1324] - appState directive should be renamed to something more generic\n\n\n[MLHR-1327] - Assume lodash is global throughout console\n\n\n[MLHR-1343] - Use userStorage to store height of stram events\n\n\n[MLHR-1351] - Allow transformResponse to set a fetchError\n\n\n[MLHR-1353] - Centralize location of breadcrumbs\n\n\n[MLHR-1360] - Put dashboards on the left\n\n\n\n\nNew Feature\n\n\n\n\n[MLHR-419] - DAG Widget with AngularJS\n\n\n[MLHR-1220] - Send the \"unsubscribe\" webSocket message for topic when no more listeners\n\n\n[MLHR-1239] - AngularJS Migration - Physical DAG View\n\n\n[MLHR-1240] - AngularJS Migration - Logical DAG View\n\n\n[MLHR-1243] - Angular JS Migration - Current/Recover Window Id Metrics\n\n\n[MLHR-1245] - AngularJS Migration - Partitions/Container Metrics\n\n\n[MLHR-1246] - AngularJS Migrations - DAG View Zoom\n\n\n[MLHR-1247] - AngularJS Migration - Physical View\n\n\n[MLHR-1248] - AngularJS Migration - Container View\n\n\n[MLHR-1249] - AngularJS Migration - Landing Page High-Level Metrics\n\n\n[MLHR-1250] - AngularJS Migration - Charts\n\n\n[MLHR-1251] - AngularJS Migration - Gauge Widget\n\n\n[MLHR-1252] - AngularJS Migration - DAG View Base Renderer\n\n\n[MLHR-1278] - AngularJS Migration - App Instance Page - Logical View\n\n\n[MLHR-1279] - AngularJS Migration - App Instance Page - Physical View\n\n\n[MLHR-1280] - AngularJS Migration - App Instance Page - Physical DAG View\n\n\n[MLHR-1283] - AngularJS Migration - App Instance Page - Metric View\n\n\n[MLHR-1298] - AngularJS Migration - Physical Operator Page - Overview\n\n\n[MLHR-1299] - AngularJS Migration - Physical Operator Page - Port List\n\n\n[MLHR-1300] - AngularJS Migration - Physical Operator Page - Properties\n\n\n[MLHR-1303] - AngularJS Migration - Logical Operator Page - Overview\n\n\n[MLHR-1305] - AngularJS Migration - Logical Operator Page - Partitions\n\n\n[MLHR-1306] - AngularJS Migration - Logical Operator Page - Chart\n\n\n[MLHR-1308] - AngularJS Migration - Logical Operator Page - Properties\n\n\n[MLHR-1313] - AngularJS Migration - App Instance Page - Physical View - Container List\n\n\n[MLHR-1314] - AngularJS Migration - App Instance Page - Physical View - Operators\n\n\n[MLHR-1315] - AngularJS Migration - App Instance Page - Logical View - Chart\n\n\n[MLHR-1316] - AngularJS Migration - App Instance Page - Logical View - Stream List\n\n\n[MLHR-1321] - AngularJS Migration - Distribution Build with Gulp\n\n\n[MLHR-1334] - Front-End Build with Gulp - WebSocket Proxy\n\n\n[MLHR-1337] - Front-End Build with Gulp - JS/CSS Revisions\n\n\n[MLHR-1338] - Front-End Build with Gulp - Angular Templates Injection\n\n\n[MLHR-1341] - create userStorage service for user settings\n\n\n[MLHR-1358] - Breadcrumbs to jump between collection elements\n\n\n[MLHR-1361] - Logical/Physical DAG - Stream Locality Legend\n\n\n[MLHR-1365] - AngularJS Migration - Widget Settings Modal - Height Management\n\n\n\n\nTask\n\n\n\n\n[MLHR-1208] - Convert contrib apps to app bundles\n\n\n[MLHR-1244] - Update tuple recorder topic to use \"applications.\n.tupleRecorder.\n\"\n\n\n[MLHR-1254] - Create a test case for UniqueValueCountAppender\n\n\n[MLHR-1264] - Start Logical Operator Page\n\n\n[MLHR-1265] - Start Physical Operator Page\n\n\n[MLHR-1266] - Start Container Page View\n\n\n[MLHR-1271] - Container Page: overview\n\n\n[MLHR-1273] - Container Page: operator list\n\n\n[MLHR-1275] - Container Page: log viewer\n\n\n[MLHR-1276] - Container Page: page module\n\n\n[MLHR-1284] - Stream View: overview\n\n\n[MLHR-1285] - Stream View: page module\n\n\n[MLHR-1289] - Port View: page module\n\n\n[MLHR-1302] - Operations Landing Page: memory gauge\n\n\n[MLHR-1311] - Vertical Resize (malhar-angular-dashboard)\n\n\n[MLHR-1318] - Move webSocket service to malhar-angular-widgets\n\n\n[MLHR-1326] - Create .jshintrc for IDE/editor, grunt tasks\n\n\n[MLHR-1330] - Document gulp usage in README\n\n\n[MLHR-1333] - add \"gulp coverage\" task\n\n\n[MLHR-1339] - Need a dynamic partitioner for File processing operator where the count of the operator instances is controlled by the backlog present\n\n\n[MLHR-1348] - Change app bundle to app package on all demos\n\n\n[MLHR-1349] - Link tables to userStorage service\n\n\n[MLHR-1372] - Adding the tool-tip capability to console\n\n\n[MLHR-1376] - Add counters and counters aggregator to mobile demo\n\n\n\n\nVersion 1.0.3\n\n\nBug\n\n\n\n\n[SPOI-2620] - S3 reader error\n\n\n[SPOI-2673] - Move the Kafka Exactly Once Producer to Malhar Library\n\n\n[SPOI-2775] - Log file being processed on seperate port to monitor progress by input operators.\n\n\n[SPOI-2813] - Send top gateways in descending order\n\n\n[SPOI-2814] - Change field name of input event in CDREvent\n\n\n[SPOI-2815] - Show top 3 gateways using a widget\n\n\n[SPOI-2816] - No query poll in alert information view\n\n\n[SPOI-2817] - For non-finalized buckets threshold flags should not be set\n\n\n[SPOI-2819] - Send data continuously to the application\n\n\n[SPOI-2820] - Provide more alerts in the UI\n\n\n[SPOI-2837] - Operators crashing sporadically when alert information requested from frontend\n\n\n[SPOI-2838] - Clear out the cdr event files and aggregates are cleared\n\n\n[SPOI-2844] - Machine data demo waits long before starting\n\n\n[SPOI-2845] - Recovery is failing in cdr events output operator after upstream operator redeploys\n\n\n[SPOI-2854] - Buckets are not being finalized correctly when there is a bucket gap in input data\n\n\n[SPOI-2861] - GATEWAY_CONNECT_ADDRESS not set during install\n\n\n[SPOI-2866] - Change lablel of top gateways to top 3 gateways\n\n\n[SPOI-2867] - Change null to empty value in the UI for alert event information\n\n\n[SPOI-2869] - Need a cleanup script to delete old files from hdfs\n\n\n[SPOI-2882] - Backup the allatory log file for flume integration build as well\n\n\n[SPOI-2883] - NPE in bufferserver\n\n\n[SPOI-2892] - DTGateway WS fails to get YARN logs\n\n\n[SPOI-2894] - Investigate bucket manager for cdr event storage\n\n\n[SPOI-2900] - First few events for an application do not get published\n\n\n[SPOI-2902] - Documentation for sandbox missing images\n\n\n[SPOI-2910] - PartitioningTest.testDynamicDefaultPartitioning sometimes fails and sometimes succeeds \n\n\n[SPOI-2913] - Events API returns no events when limit and offset not supplied\n\n\n[SPOI-2919] - Change cleanup to cleanup folders older than an hour\n\n\n[SPOI-2920] - make stats recorder asynchronous so that it won't block stram\n\n\n[SPOI-2921] - Test Node Locality feature on HDP\n\n\n[SPOI-2932] - Historical containers don't show up on gateway if there is a second attempt on application\n\n\n[SPOI-2936] - Historical data store high level design\n\n\n[SPOI-2937] - Gateway throwing exception while launching app \n\n\n\n\nImprovement\n\n\n\n\n[SPOI-2811] - Show input record in alert information dialog\n\n\n[SPOI-2812] - Show top gateways for all defect aggregates and possibly all aggregates\n\n\n[SPOI-2881] - Show actual memory usage in dashboard\n\n\n[SPOI-2891] - DTGateway default log setting too verbose\n\n\n[SPOI-2911] - Create a Bean2String codec\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-2228] - Gateway to load and to allow manipulation of app bundles\n\n\n[SPOI-2753] - Front End POC - Aggregates Table\n\n\n[SPOI-2757] - Front End POC - Kafka Operations\n\n\n[SPOI-2758] - Front End POC - Historical Data Navigation\n\n\n[SPOI-2808] - Support YARN log aggregation for dead container log retrieval in the gateway\n\n\n[SPOI-2826] - Maven archetype for assembling app bundle\n\n\n[SPOI-2827] - CLI to load app bundles\n\n\n[SPOI-2828] - Supports launching apps in app bundle in CLI\n\n\n[SPOI-2829] - Gateway to allow uploading and changing configuration of existing App Bundles\n\n\n[SPOI-2849] - Demonstrate Datatorrent RTS as front end ingestion for Spark.\n\n\n[SPOI-2879] - Kafka Request/Response Debug Collapsible Panel\n\n\n[SPOI-2896] - Unique identifier for stram events\n\n\n\n\nStory\n\n\n\n\n[SPOI-2835] - Streamlet Design - 1.0.3\n\n\n[SPOI-2836] - Kafka integration for 1.0.3\n\n\n[SPOI-2851] - Platform Excellence - 1.0.3\n\n\n\n\nTask\n\n\n\n\n[SPOI-2212] - Determine logistics of application bundles\n\n\n[SPOI-2521] - Add counters for checkpointo operations\n\n\n[SPOI-2546] - Pi demo does not seem to work in 512MB\n\n\n[SPOI-2547] - Twitter demo needs 1024MB\n\n\n[SPOI-2684] - Automated distro certification - collect minimum requirements\n\n\n[SPOI-2786] - Include setters and getters for output fields in the event\n\n\n[SPOI-2795] - Add top 3 gateways to alert aggregate csv in HDFS\n\n\n[SPOI-2802] - Dynamic partitioning for S3 input operator.\n\n\n[SPOI-2822] - Add operator to write cdr events to hdfs\n\n\n[SPOI-2823] - Fetch events from hdfs for alert events query response\n\n\n[SPOI-2846] - Partition Aggregations operator\n\n\n[SPOI-2859] - Separate out query processing from aggregations operator\n\n\n[SPOI-2870] - Application template classpath does not include malhar libraries\n\n\n[SPOI-2874] - Add Port Queue Capacity usage as part of the operator stats\n\n\n[SPOI-2878] - Move the gateway randomization to generator\n\n\n[SPOI-2890] - Update support contacts to malhar-users group\n\n\n[SPOI-2914] - Configure Store operator to join logger and tracker stream.\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-2747] - Total emitted window of an operator keeps on increasing even when it has stopped emitting\n\n\n[SPOI-2850] - Sort out the dependencies for dt-flume so we do not include the jars which are already part of dt dist and hadoop dist\n\n\n[SPOI-2873] - Deprecate ShipContainingJars.\n\n\n[SPOI-2895] - launch -license does not work\n\n\n[SPOI-2907] - Counters appear in physical plan only when a Counters aggregator or a stats listener is set on the operator\n\n\n\n\nBug\n\n\n\n\n[MLHR-1176] - Update demo section of README.md\n\n\n[MLHR-1219] - Non-chrome browsers cannot parse date format from date picker\n\n\n[MLHR-1227] - AbstractKafkaOutputOperator fails with java.lang.NoClassDefFoundError: com/yammer/metrics/Metrics\n\n\n[MLHR-1235] - close button on gateway info modal does not close the modal\n\n\n\n\nImprovement\n\n\n\n\n[MLHR-1213] - Use trackBy for malhar-angular-table\n\n\n[MLHR-1216] - DB key value store manager enhancement\n\n\n[MLHR-1218] - Clicking on an open stram log event should close it in UI\n\n\n\n\nNew Feature\n\n\n\n\n[MLHR-1212] - Make scrollbar draggable on malhar-angular-table\n\n\n[MLHR-1225] - Logical Operator Table\n\n\n[MLHR-1226] - Stram Event widget\n\n\n[MLHR-1229] - Provide better widget options dialog\n\n\n[MLHR-1231] - Custom template/controller for widget options\n\n\n\n\nTask\n\n\n\n\n[MLHR-1205] - Restructure directory structure in angular console\n\n\n[MLHR-1206] - Change malhar pom to use provided scope to prevent DT and hadoop jars from being sucked into the runtime classpath\n\n\n[MLHR-1210] - Create script to automatically add scripts to index.html\n\n\n[MLHR-1224] - Kafka enabled AdsDimensions demo\n\n\n\n\nVersion 1.0.2\n\n\nBug\n\n\n\n\n[SPOI-431] - Fix mergeSort operator to actually do mergeSort\n\n\n[SPOI-2499] - Provide license delegation tokens in secure environment\n\n\n[SPOI-2500] - Provide new delegation token before an old one expires in secure enviroment\n\n\n[SPOI-2501] - dt.log not written under CDH5 GA\n\n\n[SPOI-2506] - [MapR]FileSystem.mkdirs() doesn't work for existing folders in MapRFileSystem\n\n\n[SPOI-2605] - DT install as root cannot use /user/chetan/dt0528 as DFS location and not printing out detailed error\n\n\n[SPOI-2608] - LicensingAppMaster's name should not be obfuscated\n\n\n[SPOI-2617] - Memory usage counted by license agent and not released after app is killed\n\n\n[SPOI-2619] - Front End Server - fill missing slots for time series\n\n\n[SPOI-2630] - Not able to launch apps in cloudera cluster\n\n\n[SPOI-2634] - Uninstall is removing entire datatorrent folder as opposed to the release folder only\n\n\n[SPOI-2639] - dtcli allows apps to be launched before configuration is finished\n\n\n[SPOI-2668] - Parent jira for Kafka work for 1.0.2\n\n\n[SPOI-2678] - Put Fraud demo into the demo UI\n\n\n[SPOI-2679] - Front End Server - Query ID as Query JSON\n\n\n[SPOI-2683] - Appmaster Logs are not shown in the Console\n\n\n[SPOI-2689] - set-operator-property produces NPE on stram \n\n\n[SPOI-2699] - Mobile demo app mis-behaving \n\n\n[SPOI-2703] - Gateway fails to start in devel mode with hadoop 2.3.0\n\n\n[SPOI-2705] - Demo UI instructions missing from install README\n\n\n[SPOI-2706] - Logical plan change fails w/ obfuscated build\n\n\n[SPOI-2709] - Partition events should not be registered if same no. partitions result\n\n\n[SPOI-2710] - Containers running the unifier are not released\n\n\n[SPOI-2723] - Design and develop the File Ingestion app\n\n\n[SPOI-2737] - Gateway installation replaces dt-site.xml with invalid version\n\n\n[SPOI-2744] - ZIP of version's docs not being made available for download\n\n\n[SPOI-2748] - dtdemos service fails to stop in sandbox\n\n\n[SPOI-2749] - docs distribution files at root path\n\n\n[SPOI-2759] - Installation instructions for user setup\n\n\n[SPOI-2760] - Get container log content fails with 500\n\n\n[SPOI-2761] - Remove dtadmin reference from install wizzard\n\n\n[SPOI-2762] - EMR configuration issues\n\n\n[SPOI-2779] - dtcli failed to read the license file which was uploaded through UI\n\n\n[SPOI-2798] - Request for log content outside of range results in 500, long request time\n\n\n[SPOI-2839] - set-operator-property broken in master\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-2602] - Containers published via websocket should only contain live containers and recently finished containers\n\n\n[SPOI-2610] - Parent: Automatic unobfuscation tool\n\n\n[SPOI-2613] - Create web tool to run unobfuscator\n\n\n[SPOI-2614] - Automate transfer of allatori-log.xml to web server\n\n\n[SPOI-2712] - Add window_width to app info REST call\n\n\n[SPOI-2736] - Merge /systemAlerts/alerts and /systemAlerts/inAlerts API calls\n\n\n[SPOI-2751] - Include sandbox README.html with docs.zip and on website\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-950] - Specify memory requirements on per operator basis (duplicate)\n\n\n[SPOI-2220] - Gateway App Bundle API spec\n\n\n[SPOI-2339] - Enable container size for each operator(s)\n\n\n[SPOI-2515] - Ability to dynamically change the logger level for any instantiated loggers within the application\n\n\n[SPOI-2523] - System alerts for application state and metrics\n\n\n[SPOI-2525] - Add operator serialization check in local mode\n\n\n[SPOI-2528] - Expose \"counters\" in REST API \n\n\n[SPOI-2575] - Kafka Pub/Sub Protocol\n\n\n[SPOI-2588] - Record physical counters per window\n\n\n[SPOI-2589] - Expose counters logical and physical through web services/ web socket\n\n\n[SPOI-2606] - Initializing loggers with levels specified in the configuration\n\n\n[SPOI-2627] - Front End Server - \"Countdown\" usage\n\n\n[SPOI-2629] - Front End Server - Support of Multiple Partitions\n\n\n[SPOI-2631] - Front End Server - Cache Expiration Strategy\n\n\n[SPOI-2632] - Front End Server - Kafka Reconnect\n\n\n[SPOI-2633] - Front End Server - Page Performance (Visibility API)\n\n\n[SPOI-2638] - Front End Server - Kafka Errors Handling\n\n\n[SPOI-2645] - Front End Server - Kafka SimpleConsumer\n\n\n[SPOI-2652] - Develop and Stage Hadoop Summit Landing Page\n\n\n[SPOI-2696] - Front End Server - Top N Metrics\n\n\n[SPOI-2697] - Front End Server - Dynamic Publisher and Site\n\n\n[SPOI-2698] - Front End Server - Kafka Troubleshooting\n\n\n[SPOI-2752] - Front End Server - LRU Cache\n\n\n[SPOI-2756] - Front End POC - Alert Modal\n\n\n[SPOI-2804] - Create a hbase operator that uses a config to map incoming csv tuples to hbase table data and saves them in hbase\n\n\n\n\nStory\n\n\n\n\n[SPOI-2455] - HDFS storage layer for 1.0.2\n\n\n[SPOI-2565] - Front End Server - Define cache policy for push data in Node.js\n\n\n[SPOI-2571] - Parent jira for Hadoop Summit work\n\n\n[SPOI-2590] - Parent: GA Testing\n\n\n[SPOI-2611] - Parent jira for facilitating trouble shooting and debugging in 1.0.2\n\n\n[SPOI-2615] - Parent jira for security work in 1.0.2\n\n\n[SPOI-2662] - Parent jira for licensing security for 1.0.2\n\n\n[SPOI-2664] - HDFS Storage, Ingestion, Access application data and file storage for 1.0.2\n\n\n[SPOI-2667] - Automate distro certification and make it part of CI for 1.0.2\n\n\n[SPOI-2781] - Determine which fields are present in output that are not present in input\n\n\n\n\nTask\n\n\n\n\n[SPOI-2341] - Update on authentication and authorization in Yarn-open source\n\n\n[SPOI-2434] - Short document on deduper checkpointing scheme\n\n\n[SPOI-2440] - Kafka support in POC for 1.0.2\n\n\n[SPOI-2509] - Investigate how to use Kafka to replace the pub/sub mechanism in Gateway\n\n\n[SPOI-2532] - Join Operator\n\n\n[SPOI-2534] - Add support of getting container info of dead applications and dead containers\n\n\n[SPOI-2535] - Add support for retrieving Aggregated Counters from Response processStats of StatsListener\n\n\n[SPOI-2542] - Get user to look at instructions before .bin file download\n\n\n[SPOI-2544] - Twitter demo \"feedMultiplier\" should be RW and test to ensure that it can be changed in runtime\n\n\n[SPOI-2548] - Memory gauge always shows 100%\n\n\n[SPOI-2549] - Total memory on right corner needs to be discussed\n\n\n[SPOI-2552] - Need to list certified Distros on site\n\n\n[SPOI-2558] - Help set up Kafka operations\n\n\n[SPOI-2568] - Making Kafka Producer Exactly Once\n\n\n[SPOI-2569] - Add the Rewind Feature \n\n\n[SPOI-2573] - Integrate the Scoring of the quality logs  into the DAG\n\n\n[SPOI-2574] - Add counter calculation in hdfs operator\n\n\n[SPOI-2584] - DT counters as Key,Val (String,Number)\n\n\n[SPOI-2585] - Troubleshooting work for reporting min, max, ave, ... on resources\n\n\n[SPOI-2591] - GA Testing: CDH5 end-to-end\n\n\n[SPOI-2592] - GA Testing: HDP2 end-to-end\n\n\n[SPOI-2593] - GA Testing: MapR end-to-end\n\n\n[SPOI-2594] - GA Testing: UI end-to-end\n\n\n[SPOI-2595] - GA Testing: dtcli / gateway end-to-end\n\n\n[SPOI-2596] - GA Testing: Apache Hadoop end-to-end\n\n\n[SPOI-2597] - GA Testing: High Availability / Recovery\n\n\n[SPOI-2598] - GA Testing: Application configuration and launch\n\n\n[SPOI-2599] - GA Testing: Sandbox functionality\n\n\n[SPOI-2600] - GA Testing: Sandbox UX\n\n\n[SPOI-2621] - Top 10 support\n\n\n[SPOI-2622] - Partitioning of DimensionStore operator\n\n\n[SPOI-2624] - Front End Server - Architecture\n\n\n[SPOI-2626] - Setting up of number of partitions for Kafka Producer\n\n\n[SPOI-2636] - Twitter demo app with hashtag top 10\n\n\n[SPOI-2640] - Setup UI server for HDP grid for Hadoop Summit\n\n\n[SPOI-2641] - Setup Ambari for Hadoop Summit Demo\n\n\n[SPOI-2642] - Run Twitter demo on HDP cluster for Hadoop Summit\n\n\n[SPOI-2643] - Setup Mobile demo for HDP demo at Hadoop Summit\n\n\n[SPOI-2644] - Set up machine data demo on HDP cluster for Hadoop Summit\n\n\n[SPOI-2646] - Setup Ads demo on HDP cluster for Hadoop Summit\n\n\n[SPOI-2647] - Add the new Twitter HashTag top 10 demo to Frontend server for Summit\n\n\n[SPOI-2650] - Change default license memory settings\n\n\n[SPOI-2651] - Milestone 1 update\n\n\n[SPOI-2670] - Launch twitter hashtags on CDH and HDP clusters\n\n\n[SPOI-2671] - Update CDH cluster DT UI\n\n\n[SPOI-2672] - Move the HDFSOutputOperator to Malhar Library\n\n\n[SPOI-2676] - Get events, tuple records from kafka\n\n\n[SPOI-2680] - Make more managable kafka cluster\n\n\n[SPOI-2682] - Customer Demos Support\n\n\n[SPOI-2685] - Automated distro certification - virtualization review\n\n\n[SPOI-2686] - Automated distro certification - provisioning review\n\n\n[SPOI-2687] - Certify DT on Pivotal HD\n\n\n[SPOI-2688] - Make twitter HashTags links in demo\n\n\n[SPOI-2690] - Upgrade security sections in guides\n\n\n[SPOI-2700] - Test DimensionStore operator checkpointing and recovery\n\n\n[SPOI-2714] - Document stram event types\n\n\n[SPOI-2732] - Create a multi level key map with multiple keys\n\n\n[SPOI-2733] - Create a CSV lookup class to load CSV and lookup values based on row keys\n\n\n[SPOI-2735] - Ensure all dimemsions are implemented and covered by test\n\n\n[SPOI-2763] - Create a csv parser framework that can handle the different csv file formats\n\n\n[SPOI-2764] - Create a template application\n\n\n[SPOI-2765] - Create the template configuration file\n\n\n[SPOI-2766] - Create kafka input for the application \n\n\n[SPOI-2767] - Add a directory scan operator that progressively scans for new folders and files for input data to the application\n\n\n[SPOI-2768] - Add a kafka upload script to upload data to the application\n\n\n[SPOI-2769] - Create an upload script that uploads application files to hdfs\n\n\n[SPOI-2773] - Add a property to clear the aggregates in the AggregationsOperator\n\n\n[SPOI-2785] - Include the input event string in the CDR event\n\n\n[SPOI-2789] - Add query support for getting events for alerts\n\n\n[SPOI-2790] - Scale the input events into millions\n\n\n[SPOI-2792] - Send top 3 gateway list in alert event details\n\n\n[SPOI-2793] - Add the capability to finalize a bucket \n\n\n[SPOI-2794] - Add ability to clear out old results when replaying data\n\n\n[SPOI-2797] - Add query support to send the latest five minute buckets and aggregates\n\n\n[SPOI-2800] - Install a kafka web-console in our cluster\n\n\n[SPOI-2805] - Investigate yarn log aggregation to see how this fits into our container logs API\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-2725] - Create skeleton App\n\n\n[SPOI-2728] - Convert the Skeleton Operators into Real Operators - Part I\n\n\n[SPOI-2739] - File input operator\n\n\n[SPOI-2740] - DAG level unit test\n\n\n[SPOI-2776] - Test and Certify RPM packaging for Cloudera\n\n\n[SPOI-2777] - Gateway throws a bunch of exceptions when trying to install using rpm\n\n\n\n\nBug\n\n\n\n\n[MLHR-724] - RabbitMQ test timing issue\n\n\n[MLHR-1128] - Breadcrumbs should say physical operator, not operator on port page\n\n\n[MLHR-1129] - DAG should not display underneath controls\n\n\n[MLHR-1137] - Memory gauge always shows 100%\n\n\n[MLHR-1142] - Front End Server - Kafka Reconnect\n\n\n[MLHR-1143] - KafkaOutputOperator is not configurable for Kafka Producer properties\n\n\n[MLHR-1144] - Container Actions - use wording that's not confusing\n\n\n[MLHR-1157] - Parent JIRA for Front End Server\n\n\n[MLHR-1159] - Stop Recording function does not give visual indication of stopped recording unless refreshed\n\n\n[MLHR-1169] - Update readme for console repo\n\n\n[MLHR-1170] - Appmaster Logs are not shown in the Console\n\n\n[MLHR-1185] - Stram Event widget should check physical plan for existence of operator\n\n\n[MLHR-1186] - some numbers in logical dag hidden behind control\n\n\n[MLHR-1189] - AbstractHdfsTupleFileOutputOperator output port is not optional\n\n\n[MLHR-1192] - Parametrize the demos\n\n\n[MLHR-1195] - App master container is not showing proper metrics\n\n\n[MLHR-1202] - failureCount missing for physical operators\n\n\n\n\nImprovement\n\n\n\n\n[MLHR-1132] - Improvements to errors in the stram events widget\n\n\n[MLHR-1146] - Only get active containers for initial container list\n\n\n[MLHR-1167] - UI support for searching log levels of classes\n\n\n[MLHR-1168] - Validate input for log level widget\n\n\n[MLHR-1194] - Align app counts with labels in cluster overview widget\n\n\n[MLHR-1209] - Support JMS providers other than ActiveMQ\n\n\n\n\nNew Feature\n\n\n\n\n[MLHR-921] - Installer - List of Errors/Codes\n\n\n[MLHR-1111] - Container Log Widget, using new REST call\n\n\n[MLHR-1121] - Front End Server - Kafka Query JSON\n\n\n[MLHR-1122] - Front End Server - Kafka Keep Alive\n\n\n[MLHR-1123] - Front End Server - Kafka Parameterized Queries\n\n\n[MLHR-1124] - Front End Server - Kafka Response Debug/Format\n\n\n[MLHR-1125] - Front End Server - Kafka Pub/Sub Protocol for Widgets\n\n\n[MLHR-1130] - Widget to change the log levels of classes dynamically\n\n\n[MLHR-1133] - Put \"float\" control in widget config popup\n\n\n[MLHR-1134] - Front End Server - Editable JSON request\n\n\n[MLHR-1135] - Front End Server - Setup Instructions\n\n\n[MLHR-1136] - Front End Server - Packaging/Distribution\n\n\n[MLHR-1147] - Front End Server - Selecting Metrics\n\n\n[MLHR-1149] - Front End Server - Use Latest Kafka Offset\n\n\n[MLHR-1151] - Front End Server - Data Validation\n\n\n[MLHR-1152] - Front End Server - Page Performance (Visibility API)\n\n\n[MLHR-1175] - Widget Library - Select Widget\n\n\n[MLHR-1179] - AngularJS Migration - Application Instance Page\n\n\n[MLHR-1181] - AngularJS Migration - Application Overview Widget\n\n\n[MLHR-1182] - Angular JS Migration - App Structure\n\n\n[MLHR-1187] - UI for adding system alerts\n\n\n[MLHR-1193] - Create Basic Counters and its aggregator in library\n\n\n\n\nStory\n\n\n\n\n[MLHR-1178] - AngularJS Migration for 1.0.2\n\n\n\n\nTask\n\n\n\n\n[MLHR-960] - Create a Tableau output adapter\n\n\n[MLHR-1090] - Create first resource objects (models) for angular console\n\n\n[MLHR-1106] - Improve ended app page\n\n\n[MLHR-1139] - Add HTTP Get Operator\n\n\n[MLHR-1140] - Add HDFS output operator that writes to file names specified in tuple\n\n\n[MLHR-1160] - Create HDFS Output Operator to write with exactly once semantics\n\n\n[MLHR-1162] - Implement app list with angular\n\n\n[MLHR-1164] - Wireframe for UI DAG builder\n\n\n[MLHR-1171] - Make twitter hashTab a link in twitter demo\n\n\n[MLHR-1172] - Create a Cassandra read operator\n\n\n[MLHR-1173] - Create a Cassandra write operator\n\n\n\n\nSub-task\n\n\n\n\n[MLHR-1163] - Wireframe for DAG builder\n\n\n\n\nVersion 1.0.1\n\n\nBug\n\n\n\n\n[SPOI-1404] - Create seperate environment for node0 for demos\n\n\n[SPOI-1952] - Operation and Install guide review\n\n\n[SPOI-2203] - Configure adsDimesions and machine data to use minimum memory\n\n\n[SPOI-2340] - License Agent is persisting state in a wrong location\n\n\n[SPOI-2397] - Monitoring and fixing the bugs in Customer demos\n\n\n[SPOI-2408] - Verify DT Platform on HDP 2.1\n\n\n[SPOI-2410] - Master script to restart hadoop on the cluster\n\n\n[SPOI-2411] - Not able to starte license with 0.9.5 release\n\n\n[SPOI-2420] - Core doesn't compile with hadoop 2.4\n\n\n[SPOI-2448] - Test xml ingestion demo on a cluster\n\n\n[SPOI-2449] - Test golden gate demo on a cluster\n\n\n[SPOI-2450] - Test twitter sentiment demo on a cluster\n\n\n[SPOI-2474] - Containers not provisioned diagnostics error\n\n\n[SPOI-2477] - LicenceAgent client fails with NPE\n\n\n[SPOI-2489] - Operator downstream of EXACTLY_ONCE operator not checkpointing\n\n\n[SPOI-2490] - HDFS \n\n\n[SPOI-2491] - Testing the HDFS drain for Flume Sink\n\n\n[SPOI-2493] - Add HDFS operator updates into Malhar\n\n\n[SPOI-2496] - viewdag does not work any more\n\n\n[SPOI-2503] - Provide a rewind feature to replay tuples from a previous time\n\n\n[SPOI-2505] - Sandbox demos need customization\n\n\n[SPOI-2512] - Apps fail to start due to serialization errors on CDH5\n\n\n[SPOI-2533] - Fraud app needs debugging\n\n\n[SPOI-2543] - Applications not reporting the license information to License Agent if LA is restarted\n\n\n[SPOI-2550] - License Agent Dying\n\n\n[SPOI-2554] - Third party libraries which are required for demos are missing from /opt/datatorrent/releases/version/lib/\n\n\n[SPOI-2556] - Add demo guides to documentation index\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-2210] - Default license generation should be part of build process\n\n\n[SPOI-2468] - README for certifications and benchmarks\n\n\n[SPOI-2482] - DTCli should handle ^C more gracefully\n\n\n[SPOI-2494] - Add HTTP Get operator to Malhar\n\n\n[SPOI-2555] - Include README, LICENSE, and CHANGELOG in docs\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-406] - Document and test download to work with Hortonworks HDP 2.0\n\n\n[SPOI-408] - Certify with MapR (Hadoop 2.3) release in 1.0.1\n\n\n[SPOI-1146] - API for retrieving raw Container Logs\n\n\n[SPOI-1834] - Pub-Sub/REST mechanisms for exceptions thrown by an application\n\n\n[SPOI-2080] - Application to certify installation\n\n\n[SPOI-2253] - Username/password feature in the installer\n\n\n[SPOI-2381] - dt-site.xml logistics\n\n\n[SPOI-2387] - Aggregate Logical Operators on Gateway\n\n\n[SPOI-2402] - Update dt-site.xml with container memory less than 512MB  after getting the app-memory-automation results for all the apps.\n\n\n[SPOI-2460] - dt-site.xml new logistics implementation\n\n\n[SPOI-2470] - Create a certification property file for Distro/Install Certification\n\n\n[SPOI-2486] - Workaround for \"FileSyste.getScheme is not overrided by MapRFileSystem\"\n\n\n[SPOI-2519] - add grep feature when getting container log\n\n\n[SPOI-2526] - Add REST call to return info of historical containers\n\n\n\n\nTask\n\n\n\n\n[SPOI-62] - Buffer Server logging\n\n\n[SPOI-1206] - Review Licensing of Third Party Libraries\n\n\n[SPOI-1745] - Document Demo Application Setup\n\n\n[SPOI-1784] - License agent app to have webservice\n\n\n[SPOI-2199] - Readme file for AWS installation\n\n\n[SPOI-2223] - Test if all the applications with reduced container memory sizes run in sandbox 0.9.4.\n\n\n[SPOI-2348] - Enable checkpoint for aggregator operator\n\n\n[SPOI-2354] - Certify demos on 0.9.4 sandbox\n\n\n[SPOI-2370] - Fully automate Hadoop cluster restart\n\n\n[SPOI-2379] - Certify DT with Hadoop 2.3/CDH\n\n\n[SPOI-2392] - Add more basic demos to sandbox launcher\n\n\n[SPOI-2393] - Certify Amazon EMR Hadoop 2 version\n\n\n[SPOI-2414] - Stop the nightly benchmarking jenkins jobs \n\n\n[SPOI-2415] - remove the \"page has not loaded since 60 second\" message at top\n\n\n[SPOI-2422] - Add HDFS cache limit on FlumeAgent operator\n\n\n[SPOI-2424] - Log metrics across all poc operators to a file\n\n\n[SPOI-2425] - Compare HDFS with customer site\n\n\n[SPOI-2426] - Benchmark HDFS drain rate by agent operator\n\n\n[SPOI-2427] - Enable dedupper to be run with checkpointing turned OFF\n\n\n[SPOI-2428] - Evaluate bigger bucket size for checkpointing for deduper\n\n\n[SPOI-2430] - Number validation for poc\n\n\n[SPOI-2432] - Compute the checkpoint I/O for dedupper\n\n\n[SPOI-2439] - Compute deduper HDFS I/O for current checkpointing scheme\n\n\n[SPOI-2441] - Estimate the feed time for HDFS agent replay\n\n\n[SPOI-2445] - Certify Cloudera CDH 5.0 GA\n\n\n[SPOI-2451] - Gateway Load Test\n\n\n[SPOI-2457] - Widget Library\n\n\n[SPOI-2458] - Generate certification report for HDP 2.1\n\n\n[SPOI-2466] - Monitoring and notifications for cluster\n\n\n[SPOI-2475] - Document process to certify demo re-start\n\n\n[SPOI-2476] - Document process for grid restart\n\n\n[SPOI-2483] - Node.js Kafka Client POC\n\n\n[SPOI-2513] - Create demos ui service wrapper compatible with CentOS\n\n\n[SPOI-2517] - Configure customer demos ui for as a service\n\n\n[SPOI-2531] - HDFSOutputOperator write with exactly once semantics\n\n\n[SPOI-2535] - Add support for retrieving Aggregated Counters from Response processStats of StatsListener\n\n\n[SPOI-2537] - Need to list previous versions on datatorrent site\n\n\n[SPOI-2540] - Fix URL for AWS EMR document\n\n\n[SPOI-2551] - Define a process on backend to ensure archive of previous downloads works for customers\n\n\n[SPOI-2553] - Holding ticket for Malhar maintenance work\n\n\n\n\nSub-task\n\n\n\n\n[MLHR-774] - Terms of service license\n\n\n[MLHR-976] - Implement table module for angular\n\n\n\n\nBug\n\n\n\n\n[MLHR-998] - Compilation error while using UniqueValueCount operator.\n\n\n[MLHR-1054] - Update topic and rest calls for Logical/Physical Split\n\n\n[MLHR-1056] - Rename demo apps to include Demo suffix\n\n\n[MLHR-1089] - Error handling for license page\n\n\n[MLHR-1093] - AbstractKafkaInputOperator is not committing the txn to Kafka in commit call back\n\n\n[MLHR-1096] - Get last N events when tailing stram events\n\n\n[MLHR-1108] - Logical DAG resizing\n\n\n[MLHR-1113] - AdsDimension demo fails to launch\n\n\n[MLHR-1114] - FraudDetect demo fails to launch\n\n\n[MLHR-1115] - MachineData demo fails to launch\n\n\n[MLHR-1127] - HdfsTextFileInputOperator Fails during checkpointing\n\n\n[MLHR-1138] - Remove unused CPU columns in logical operator table\n\n\n\n\nImprovement\n\n\n\n\n[MLHR-1092] - Add environment variable override to rename demo apps\n\n\n[MLHR-1098] - Improved code coverage for malhar-angular-dashboard\n\n\n[MLHR-1099] - Extend dataModelOptions with non-serializable defaults\n\n\n[MLHR-1100] - Improved widget directive in malhar-angular-dashboard\n\n\n[MLHR-1116] - Use negative offset to get last N events for stram events widget\n\n\n[MLHR-1119] - Add a confirm box when killing app master container\n\n\n[MLHR-1120] - Save range selection between page loads for stram events\n\n\n\n\nNew Feature\n\n\n\n\n[MLHR-783] - DAG should adjust in height as the widget height changes\n\n\n[MLHR-784] - WidgetOutputOperator - Schema Update\n\n\n[MLHR-984] - STRAM decisions widget\n\n\n[MLHR-1070] - Web Demos - Upgrade to AngularJS 1.2.16\n\n\n[MLHR-1076] - Widget Library - WebSocket\n\n\n[MLHR-1077] - Widget Library - Data Models\n\n\n[MLHR-1087] - Dashboard Component - AngularJS Scope Documentation\n\n\n[MLHR-1088] - Dashboard Web App - Dependencies Documentation\n\n\n[MLHR-1095] - Node.js Kafka Integration\n\n\n[MLHR-1097] - Implement storage on malhar-angular-dashboard\n\n\n[MLHR-1118] - Add storage hash for clearing out invalid dashboards\n\n\n\n\nStory\n\n\n\n\n[MLHR-719] - Time Series Forecasting\n\n\n[MLHR-1060] - Widget Library\n\n\n[MLHR-1061] - Dashboard Component\n\n\n[MLHR-1082] - Parent jira for Real Time ETL Application\n\n\n\n\nTask\n\n\n\n\n[MLHR-700] - Develop operator for calculating Coefficient of Determination (RSquare)\n\n\n[MLHR-841] - Demo guide for machine data application\n\n\n[MLHR-850] - Demo guide for Ads Dimension demo\n\n\n[MLHR-852] - Demo guide for Twitter (rolling topwards) demo application\n\n\n[MLHR-874] - Demo guide for charting demo - Yahoo finance\n\n\n[MLHR-896] - widget output operator - widget type should not be decided in backend\n\n\n[MLHR-950] - Create a demo for distributed distinct\n\n\n[MLHR-953] - Set up \"upgrade path\" for CustomerApplications\n\n\n[MLHR-983] - UI to expose the \"events\" API of the gateway\n\n\n[MLHR-1042] - ETL: Consolidate output operators properties\n\n\n[MLHR-1063] - Create contributing section on the main page of malhar repo\n\n\n[MLHR-1064] - Create contributing secion on main page of UI repos\n\n\n[MLHR-1065] - create line chart operator\n\n\n[MLHR-1075] - add .travis.yml file to ui-console\n\n\n[MLHR-1078] - Update license header script, keep year updated\n\n\n[MLHR-1080] - create real time chart output operator\n\n\n[MLHR-1083] - Consolidate realtime output operator properties\n\n\n[MLHR-1084] - Consolidate aggregate operator properties\n\n\n[MLHR-1085] - Consolidate linechart operator properties\n\n\n[MLHR-1104] - Design App Bundle and Upload/Launch Feature\n\n\n\n\nVersion 1.0.0\n\n\nBug\n\n\n\n\n[SPOI-2086] - creation of logs directory under .dt folder of user\n\n\n[SPOI-2497] - Remove hard-coded \"hdfs\" scheme \n\n\n[SPOI-2501] - dt.log not written under CDH5 GA\n\n\n[SPOI-2577] - After installation, I cannot launch license agent\n\n\n[SPOI-2604] - The example DFS directory should be changed from /home/... to /user/...\n\n\n[SPOI-2607] - Install page should only list error (not warnings) issues\n\n\n[SPOI-2609] - When yarn.scheduler.minimum-allocation-mb is set to 256, PiDemo fails with NPE\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-2522] - Replace InputPort.getStreamCodec method with an attribute on InputPort called StreamCodec\n\n\n[SPOI-2576] - Change local install from .datatorrent to datatorrent directory\n\n\n[SPOI-2603] - installer complains about gateway has trouble starting but it actually runs OK\n\n\n[SPOI-2637] - Improve the Getting Started Guide\n\n\n\n\nTask\n\n\n\n\n[SPOI-2520] - Change the name of the CustomStats to Counters\n\n\n[SPOI-2578] - Check dir before mkdir call (MapR requirement)\n\n\n[SPOI-2579] - Take out launch from gateway webservice spec document\n\n\n[SPOI-2649] - Update software license\n\n\n\n\nBug\n\n\n\n\n[MLHR-1101] - Install errors formatting\n\n\n[MLHR-1103] - Change installer instructions for DFS directory path\n\n\n\n\nVersion 0.9.5.1\n\n\nBug\n\n\n\n\n[SPOI-2554] - Third party libraries which are required for demos are missing from /opt/datatorrent/releases/version/lib/\n\n\n[MLHR-1113] - AdsDimension demo fails to launch\n\n\n[MLHR-1114] - FraudDetect demo fails to launch\n\n\n[MLHR-1115] - MachineData demo fails to launch\n\n\n\n\nVersion 0.9.5\n\n\nSub-task\n\n\n\n\n[SPOI-1692] - Certify fault tolerance for multiple containers failure\n\n\n[SPOI-1693] - Develop automated test scenario for StrAM failure to certify one of high availability aspects. \n\n\n[SPOI-2008] - Setup virtual cluster for high availability certification\n\n\n[SPOI-2009] - WordCount app to be used in certification of high availability\n\n\n[SPOI-2127] - Error/Warning message not clear - nightly build installer (datatorrent-dist-0.9.4-SNAPSHOT.bin) if executed as non-root user.\n\n\n[SPOI-2128] - Nightly build installer needs to check if port 9090 is available before attempting to run dtgateway service.\n\n\n[SPOI-2129] - Error/Warning message not clear while updating the Hadoop configuration through Installer-Web-based UI\n\n\n[SPOI-2247] - Gateway user authentication REST API spec\n\n\n[SPOI-2248] - Implement user authentication and basic authorization in gateway\n\n\n[SPOI-2255] - Password protect the web socket\n\n\n[SPOI-2257] - Support HTTPS in gateway\n\n\n[SPOI-2277] - Support HTTPS and auth in pubsub connection from stram to Gateway\n\n\n[SPOI-2291] - Copy GW configurations to HDFS on every write\n\n\n\n\nTechnical task\n\n\n\n\n[SPOI-2005] - Verify and add Machine demo to app memory usage automation\n\n\n\n\nBug\n\n\n\n\n[SPOI-1801] - Mobile app has two logical input gen - Need to handle containers on servers with two diff clocks\n\n\n[SPOI-1890] - Application behavior when resources are not available\n\n\n[SPOI-1954] - Incorrect Processed Stats\n\n\n[SPOI-1995] - Sandbox instructions are outdated\n\n\n[SPOI-1998] - Killing of node manager on node running License AM\n\n\n[SPOI-2067] - Move apps from contrib to demos\n\n\n[SPOI-2152] - Stabillity of mobile demo\n\n\n[SPOI-2194] - Gateway start script prints out repetitive message\n\n\n[SPOI-2205] - Make generate license tool work with web tool to generate 1TB license in the new flow\n\n\n[SPOI-2207] - Deduper showed high latency and crashed in staging environment.\n\n\n[SPOI-2221] - Analyze FileSystem.get calls and if necessary replace them with FileSystem.newInstance\n\n\n[SPOI-2278] - Relative hdfs path should be created under application directory\n\n\n[SPOI-2283] - Do not make assumption about FSStorageAgent being the StorageAgent or the default configuration for it being the configuration the user set\n\n\n[SPOI-2284] - User 'dtadmin' should be added to the group having access to hadoop services and hdfs\n\n\n[SPOI-2285] - uninstall.sh does not uninstall the platform\n\n\n[SPOI-2288] - NullPointerException in buffer server\n\n\n[SPOI-2289] - FSStorageAgent store throws java.nio.channels.ClosedChannelException \n\n\n[SPOI-2295] - Monitoring and fixing the bugs in Customer demos\n\n\n[SPOI-2297] - Mobile Demo going to unstable state due to out of order tuple sequence error\n\n\n[SPOI-2298] - Events returned from REST API has funky keys\n\n\n[SPOI-2302] - Normalize WebSocket \n REST format for Stram Events\n\n\n[SPOI-2314] - Fix application names\n\n\n[SPOI-2315] - node1 application launch-xxx macros not working after upgrade\n\n\n[SPOI-2342] - DTCLI ignoring annotated Application Names\n\n\n[SPOI-2344] - dtcli fails to load dt-env.sh\n\n\n[SPOI-2349] - Update default license to 4 months\n\n\n[SPOI-2357] - Unable to launch application using launch alias, if the application names shown in the dtcli list are picked from dt-site.xml.\n\n\n[SPOI-2358] - Installer bin extrac does not preserve permissions\n\n\n[SPOI-2359] - Gateway fails to restart due to missing JAVA_HOME\n\n\n[SPOI-2361] - After global install local .dt directory may be missing when executing dtcli\n\n\n[SPOI-2364] - Catastrophic Error- tuples out of sequence in Generic Node\n\n\n[SPOI-2365] - Hadoop Shell throws exception for dt.attr.APPLICATION_PATH\n\n\n[SPOI-2371] - DTCLI updating the dt-site.xml\n\n\n[SPOI-2373] - Sandbox dt-site.xml is missing configurations related to Map reduce applications.\n\n\n[SPOI-2374] - The apps in sandbox use 1GB as the container size and not 512MB as specified under misc/sandbox/conf/dt/dt-site.xml\n\n\n[SPOI-2375] - configuration from dt-site.xml in user's directory is not picked by dtcli\n\n\n[SPOI-2376] - Gateway classpath references invalid directory\n\n\n[SPOI-2380] - get-physical-operator-properties wrong help text\n\n\n[SPOI-2389] - DataTorrent discovers override dt-site.xml in CLASSPATH\n\n\n[SPOI-2390] - DTGateway is killed after terminal is closed\n\n\n[SPOI-2391] - dtdemos service does not stop on sandbox\n\n\n[SPOI-2395] - Missing FraudDetection/AdsDeminsions/MachineData demos from sandbox.\n\n\n[SPOI-2396] - gateway not started after stopping and starting hadoop/datatorrent services in sandbox.\n\n\n[SPOI-2398] - Extra directory created by gateway in user home\n\n\n[SPOI-2400] - An operator exits normally when there is OutOfMemory error \n\n\n[SPOI-2401] - The AppMaster finishes after retrying 6 times to deploy input operator\n\n\n[SPOI-2412] - App restart HDFS file permission issue on CDH cluster\n\n\n[SPOI-2417] - DTGateway dt-site.xml backup location\n\n\n[SPOI-2419] - Ensure principal name propagates to containers in non-secure mode\n\n\n[SPOI-2435] - Latest changes to latency calculation break tests.\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-1376] - Have app specific configuration to stram-site.xml in sandbox\n\n\n[SPOI-1855] - Ability to continue/shutdown application\n\n\n[SPOI-1986] - In Logical View do not include the unifier stats\n\n\n[SPOI-2001] - Update Release Process document (GDrive)\n\n\n[SPOI-2222] - Constraint format in license file to be in JSON\n\n\n[SPOI-2287] - Allow specifying a complete list of dependencies while deploying the application\n\n\n[SPOI-2335] - Make BaseOperator Java Serializable\n\n\n[SPOI-2367] - Create service wrapper for demos server\n\n\n[SPOI-2385] - Improve the performance of HDFSStorage for POC\n\n\n[SPOI-2404] - Reset the Application Failure Count upon successful recovery\n\n\n[SPOI-2405] - Add more params to /ws/v1/applications/{appid}/events call\n\n\n[SPOI-2407] - Need ability to supply the class name representing the property\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-91] - Design and implement a socket adapter operator\n\n\n[SPOI-2007] - Automation to certify High Availability\n\n\n[SPOI-2012] - Run the app-memory-usage with lower container sizes starting from 128M to 512M when the grid is restarted.\n\n\n[SPOI-2249] - Simple user authentication / authorization support in the UI\n\n\n[SPOI-2355] - Add REST call to make existing license file the current license\n\n\n[SPOI-2386] - REST call to retrieve the datatorrent conf directory\n\n\n[SPOI-2399] - Starter Web Application\n\n\n\n\nStory\n\n\n\n\n[SPOI-2215] - Password protect the dashboard\n\n\n\n\nTask\n\n\n\n\n[SPOI-746] - DT Phome home customer side work\n\n\n[SPOI-1642] - Certify support for node failure\n\n\n[SPOI-2045] - Discuss license cleanup/improvements\n\n\n[SPOI-2099] - Show mandatory fields with '*' in the installer\n\n\n[SPOI-2101] - Say \"Terms of Service\" on the text\n\n\n[SPOI-2103] - Show error message on the side\n\n\n[SPOI-2117] - An error during uninstall\n\n\n[SPOI-2118] - 1 yr license mail not getting mailed\n\n\n[SPOI-2145] - Set up separate access to jars for big customers \n\n\n[SPOI-2178] - Filters HashMap from HDFSLoader\n\n\n[SPOI-2190] - Create web page (navigator and table view) for 5 dimensions and 2 computations\n\n\n[SPOI-2201] - Create a upgrade license page\n\n\n[SPOI-2202] - Upgrade Machine Data to latest code\n\n\n[SPOI-2211] - Getting too many deprecated warnings in demos\n\n\n[SPOI-2213] - Be able to diagnose Stram decisions\n\n\n[SPOI-2214] - Ability to access Stram decisions for failed/finished/killed app\n\n\n[SPOI-2216] - Upgrade Twitter Demo to latest Code\n\n\n[SPOI-2217] - Upgrade Fraud Detection Demo to latest Code\n\n\n[SPOI-2218] - Upgrade Ads Dimension Demo to latest Code\n\n\n[SPOI-2219] - Upgrade Mobile Locator Demo to latest Code\n\n\n[SPOI-2224] - Fraud demo issues \n\n\n[SPOI-2225] - Issues with Machine Data Demo\n\n\n[SPOI-2226] - Issues with Ads demo\n\n\n[SPOI-2233] - Remove max num container parameter that is included by default\n\n\n[SPOI-2234] - Give warning when operators are automatically inlined\n\n\n[SPOI-2237] - CPU column on logical view\n\n\n[SPOI-2243] - Use hammerdb to generate load against Oracle\n\n\n[SPOI-2256] - Document certification suite used to certify datatorrent platform\n\n\n[SPOI-2286] - Implement Goldengate java handler to capture db change and send to kafka\n\n\n[SPOI-2290] - Hide configuration and launch app for GA\n\n\n[SPOI-2292] - Add to README: not recommended to launch more than one gateway\n\n\n[SPOI-2293] - Manage license file on HDFS\n\n\n[SPOI-2303] - Make DFS error keys more fine grained \n\n\n[SPOI-2309] - Use connection to RM to guess the cluster accessible IP address for GATEWAY_CONNECT_ADDRESS\n\n\n[SPOI-2310] - Allow different filesystem from fs.defaultFS for dt.dfsRootDirectory\n\n\n[SPOI-2338] - The \"#\" in the boxes should not change\n\n\n[SPOI-2351] - Bundle the default passwd file (with admin/admin) in the installer\n\n\n[SPOI-2362] - Create service wrappers for Apache Hadoop on Sandbox\n\n\n[SPOI-2363] - Update dtdemo sandbox script to work with Hadoop/DTGateway service wrappers\n\n\n[SPOI-2394] - Install wizard shows up again after gateway restart\n\n\n[SPOI-2416] - Separate malhar and malhar-ui-console\n\n\n[SPOI-2418] - Create an ability for the DTFlumeSink to backoff if it suspects that the DAG is not processing the data in healthy fashion.\n\n\n[SPOI-2421] - Update end-user documentation\n\n\n\n\nSub-task\n\n\n\n\n[MLHR-726] - Time Series Forecasting Operator using Holt's Linear Trend Model\n\n\n[MLHR-933] - Time Series Forecasting using Holt-Winters' Seasonal Method\n\n\n[MLHR-965] - create non transactional output operator\n\n\n[MLHR-966] - create transactional output operator\n\n\n[MLHR-967] - create a data store writer implementation to use in real time etl app\n\n\n[MLHR-1036] - Create incremental model for Holt's Linear Trend Forecasting Algorithm\n\n\n[MLHR-1040] - Develop incremental model creation for Holt-Winters' Multiplicative Method Time-Series Forecasting\n\n\n\n\nBug\n\n\n\n\n[MLHR-778] - Logical Operators list has no palette\n\n\n[MLHR-857] - Machinedata demo stop working with new Redis operator\n\n\n[MLHR-963] - create output operator to add historical data to supplied datastore\n\n\n[MLHR-964] - create interface for data stores that can be used by output operator to persist historical data\n\n\n[MLHR-968] - Remove test related code from AbstractTransactionableStoreOutputOperator\n\n\n[MLHR-969] - Create a manageable version of our demo docs to be included with our installation\n\n\n[MLHR-978] - create resource leak for the hdfs input/output operators\n\n\n[MLHR-979] - AdsDimension: Redis operator shows high latency\n\n\n[MLHR-986] - Page Not Found errors from the web-site\n\n\n[MLHR-1000] - Invalid IP address selected by default during upgrade\n\n\n[MLHR-1005] - CPU column on logical view\n\n\n[MLHR-1008] - twitter application failed with exception\n\n\n[MLHR-1015] - State is missing from physical operator overview widget\n\n\n[MLHR-1019] - Make gateway ip and port inputs on same line in installer\n\n\n[MLHR-1020] - License screen does not reflect new license upload\n\n\n[MLHR-1021] - Unifiers should not have link to nonexistent logical operator\n\n\n[MLHR-1022] - Update installer license text to reflect correct trial period\n\n\n[MLHR-1028] - Cannot kill application in state ACCEPTED\n\n\n[MLHR-1030] - Sorting on logical operators list fails until you sort on name\n\n\n[MLHR-1031] - UI Showing wrong total Memory usage\n\n\n[MLHR-1049] - Streams not showing up in dag overview\n\n\n[MLHR-1050] - Remove the \"Development\" mode\n\n\n[MLHR-1055] - Unnecessary gateway restart requested during installation navigation\n\n\n[MLHR-1057] - Overview in dag widget not accurate to visible area\n\n\n[MLHR-1058] - Change install wizard from widget in a page to just a page\n\n\n[MLHR-1066] - Install Wizard tab order and auto-focus\n\n\n[MLHR-1067] - Install Wizard invalid DFS directory check\n\n\n[MLHR-1072] - Socket input operator test fail\n\n\n[MLHR-1074] - Remove stram events page/widget from 0.9.5\n\n\n\n\nImprovement\n\n\n\n\n[MLHR-721] - Migrate database cache lookup to the new db framework\n\n\n[MLHR-985] - AdsDimension: Plug-in Dimension Unifier \n\n\n[MLHR-1010] - Remove info icon from installer text\n\n\n[MLHR-1011] - Change \"property\" to \"item\" in install step tables\n\n\n[MLHR-1012] - Review and edit installer text\n\n\n[MLHR-1013] - Installer: Rework Hadoop and System screens\n\n\n[MLHR-1035] - Remove darker grey background from console\n\n\n[MLHR-1046] - Operators/Containers not being subscribed to on App Page\n\n\n[MLHR-1062] - Install Wizard examples of Hadoop and DFS path\n\n\n[MLHR-1069] - License summary section in Install Wizard\n\n\n\n\nNew Feature\n\n\n\n\n[MLHR-684] - Invalidate dashboard state from previous versions\n\n\n[MLHR-881] - ETL Web App - Packaging\n\n\n[MLHR-882] - ETL Web App - Dashboard\n\n\n[MLHR-885] - ETL Web App - Sample Data Generation\n\n\n[MLHR-997] - ETL Web App - Historical Data from MongoDB\n\n\n[MLHR-1014] - Remove config properties edit page, replace with config home\n\n\n[MLHR-1052] - Bar Chart Widget\n\n\n[MLHR-1053] - Widget Library - Demo Application\n\n\n\n\nStory\n\n\n\n\n[MLHR-703] - Logstream UI\n\n\n[MLHR-709] - Widgets Library as Independent Project\n\n\n\n\nTask\n\n\n\n\n[MLHR-661] - Migrate JDBC adapters to use the new database adapter interface\n\n\n[MLHR-866] - Annotate all stateless operators in Malhar as such\n\n\n[MLHR-868] - Demo Guide for Map-Reduce Operator - LogsCountApplication\n\n\n[MLHR-869] - Demo guide for Map-Reduce Operator - NewWordCountApplication.\n\n\n[MLHR-870] - Demo guide for Map-Reduce Operator - InvertedIndexApplication.\n\n\n[MLHR-922] - Machine data demo is using deprecated attributes. Gives warnings on launch\n\n\n[MLHR-951] - Remove deprecated warnings from demos\n\n\n[MLHR-954] - Need to debug older demos\n\n\n[MLHR-961] - Develop a Goldengate input adapter\n\n\n[MLHR-970] - Re-create the application that we build for iAd Poc in Malhar\n\n\n[MLHR-1006] - In Logical View do not include the unifier stats\n\n\n[MLHR-1007] - Simple user authentication/authorization support in UI\n\n\n[MLHR-1018] - Need a Kafka-HBase app\n\n\n[MLHR-1023] - Update installer text on welcome screen\n\n\n[MLHR-1024] - Create issues summary page under configuration\n\n\n[MLHR-1025] - Add progress status indicator for installer wizard\n\n\n[MLHR-1026] - Reorder install wizard screens\n\n\n[MLHR-1027] - Remove external upgrade link from licensing screen\n\n\n[MLHR-1041] - ETL : Consolidate input operator properties in json format\n\n\n[MLHR-1044] - ETL: Add input operator to ETL Application in ETL branch\n\n\n[MLHR-1071] - Create stateless deduper - aka deduper which forgets its state upon failure or repartitioning\n\n\n\n\nVersion 0.9.4\n\n\nBug\n\n\n\n\n[SPOI-455] - Cleanup maven repository workaround in install.sh\n\n\n[SPOI-1636] - Update all node1 demo to 0.9.3 release\n\n\n[SPOI-1774] - Thread local performance drop from 65M to 40 M tuplesProcessed/sec\n\n\n[SPOI-1775] - MROperator demo applications fail when launched from Sandbox\n\n\n[SPOI-1847] - Intermittent - WebSocket Publishing ignores endWindow\n\n\n[SPOI-1864] - POC partitioned operators not getting correct initial state\n\n\n[SPOI-1866] - Datatorrent applications not starting on CDH5 Vagrant cluster\n\n\n[SPOI-1867] - Investigate InstallAnywhere use for DataTorrent installation\n\n\n[SPOI-1882] - When not able to contact license agent, the application should not die\n\n\n[SPOI-1883] - Stram crashes with Unknown-Container issue\n\n\n[SPOI-1885] - Determine minimum amount of memory needed to run twitter app\n\n\n[SPOI-1886] - Determine minimum amount of memory needed to run mobile app\n\n\n[SPOI-1888] - Install license in the new console configuration\n\n\n[SPOI-1889] - Licensing needs to support eval and free\n\n\n[SPOI-1902] - Dynamic MxN partitioning does not handle scale down to single M instance \n\n\n[SPOI-1903] - MiniClusterTests fails because ~/.dt/dt-site.xml dependency\n\n\n[SPOI-1906] - history does not get flushed to the history file until the next command prompt\n\n\n[SPOI-1917] - Licensing error\n\n\n[SPOI-1922] - Loading license does not work\n\n\n[SPOI-1923] - Close button does not work in several pop-up windows\n\n\n[SPOI-1936] - No uninstall script available with the installer. \n\n\n[SPOI-1953] - Installation video on our website should be refreshed to reflect the latest version(s)\n\n\n[SPOI-1960] - application incorrectly marked as succeeded\n\n\n[SPOI-1968] - 404 for logical plan url\n\n\n[SPOI-1970] - Negative requested container count logged by Stram\n\n\n[SPOI-1976] - Fix AdsDimensions in certification \n\n\n[SPOI-1992] - the latency in the application overview freezes after a container gets killed.\n\n\n[SPOI-1999] - Evaluate Yarn cluster issue\n\n\n[SPOI-2002] - Gateway fails to load properties\n\n\n[SPOI-2037] - Redirect to welcome page on first run / install\n\n\n[SPOI-2039] - DTGateway logs to .dt/logs in service mode\n\n\n[SPOI-2040] - Apache logs application under contrib folder fails to run\n\n\n[SPOI-2047] - dtgateway service not starting when machine boots\n\n\n[SPOI-2049] - Better error message on invalid hadoop directory\n\n\n[SPOI-2051] - RPM install reports install script failure\n\n\n[SPOI-2052] - DTGateway restart fails when running with -service\n\n\n[SPOI-2055] - dtgateway service reports OK on startup failure\n\n\n[SPOI-2056] - LicensingAgentProtocolHelper.getLicensingAgentProtocol gets stuck when YARN is not running\n\n\n[SPOI-2057] - DTCLI is not working after intallation\n\n\n[SPOI-2058] - launch-demos macro not available after installing the platform from self extracting intaller\n\n\n[SPOI-2059] - Show understandable error message if the root hdfs directory could't be created\n\n\n[SPOI-2060] - UI Shows Nan B in Allocated Memory\n\n\n[SPOI-2062] - Gateway needs to check hadoop version\n\n\n[SPOI-2064] - dtgateway-ctl stop doesn't work\n\n\n[SPOI-2065] - Readme File is not udpated\n\n\n[SPOI-2066] - installer not recognizing -l option\n\n\n[SPOI-2070] - Installer: echo_success command doesn't work with Ubuntu\n\n\n[SPOI-2073] - Invalid Ip Address in Installer UI\n\n\n[SPOI-2074] - install script needs to check hadoop version\n\n\n[SPOI-2075] - lauch-local-demos\n\n\n[SPOI-2076] - Licensing agent RPC gives NPE\n\n\n[SPOI-2077] - Installer: we need a separate page for hadoop installation directory \n\n\n[SPOI-2078] - Change the certification and benchmarking script and code to use the new location of benchmarking apps\n\n\n[SPOI-2083] - install script, when run by rpm, complains about invalid group dtadmin\n\n\n[SPOI-2088] - Map Reduce demo applications still show the classnames when listed using launch-demos\n\n\n[SPOI-2089] - demo applications displayed after running the launch-demos command should be in alphabetical order\n\n\n[SPOI-2090] - Error while requesting evaluation license from Datatorrent.com\n\n\n[SPOI-2094] - Installer throws failed message while stopping gateway \n\n\n[SPOI-2097] - launch-demos macro not available after installing the platform from self extracting intaller\n\n\n[SPOI-2098] - App names still have full classpath\n\n\n[SPOI-2120] - Installer - Restart Modal is not closed after Restart Failed (Happened Once)\n\n\n[SPOI-2132] - Ensure HDFS does not blow up with millions of files per sec\n\n\n[SPOI-2133] - Delete old files to ensure NN does not crash\n\n\n[SPOI-2134] - Send POC1 to customer\n\n\n[SPOI-2143] - Spelling error and reference to $HOME/.datatorrent\n\n\n[SPOI-2146] - Move Kafka benchmark apps to contrib folder\n\n\n[SPOI-2148] - Installer - Disable Closing Modals on Click\n\n\n[SPOI-2149] - Address the confusion around gateway address.\n\n\n[SPOI-2151] - User is not able to change the defaultFS during installation\n\n\n[SPOI-2153] - Cryptic error message when launching app on node0\n\n\n[SPOI-2157] - Getting logical plan returns error when one of the getters is bad\n\n\n[SPOI-2159] - gateway is polling resourcemanager for appinfo w/o subscriber\n\n\n[SPOI-2163] - Change directory before DTGateway launch\n\n\n[SPOI-2165] - Installer - add reload ability to System screen\n\n\n[SPOI-2170] - DTGateway classpath is duplicated after restart\n\n\n[SPOI-2171] - Remove reload button from System configuration screen\n\n\n[SPOI-2172] - Installer may display invalid port after starting DTGateway\n\n\n[SPOI-2173] - Installer base location change not working\n\n\n[SPOI-2192] - CLI command for getting list of operator classes from a jar\n\n\n[SPOI-2193] - CLI command for getting properties of a operator class from a jar\n\n\n[SPOI-2206] - dag view does not get rendered property in Firefox\n\n\n[SPOI-2227] - (Re)start license agent when license file is uploaded\n\n\n[SPOI-2229] - container local operators not redeployed\n\n\n[SPOI-2238] - Installer complains about sudo running as root\n\n\n[SPOI-2241] - DAG Firefox 28 Support\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-1311] - Review platform documentation\n\n\n[SPOI-1567] - Certify against commercial Hadoop distributions\n\n\n[SPOI-1769] - Trying to kill a non dt app can return a better message\n\n\n[SPOI-1844] - Ship project-template with a log4j.properties file with debugging level set to INFO\n\n\n[SPOI-1852] - WebSocket client recovery logging\n\n\n[SPOI-1853] - Create WebSocket clients in containers on demand\n\n\n[SPOI-1854] - Option to retrieve only running and recently-ended apps\n\n\n[SPOI-1856] - Need something that is like LicensingAgentClient but not specific for stram.\n\n\n[SPOI-1857] - Gateway to warn about available licensed memory being low\n\n\n[SPOI-1858] - CLI to directly connect to license agent to get live license info\n\n\n[SPOI-1859] - Gateway to directly connect to license agent to get live license info\n\n\n[SPOI-1861] - Gateway command to restart itself\n\n\n[SPOI-1869] - Add UI build script to dist build file\n\n\n[SPOI-1874] - the first operator that stalls for more than specific period, take it out so as to unclog the processing\n\n\n[SPOI-1899] - Add appmaster container to container list\n\n\n[SPOI-1904] - Updates needed to the README file\n\n\n[SPOI-1911] - Run certification as part of nighty build\n\n\n[SPOI-1915] - Using $DT_HOME in README \n\n\n[SPOI-1961] - Show the activation date of the license with list-license-agent command \n\n\n[SPOI-1965] - The file demos.jar should be installed by default by the Installer\n\n\n[SPOI-1978] - Manual eval request (by e-mail) - template  \n\n\n[SPOI-1980] - DT Version in license request and generated license\n\n\n[SPOI-1981] - Approve / update license verification e-mail\n\n\n[SPOI-1997] - Certify against commercial Hadoop distributions\n\n\n[SPOI-2026] - Add support to LogicalPlan for testing dag serialization\n\n\n[SPOI-2150] - Update Readme file for the local install\n\n\n[SPOI-2155] - Installer - Validate Fields on blur event\n\n\n[SPOI-2156] - Installer - Navigation Code Cleanup\n\n\n[SPOI-2158] - Installer - CSS Classes\n\n\n[SPOI-2160] - Installer font size\n\n\n[SPOI-2166] - Configuration screen navigation panel\n\n\n[SPOI-2174] - Notify user with installation location and version\n\n\n[SPOI-2175] - Notify user about local DTGateway management during installation\n\n\n[SPOI-2188] - Installer - Register Instructions\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-328] - Add annotation to declare the operator stateless\n\n\n[SPOI-393] - High Availability for STRAM\n\n\n[SPOI-868] - Setting operator properties as different types\n\n\n[SPOI-1182] - Add Key-based filter functionality to malhar library for Min, Max, SimpleMovingAverage, StandardDeviation like operators\n\n\n[SPOI-1654] - Logstream - aggregate top hits and bytes for URL, geo DMA, IP, URL/status code, url\n\n\n[SPOI-1747] - create a filter operator to output multiple records based on filter\n\n\n[SPOI-1756] - configuration for input adaptor\n\n\n[SPOI-1757] - configuration for filter operator\n\n\n[SPOI-1758] - configuration for dimension operator\n\n\n[SPOI-1759] - configuration for aggreagation operator\n\n\n[SPOI-1760] - configuration for web socket output\n\n\n[SPOI-1849] - add dt HDFS directory in configuration \n\n\n[SPOI-1907] - Installer: HDFS directory creation attempt via Gateway (as part of config updates) \n\n\n[SPOI-1909] - Port re-selection by Gateway if 9090, 9091, 9092, etc are taken\n\n\n[SPOI-1913] - Automate verifing the app memory for the demos\n\n\n[SPOI-1928] - gateway needs to be able to start with standalone hadoop jar (without hadoop installation)\n\n\n[SPOI-1929] - /ws/v1/about to include java version and hadoop location\n\n\n[SPOI-1930] - New installation script\n\n\n[SPOI-1966] - dtcli should be enabled to list app names (if available) as opposed to app class path\n\n\n[SPOI-1974] - Add throughput,  totalTuplesProcessed and elapsed time to performance benchmarking\n\n\n[SPOI-1975] - Display throughput, tuplesProcessed per sec and latency in a tabular format.\n\n\n[SPOI-1987] - Copy License to Front-End Distribution\n\n\n[SPOI-2003] - Verify and add all the demos except Machine data to app memory usage automation\n\n\n[SPOI-2011] - Make a separate jar file for performance benchmarking demos\n\n\n[SPOI-2013] - Support for doc link as an attribute for the application\n\n\n[SPOI-2018] - Have a launch-performance command in dtcli\n\n\n[SPOI-2021] - Rename all the apps under contrib to have meaningful names\n\n\n[SPOI-2023] - Make a launch-contrib command available in stram cli\n\n\n[SPOI-2027] - Packaging benchmarking demos\n\n\n[SPOI-2042] - redirect user to welcome screen if dt.configStatus is not \"complete\"\n\n\n[SPOI-2044] - set property dt.configStatus to \"complete\" when the user has completed the config wizard\n\n\n[SPOI-2122] - Installer - Offline Email Template\n\n\n[SPOI-2147] - Provide separate dt benchmarking package scripts to throughput and hdfs operators benchmarking\n\n\n[SPOI-2195] - Gateway REST API to return operator classes in a jar given superclass (or not)\n\n\n[SPOI-2196] - Gateway REST API to return properties of an operator in a jar \n\n\n[SPOI-2200] - Installer - License Flow\n\n\n\n\nStory\n\n\n\n\n[SPOI-1608] - Platform Benchmarking (Platform1 and Platform2)\n\n\n\n\nTask\n\n\n\n\n[SPOI-722] - Document ads demo (add comments to codebase)\n\n\n[SPOI-1403] - HDFS Operator Benchmark\n\n\n[SPOI-1411] - Deprecate old Malhar webdemos once logstream is available\n\n\n[SPOI-1513] - datatorrent.com webapp development - app testing\n\n\n[SPOI-1610] - Develop benchmarking app for AdsDimension App (exactly once semantics) - Platform1\n\n\n[SPOI-1612] - Benchmarking Ads Dimension demo app on Morado cluster (exactly once semantics) - Platform1\n\n\n[SPOI-1618] - Benchmarking Machine Data app with Platform1\n\n\n[SPOI-1694] - Document DT SandBox preparation\n\n\n[SPOI-1730] - Default License in git needs to be replaced by the license cut by the real key \n\n\n[SPOI-1732] - Create the real public/private key for licenses and store the private key in a safe place.\n\n\n[SPOI-1745] - Document Demo Application Setup\n\n\n[SPOI-1788] - CLI commands for licensing\n\n\n[SPOI-1794] - Create license info as a string \n\n\n[SPOI-1796] - Soft enforcement for normal paid app. 200% bump?\n\n\n[SPOI-1848] - Gateway to support changing and getting config parameters\n\n\n[SPOI-1851] - Document virtual cluster setup\n\n\n[SPOI-1862] - Working on creating Wire Frames for the Installation of DT Platform\n\n\n[SPOI-1863] - Make installer work w/o Maven\n\n\n[SPOI-1865] - Allow user to configure application classpath\n\n\n[SPOI-1868] - Support Book Keeping in the HDFSStorage\n\n\n[SPOI-1870] - Validate dtcli generate-license-request\n\n\n[SPOI-1872] - Modify generatelicense process\n\n\n[SPOI-1873] - License process via console\n\n\n[SPOI-1876] - Application Developer Guide Improvements\n\n\n[SPOI-1877] - Download and build JDK Standrd Doclet Source as part of DT \n\n\n[SPOI-1878] - DT Console Web UI Testing on Chrome\n\n\n[SPOI-1879] - DT Console Web UI Testing for Demo Apps on Chrome\n\n\n[SPOI-1884] - Operater developer guide review \n\n\n[SPOI-1891] - Add allatori documentation\n\n\n[SPOI-1892] - Add automatic build of front components to distribution\n\n\n[SPOI-1893] - Quick Start Guide\n\n\n[SPOI-1914] - cli get-app-info to include info from hadoop\n\n\n[SPOI-1926] - Call license web service and return license file\n\n\n[SPOI-1927] - Returns license request blob for UI to assemble mailto link\n\n\n[SPOI-1939] - Twitter Top Counter Demo Applications Guide\n\n\n[SPOI-1940] - RPM packaging for installer\n\n\n[SPOI-1941] - Include demo UI into installer\n\n\n[SPOI-1942] - Remove Allatori code expiration for GA\n\n\n[SPOI-1943] - include more info in the license request \n\n\n[SPOI-1945] - Add REST call to gateway to post license file\n\n\n[SPOI-1946] - virtual cluster configuration changes\n\n\n[SPOI-1947] - Create license@datatorrent.com\n\n\n[SPOI-1949] - Java application (with main method) that returns information given a license request blob\n\n\n[SPOI-1956] - License generation key pair expiration / private key protection\n\n\n[SPOI-1963] - Evaluate Doclava Doclet from Google\n\n\n[SPOI-1982] - E-mail verification success web page \n\n\n[SPOI-1988] - Review Quick Start Guide\n\n\n[SPOI-1989] - AdsDimension - Demo Applications Guide\n\n\n[SPOI-1990] - Twitter Rolling Top Words Counter - Demo Applications Guide\n\n\n[SPOI-2004] - Installer testing for GA\n\n\n[SPOI-2006] - Grant Google Analytics Access To Following People\n\n\n[SPOI-2010] - Configure the apps to use minimum memory as verified by app-memory-usage-automation.\n\n\n[SPOI-2015] - Get Machine data into contrib.jar\n\n\n[SPOI-2016] - Fraud Detection in contrib.jar\n\n\n[SPOI-2017] - Quick Start Guide version 2\n\n\n[SPOI-2019] - List NxN performance apps (different event size vs different stream locality\n\n\n[SPOI-2025] - Getting Start Guide - Launch this copy.\n\n\n[SPOI-2032] - Certify Cloudera CDH 5.0\n\n\n[SPOI-2048] - Uninstall script\n\n\n[SPOI-2050] - Start gateway as service flag\n\n\n[SPOI-2069] - Test Installer\n\n\n[SPOI-2091] - Update installation license agreement\n\n\n[SPOI-2093] - Verify demo UI is bundled with installer\n\n\n[SPOI-2100] - For terms of service box, change \"continue\" to \"accept and continue\"\n\n\n[SPOI-2106] - Change the message on 1 yr registration\n\n\n[SPOI-2107] - Change the message on 1 yr registration\n\n\n[SPOI-2108] - Put timeglass and \"loading\" or spinning.... while Hadoop system properties are being loaded\n\n\n[SPOI-2109] - Gateway down creates bad error message\n\n\n[SPOI-2110] - Remove errors popping on the right hand of console\n\n\n[SPOI-2111] - If gateway outage is discovered add a message to get them back\n\n\n[SPOI-2112] - Change the message on Hadoop screen\n\n\n[SPOI-2114] - Error if HDFS does not exist\n\n\n[SPOI-2115] - Create a list of issues summary screen\n\n\n[SPOI-2116] - Show more instructions on the completed screen\n\n\n[SPOI-2131] - Ingestion POC\n\n\n[SPOI-2136] - Do cartesian products for key, val pair\n\n\n[SPOI-2142] - Allow customization of cartesian product \n\n\n[SPOI-2161] - prereq message on welcome screen\n\n\n[SPOI-2162] - DFS error message\n\n\n[SPOI-2164] - DFS location validation\n\n\n[SPOI-2167] - Evaluate errors in Cloudera certification\n\n\n[SPOI-2223] - Test if all the applications with reduced container memory sizes run in sandbox 0.9.4.\n\n\n[SPOI-2230] - Uninstaller for RPM\n\n\n[SPOI-2231] - Provide Environment with Running Demos\n\n\n[SPOI-2240] - Set Up DataTorrent Demos on Dev Environment\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-1682] - Too many mbassy threads!!!\n\n\n[SPOI-1718] - Update HA Documentation\n\n\n[SPOI-1729] - Restore operator recovery checkpoints in AM recovery \n\n\n[SPOI-1786] - Users should be able to generate license at datatorrent.com\n\n\n[SPOI-1931] - Installer - determine OS type and version\n\n\n[SPOI-1932] - installer with sudo/root user creation, service installs\n\n\n[SPOI-1933] - service wrappers for DTGateway\n\n\n[SPOI-1934] - HDFS directory creation during install\n\n\n[SPOI-1935] - Search for hadoop binaries standard paths\n\n\n[SPOI-1958] - Create HDFS Word input operator\n\n\n[SPOI-1962] - Add test to jekins nightly build\n\n\n[SPOI-1971] - Verify launch script for other apps from demo \n\n\n[SPOI-1972] - When some app fail, the main monitor should still keep looking at the other apps\n\n\n[SPOI-2028] - Provide shell script 'benchmark-throughput' to produce a single summary table \n\n\n[SPOI-2030] - Provide a list of individual demos used for benchmarking through dtcli benchmarking, so that the user can launch the demos individually.\n\n\n[SPOI-2031] - Package benchmarking suite into the installer and sandbox\n\n\n[SPOI-2046] - Update licenses location in installation script\n\n\n[SPOI-2053] - Certify CDH5.0 as part of Cloudera certification for inclusion in their process\n\n\n[SPOI-2054] - Certify installer on HW\n\n\n[SPOI-2079] - Run certification on bin install\n\n\n[SPOI-2085] - Change the certification resource xml files to contain the certification type.\n\n\n\n\nBug\n\n\n\n\n[MLHR-729] - Columns in table wrap in Firefox\n\n\n[MLHR-730] - Dev server does not escape double-quotes in error message\n\n\n[MLHR-732] - Links Outline in Firefox\n\n\n[MLHR-754] - JarList and DepJarList headers point to nonexistent text items\n\n\n[MLHR-767] - \"FINISHING\" has no icon in status\n\n\n[MLHR-785] - NaN in file size column of jar list\n\n\n[MLHR-809] - Inconsistent landing page across browsers\n\n\n[MLHR-814] - Selecting operators in the logical operators widget does not activate any actions\n\n\n[MLHR-816] - Names of partitions in the partitions widget should not be shown as links\n\n\n[MLHR-817] - Clickin on 'outputPort' leads to PageNotFound error\n\n\n[MLHR-824] - Don't show license agent detail when license agent is not running\n\n\n[MLHR-826] - Dep Jars fail to load in specify dep jars modal\n\n\n[MLHR-830] - Optimize RandomWordInput operator used in perfromance benhcmarking to use local final variables to improve performance. \n\n\n[MLHR-835] - Fix hard-coded file path in com.datatorrent.demos.wordcount.WordCountInputOperator\n\n\n[MLHR-836] - Need AbstractHDFSOutputAdapter\n\n\n[MLHR-863] - Add license headers to AbstractHdfsOutputOperator\n\n\n[MLHR-879] - Installer - Issues Management\n\n\n[MLHR-887] - Input operator that tails a growing log file in a directory\n\n\n[MLHR-899] - Give default name for all demo applications\n\n\n[MLHR-908] - Mark installation complete with dt.configStatus property\n\n\n[MLHR-911] - UI Shows Nan B in Allocated Memory\n\n\n[MLHR-912] - Installer - System Section - Show Field Specific Errors\n\n\n[MLHR-913] - Installer - System Section - Server-Side Error Messages\n\n\n[MLHR-918] - Yahoo finance with Alerts : Modify to accept multiple ticker symbols and remove hard-coded values.\n\n\n[MLHR-920] - Give desciptive names to benchmarking apps\n\n\n[MLHR-924] - Tail Operator should take care of the truncation of file\n\n\n[MLHR-930] - UI container list should not show time for last heartbeat for if the value is \"-1\"\n\n\n[MLHR-938] - gateway address property change\n\n\n[MLHR-939] - Put GatewayRestart into \"actions\" hash in settings.js\n\n\n[MLHR-940] - Delete the space in the name\n\n\n[MLHR-943] - Change the method name from isConnected to Connected in the db api\n\n\n[MLHR-991] - DAG Stream Locality\n\n\n\n\nImprovement\n\n\n\n\n[MLHR-731] - Compile LESS on the fly in dev environment\n\n\n[MLHR-735] - Remove unused bundling make commands, update README\n\n\n[MLHR-745] - Add icon to left widget manager drawer\n\n\n[MLHR-746] - Use \"cores\" not % cpu for cluster metric total cpu usage\n\n\n[MLHR-747] - Remove \"#\" for \"number of container\", et al labels\n\n\n[MLHR-748] - Only load all applications from RM on demand\n\n\n[MLHR-749] - Improve notification history pane\n\n\n[MLHR-750] - Normalize labels for everything in console\n\n\n[MLHR-751] - Add lock where close dashboard icon would be for default dashboard\n\n\n[MLHR-753] - Remove avg app age in cluster metrics\n\n\n[MLHR-755] - Change \"max alloc mem\" to \"peak alloc mem\" in cluster metrics\n\n\n[MLHR-756] - Add memory levels to tooltip of license mem gauge in top right\n\n\n[MLHR-768] - Remove beefy from npm shrinkwrap\n\n\n[MLHR-770] - Clean up BaseUtil, BaseModel, BaseCollection, add tests\n\n\n[MLHR-781] - shorten link to log file in container page\n\n\n[MLHR-798] - Add \"config.adsdimensions.redis.dbindex\" configuration in webdemo\n\n\n[MLHR-873] - Add equals and hashcode to JdbcOperatorBase\n\n\n[MLHR-931] - ETL: Create a converter api and provide an implementation for Json Object to flat map conversion\n\n\n[MLHR-935] - Have \"silentErrors\" option for models and collections\n\n\n[MLHR-937] - ETL: Create a unifier for DimensionComputation operator\n\n\n[MLHR-947] - Improve overall look and feel of install wizard\n\n\n[MLHR-948] - Remove mock issues from SummaryView in installer wizard\n\n\n[MLHR-993] - Demo UI - Default Applications Names for Application Discovery\n\n\n\n\nNew Feature\n\n\n\n\n[MLHR-688] - Discard Real-Time Updates When Page is not Active\n\n\n[MLHR-757] - Dashboard - Save Widget Width to Local Storage\n\n\n[MLHR-759] - Dashboard - Widget Definitions Collection\n\n\n[MLHR-763] - Line Chart Widget - hAxis options\n\n\n[MLHR-764] - Dashboard App - Meteor Data Source\n\n\n[MLHR-765] - Shutdown Container Interface\n\n\n[MLHR-773] - Configuration wizard page\n\n\n[MLHR-788] - Web Demos - Redis and MongoDB Config\n\n\n[MLHR-789] - Web Demos - Fraud Demo MongoDB Database Name\n\n\n[MLHR-790] - Web Demos - Single Configuration File\n\n\n[MLHR-791] - Web Demos - Start Script\n\n\n[MLHR-792] - Web Demos - Single Page App\n\n\n[MLHR-793] - Web Demos - JS/CSS Bundle\n\n\n[MLHR-794] - Web Demos - WebSocket Pub/Sub\n\n\n[MLHR-795] - Web Demos - Resources Clean Up on Scope Destroy\n\n\n[MLHR-796] - Web Demos - Distribution Files\n\n\n[MLHR-797] - Web Demos - Running Instructions\n\n\n[MLHR-804] - Config Page (Manage Properties)\n\n\n[MLHR-805] - Web Demos - Distribution Package Instructions\n\n\n[MLHR-806] - Installer - License Requests (REST API Calls)\n\n\n[MLHR-807] - Installer - License Text\n\n\n[MLHR-818] - Installer - License Section\n\n\n[MLHR-819] - Installer - System Properties Section\n\n\n[MLHR-820] - Console - Node.js Dev Mode\n\n\n[MLHR-821] - Web Demos - Distribution Package Launch Script\n\n\n[MLHR-831] - Installer - License Flow\n\n\n[MLHR-834] - Installer - License - Registration\n\n\n[MLHR-858] - Installer - Gateway Restart\n\n\n[MLHR-861] - Installer - System Properties - IP List\n\n\n[MLHR-864] - Installer - Handling Hadoop Not Found\n\n\n[MLHR-903] - Installer - WebSocket DataSource Disconnect\n\n\n[MLHR-909] - Installer - Restart Confirmation\n\n\n[MLHR-914] - Installer - Error Messages\n\n\n[MLHR-919] - License Bar\n\n\n[MLHR-923] - Installer - Properties Update \"Loading\" Indicator\n\n\n[MLHR-925] - Installer Update\n\n\n[MLHR-934] - Allow overrides to $.().modal(options) for Modals\n\n\n\n\nStory\n\n\n\n\n[MLHR-705] - Node.js Pub/Sub Service\n\n\n[MLHR-708] - Evaluate Node.js Pub/Sub Services\n\n\n[MLHR-710] - Pie Chart Data Model\n\n\n[MLHR-712] - Dashboard App - Historical Data Support\n\n\n[MLHR-714] - Dashboard App - MongoDB Integration\n\n\n[MLHR-715] - Create Dashboard from Running App - Widgets Auto Discovery\n\n\n[MLHR-736] - Console Firefox Support\n\n\n[MLHR-737] - Console Safari Support\n\n\n[MLHR-801] - Installer/Config UI\n\n\n[MLHR-802] - Web Demos - Distribution Package\n\n\n[MLHR-803] - Installer (Wizard) Page\n\n\n\n\nTask\n\n\n\n\n[MLHR-349] - Add build script to Malhar/front\n\n\n[MLHR-402] - Logstream - aggregate top hits and bytes for URL, geo DMA, IP, URL/status code, url\n\n\n[MLHR-651] - Use compatible version of jersey/jackson/jetty in Malhar\n\n\n[MLHR-659] - Migrate MongoDB adapters to use the new database adapter interface\n\n\n[MLHR-741] - Web Apps (Demos) Firefox Support\n\n\n[MLHR-744] - Web Apps (Demos) Safari Support\n\n\n[MLHR-760] - Dashboard App - Meteor Integration\n\n\n[MLHR-761] - Dashboard App - Derby.js Integration\n\n\n[MLHR-762] - create install script for ui\n\n\n[MLHR-839] - Review guide on MachineData app\n\n\n[MLHR-842] - Demo guide for Pi Application\n\n\n[MLHR-843] - Demo guide for Twitter Top URL Counter demo application\n\n\n[MLHR-851] - Demo guide for Fraud detection demo application\n\n\n[MLHR-853] - Demo guide for Mobile demo application\n\n\n[MLHR-854] - Demo Guide for Word-Count Application\n\n\n[MLHR-855] - Demo guide for Pi Calculator application\n\n\n[MLHR-856] - Demo guide for Twitter Rolling Top Words Application\n\n\n[MLHR-859] - upgrade kryo to 2.23\n\n\n[MLHR-871] - Demo guide for Twitter Top URL Counter - Launch This Copy.\n\n\n[MLHR-872] - Demo Guide for Word-Count Application - Launch this copy.\n\n\n[MLHR-875] - Demo guide for Yahoo finance application\n\n\n[MLHR-876] - Demo guide for Yahoo finance alerting application\n\n\n[MLHR-877] - Demo guide for Yahoo finance application with Derby SQL\n\n\n[MLHR-892] - ETL logstream application - study the log stream application\n\n\n[MLHR-893] - ETL- Use the generic dimension operator that was created for a POC in Log stream \n\n\n[MLHR-900] - ETL- Operators used by logstream application need to be generic and moved to library\n\n\n[MLHR-904] - Fix the nightly and trigger builds broken due to removal of api.codec and api.util\n\n\n[MLHR-905] - Dedup: Make deduper and bucket manager part of malhar library\n\n\n[MLHR-910] - Demo guide for Twitter Rolling Top Words Application - Launch This Copy\n\n\n[MLHR-915] - CLONE - Demo guide for Pi Application - Launch this copy\n\n\n[MLHR-916] - CLONE - Demo guide for Pi Calculator application - Launch this copy\n\n\n[MLHR-917] - CLONE - Demo guide for Mobile demo application - Launch this copy\n\n\n[MLHR-936] - Create new Redis Store using Lettuce redis client\n\n\n[MLHR-949] - Add confirmation to DTGateway restart button in System Properties\n\n\n[MLHR-962] - ETL : Create a sifter operator \n\n\n[MLHR-980] - CLONE - Demo guide for Yahoo finance alerting application - launch this copy\n\n\n[MLHR-981] - CLONE - Demo guide for Yahoo finance application - launch this copy\n\n\n[MLHR-982] - CLONE - Demo guide for Yahoo finance application with Derby SQL - launch this copy\n\n\n\n\nSub-task\n\n\n\n\n[MLHR-678] - Time Series Forecasting with Simple Linear Regression\n\n\n[MLHR-718] - Time Series Forecasting using Simple/Single Exponential Smoothing\n\n\n[MLHR-726] - Time Series Forecasting Operator using Holt's Linear Trend Model\n\n\n[MLHR-727] - Develop application for a telecom related use case for time series forecasting with Simple Linear Regression and CMA smoothing\n\n\n[MLHR-932] - Create Centered Moving Average Smoothing Operator\n\n\n\n\nVersion 0.9.3\n\n\nNew Feature\n\n\n\n\n[SPOI-261] - Design a general purpose read from stream and write to cassandra\n\n\n[SPOI-400] - Each streaming application should license check\n\n\n[SPOI-1622] - Input operator - XML parser\n\n\n[SPOI-1647] - LogStream UI \n\n\n[SPOI-1770] - Gateway should expose list of available topics\n\n\n[SPOI-1778] - Open readme on sandbox startup\n\n\n[SPOI-1804] - Start license app on launch app if not running\n\n\n[SPOI-1805] - command to show license file info in cli\n\n\n[SPOI-1812] - Create REST call for specific license agent, given a license id\n\n\n[SPOI-1823] - Gateway REST API - Get Running Applications List\n\n\n[SPOI-1829] - Semantic URLs for Web Apps\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-1202] - Provide a way to check whether an operator is partitioned\n\n\n[SPOI-1783] - Add allocatedMB to main application list\n\n\n[SPOI-1789] - Change frequency of heartbeats to license app\n\n\n[SPOI-1795] - License file to clearly state hard enforcement or soft enforcement\n\n\n[SPOI-1816] - support simple variable substitution in the cli\n\n\n[SPOI-1835] - support gateway status command\n\n\n\n\nStory\n\n\n\n\n[SPOI-1320] - Support MQTT protocol\n\n\n[SPOI-1542] - Input operator - Directory Scan\n\n\n\n\nBug\n\n\n\n\n[SPOI-1696] - Make de-duper dynamically partitionable\n\n\n[SPOI-1704] - Stram enforcement to lock physical plan changes when license memory limit is reached\n\n\n[SPOI-1706] - Design a enforcement format for the license policy\n\n\n[SPOI-1711] - Encryption/obfuscation of communication between stram and license agent\n\n\n[SPOI-1779] - Update sandbox documentation terminology\n\n\n[SPOI-1802] - Provide total license memory via stram web services\n\n\n[SPOI-1807] - Unknown outage on Machine data demo\n\n\n[SPOI-1809] - Suppress expected error message in dtgateway-ctl  \n\n\n[SPOI-1810] - sample-stram-site.xml generates warnings\n\n\n[SPOI-1813] - Add SNAPSHOT repository to install pom\n\n\n[SPOI-1815] - Make stram memory reporting to license manager asynchronous \n\n\n[SPOI-1818] - Chance \"className\" in license file to id\n\n\n[SPOI-1820] - dtcli script doesn't exit when maven command fails\n\n\n[SPOI-1826] - Update documentation title\n\n\n[SPOI-1827] - Install script errors\n\n\n[SPOI-1831] - CLI warning about trouble with license manager\n\n\n[SPOI-1833] - Use encrypted byte arrays for RPC wire protocol for licensing\n\n\n[SPOI-1840] - Change default license memory limit to 25GB\n\n\n[SPOI-1841] - Make stram memory enforcement tolerances property settings for the enforcer\n\n\n[SPOI-1842] - Investigate the possibility of engine obfuscation jar not containing any references to license package path\n\n\n[SPOI-1880] - GET nonexistent container returns 500 error\n\n\n\n\nTask\n\n\n\n\n[SPOI-1375] - All sandbox apps must work in 8G VM. Need to test each\n\n\n[SPOI-1467] - DB lookup for Cassandra\n\n\n[SPOI-1507] - datatorrent.com webapp development - pilot test of Angular and WP integration\n\n\n[SPOI-1509] - datatorrent.com webapp development - db design\n\n\n[SPOI-1511] - datatorrent.com webapp development - app design\n\n\n[SPOI-1512] - datatorrent.com webapp development - app dev\n\n\n[SPOI-1515] - datatorrent.com webapp deveopment - integrate standalone app with cms\n\n\n[SPOI-1516] - datatorrent.com webapp development - add GA info during registration\n\n\n[SPOI-1517] - datatorrent.com webapp development - background jobs\n\n\n[SPOI-1617] - Benchmarking Performance app with Platform2\n\n\n[SPOI-1641] - Benchmarking Ads Dimension App - Platform2\n\n\n[SPOI-1715] - Show remainingLicensedMB and allocatedMB in UI for each application\n\n\n[SPOI-1763] - Provide support for Accumulo NoSQL db\n\n\n[SPOI-1780] - Sandbox - activate license automatically\n\n\n[SPOI-1781] - Sandbox - increase memory to 8GB\n\n\n[SPOI-1782] - License App should use much less memory (256MB or less?)\n\n\n[SPOI-1787] - Add license instructions to README\n\n\n[SPOI-1790] - Ensure update to license app on any resource change by StrAM\n\n\n[SPOI-1791] - Hard enforcement for free license (6GB), and eval license\n\n\n[SPOI-1793] - Hide sub-license and make license object behave as \u201cwhat is license data right now?\u201d\n\n\n[SPOI-1798] - Date format change in license file\n\n\n[SPOI-1799] - Change name \"Sublicense\" to \"Section\" or \"LicenseSection\"\n\n\n[SPOI-1800] - Webservice specs for Gateway for license info\n\n\n[SPOI-1825] - Update end user documentation\n\n\n[SPOI-1832] - Support CDH default log4j setup\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-1451] - Show critical path\n\n\n[SPOI-1721] - Augment the Partitionable interface to inform of all the partitions which actually were deployed\n\n\n[SPOI-1733] - Container heartbeat RPC failover\n\n\n\n\nNew Feature\n\n\n\n\n[MLHR-5] - UI component for license information\n\n\n[MLHR-6] - Google Line Chart Widget\n\n\n[MLHR-7] - Gauge Widget\n\n\n[MLHR-8] - Top N Widget\n\n\n[MLHR-9] - Compile Widget from HTML Template\n\n\n[MLHR-10] - Make widgets resizable and renameable in ui-builder\n\n\n[MLHR-11] - Dashboard Component Grunt Tasks\n\n\n[MLHR-653] - Dynamically Connect Widgets to WebSocket Topics\n\n\n[MLHR-655] - Create serializing mechanism for instantiated widgets and dashboard(s)\n\n\n[MLHR-656] - Add/Compile Widgets from templateUrl\n\n\n[MLHR-664] - Support MQTT protocol\n\n\n[MLHR-668] - Set up widget configure dialog\n\n\n[MLHR-669] - Visual Data Demo App\n\n\n[MLHR-671] - Add allocatedMB column in main application list\n\n\n[MLHR-673] - Dashboard App - Notification Service\n\n\n[MLHR-674] - Explicit Saving/Loading of Dash configurations in ui builder\n\n\n[MLHR-687] - Dashboard App - Filter WebSocket Topics\n\n\n[MLHR-689] - Dashboard App - Widget Options Modal\n\n\n[MLHR-690] - Dashboard App - Widgets Schema\n\n\n[MLHR-691] - WebSocket Topics Debugger Widget\n\n\n[MLHR-692] - JSON Widget\n\n\n[MLHR-693] - Progressbar Widget\n\n\n[MLHR-695] - Pie Chart Widget\n\n\n[MLHR-696] - Dashboard App - Development/Production Scripts\n\n\n[MLHR-697] - Dashboard App - Node.js Configuration\n\n\n[MLHR-698] - Dashboard App - WebSocket/REST API Configuration\n\n\n\n\nImprovement\n\n\n\n\n[MLHR-686] - Deglobalize the visibly component\n\n\n\n\nBug\n\n\n\n\n[MLHR-4] - Use new livechart module for OpChart Widget\n\n\n[MLHR-13] - Console status column display\n\n\n[MLHR-648] - Update Issues section in README files \n\n\n[MLHR-650] - Changing metrics on logical dag fails\n\n\n[MLHR-654] - Some widgets' height changes when changing width\n\n\n[MLHR-667] - Add UI version to console\n\n\n[MLHR-670] - Memory leak in console\n\n\n[MLHR-677] - Widgets Data Models\n\n\n[MLHR-680] - Update license information dialog for new REST call info\n\n\n[MLHR-725] - WindowId Formatter\n\n\n[MLHR-739] - Stream Locality Toggle fails for DAG view\n\n\n\n\nStory\n\n\n\n\n[MLHR-1] - Reusable Dashboard Component with AngularJS\n\n\n[MLHR-2] - Dashboard Widgets\n\n\n[MLHR-3] - Dashboard App\n\n\n\n\nTask\n\n\n\n\n[MLHR-321] - Directory Scan operator\n\n\n[MLHR-452] - Create a De-duplication operator\n\n\n[MLHR-603] - Supports upload of dependency jars\n\n\n[MLHR-638] - Test streaming application for dynamic partition\n\n\n[MLHR-645] - More fields in drop down in logicalDAG widget\n\n\n[MLHR-646] - Document issue tracking location in README\n\n\n[MLHR-652] - Parallel Simple Linear Regression\n\n\n[MLHR-657] - Migrate memcache adapters to use the new database adapter interface\n\n\n[MLHR-662] - Migrate Redis adapters to use the new database adapter interface\n\n\n[MLHR-663] - Design new DB adapters interface\n\n\n[MLHR-666] - DB lookup for Cassandra\n\n\n\n\nVersion 0.9.2\n\n\nBug\n\n\n\n\n[SPOI-1327] - AtLeastOnceTest.testInlineOperatorsRecovery intermittent failure\n\n\n[SPOI-1342] - DTCli should check license and relay the information with each application launched\n\n\n[SPOI-1383] - Last window id and recovery window id do not update on 0.9\n\n\n[SPOI-1439] - Gateway should be secured\n\n\n[SPOI-1445] - Add version detection for Gateway\n\n\n[SPOI-1456] - Free Memory in container widget changes too rapidly\n\n\n[SPOI-1540] - Specification of license handlers and enforcers in the license file.\n\n\n[SPOI-1632] - jar upload fails\n\n\n[SPOI-1634] - Uptime at 1Billion events/s (Machine data)\n\n\n[SPOI-1635] - Update node1 with latest machine data demo\n\n\n[SPOI-1676] - Incremental Obfuscation of dt-flume directory\n\n\n[SPOI-1677] - Supports uploading of dependency jars\n\n\n[SPOI-1678] - When loading jars, make sure they are in their separate space so they don't conflict with gateway, cli and other jars\n\n\n[SPOI-1679] - When uploading jar and when dependencies are not met, allow the upload with a message about dependencies\n\n\n[SPOI-1680] - gateway throws errors when retrieving web service info from stram\n\n\n[SPOI-1687] - Support launching jar and showing logical plan from HDFS \n\n\n[SPOI-1688] - Map Reduce Monitor Does Not Publish WebSocket Data\n\n\n[SPOI-1697] - Update demo configuration on node2\n\n\n[SPOI-1703] - Update auto provisioning with DataTorrent 0.9.1 and GCE GA\n\n\n[SPOI-1707] - License agent should handle license expiry\n\n\n[SPOI-1708] - Stram should store license expiry\n\n\n[SPOI-1709] - Show license object information in gateway\n\n\n[SPOI-1710] - License cutting utility\n\n\n[SPOI-1712] - Gateway to gracefully handle stram being a newer version than itself\n\n\n[SPOI-1714] - Dynamic partition stop working if you start from only 1  partition\n\n\n[SPOI-1727] - ApplicationInfoAutoPublisher unit test error\n\n\n[SPOI-1728] - StramEvent exception prevents package name obfuscation\n\n\n[SPOI-1739] - recordingStartTime of operator stats is showing -1 from time to time\n\n\n[SPOI-1743] - Tuple recording on port is not showing up in web services\n\n\n[SPOI-1744] - Recording says ended even if the recording is still going on\n\n\n\n\nImprovement\n\n\n\n\n[SPOI-1098] - event recorder logging improvements\n\n\n[SPOI-1370] - Make the partition logic available to the end-users\n\n\n[SPOI-1448] - DAG Visualization - Stream Types\n\n\n[SPOI-1603] - BufferServerStatsCollection - dont check against bufferserverpublisher and subscriber\n\n\n[SPOI-1613] - Update the User Interface guide to reflect latest version (0.9.1)\n\n\n\n\nNew Feature\n\n\n\n\n[SPOI-165] - Parent jira for authentication\n\n\n[SPOI-170] - Stream should authenticate before allowing an operator to connect\n\n\n[SPOI-258] - Develop Flume Sink and corresponding DT input adapter\n\n\n[SPOI-327] - Parent jira for Security\n\n\n[SPOI-401] - Licensing alert mechanisms\n\n\n[SPOI-411] - Ability to modify (add, upgrade, downgrade) license while the app is running\n\n\n[SPOI-436] - Provide Web Service for obtaining license information (Usage limits, etc)\n\n\n[SPOI-729] - Include license data in DT phone home\n\n\n[SPOI-872] - Logical View of Running Application\n\n\n[SPOI-975] - Support DataLocal Functionality\n\n\n[SPOI-1406] - Add log file path and/or URL to each container info map\n\n\n[SPOI-1621] - Input operator - CDR parser using CSV \n\n\n[SPOI-1699] - Add locality (and maybe id?) to physical streams in REST calls\n\n\n\n\nTask\n\n\n\n\n[SPOI-1689] - Map Reduce Monitor Web App\n\n\n[SPOI-739] - Hadoop 2.2 certification\n\n\n[SPOI-763] - Competition study\n\n\n[SPOI-1140] - Annotate dag visualization with stream throughout and other data\n\n\n[SPOI-1246] - Support versioning for Gateway to STRAM communications \n\n\n[SPOI-1253] - Create DataTorrent Application which provides licensing server functionality\n\n\n[SPOI-1389] - ContainerList view should show the log file name (stderr, stdout) in the info widget\n\n\n[SPOI-1405] - Design macros for node0 and node1\n\n\n[SPOI-1609] - Competitive analysis - DT (Platform1 and Platform2)\n\n\n[SPOI-1611] - Benchmarking Ads Dimension on Morado cluster (at-least-once semantics) - Platform1\n\n\n[SPOI-1616] - Benchmarking Performance app with Platform1\n\n\n[SPOI-1670] - Ensure that Dedup operator is fault tolerant\n\n\n[SPOI-1673] - use public/private key encryption for dt phone home\n\n\n[SPOI-1675] - Map Reduce Jobs\n\n\n[SPOI-1686] - Launch separate process when loading classes from application jars\n\n\n[SPOI-1722] - Create a utility to create default license\n\n\n[SPOI-1724] - Create a command line utility to generate customer license\n\n\n[SPOI-1736] - CLI warning on License Violation\n\n\n[SPOI-1742] - Update end-user documentation\n\n\n\n\nSub-task\n\n\n\n\n[SPOI-919] - Certify secure mode with Hadoop 2.2.0\n\n\n[SPOI-966] - Create a licensing agent application\n\n\n[SPOI-1413] - Flume sink part \n\n\n[SPOI-1414] - DT Input Adapter for Flume\n\n\n[SPOI-1475] - Augment Kafka operator to dynamically adapt to load and broker/partition changes\n\n\n[SPOI-1538] - Develop Ads Dimension on Morado cluster (at-least-once semantics) Platform1\n\n\n[SPOI-1713] - Secure communication between gateway and stram\n\n\n[SPOI-1720] - Ensure that the Partionable interface and StatsListener interface callbacks are made from the same thread\n\n\n[SPOI-1723] - The default license generation should be integrated with build\n\n\n[SPOI-1731] - Sync execution layer deployment state after recovery\n\n\n\n\nGitHub - DataTorrent/Malhar\n\n\n\n\n[616] - fix #615 Update Web Apps Instructions\n\n\n[615] - Update Web Apps Instructions\n\n\n[614] - corrected typo\n\n\n[613] - Fixes #599. Upload and specify dependency jars\n\n\n[612] - fixes #597\n\n\n[611] - fixes #610\n\n\n[610] - Telecom tests failing\n\n\n[609] - Github 597\n\n\n[608] - #fix 607 Machine Data Demo Day Format\n\n\n[607] - Machine Data Demo Day Format\n\n\n[606] - fixes #457 added xml parse operator to parse and pick values from xml nodes and att...\n\n\n[605] - added the history for hadoop 1.x\n\n\n[603] - Map Reduce Monitor - Elapsed Time\n\n\n[602] - Map Reduce Monitor - Elapsed Time\n\n\n[601] - Map Reduce Monitor - Bootstrap JS, Server Errors Notification, Header Alignment\n\n\n[599] - Provide UI for uploading and specifying dependency jars\n\n\n[598] - Map Reduce Monitor - Server Errors Notification\n\n\n[597] - Map Reduce Monitor App - CPU/Memory History\n\n\n[595] - Map Reduce Monitor (History Charts, Animations, Readme, AngularJS Upgrade)\n\n\n[594] - Map Reduce Monitor - Production Files (dist)\n\n\n[593] - Map Reduce Monitor - AngularUI Bootstrap Progressbar issue with ngAnimate\n\n\n[592] - Map Reduce Monitor Update (Readme, App List, History Charts)\n\n\n[591] - Map Reduce Monitor - Map/Reduce History Charts\n\n\n[590] - Fixes #401, adds zoom to physical DAG\n\n\n[589] - Map Reduce Monitor - App List Columns\n\n\n[586] - Map Reduce Monitor - App Id AngularJS Filter\n\n\n[585] - Fixes #569, Cosmetic changes\n\n\n[584] - Map Reduce Monitor - Show Active Job First\n\n\n[581] - Map Reduce Monitor Update (Loading Indicator, Animations, Delayed Promise)\n\n\n[580] - Map Reduce Monitor - AngularJS Animations\n\n\n[579] - Map Reduce Monitor - Upgrade to AngularJS 1.2.6\n\n\n[578] - Map Reduce Monitor - App List Loading Indicator\n\n\n[577] - fixes #553, fixes #575\n\n\n[576] - Map Reduce Monitor - AngularJS Delayed $q Promise\n\n\n[575] - Map Reduce Monitor App - Send Job Stats immediately on Subscribe Request\n\n\n[573] - Map Reduce Monitor Update (App List Grid, Progress Bars, Job Removal)\n\n\n[572] - fixes #570\n\n\n[571] - Map Reduce Monitor - Remove Job on WebSocket Message\n\n\n[570] - clipPath issue when multiple charts on the same page\n\n\n[569] - Various cosmetic updates for console\n\n\n[568] - fixes #542, tooltip no longer obstructed by graph lines\n\n\n[567] - Bird's Eye View for Physical DAG view\n\n\n[566] - fixes #544, windowIds can now handle initial value of -1 or 0\n\n\n[565] - Map Reduce Monitor - Job Selection\n\n\n[564] - fixes #357, added logical operator page\n\n\n[563] - Map Reduce Monitor - App List ng-grid\n\n\n[561] - Map Reduce Monitor - App List Table Filter\n\n\n[560] - Fixed exception with KryoSerializableStreamCodec #559\n\n\n[558] - CDR simulator #524\n\n\n[557] - Github 525\n\n\n[556] - set the name of the io threads created by ning asynchttpclient\n\n\n[554] - Squashed commit of the following:\n\n\n[553] - Map Reduce Monitor App - Store Map/Reduce Progress History\n\n\n[551] - fixes #550\n\n\n[550] - Map Reduce Monitor App - App Should Broadcast Special Message on Unsubscription\n\n\n[549] - Map Reduce Monitor - Stop Updates after Job Unsubscribe\n\n\n[548] - Map Reduce Monitor - Found Job Notification\n\n\n[547] - Fix Github #545\n\n\n[544] - Console does not handle initial windowId\n\n\n[542] - Tooltip for line graphs show up behind graphs after turning series on and off\n\n\n[541] - fixes #535\n\n\n[539] - Map Reduce Monitor - Merge Progress Bars with Progress Table\n\n\n[538] - Map Reduce Monitor - Combine Map/Reduce Counters\n\n\n[536] - Map Reduce Monitor - App List Running Jobs Progress Bar\n\n\n[535] - Console breaks when switching to other page\n\n\n[534] - Fixes #510, Unsubscribe Logical Operators when not used by any widget\n\n\n[533] - Fixes #521, Refactored WindowId usage\n\n\n[532] - Map Reduce Monitor - Counters\n\n\n[530] - Map Reduce Monitor - App List Sort\n\n\n[526] - Map Reduce Monitor - Counters\n\n\n[525] - CDR processing DAG prototype\n\n\n[523] - Github 512\n\n\n[521] - Normalize all WindowId objects by overriding \"set\" method of appropriate models\n\n\n[519] - Map Reduce Monitor - Header Alignment on Resize\n\n\n[518] - Map Reduce Web App\n\n\n[517] - Map Reduce Monitor - License Headers\n\n\n[516] - Map Reduce Monitor - Map Reduce Jobs List\n\n\n[515] - Map Reduce Monitor - AngularJS Modules Definition\n\n\n[514] - Map Reduce Monitor - Readme (Deployment and Running Instructions)\n\n\n[513] - Map Reduce Monitor - Job Query Loading Indicator\n\n\n[512] - Support Normalization Operator\n\n\n[511] - Map Reduce Monitor - AngularJS Settings Provider\n\n\n[510] - Unsubscribe logicalOperators on InstancePage when not in use by widget\n\n\n[509] - Fixes #505. Also removes one more instance of free memory metric for containers\n\n\n[507] - Map Reduce Monitor - Single Config (Server and Client)\n\n\n[505] - Add processed and emitted metrics to container overview widget\n\n\n[504] - fixes #356, container log url now available in container info widget\n\n\n[503] - Map Reduce Monitor - Active Job Highlight\n\n\n[502] - Map Reduce Monitor - AngularJS Parent Scope Event Propagation\n\n\n[501] - Fixes #364, removed free memory from container metrics\n\n\n[500] - Map Reduce Monitor - Mock Server\n\n\n[499] - Adding support for R. Basic operations - min, max and std deviation support added. Also adding support to run R scripts.\n\n\n[498] - Rsupport pull\n\n\n[496] - Map Reduce Monitor - Progress Line Chart\n\n\n[495] - Map Reduce Monitor - Running MAPREDUCE Applications Discovery\n\n\n[494] - fixes #420, can now set explicit height for widgets\n\n\n[493] - fix #488 added delay before reconnection\n\n\n[492] - CPU/RAM Metrics for Map Reduce Jobs\n\n\n[491] - fix #488 added delay before reconnection\n\n\n[489] - Map Reduce Monitor - Job Controller\n\n\n[488] - WebSocketOutputOperator should wait a specified number of seconds before reconnection\n\n\n[487] - Using uniform naming convention for applications. Fixed incorrect application names. Fixes #486.\n\n\n[486] - Application names are not uniform\n\n\n[485] - CPU/RAM Metrics for Map Reduce Jobs (Map Reduce Monitor App)\n\n\n[484] - Map Reduce Monitor - AngularJS UI-Router Nested Views\n\n\n[483] - Enhance the AbstractSlidingWindow #480, Add a SortedSlidingWindow operator #423\n\n\n[482] - fixes #411. bundling on server.js, monkeypatching fs to avoid EMFILE\n\n\n[479] - fix #443 reconnection when the connection is dropped\n\n\n[478] - fix #443 Handles reconnection when the connection is dropped\n\n\n[477] - Improvements to LogicalDagWidget. Fixes #399, #473, #475, #476\n\n\n[476] - Logical DAG Widget: Limit scroll scale extent\n\n\n[475] - Logical DAG Widget: add ability to reset initial dag view\n\n\n[474] - Map Reduce Monitor - AngularJS UI-Router\n\n\n[473] - Logical DAG Widget: only zoom when alt/option is held down\n\n\n[472] - Map Reduce Monitor App - App Does not Publish Completed Maps\n\n\n[471] - Map Redice Monitor - Reduce Progress Grid\n\n\n[470] - Map Redice Monitor - Map Progress Grid\n\n\n[469] - Map Redice Monitor - AngularJS Percentage Filter\n\n\n[468] - Map Reduce Monitor - Monitored Jobs Grid\n\n\n[467] - Add a general CSV parser operator to parse string, byte[] input to Map #451\n\n\n[466] - Map Redice Monitor - AngularJS Util Service\n\n\n[465] - Map Reduce Monitor - Unsubscribe Action\n\n\n[464] - Map Reduce Monitor - Client-Side Settings\n\n\n[463] - Map Reduce Monitor - WebSocket Unsubscribe\n\n\n[461] - Map Reduce Monitor -  Multiple Jobs Monitoring\n\n\n[460] - Map Reduce Monitor - Progress Bar Animation\n\n\n[459] - Map Reduce Monitor - Upgrade to AngularJS 1.2.4\n\n\n[458] - added xml parser operator and its test, fixes #457\n\n\n[456] - Github 444\n\n\n[454] - Map Reduce Monitor - AngularUI Bootstrap\n\n\n[453] - Map Reduce Monitor - Production Build with Grunt\n\n\n[452] - Map Reduce Monitor - jshint\n\n\n[451] - CSV input operator (CDR processing)\n\n\n[450] - Map Reduce Monitor - Progress Bars \n\n\n[449] - Map Reduce Monitor App - WebSocket Query\n\n\n[448] - Map Reduce Monitor - Error Notifications with pnotify \n\n\n[444] - Map Redice Monitor App - Publish Map/Reduce Updates as Array\n\n\n[443] - Map Reduce Monitor App WebSocket Issue\n\n\n[442] - Map Reduce Monitor - Node.js Proxy for Hadoop ResourceManager\n\n\n[441] - Map Reduce Monitor - REST Service\n\n\n[439] - Map Reduce Monitor - Server Configuration\n\n\n[438] - Map Reduce Monitor - Settings\n\n\n[436] - Map Reduce Monitor - Job Progress Grid\n\n\n[435] - Map Reduce Monitor - WebSocket Service with AngularJS provider\n\n\n[434] - Map Reduce Monitor - Unit Tests\n\n\n[433] - Map Reduce Monitor - AngularJS Directives (widgets)\n\n\n[432] - Map Reduce Monitor - Page Layout with Bootstrap\n\n\n[431] - Map Reduce Monitor - Node.js Server\n\n\n[430] - Map Reduce Monitor - Yeoman Generated App\n\n\n[428] - Normalization operator (CDR processing)\n\n\n[427] - Filter operator (CDR Processing)\n\n\n[426] - Enrichment operator (CDR processing)\n\n\n[425] - Aggregator operator (CDR processing)\n\n\n[422] - Github 421\n\n\n[421] - Create RedisOperator taking String,String for performance\n\n\n[420] - Allow widgets to have adjustable height\n\n\n[419] - DAG Styling, DAG Firefox Issue\n\n\n[418] - Logical DAG - Firefox Bottom Margin Issue\n\n\n[417] - Logical DAG Styling\n\n\n[416] - Fix jquery build error\n\n\n[415] - Organized scripts and server\n\n\n[414] - fix #408, fix #413 Logical DAG - Show Stream Locality on Demand\n\n\n[413] - Logical DAG - Right Aligned Legned and Show Locality Link\n\n\n[412] - Improve Front Dev Environment\n\n\n[411] - Improved dev environment for front\n\n\n[410] - Map Reduce Monitor Web App\n\n\n[409] - fix #393 Front Node.js Proxy\n\n\n[408] - Logical DAG - Show Stream Locality on Demand\n\n\n[407] - fixes #373\n\n\n[401] - Physical DAG - Smart Zoom\n\n\n[399] - Physical DAG - Bird's-Eye View\n\n\n[393] - Front Node.js Proxy\n\n\n[375] - fixes #367, improves reload time during dev on front\n\n\n[374] - fixes #367, improves reload time during dev on front\n\n\n[372] - Logical DAG Legend Styling\n\n\n[371] - Logical DAG Legend\n\n\n[370] - Fix issue316 issue317 pull\n\n\n[369] - Logical DAG - Legend\n\n\n[368] - Squashed commit of the following:\n\n\n[367] - Precompile templates for better dev process\n\n\n[366] - Documenting demos pull\n\n\n[365] - Normalized all \"processed\" and \"emitted\" labels\n\n\n[364] - Remove free memory from container metrics in UI\n\n\n[362] - Dependency to dagre-d3 fork\n\n\n[361] - Logical DAG - Stream Locality\n\n\n[360] - Update physical operators collection to fetch from physical plan\n\n\n[359] - Add source and sinks to physical operator list \n\n\n[358] - Normalize processed/s emitted/s labels across data tables and dag view\n\n\n[357] - Create Logical Operator Page\n\n\n[356] - ContainerList view should show the log file name (stderr, stdout) in the info widget\n\n\n[355] - fixes #349, recently-launched app does not request operator list\n\n\n[354] - Make partitionable kafka input operator adjust partitions ifself for kafka partition change(del/add)\n\n\n[353] - Upgrade kafka to 0.8 release\n\n\n[352] - fixes #322\n\n\n[351] - Non-partitioned operators\n\n\n[350] - Key/Value lookup Storage Manager changes\n\n\n[349] - Application launch error in console\n\n\n[348] - Fixes #339, switches cluster metrics to websocket topic\n\n\n[347] - fix #346 Physical DAG - Remove Container IDs\n\n\n[346] - Physical DAG - Remove Container IDs\n\n\n[345] - Added sensible default display for avg app age field in cluster metrics widget #341\n\n\n[344] - Added build cmd to travis script, fixes #343\n\n\n[343] - Build step for front not in travis script\n\n\n[342] - Fix for #328\n\n\n[341] - Cluster overview display items (initial launch)\n\n\n[339] - Cluster stats should come from WebSocket topic\n\n\n[337] - Add Write failed: Broken pipe", 
            "title": "Release Notes"
        }, 
        {
            "location": "/release_notes/#datatorrent-rts-release-notes", 
            "text": "", 
            "title": "DataTorrent RTS Release Notes"
        }, 
        {
            "location": "/release_notes/#version-320", 
            "text": "", 
            "title": "Version 3.2.0"
        }, 
        {
            "location": "/release_notes/#new-feature", 
            "text": "[SPOI-6351] - Add feature to REST API to get queue information from cluster", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvement", 
            "text": "[SPOI-5777] - Kafka start offset should have user option to read from latest or earliest  [SPOI-5828] - Stackstraces should not be shown on errors  [SPOI-6641] - Implement \"forever\" bucket in DimensionComputation  [DTIN-40] - Observed unused variables in SplunkBytesInputOperator  [DTIN-69] - Move Query Operators implementation to newer one  [DTIN-50] - [dtIngest] add parameter \"parallel readers\"", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug", 
            "text": "[SPOI-5104] - Ingestion: Failed to copy data when inputs include a directory and a subdirectory  [SPOI-5216] - FileSplitter fails with ConcurrentModificationException  [SPOI-5571] - S3 : Copying data failed with RuntimeException saying 'Unable to move file'  [SPOI-5809] - Kryo Exception In Stateful Stream Codec When Operator Is Killed From UI and Comes Back Up  [SPOI-5823] - Downstream container falling behind when buffer spooling is enabled  [SPOI-5899] - Ability to retrieve schemas from an app package  [SPOI-6053] - Schema Generator - bool getter should be isBool and not getBool  [SPOI-6073] - AbstractFileOutputOperator not finalizing the file after the recovery  [SPOI-6079] - Installation wizard: 'continue' button is misplaced in last step of installation  [SPOI-6098] - 'single run' doesnt work  [SPOI-6202] - Sandbox 3.1 has community license instead of enterprise  [SPOI-6204] - Sandbox 3.1 loses uploaded license after reboot  [SPOI-6222] - HDFS recovery in sandbox causes license to degrade to community  [SPOI-6236] - Add 1s aggregations to App Data Tracker  [SPOI-6298] - Dimensions Store Can Become Blocked  [SPOI-6383] - Sometimes expired query is still executed.  [SPOI-6393] - Markdown code blocks render with invalid syntax highlights in console  [SPOI-6446] - Merge PojoEnrichment and TupleEnrichment  [SPOI-6505] - Exceptions from asm when uploading apps  [SPOI-6603] - Warning messages shown on console are not completely visible  [SPOI-6709] - Temporarily Remove Methods Which Override Their Return Type In MEGH From Semver Checks  [SPOI-6846] - DT dashboard guide link is not working  [SPOI-6855] - Broken links observed on summary page while DT RTS configuration   [SPOI-6856] - Correct docs index.html links and title  [DTIN-51] - bandwidth option was removed during merge  [DTIN-101] - For message-based-input to message-based-output, compression   encryption options should not be visible on config UI  [DTIN-102] - [Ingestion UI] If kafka as output type, then Brokerlist is the configuration parameter, not zookeeper quorum  [DTIN-112] - Message based input to FTP output fails with IOException while appending the data  [DTIN-113] - Kafka input to kafka output fails in MessageWriter with EOFException while fetching output topic metadata  [DTIN-123] - All files are not getting copied when bandwidth option is specified  [DTIN-124] - For kafka to hdfs, if offset is set to 'Earliest', messages are not consumed from the beginning  [DTIN-126] - 'Compact files' option should be disabled for message based Input type  [DTIN-129] - StreamCorruptedException while decrypting AES/PKI encrypted file  [DTIN-130] - Text box for 'Bandwidth to use' should accept integer values only  [DTIN-155] - Messages are dropped when bandwidth option is enabled  [DTIN-160] - Copying data from S3 to HDFS fails with NoSuchMethodError  [DTIN-161] - Message based input to S3N output fails with IOException while appending the data  [DTIN-162] - JMS messages are not fully consumed in case of JMS to kafka  [DTIN-164] - Data values in 'Table' widget flicker when encryption is enabled  [DTIN-165] - Data source names for dtingest should not contain 'null'  [DTIN-174] - Append doesn't work for S3, FTP filesystems and ObjectOutputStream  [DTIN-176] - Parallel read is not working in case of FTP and S3 as input  [DTIN-186] - FileMerger failed with unable to merge file exception", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#task", 
            "text": "[SPOI-5173] - DAG validation: Attribute values Serializable  [SPOI-5403] - Ingestion Splunk integration  [SPOI-5801] - Make a MapR partner (datatorrent) sandbox  [SPOI-5958] - dtView Integration for ingestion metric visualization  [SPOI-6241] - Change in enterprise license  [SPOI-6439] - Run benchmark for 3.2.0 release  [SPOI-6691] - Decouple malhar version from dt version in dtingest pom dependancies  [DTIN-20] - Accept bandwidth limit in units other than byes/s in dtingest script  [DTIN-38] - Needs to check the Query frequency option is available for Splunk Input Operator  [DTIN-47] - Merge code from release-1.0.1 branch to ingegration-1.1.0 branch  [DTIN-54] - Disabling the splunk from dtIngest script  [DTIN-71] - Integrate release 1.0.1 branch with integration-1.1.0", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task", 
            "text": "[SPOI-5369] - Integrate schema support in Cassandra Input/Output  [SPOI-5437] - Create Jdbc Pojo input operator and integrate schema support in Jdbc POJO input/output  [SPOI-5819] - Bandwidth control for file based sources  [SPOI-5820] - Bandwidth control for message based sources  [SPOI-5959] - Metrics for compression, encryption  [SPOI-6337] - Expose bandwidth metrics  [SPOI-6441] - Change console home page to be welcome screen instead of operations summary", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#version-311", 
            "text": "", 
            "title": "Version 3.1.1"
        }, 
        {
            "location": "/release_notes/#improvement_1", 
            "text": "[SPOI-5828] - Stackstraces should not be shown on errors", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug_1", 
            "text": "[SPOI-5786] - AppDataTracker Custom Metric Store Deadlock  [SPOI-6032] - App Builder should not show property from super class  [SPOI-6049] - DimensionStoreHDHT Should always set meta data on aggregates in processEvent, even if the aggregate is received in a committed window.  [SPOI-6073] - AbstractFileOutputOperator not finalizing the file after the recovery  [SPOI-6090] - Ingestion: Error decrypting files for message based sources  [SPOI-6147] - Launch issue with \"Starter Application Pack\"  [SPOI-6202] - Sandbox 3.1 has community license instead of enterprise  [SPOI-6203] - -ve Memory reported for Application Master  [SPOI-6204] - Sandbox 3.1 loses uploaded license after reboot  [SPOI-6222] - HDFS recovery in sandbox causes license to degrade to community  [SPOI-6286] - App Data Tracker Number Format Exception In Idempotent Storage Manager  [SPOI-6304] - Fix netlet dependency  [SPOI-6313] - Work Around APEX-129  [SPOI-6333] - RandomNumberGenerator in apex-app-archetype does not use numTuples property", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#task_1", 
            "text": "[SPOI-5755] - Gateway should show \"HDFS is not up yet\"  [SPOI-6148] - Update website with release 3.1  [SPOI-6241] - Change in enterprise license", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#version-310", 
            "text": "", 
            "title": "Version 3.1.0"
        }, 
        {
            "location": "/release_notes/#new-feature_1", 
            "text": "[SPOI-4670] - Enable message schema management in App Builder  [SPOI-5844] - Retrieve older versions of schemas", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvement_2", 
            "text": "[SPOI-5611] - Simplify PubSub operators to supply Gateway connect address URI by default", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug_2", 
            "text": "[SPOI-4380] - MxN unifier not removed when partition is removed from physical plan.  [SPOI-5338] - Cleanup OperatorDiscoverer class to remove reflection and use ASM  [SPOI-5582] - Ingestion: Fails with \"Unable to merge file\" with FTP as destination  [SPOI-5697] - Unable to launch ingestion app through Ingestion wizard  [SPOI-5743] - App package import also needs to do the typegraph stuff as upload  [SPOI-5831] - Add getter for properties for Twitter Demo  [SPOI-5833] - Schema class not loaded during validation  [SPOI-5841] - e2e files being added to app/index.html with gulp inject:scripts  [SPOI-5857] - Gateway has trouble getting to STRAM until after restart  [SPOI-5943] - chicken and egg problem for entering kerberos credentials data to dt-site.xml  [SPOI-5946] - Port compatibility when port require schemas  [SPOI-6002] - AppBuilder fails to load operator due to NullPointerException", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#task_2", 
            "text": "[SPOI-5307] - Mocha based tests for Gateway API calls  [SPOI-5749] - Certify MapR sandbox  [SPOI-5750] - Create Splunk forwarder operators  [SPOI-5751] - Create splunk forwarder input operator  [SPOI-5752] - Create splunk forwarder output operator  [SPOI-5753] - Create splunk forwarder configuration specification  [SPOI-5754] - Change node1 twitter demos settings to 5 mins in node1 conf demo conf file  [SPOI-5755] - Gateway should show \"HDFS is not up yet\"  [SPOI-5756] - Test license expiry on sandbox as part of sandbox testing  [SPOI-5764] - Gateway to show better error messages on all 500 errors  [SPOI-5803] - Support of custom aggregation for ADT  [SPOI-5893] - Changes for retrieving container and operator history information", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_1", 
            "text": "[SPOI-4946] - Handle new @useSchema and @description property metadata in UI  [SPOI-4980] - Schema Management in the UI  [SPOI-5505] - handle Object-to-Object port compatibility with and without schema  [SPOI-5506] - handle Object-to-Pojo port compat with schema  [SPOI-5513] - handle port compatibility with generic type ports  [SPOI-5747] - Automatically generate new eval license when sandbox starts up", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#version-300", 
            "text": "", 
            "title": "Version 3.0.0"
        }, 
        {
            "location": "/release_notes/#sub-task_2", 
            "text": "[SPOI-1901] - Dynamic property changes lost on AM restart   [SPOI-4820] - Add an api call to retrieve all the application, operator and port attributes available for the app-package  [SPOI-4891] - Example for schema meta data and property doclet tag  [SPOI-4968] - Capture @useSchema and @description doclet tags from docblocks  [SPOI-4972] - Design Schema API for managing schemas  [SPOI-4973] - Create a Schema resource which will contain schema calls  [SPOI-4987] - Generate pojo class using schema provided in json  [SPOI-4999] - Implement the rest call to save schema on the backend  [SPOI-5211] - Support custom visualization link  [SPOI-5237] - Type of attributes are wrong when it is an inner class or enum  [SPOI-5357] - LogicalNode may swallow END_STREAM message in catchUp  [SPOI-5361] - Button for dt.phoneHome.enable property on system config page  [SPOI-5527] - Implement Queue Option", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#bug_3", 
            "text": "[SPOI-4321] - Datatorrent Core Trigger Jenkings Job hangs  [SPOI-4633] - Resolve type variable across the type hierarchy   [SPOI-4789] - Time calculated from window Id using WindowGenerator.getMillis is incorrect at times   [SPOI-4884] - Intermittent failure for CustomMetricTest  [SPOI-4896] - Kafka operator stop consuming from kafka cluster after it is restarted.  [SPOI-4941] - For kafka operator in app builder certain properties need to be set twice  [SPOI-5006] - If incompatible change made in platform, we need to recompute the typegraph automatically  [SPOI-5074] - Code/fix code that leads to classloader leaks in Gateway  [SPOI-5087] - Bucket ID Tagger In app data tracker has very high latency  [SPOI-5323] - Malhar operators are not packaged with distribution for 3.0  [SPOI-5359] - License Type, License ID, and Features should not just be empty on LicenseInfo page  [SPOI-5360] - When license upload fails with no message, install wizard should not have an unaddressed colon  [SPOI-5379] - NoClassDefFoundError due to indirect reference to dt-common classes  [SPOI-5385] - No error message is returned when DTCli gets an NPE  [SPOI-5389] - Support for RM and HDFS delegation token renewal in secure HA environments  [SPOI-5433] - Asm code not working for jdk 1.8  [SPOI-5469] - Datatorrent DTX trigger Jenkins Job fails on timeout  [SPOI-5472] - Undeploy heartbeat requests are not processes if container is idle  [SPOI-5484] - Ingestion FTP as output does not work with vsftpd  [SPOI-5497] - Could not open pi demo in 3.0.0 RC2  [SPOI-5498] - Typegraph exception with 3.0.0 RC2  [SPOI-5500] - Saving an app whose streams have an assigned schema (not handwritten java class) fails with 404  [SPOI-5504] - Make temporary file names unique to avoid lease expiry  [SPOI-5530] - Failed to edit pidemo JSON based application  [SPOI-5536] - App jar is missing from typegraph  [SPOI-5552] - Non-concrete classes being returned with assignableClasses call  [SPOI-5562] - Boolean Operator Property Values Are Blank In the Operator Properties Table  [SPOI-5569] - Launching AdsDimensionsDemoGeneric fails with ClassNotFoundException   [SPOI-5577] - Sometimes CustomMetrics Store Returns No Data When There is Data  [SPOI-5578] - Sometimes Custom Metrics Data Source Is Not Accessible From Widgets  [SPOI-5582] - Ingestion: Fails with \"Unable to merge file\" with FTP as destination  [SPOI-5583] - DFS root directory check is failing on MapR cluster  [SPOI-5593] - App Builder search tooltip  [SPOI-5597] - Bad log level WARNING in UI  [SPOI-5604] - Pressing \"Enter\" in newDashboardModal closes the modal  [SPOI-5607] - Sales Enrichment operator needs to be recovered and added to Sales Dimensions demo  [SPOI-5612] - AppBuilder can not deserialize instance of java.net.URI with PubSubWebSocketAppData operators  [SPOI-5627] - error message from install script from 3.0.0-RC4   [SPOI-5628] - Update dt-site of sandbox for new Sales demo enrichment operator  [SPOI-5629] - Data visualization links broken with APPLICATION_DATA_LINK   [SPOI-5632] - If there are no uncategorized operators, dont show an uncategorized group  [SPOI-5636] - dtcp is not getting packaged in installation (RC4)  [SPOI-5637] - Allatori Configuration errors in ingestion pom.xml  [SPOI-5640] - dtcp: When invalid value is provided for scan interval no error is thrown by dtcp, just exits out  [SPOI-5643] - When An Application Is Restarted Under A Different User Name Custom Metrics Data is Not Saved.  [SPOI-5644] - isChar validation should strip \\u000 character  [SPOI-5646] - Megh demos is failing release build  [SPOI-5647] - launchPkgApplicationDropdown tries to push an alert to scope.alerts, which is undefined  [SPOI-5653] - When Gateway Restarts App Data Tracker It Should Do It Using HA Mode  [SPOI-5655] - If gateway fails to launch (e.g., port 9090 is in use), installer should inform user  [SPOI-5656] - Update Bucket ID Tagger To use only user name for determining bucket Ids  [SPOI-5661] - Set default store in HDHTReader  [SPOI-5662] - Improve defaults of Enrichment Operator In Sales Demo  [SPOI-5664] - Omit Aggregator Registry From UI  [SPOI-5672] - App Data Tracker Compliation is failing  [SPOI-5673] - Unable to launch ingestion app with JMS as input source   [SPOI-5675] - Kafka keys population error  [SPOI-5676] - Fix API doc generation  [SPOI-5685] - Complete App Builder category and property name fixes  [SPOI-5688] - Correct interactive demo tutorials available in the Learn section  [SPOI-5689] - Installer produces error message about removing docs directory  [SPOI-5690] - Images missing from lean section tutorials after installing release build  [SPOI-5693] - Console does not upload default DT application packages  [SPOI-5697] - Unable to launch ingestion app through Ingestion wizard  [SPOI-5704] - App package references in docs  [SPOI-5705] - Build version incorrect  [SPOI-5706] - api and user docs not viewable through gateway  [SPOI-5707] - Packaging utilities jar in Ingestion  [SPOI-5708] - Update netlet dependency to release for 3.0  [SPOI-5712] - Unable to launch dtingest app using dtingest command line utility but able to launch via UI", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_3", 
            "text": "[SPOI-4513] - Expose app data tracker stats as \"pseudo-datasource\" for each application  [SPOI-4889] - Make BucketIdTagger fault tolerant  [SPOI-5320] - License URL should open in new window   [SPOI-5494] - Support \"queue\" query parameters from launch app modal  [SPOI-5535] - Console should validate duplicate app name in the same app package  [SPOI-5548] - Check compaction keys from UI  [SPOI-5603] - Add syntax highlighting to Markdown code blocks  [SPOI-5611] - Simplify PubSub operators to supply Gateway connect address URI by default  [SPOI-5623] - License upgrade link should open in separate tab  [SPOI-5633] - Improve markdown content display styles  [SPOI-5657] - Redirect to welcome screen of the Learn section after install  [SPOI-5694] - Ingestion default app-config should populate meaningful defaults", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_2", 
            "text": "[SPOI-4672] - Support for secure HA environments  [SPOI-5288] - Add \"launch\" button to each item in the list of app packages  [SPOI-5304] - Support ingestion install mode  [SPOI-5313] - Obfuscate ADT in the distribution build  [SPOI-5561] - Markdown documentation support in the Console", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#task_3", 
            "text": "[SPOI-4228] - The location of App Data Tracker from installation and from devel mode  [SPOI-4658] - App Data Tracker Technical Doc  [SPOI-4879] - Hide operators that have no input ports from App builder  [SPOI-4983] - Installer for Ingestion app  [SPOI-4984] - Obfuscate ingestion jar  [SPOI-5029] - Move new x-java dependencies in appDataFramework branch to core  [SPOI-5032] - Review comparison for appDataFramework pull request to the master  [SPOI-5050] - Licensing-related changes to the UI for 3.0  [SPOI-5255] - Support enhancements to uiTypes  [SPOI-5314] - Include obfuscated App Data Tracker in the install bundle  [SPOI-5370] - Megh obfuscation  [SPOI-5502] - Allatori obfuscates operator class names even config has keep-name on class * implements com.datatorrent.api.Operator  [SPOI-5555] - [dtcp] set default app package for ingestion  [SPOI-5574] - Update installer README  [SPOI-5608] - Docs for backward compatibility in 3.0   [SPOI-5610] - Add testing to Markdown support models, pages, directives  [SPOI-5621] - Include Ingestion utilities in DT installer  [SPOI-5624] - [Ingestion] Hide message output destination when input is file source  [SPOI-5635] - Changing product name to dtIngest  [SPOI-5638] - Post ingestion packge to central DT maven repository  [SPOI-5639] - App Data Tracker Release Job  [SPOI-5645] - support \"short\" primitive type in app builder  [SPOI-5658] - Dummy ticket for Apex-17 - Change CustomMetric annotation to AutoMetric and enhancements", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#bug_4", 
            "text": "[MLHR-1726] - Bug in PojoUtils  [MLHR-1742] - Create a wrapper method in POJOUtils that returns the appropriate getter for primitives  [MLHR-1752] - PojoUtils create/constructSetter does not handle boxing/unboxing  [MLHR-1756] - Exclude hadoop-common and other hadoop libraries from application  [MLHR-1789] - Complete Category and property name fixes", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_4", 
            "text": "[MLHR-1725] - Add Support for Setters to PojoUtils  [MLHR-1757] - change scope of dt-engine in maven build", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#version-210", 
            "text": "", 
            "title": "Version 2.1.0"
        }, 
        {
            "location": "/release_notes/#bug_5", 
            "text": "[SPOI-3732] - Get User Info APIs creates users  [SPOI-3816] - Ingestion UI: Node server should store pipelines on hdfs rather than local filesystem  [SPOI-3820] - Ingestion UI: Rest calls to gateway from Node server fails  [SPOI-3845] - BlockSynchronizer  sometimes misbehaves when BlockReader is killed.   [SPOI-3862] - Ingestion UI - left/right margins have no width  [SPOI-3876] - Review the changes done to DimensionsComputation for HadoopWorld Demo  [SPOI-3940] - Demo app package version stuck at v1.0-SNAPSHOT  [SPOI-4029] - PiJavaScript demo pi calc operator fails to re deploy  [SPOI-4067] - Launch of ingestionApp using DTCP is failing  [SPOI-4070] - Stray directory /opt/datatorrent/current/datatorrent  is created after installation.    [SPOI-4077] - Ingestion UI: Create option to scan the directory recursively  [SPOI-4079] - Gateway does not retain listen address specified during installation  [SPOI-4080] - Gateway guard hides errors with restarts  [SPOI-4081] - Ingestion UI: pipeline table doesn't auto refreshes  [SPOI-4085] - UI: wrong application selection after sorting in pipeline instances table  [SPOI-4087] - Relaunch functionality keeps relaunching multiple apps  [SPOI-4094] - Ingestion app tests failing and empty directory getting created under target that are not removed  [SPOI-4128] - Need to explain difference between properties   state holding data structure  [SPOI-4175] - Remove directory prop from base block reader and ftp block reader from ingestion   [SPOI-4229] - committed function not getting invoked in local mode  [SPOI-4352] - App Builder uiType not present for several classes  [SPOI-4362] - Remove dependencies on DirectoryScanner from OperatorDiscoverer when used in stram  [SPOI-4691] - DT console shows negative memory size for killed application  [SPOI-4702] - IndexOutOfBoundsException in Stram due to counters in AbstractBlockReader", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_5", 
            "text": "[SPOI-3592] - Clean up appInstance widget definitions  [SPOI-3783] - Remove gateway port automatic re-selection on startup feature  [SPOI-3932] - demos ui references web services v1  [SPOI-3944] - Ingestion UI : Display pipelines progress  [SPOI-3945] - UI: Add multiple input support  [SPOI-3946] - Create an auto scaling scheme for the Block Reader and Writer   [SPOI-4003] - Ingestion app: add a feature to re-try failed blocks  [SPOI-4026] - Aggregated counters are not published via web-socket in the logical operators topic  [SPOI-4048] - Block application launch during critical system issues  [SPOI-4078] - Gateway address argument validation during installation and launch  [SPOI-4551] - Enhance Gateway API to return AppIDs for a given application name  [SPOI-4578] - Allow supporting archive jars in configuration definition for app packages  [SPOI-4728] - Support -originalAppId when launching apps through the DT Gateway API  [SPOI-4751] - Upgrade sniffer maven plugin to the latest version", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_3", 
            "text": "[SPOI-3028] - [HDHT] Hive Interoperability  [SPOI-3029] - [HDHT] Export to ORC Format  [SPOI-4050] - AppData UI Dashboard page  [SPOI-4093] - Ingestion UI: Control for triggering scan of files  [SPOI-4636] - Added $? feature in dtcli and use it in CLIProxy  [SPOI-4718] - CLI commands to support variable arguments", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#task_4", 
            "text": "[SPOI-3242] - Rethrow the exceptions being caught in catch block of operators.  [SPOI-3502] - Create the landing page for the UI of ingestion application  [SPOI-3504] - [Ingestion UI] Create views for creating a pipeline and its description  [SPOI-3762] - Provide installation version option for installer  [SPOI-3763] - Update document export tokens  [SPOI-3870] - Implement transformation functions for rainier poc1  [SPOI-3871] - Filter records based field values for rainier poc1  [SPOI-3872] - Generate audit and bad record logs for rainier poc1  [SPOI-3970] - Ingestion UI : Use the console package   [SPOI-3981] - Remove unused imports  [SPOI-4040] - Add an operator that keeps track of failed files  [SPOI-4122] - [UI] : Display list of skipped files on UI when overwrite flag is false  [SPOI-4172] - After effects of removing threshold property from BlockReader in ReaderWriter partitioner/stats-listener    [SPOI-4369] - [AppData][AppDataTracker] Deserializer operator   [SPOI-4385] - Create metrics aggregators - sum, min, max, count  [SPOI-4544] - Custom metrics can be cumulative or per window which can be statically declared by the operator developer  [SPOI-4653] - Create Aggregators registry in AppDataTracker  [SPOI-3767] - Auto scaling of BlockReader using partitioning  [SPOI-4748] - Add data query to custom metrics store", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#bug_6", 
            "text": "[MLHR-1614] - AbstractFSWriter in append mode is not fault-tolerant  [MLHR-1620] - Remove close file from AbstractFSWriter  [MLHR-1637] - Cleanup skipping of records from setup of FSDirectoryInputOperator   [MLHR-1643] - FileSplitter recovery fails  [MLHR-1644] - Add Mock Server Libraries in unit tests of database/key value store operators  [MLHR-1653] - Remove JavaScriptOperatorBenchMark form library  [MLHR-1656] - AbstractFileOutputOperator LeaseExpired exception when cache reaches its threshold  [MLHR-1668] - Stateless Partitioner in case of parallel partition ignores parallel partition count (except the first time define partitions is called)  [MLHR-1687] - AbstractBlockReader threshold breaks the idempotency  [MLHR-1708] - Duplicate data read from kafka if kafka partitions are less than DT partitions  [MLHR-1712] - The directory under which the idempotent state is stored should be relative to the app directory so that the state is copied on relaunch  [MLHR-1723] - FTPStringInputOperatorTest fails on Windows OS", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_6", 
            "text": "[MLHR-1547] - Integration of Idempotent storage manager to Directory scanner  [MLHR-1621] - Parititon Couchbase Output Operator  [MLHR-1632] - Add couchbase mock to couchbase tests.  [MLHR-1634] - Improve the BlockReader partitioning scheme to accommodate ingestion rate  [MLHR-1641] - Improve the BlockReader  [MLHR-1661] - Ability to override the stream-codec of input port in AbstractFileOutputOperator  [MLHR-1684] - Improve file splitter not to emit all files  in one shot and hold scanned file names in memory  [MLHR-1694] - BasicCounters improvements", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_4", 
            "text": "[MLHR-1355] - Supports secure hadoop cluster in the installer  [MLHR-1497] - Operators for ElasticSearch  [MLHR-1578] - UI Auth: UI to show different tabs and/or buttons for different user permissions", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#version-201", 
            "text": "", 
            "title": "Version 2.0.1"
        }, 
        {
            "location": "/release_notes/#bug_7", 
            "text": "[SPOI-4379] - Operator removed from physical plan due to invalid SHUTDOWN status  [SPOI-4381] - Queue size missing on physical operator page   [SPOI-4382] - queueSize port metric reports bogus values  [SPOI-4384] - Recovery fails due to corrupted checkpoints.  [SPOI-4437] - DTCli not recognizing $HOME variable set", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_7", 
            "text": "[SPOI-4728] - Support -originalAppId when launching apps through the DT Gateway API  [SPOI-4390] - Replace 'start time' on console to 'Up time'", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#version-200", 
            "text": "", 
            "title": "Version 2.0.0"
        }, 
        {
            "location": "/release_notes/#bug_8", 
            "text": "[SPOI-4046] - Gateway /containers?states={state} call returns erroneous state information  [SPOI-4057] - License can not be upgraded to Evaluation / Production License  [SPOI-4037] - CONTAINERS_MAX_COUNT in sandbox prevents demos launch from dtcli  [SPOI-4008] - HDHT Error while flushing write cache  [SPOI-3837] - Partitioner interface semantics broken in 2.0.0 physical plan implementation]  [SPOI-3901] - twitter demo app package launch throws NoClassDefFoundError  [SPOI-3900] - Gateway gets 400 error when phone home  [SPOI-3922] - dtcli does not show output port attributes.  [SPOI-3934] - DT CLI Needs to validate user inputs  [SPOI-3054] - # Of Tuples Produced != Number Of Tuples Output By Unifier When Repartitioning  [SPOI-3210] - HDHT DTFile reader bug  [SPOI-3340] - Settings in ~/.dt/dt-site.xml don't override app package defaults  [SPOI-3349] - Can we document how to enable password security in gateway  [SPOI-3365] - Launch App Package does not honor -local option  [SPOI-3369] - Implement the pam authentication as a hadoop authentication handler  [SPOI-3396] - launch command should be able to specify local conf file when launching app package  [SPOI-3397] - DT Gateway doesn't start on Mapr 4.0.1  [SPOI-3402] - Checking the UID for Dtadmin  [SPOI-3419] - KafkaAdsDimensionsDemo demo available with installer fails  [SPOI-3420] - Installation issues on HDP 2.2  [SPOI-3469] - gateway, dtcli fail to start in development mode  [SPOI-3503] - AppPackage tests produce 100+GB files  [SPOI-3522] - MaxEventsPerSecond for Flume Ingestor should adjust for partitioned instances  [SPOI-3528] - Installer does not show relevant error if nonexistent env file passed as argument and finishes silently with defaults  [SPOI-3548] - browser inconsistencies with table  [SPOI-3550] - Confusing Breadcrumb navigation on Application Page  [SPOI-3555] - Unifier names should not be links to nonexistent pages  [SPOI-3594] - Systems Diagnostic produces conflicting results for Hadoop installation  [SPOI-3595] - Correctly Implement the clone method in platform.  [SPOI-3608] - User Profile and User Management should not show when auth is disabled  [SPOI-3613] - RBAC Configuration screen is completely missing after recent changes  [SPOI-3628] - Pubsub websocket auth not working with kerberos frontend authentication  [SPOI-3638] - Cant assign roles to user in console  [SPOI-3644] - Develop Tab Can not be seen on naviagation bar   [SPOI-3646] - Error creating new config xml file in package  [SPOI-3647] - MR Operator demo throws null pointer exception  [SPOI-3648] - Cant add admin permissions to existing roles  [SPOI-3654] - In passwd policy user can not change own password using ui console  [SPOI-3657] - Can't Launch Apps Using ui console if user is not the same as dtgateway user  [SPOI-3663] - modify certification scripts and gateway proxy to upload demo jars from repackaged malhar demos  [SPOI-3665] - DataTorrent logo link broken in console  [SPOI-3670] - Update DTCliTest to create app package like AppPackageTest.java  [SPOI-3672] - Uploaded license not used when launching app  [SPOI-3674] - License info not updated after license upload until page reload  [SPOI-3675] - Display Error in Sandbox. License Manager Error:null  [SPOI-3682] - doubleclick select active kills containers  [SPOI-3684] - Gateway deletes JSON app silently (no error) on PUT  [SPOI-3685] - Error message in launch app modal is not red  [SPOI-3686] - Launch properties do not get sent with the launch request  [SPOI-3689] - AppPackages APIs give 404 not found messages  [SPOI-3690] - APIs falsely report role assignments  [SPOI-3691] - API /ws/v1/config/hadoopInstallDirectory does not work  [SPOI-3708] - Gateway shows License error after license agent gets back to normal  [SPOI-3710] - Sandbox packaging invalid due to HDFS dependency  [SPOI-3712] - Exclamation point in DT cli tries to expand \"event\"  [SPOI-3713] - When CLI has a problem running a CLIProxy command, there is out of memory error from the gateway  [SPOI-3714] - Launch button from other screens should bring up launch modal  [SPOI-3716] - locality performance benchmarks operators failing with OOM exception  [SPOI-3717] - Insufficient license memory for Node Local Performance benckmark application  [SPOI-3718] - change high availability certification benchmarking to compare number of active containers instead of total number of containers  [SPOI-3720] - License manager not found returned from Gateway even though it returns valid license including agent info  [SPOI-3721] - Changes to XMLConfig to incorporate changed properties  [SPOI-3725] - NullPointerException in Certification-initDemo for PerformanceBenchmarkForFixedNumberOfTuples  [SPOI-3729] - Thread local validation failure for multiple ports between two operators  [SPOI-3732] - Get User Info APIs creates users  [SPOI-3733] - Can't Launch Apps Using UI or gateway APIs in kerberos environment  [SPOI-3734] - Remove macro from cli  for application or add relevant application  [SPOI-3754] - Gateway can only restore roles once", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_8", 
            "text": "[SPOI-2036] - Ensure that definePartitions and partitioned are called for parallel partitioned operators  [SPOI-2955] - Provide tool-tip capability to RTS UI  [SPOI-3194] - Support PAM authentication mechanism as an agent for LDAP  [SPOI-3304] - Reorganize code to ensure easy consumability  [SPOI-3345] - Delete OperatorContext.PartitionTPSMax and .PartitionTPSMin   [SPOI-3346] - Delete OperatorContext.InitialPartitionCount  [SPOI-3366] - Update Eval license for RTS 2.0  [SPOI-3401] - Switch installer, gateway, console from using HADOOP_PREFIX to hadoop executable  [SPOI-3404] - Provide support for viewing/collecting YARN logs across the cluster  [SPOI-3415] - Remove defaults from dt-site.xml and add version  [SPOI-3418] - Remove dependency on /etc/datatorrent  [SPOI-3447] - Partitioning of couchbase input operator  [SPOI-3525] - Messages shown in installer and uninstaller are longer than 80 characters  [SPOI-3554] - Log-Level Setter for angular  [SPOI-3626] - appPackageDisplayName in /ws/v1/appPackages  [SPOI-3667] - Confirmation box to shutdown or kill apps should have \"warm\" colors  [SPOI-3668] - DTGateway logs are flooded with permission WARN logs  [SPOI-3694] - Populate launch properties with required properties of app  [SPOI-3697] - Add content-disposition so that app package download has a reasonable file name when downloaded from browser  [SPOI-3698] - Support URI codec out the box so that UI can set URI operator properties  [SPOI-4066] - Portable environment settings across releases", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_5", 
            "text": "[SPOI-3393] - Track recordings by a generated ID rather than startTime  [SPOI-3422] - Restore default roles with backend  [SPOI-3683] - Add download button to app package  [SPOI-3753] - \"Reset to Defaults\" for user roles", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#task_5", 
            "text": "[SPOI-3927] - Support per application configuration defaults and required properties  [SPOI-2809] - Implement password authentication as a hadoop authentication type  [SPOI-3115] - Update document with password authentication once UI supports user mgmt  [SPOI-3239] - HDHT store removal support  [SPOI-3351] - Abstract the reconciler used in Guava and Rainier to malhar  [SPOI-3368] - Configuration handling on major version upgrade  [SPOI-3371] - CLI to doAs the user specified by HADOOP_USER_NAME if security is enabled  [SPOI-3372] - Gateway REST service to only return rows the user is authorized to view  [SPOI-3373] - Gateway Websocket service to only publish to subscribers with info individual subscribers are authorized to view  [SPOI-3374] - Gateway to return 403 Forbidden for resources the user is not authorized to view or operations the user is not authorized to perform  [SPOI-3375] - Gateway support for permission based authorization  [SPOI-3376] - Gateway REST call to show what permissions the current user has  [SPOI-3377] - Gateway REST call to support role and permission management  [SPOI-3378] - HTTP Header auth support in Gateway   [SPOI-3381] - Implement Kerberos group to role mapping  [SPOI-3385] - Gateway ACL support at the application level  [SPOI-3390] - LDAP/AD support in Gateway  [SPOI-3405] - Implement Kerberos security context and security context filter in Gateway  [SPOI-3443] - Ingestion Application repo and package  [SPOI-3472] - Installer testing  [SPOI-3485] - Console repository relocation  [SPOI-3486] - Create HDHT section in Application Developer Guide  [SPOI-3517] - Document JVM_OPTIONS and QUEUE_NAME features in user docs  [SPOI-3519] - Add PAM auth user docs  [SPOI-3520] - Update RBAC user docs  [SPOI-3521] - Console user docs updates  [SPOI-3536] - Create hdfs output operator to write table files  [SPOI-3537] - write hdfs writer for small files to copy to vertica  [SPOI-3538] - Combine small file writer with vertica writer  [SPOI-3540] - Sandbox updates to support app package launches from console  [SPOI-3593] - Clean up import functionality for demo apps  [SPOI-3610] - Implement RBAC features for App Packages  [SPOI-3619] - Update console for the new app package REST API  [SPOI-3627] - Convert our REST API from v1 to v2 as we have made some backward incompatible changes in the REST API  [SPOI-3631] - Test HDHT recovery  [SPOI-3632] - Upgrade and test demos on 2.0.0  [SPOI-3633] - Set certain sensitive permissions \"admin\" role only  [SPOI-3634] - Allow default app instance and app package permissions for each user  [SPOI-3635] - Remove INITITAL_PARTITION_COUNT from demo app pkg properties  [SPOI-3636] - Upgrade and test demos on 2.0.0  [SPOI-3650] - Test demos on sandbox on dt 2.0.0  [SPOI-3651] - benchmarks for dt 2.0.0  [SPOI-3652] - list of current benchmarks  [SPOI-3653] - Document property changes needed run demos on 2.0.0  [SPOI-3661] - Update security section in operations guide  [SPOI-3662] - Operations Guide: Update application configuration section  [SPOI-3669] - Remove gateway jars api usage from benchmarking and use app pkg api instead  [SPOI-3695] - Documentation on System Alerts in the Gateway  [SPOI-3700] - run app memory usage benchmarks for 2.0  [SPOI-3701] - run performance benchmarks for 2.0  [SPOI-3702] - run high availability benchmarks for 2.0  [SPOI-3703] - Run Fs output operator benchmark  [SPOI-3704] - run performance across tuple size benchmarks for 2.0  [SPOI-3705] - Change jars api usage to apps package api usage in certification  [SPOI-3706] - Convert Malhar benchmark module to app package  [SPOI-3730] - Getting Started Guide updates", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#bug_9", 
            "text": "[MLHR-1237] - Scrolling log viewer with mouse does not trigger getting more log content  [MLHR-1242] - \"last heartbeat\" in killed container list shows date in 1969  [MLHR-1443] - Attempt reconnect if web socket connection is dropped  [MLHR-1523] - Physical operator ids sorted lexicographically  [MLHR-1552] - Port JMS Input operator changes to library.  [MLHR-1553] - shutdown and kill cmds still visible when ended apps are selected  [MLHR-1588] - New installation does not walk through welcome steps  [MLHR-1590] - App Instance dashboard cannot find Logical DAG widget for starting app  [MLHR-1591] - pending undeploy does not have an icon or color associated  [MLHR-1592] - webSocket does not reconnect if user logs out then logs back in  [MLHR-1593] - Installer should be ok with 404 errors when fetching HadoopLocation and dfsRootDirectory  [MLHR-1594] - DFS permissions error during install does not provide resolution steps  [MLHR-1595] - Installer asks for login even if auth is disabled  [MLHR-1606] - Change EDIT_AND_KILL_OTHERS_APPS to MANAGE_OTHERS_APPS  [MLHR-1609] - Physical Operators have an invalid heartbeat when PENDING_DEPLOY  [MLHR-1611] - Do not try to put or edit admin role in auth management page  [MLHR-1612] - New containers from websocket do not get jvm name  [MLHR-1615] - Feedback for retrieving ended apps  [MLHR-1619] - When launching app package, allow launch time parameters to be specified  [MLHR-1622] - YahooFinanceApplication throws null pointer exception on 2.0  [MLHR-1627] - Make twitter credentials into requiredProperties in app package  [MLHR-1631] - NullPointerException in launching HDFSBenchmarking App:TupleSize property missing in appResponse", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_9", 
            "text": "[MLHR-1190] - Infinite scroll mechanism in stram events widget  [MLHR-1234] - Add version information to system diagnostics screen  [MLHR-1555] - Remove dashboard component from main operations page  [MLHR-1571] - Height of app list should be determined by available space  [MLHR-1596] - Extract two-way infinite scroll behavior into directive  [MLHR-1597] - Fix FTP Input operator  [MLHR-1602] - Partitioning of couchbase input operator", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_6", 
            "text": "[MLHR-1228] - create profile storage object  [MLHR-1261] - AngularJS Migration - Tuple Viewer  [MLHR-1262] - AngularJS Migration - Tuple Recording  [MLHR-1354] - Auth management support in the UI", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#story", 
            "text": "[MLHR-975] - Migration from Backbone to Angular", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#task_6", 
            "text": "[MLHR-1292] - Recording View  [MLHR-1331] - Update the tuple recording topic  [MLHR-1586] - Remove DirectoryScanInputOperator from library  [MLHR-1601] - Re-arrange logstream app in dt application package format  [MLHR-1608] - Create a Block reader which emits Slice and doesn't read ahead of a block boundary  [MLHR-1617] - Sensitive permission assignment need dialog box warning with consequences  [MLHR-1618] - Update console for the new app package REST API  [MLHR-1623] - Create synchronizer for asynchronously processing streaming data for committed windows  [MLHR-1626] - Write unit tests for AbstractSynchronizer", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#bug_10", 
            "text": "[SPOI-2946] - Provide a tool that generates a license report based on license audit logs  [SPOI-2974] - Better error handling when license is expired  [SPOI-3147] - Issues with dynamic partitioning of Block readers   [SPOI-3192] - Send proxy user when making web service calls from gateway  [SPOI-3267] - HDHT Operator crash when launching application against existing store  [SPOI-3271] - Change port stats propogation test to check for cumulative buffer server bytes  [SPOI-3286] - HDHT WAL recovery crashing with NegativeArraySizeException  [SPOI-3287] - Research the types of data that can be stored in hive orc files  [SPOI-3288] - NullPointerException in StreamingContainerManager-  fillLogicalOperatorInfo  [SPOI-3289] - App Packages launch not reading required properties from dt-site.xml  [SPOI-3293] - Incorrect application name when launching JSON app.  [SPOI-3294] - Configuration setting from property.xml not effective for JSON app  [SPOI-3296] - npm install fails for malhar-ui-console  [SPOI-3297] - Remove keys from metric list in app dashboard widget.  [SPOI-3300] - Two operators named \"Dimension Computation\"  [SPOI-3305] - Create Sales Demo JSON Input Generator  [SPOI-3306] - Change Event Schema to default to Sales Schema  [SPOI-3309] - DT flume sink not draining under some circumstances  [SPOI-3313] - Setting attributes with JSON and properties app needs cleanup  [SPOI-3319] - NPE in FSStatsRecorder  [SPOI-3320] - The platform should use user folder in hdfs for storage  [SPOI-3323] - Gateway resource leak when RM is not running and/or during network problem   [SPOI-3325] - HDHT DTFile clean cache when reader close  [SPOI-3338] - Application level operator properties do not override \"global\" operator properties  [SPOI-3343] - Gateway was stuck", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_10", 
            "text": "[SPOI-2918] - Document Counters  [SPOI-3188] - Ability to interpret an input operator as a regular operator   [SPOI-3312] - OperatorAnnotation that enforces checkpointing to happen at application window boundary  [SPOI-3317] - Add sales generator tuples per window randomization controls  [SPOI-3318] - Add more dimensions and aggregates to Sales demo  [SPOI-3321] - Improve data variation between categories, regions, and discounts  [SPOI-3329] - Implement simple variable substitution in properties files", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_7", 
            "text": "[SPOI-2014] - Support for doc link for an operator   [SPOI-2931] - Make sure app packages work in sandbox  [SPOI-2989] - HDHT - Recovery", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#task_7", 
            "text": "[SPOI-2663] - Certify datatorrent on a CDH5 secure cluster  [SPOI-2702] - HDS - Generic time series query  [SPOI-2843] - Tracker ticket for work related to app package in Malhar  [SPOI-2945] - Allow application to start even if license manager is not running for production license file  [SPOI-2950] - Generate monthly alerts when the memory used crosses the licensed memory  [SPOI-3022] - Run jpa and jdbc tests on vertica installation in cluster  [SPOI-3098] - Scrub all components that make it to the release list of jiras  [SPOI-3119] - Generate audit logs in the applications as well  [SPOI-3153] - Certify datatorrent on HDP secure cluster  [SPOI-3156] - Tool to generate license report from a single Hadoop grid  [SPOI-3158] - Webservice for license report  [SPOI-3160] - Uptime debugging for license mgr  [SPOI-3161] - license mgr should write memory reported by each app in the logs  [SPOI-3184] - Test case to reproduce locality issue in CDH  [SPOI-3229] - New app will launch for production license even if no memory is available in license  [SPOI-3261] - Kafka one to many dynamic partitioning pilot  [SPOI-3269] - Support contact update  [SPOI-3278] - Research elastic search functionality to build operators  [SPOI-3280] - Create a model parser that loads the model mapping file and generates sql mapping  [SPOI-3282] - Create a vertica output operator  [SPOI-3283] - Start License Manager as part of Installation  [SPOI-3284] - Watch dog process for License Manager in Gateway  [SPOI-3299] - O15 demo: Dictionary for keys  [SPOI-3307] - Add Enrichment operator data file  [SPOI-3315] - O15 demo: Update widget configuration for sales schema  [SPOI-3335] - Kafka offset manager  [SPOI-3339] - Move FSStorageAgent to Malhar  [SPOI-3352] - Process OperatorCommand returned by stats listener  [SPOI-3411] - Create a feed processor operator that converts input event to table rows  [SPOI-3412] - Create gzip input operator  [SPOI-3413] - Create pluggable functions for each column based given function name and parameters  [SPOI-3414] - Create Table controller to handle value generation for each column including user defined functions  [SPOI-3561] - Recording View: links to start and stop recording  [SPOI-3562] - Recording View: page module  [SPOI-3565] - Container Page (angular)", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#bug_11", 
            "text": "[MLHR-1267] - AngularJS Migration - Dashboard Layout Lock  [MLHR-1352] - Custom directive for breadcrumbs  [MLHR-1489] - Create a hive output operator that can write to hive orc files  [MLHR-1500] - Cleanup of the junit.framework.Assert   [MLHR-1505] - Stram event error closes when clicking on stack trace  [MLHR-1506] - Stram event collection removes events  [MLHR-1512] - UI Console - Dashboard Reset  [MLHR-1532] - Table says \"loading\" when active filter result is 0 rows  [MLHR-1541] - Add orc file output to adsdimension demo  [MLHR-1542] - App Data Framework - WebSocket Support  [MLHR-1585] - Installer fails with auth enabled", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_11", 
            "text": "[MLHR-1256] - Create a new landing dashboard  [MLHR-1432] - Add confirm for deleting an app package  [MLHR-1451] - Loading feedback for malhar-angular-table  [MLHR-1452] - Loading feedback for app packages and package apps  [MLHR-1454] - Performance Tuning for malhar-angular-table  [MLHR-1499] - Idempotent State Manager  [MLHR-1531] - App overview does not show started time  [MLHR-1540] - Remove stram events from physical view  [MLHR-1543] - Switch to gulp in malhar-angular-table  [MLHR-1549] - Short operator properties should be shown in line", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_8", 
            "text": "[MLHR-1282] - AngularJS Migration - App Instance Page - Alerts Recordings View  [MLHR-1301] - AngularJS Migration - Physical Operator Page - Recordings  [MLHR-1317] - AngularJS Migration - App Instance Page - Logical View - Log Levels  [MLHR-1362] - AngularJS Migration - App Instance Page - Stram Events Widget Resize  [MLHR-1370] - AngularJS Migration - Container View - Chart  [MLHR-1428] - Dashboard Component - Option to Disable Vertical Resize  [MLHR-1462] - Socket.IO Kafka Communication Protocol  [MLHR-1463] - UI Console - Client Settings  [MLHR-1464] - Kafka App Data - Server Socket.IO Node.js  [MLHR-1465] - Kafka Socket.IO Service - Latency  [MLHR-1466] - Kafka Server - Query Cache  [MLHR-1470] - Dashboard Builder Integration  [MLHR-1486] - UI Console - Deploy Scripts  [MLHR-1488] - App Package Dag Viewer - Handle Case when DAG is empty  [MLHR-1491] - App Data UI - Runtime Configuration for Default Dashboard/Widgets/Queries  [MLHR-1495] - Kafka Debug Widget - Kafka Producer/Consumer Topics  [MLHR-1498] - Provide Ability to Reset Dashboard Configuration   [MLHR-1502] - App Data Server - Fetch Latest Kafka Offset  [MLHR-1514] - App Data Server - EventEmitter  [MLHR-1515] - App Data Server - UML Diagrams  [MLHR-1520] - App Data UI - Table Widget  [MLHR-1521] - App Data UI - WebSocket Support  [MLHR-1530] - App Data UI - Table Widget", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#story_1", 
            "text": "[MLHR-1295] - AngularJS Migration - Logical Operator Page  [MLHR-1296] - AngularJS Migration - Physical Operator Page", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#task_8", 
            "text": "[MLHR-1270] - Container Page (angular)  [MLHR-1272] - Container Page: memory gauge  [MLHR-1274] - Container Page: metric chart  [MLHR-1281] - Stream View  [MLHR-1286] - Stream View: sources table  [MLHR-1287] - Stream View: sinks  [MLHR-1288] - Port View  [MLHR-1290] - Port View: overview  [MLHR-1291] - Port View: chart  [MLHR-1293] - Recording View: page module  [MLHR-1294] - Recording View: links to start and stop recording  [MLHR-1307] - Installation Wizard  [MLHR-1310] - Info Menu Links  [MLHR-1312] - Lock Layout (malhar-angular-dashboard)  [MLHR-1453] - System Diagnostics Page  [MLHR-1460] - Installation Wizard: welcome page  [MLHR-1461] - Installation Wizard: hadoop config page  [MLHR-1467] - Installation Wizard: license screen  [MLHR-1482] - Installation Wizard: summary screen  [MLHR-1483] - Installation Wizard: license upload  [MLHR-1496] - Create an operator for Solr  [MLHR-1516] - Annotate new FS output operators  [MLHR-1518] - WebSocket support for dimension demo  [MLHR-1574] - UI Auth: support for current password auth  [MLHR-1575] - UI Auth: Managing role and permissions", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_3", 
            "text": "[SPOI-3123] - Add troubleshooting section to Operations and Installation Guide  [SPOI-3124] - Check for vmem/pmem ratio during System Diagnostics test  [SPOI-3129] - Integrate Kafka query protocol for AdsDimension demo.  [SPOI-3154] - Setup AdsDimension demo on cluster  [SPOI-3226] - Create JSON AdInfo Generator for App Builder Demo  [SPOI-3227] - Crate JSON to Map converter for App Builder Demo  [SPOI-3228] - Create shared dimensions computation schema for App Builder Demo", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#bug_12", 
            "text": "[SPOI-1667] - Drop of Performance for Machine demo  [SPOI-2742] - Core dumps on EMR  [SPOI-2942] - Disable physical plan locking when there is no available license memory  [SPOI-2947] - Provide a tool that distributes memory and generates licenses that can be deployed on multiple clusters  [SPOI-3018] - Investigate raw performance of vertica jdbc  [SPOI-3024] - Adding Virtual Mem to Physical Memory setting in Readme file  [SPOI-3025] - Can't see the container logs from DT Console in Pivotal installation  [SPOI-3053] - Continual Repartitioning Of Operator Causes Out Of Memory Exception  [SPOI-3081] - Core StatsTest.testPortStatsPropagation Failing  [SPOI-3107] - Failed to load Logical Plan   [SPOI-3132] - User Interface Guide URL  specified in current Sandbox gives 404  [SPOI-3181] - dtgateway script: $PATH is being appended to repeatedly when gateway has trouble starting  [SPOI-3187] - HDHT query results partition duplication  [SPOI-3190] - HDHT AdsDimension demo last bar should grow  [SPOI-3195] - Operator class exception when loading app package  [SPOI-3196] - CLI returns Perm Gen Space errors when there are too many classes to inspect in app package  [SPOI-3197] - Provide meta information for operators that can be used by DAG builder  [SPOI-3198] - App Package archetype is erroneously including app package manifest in the low level jar file  [SPOI-3213] - Newer maven versions ( 3.1) do not work with copy-maven-plugin in app package archetype  [SPOI-3215] - Duplicate streams in OperatorDeploymentInfo with default partition/parallel partitioning   [SPOI-3232] - Mobile demo not dynamically partitioning  [SPOI-3262] - Issue with serialization of xml cartesian product operator  [SPOI-3319] - NPE in FSStatsRecorder", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_12", 
            "text": "[SPOI-1967] - Accept Password for Private Key  [SPOI-2997] - License file content updates  [SPOI-3083] - Ability to save DAG as a DT App Package  [SPOI-3084] - Operator Library Browser  [SPOI-3087] - Properties Section  [SPOI-3088] - Attributes Section  [SPOI-3089] - Operators needed for demo  [SPOI-3102] - FileSplitter needs to be idempotent  [SPOI-3136] - Add a check to see if stram is connected to THIS gateway", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_9", 
            "text": "[SPOI-3142] - Ability to take Logical Plan Serialization JSON to construct a DAG and launch it  [SPOI-3148] - App Package REST API needs to allow PUT of json logical plan for adding/replacing application in app package  [SPOI-3149] - Ability to launch a json-specified app within app package  [SPOI-3185] - Import demo app packages from dtgateway fs  [SPOI-3186] - Group app packages under demos folder in release  [SPOI-3189] - Add display name and description to app package's manifest  [SPOI-3199] - XML Javadoc  [SPOI-3266] - Provide Ability to Allow Cross Origin Access", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#task_9", 
            "text": "[SPOI-3039] - Print data upon license mgr app start  [SPOI-3061] - Add port tuple type in logical plan serialization  [SPOI-3067] - Remove time-bombed public/private key pair from the code  [SPOI-3068] - Make sure the release process (sandbox, installer binary) is okay with the time bound key pair removed from the code  [SPOI-3069] - Remove the entire request blob process and generate license straight from customer info and license info  [SPOI-3070] - Device ID change in license  [SPOI-3071] - Generate License web tool to prompt for password for the private key  [SPOI-3126] - O15 - Generic Dimension Computation/Store Operator  [SPOI-3128] - HDHT - AdsDimension app integration for internal demo   [SPOI-3130] - HDHT - Test files with varying block sizes  [SPOI-3134] - Add a organization field in license file  [SPOI-3162] - Kafka ingestion demo  [SPOI-3164] - Integrate Kafka Query frontend into demos  [SPOI-3165] - Ability to return JSON app as-is from app package through REST  [SPOI-3166] - Ability to delete JSON apps from app package  [SPOI-3167] - Ability to save incomplete json-based app to an app package  [SPOI-3179] - Investigate how IDEs parse javadoc  [SPOI-3182] - Complete dag annotations spec  [SPOI-3193] - Investigate pluggable authorization mechanisms supported by Hadoop  [SPOI-3200] - Generate resource file containing javadoc comments and custom tags  [SPOI-3201] - Configure default javadoc doclet in pom to include custom tags  [SPOI-3202] - Transform xml javadoc resources file to contain only comments and tags  [SPOI-3203] - Add javadoc resource file to the class jar at maven build  [SPOI-3206] - Add field and method comments to transformed xml javadoc  [SPOI-3217] - Limit events DTFlumeSink pumps into the dag  [SPOI-3219] - Help diagnose launch issue  [SPOI-3225] - Check and fix machine and twitter hash tag demos  [SPOI-3236] - displayName should be available from appPackage/applications call", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#bug_13", 
            "text": "[MLHR-1347] - Create a block reader in library which is capable of dynamic partitioning itself  [MLHR-1366] - HdfsBucketStore is not completely covered by unit test cases  [MLHR-1367] - Add Counter Aggregators For Monitoring AbstractThroughputFSDirectoryInputOperator  [MLHR-1373] - Container log download link  [MLHR-1404] - App Package File Upload  [MLHR-1407] - App List Kill App Selection Issue  [MLHR-1410] - Add a \"remove\" option to the \"set/subscribe\" method of BaseCollection  [MLHR-1412] - App Package archetype is erroneously including app package manifest in the low level jar file  [MLHR-1419] - Xml cartesian product operator has kryo serialization errors in some cases  [MLHR-1422] - UI Physical tab-  sorting by operator id is by lexical order not numerical  [MLHR-1423] - app instance page does not update when state goes ACCEPTED =  RUNNING  [MLHR-1426] - favicon not being handled in gulp build  [MLHR-1436] - Uptime shows -1 day even when the app is runing for some time  [MLHR-1438] - AdsDimensionsWithHDSDemo - Expose Aggregation in Operator Properties  [MLHR-1442] - Move the recipient property for the machine data to config  [MLHR-1557] - subscribe handler on app instance alters original ws message  [MLHR-1558] - Palette in the Apps List does not update after one app gets killed", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_13", 
            "text": "[MLHR-1427] - Make app name link to instance page as well in AppsList", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_10", 
            "text": "[MLHR-1388] - Base DAG Viewer  [MLHR-1389] - App Package List Page  [MLHR-1390] - Applications/Package Info Page  [MLHR-1391] - App Package REST API Integration  [MLHR-1392] - App Package CRUD  [MLHR-1406] - UI Support of app package import  [MLHR-1408] - Allow Shutting Down/Killing Multiple Applications  [MLHR-1424] - UI Grid 3 Integration - Fonts  [MLHR-1429] - Dashboard Component - Configurable Widget  [MLHR-1430] - Kafka UI Demo  [MLHR-1431] - Visual Data Demo Update", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#story_2", 
            "text": "[MLHR-1241] - App Package support in UI  [MLHR-1357] - UI Dashboard Front Page Design", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#task_10", 
            "text": "[MLHR-1079] - Normalize timestamps for info pages  [MLHR-1104] - Design App Package and Upload/Launch Feature  [MLHR-1105] - Polish simplified (current) app data alert feature  [MLHR-1207] - Convert demo apps to app packages  [MLHR-1309] - License Info  [MLHR-1363] - Create file splitter that breaks file into blocks and emits block metadata and a block reader   [MLHR-1415] - Clean-up Hdfs Output operators in library and incorporate the features of fault-tolerant Writer  [MLHR-1503] - Remove deprecated jdbc package from contrib", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#version-104", 
            "text": "", 
            "title": "Version 1.0.4"
        }, 
        {
            "location": "/release_notes/#bug_14", 
            "text": "[SPOI-2346] - ExactlyOnceTest#testLinearInputOperatorRecovery hangs  [SPOI-2511] - ResourceManager HA support  [SPOI-2625] - uninstall.sh should print message that reminds users of running DT applications  [SPOI-2922] - Counters Aggregator gets lost when an operator is parallel partitioned - test fix  [SPOI-2939] - AppBundles test resource refers to fixed version  [SPOI-2967] - Mobile demo dies after couple of days  [SPOI-2979] - App Bundle unit test should load properties.xml  [SPOI-2991] - [kafka-yarn] Archive resource to use less space in HDFS  [SPOI-2993] - appBundle with conflicting names in config and ApplicationAnnotation can't launch in CLI with either name  [SPOI-2995] - Generate MANIFEST.MF automatically instead of requiring user to change it  [SPOI-2999] - Investigate application integration to Vertica  [SPOI-3012] - launch application throws NPE when yarn.application.classpath is not defined  [SPOI-3015] - DTGateway log grows indefinitely  [SPOI-3019] - Investigate how many objects per second (with a single field) using JPA can be written to vertica  [SPOI-3027] - Sandbox GATEWAY_CONNECT_ADDRESS change to support VMWare  [SPOI-3055] - NullPointerException When Repartitioning Too Frequently  [SPOI-3074] - NullPointerException in AppMaster  [SPOI-3079] - NPE while launching application  [SPOI-3080] - Content-disposition: attachment for container logs  [SPOI-3091] - Null Pointer Exception/Internal Server Error When Getting Operator Stats  [SPOI-3092] - DefaultUnifier of a port in an operator does not function correctly when the port has more than one sinks", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_14", 
            "text": "[SPOI-2821] - Add bucket processed time  [SPOI-2891] - DTGateway default log setting too verbose  [SPOI-2938] - Document AppBundles development and deployment  [SPOI-3044] - gateway should show the full stack trace of the origin upon error when proxying", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_11", 
            "text": "[SPOI-2720] - API Call for determining if an application's stram can connect to the gateway  [SPOI-2981] - HDS - File Format  [SPOI-2982] - HDS - Writing Data Files  [SPOI-2983] - HDS - Bucket Meta Data  [SPOI-2984] - HDS - WAL  [SPOI-2986] - HDS - Bucket Management  [SPOI-3031] - Add ability to specify number of recording windows  [SPOI-3032] - Delete request for recording  [SPOI-3033] - Provide byte offset for each line when grepping container log  [SPOI-3049] - Ingestion streamlet design for first cut", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#task_11", 
            "text": "[SPOI-1734] - Licensing Agent Logging for Audit  [SPOI-2810] - Update document with details on password authentication  [SPOI-2906] - Write documentation on App Bundle  [SPOI-2908] - Prototype kafka-on-yarn  [SPOI-2934] - Add application id to the tuple record topic  [SPOI-2962] - Create operator to write events in bucket file and write index files for keys  [SPOI-2963] - Change query processor operator to fetch events based on indices  [SPOI-2996] - Support launch-app-bundle command without the appname argument  [SPOI-3006] - Create a LATLON to MGRS converter utility  [SPOI-3009] - Create a utility to calculate MD5 hash  [SPOI-3010] - Create a utility for Blowfish  [SPOI-3014] - Update sandbox release docs  [SPOI-3026] - Change the name app bundle to app package and the extension zip to jar  [SPOI-3036] - Review issue with ingestion  [SPOI-3050] - A first cut at design for ingestion streamlet  [SPOI-3051] - License work  [SPOI-3108] - Add REST API to get the entire hadoop configuration  [SPOI-3121] - HDS - File Format - Performance testing TFile / DTFile  [SPOI-2985] - HDS - File Format - Performance testing - HFile  [SPOI-2987] - HDS - File Format - Performance testing - MapFile  [SPOI-3122] - HDS - File Format - HFile implementation  [SPOI-3017] - HDS - File Format - Common Interface  [SPOI-3100] - Create the SinglePointCalculator   [SPOI-3103] - Create Synchronizer Operator  [SPOI-3104] - Create Persister to store the data", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#bug_15", 
            "text": "[MLHR-1223] - JDBC Store doesn't support connection properties  [MLHR-1253] - documenting keyhashvalpair  [MLHR-1255] - db api in malhar lib has KeyValueStore interface which is similar to db/cache/Store api   [MLHR-1260] - AngularJS Migrations - Metrics Grouping  [MLHR-1268] - AbstractBatchTransactionableStoreOutputOperator add to library db is same as public abstract class AbstractAggregateTransactionableStoreOutputOperator  [MLHR-1269] - Mobile demo dies after couple of days  [MLHR-1319] - AngularJS Migration - WebSocket Service  [MLHR-1320] - AngularJS Migration - Page Visibility API  [MLHR-1323] - stram events links broken (angular)  [MLHR-1325] - Wrong \"track by\" in physical operators list  [MLHR-1328] - \"inspect\" button in appslist broken  [MLHR-1335] - dtPageHref directive does not update href of element on changes  [MLHR-1336] - Container shorthand directive not watching value changes  [MLHR-1342] - Allow DAG widgets to be vertically resized (using jquery resizable)  [MLHR-1344] - AngularJS Migration - Metrics Chart  [MLHR-1345] - AngularJS Migration - Physical Operator Page - Chart  [MLHR-1346] - Test for AbstractFSDirectoryInputOperator fails sporadically  [MLHR-1359] - AngularJS Migration - Dashboard Widgets Vertical Resize  [MLHR-1364] - BucketManager while reading values doesn't ensure that latest value for a key is read  [MLHR-1368] - AngularJS Migration - App Instance Page - Overview Grouping  [MLHR-1374] - Remove all occurrences of dag.setAttribute(DAG.APPLICATION_NAME, \"xxx\") in demos code", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_15", 
            "text": "[MLHR-1263] - Migrate JDBC non-transaction output operator to new db api and move it to lib  [MLHR-1322] - Changing Machine Data Operator and Stream Names  [MLHR-1324] - appState directive should be renamed to something more generic  [MLHR-1327] - Assume lodash is global throughout console  [MLHR-1343] - Use userStorage to store height of stram events  [MLHR-1351] - Allow transformResponse to set a fetchError  [MLHR-1353] - Centralize location of breadcrumbs  [MLHR-1360] - Put dashboards on the left", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_12", 
            "text": "[MLHR-419] - DAG Widget with AngularJS  [MLHR-1220] - Send the \"unsubscribe\" webSocket message for topic when no more listeners  [MLHR-1239] - AngularJS Migration - Physical DAG View  [MLHR-1240] - AngularJS Migration - Logical DAG View  [MLHR-1243] - Angular JS Migration - Current/Recover Window Id Metrics  [MLHR-1245] - AngularJS Migration - Partitions/Container Metrics  [MLHR-1246] - AngularJS Migrations - DAG View Zoom  [MLHR-1247] - AngularJS Migration - Physical View  [MLHR-1248] - AngularJS Migration - Container View  [MLHR-1249] - AngularJS Migration - Landing Page High-Level Metrics  [MLHR-1250] - AngularJS Migration - Charts  [MLHR-1251] - AngularJS Migration - Gauge Widget  [MLHR-1252] - AngularJS Migration - DAG View Base Renderer  [MLHR-1278] - AngularJS Migration - App Instance Page - Logical View  [MLHR-1279] - AngularJS Migration - App Instance Page - Physical View  [MLHR-1280] - AngularJS Migration - App Instance Page - Physical DAG View  [MLHR-1283] - AngularJS Migration - App Instance Page - Metric View  [MLHR-1298] - AngularJS Migration - Physical Operator Page - Overview  [MLHR-1299] - AngularJS Migration - Physical Operator Page - Port List  [MLHR-1300] - AngularJS Migration - Physical Operator Page - Properties  [MLHR-1303] - AngularJS Migration - Logical Operator Page - Overview  [MLHR-1305] - AngularJS Migration - Logical Operator Page - Partitions  [MLHR-1306] - AngularJS Migration - Logical Operator Page - Chart  [MLHR-1308] - AngularJS Migration - Logical Operator Page - Properties  [MLHR-1313] - AngularJS Migration - App Instance Page - Physical View - Container List  [MLHR-1314] - AngularJS Migration - App Instance Page - Physical View - Operators  [MLHR-1315] - AngularJS Migration - App Instance Page - Logical View - Chart  [MLHR-1316] - AngularJS Migration - App Instance Page - Logical View - Stream List  [MLHR-1321] - AngularJS Migration - Distribution Build with Gulp  [MLHR-1334] - Front-End Build with Gulp - WebSocket Proxy  [MLHR-1337] - Front-End Build with Gulp - JS/CSS Revisions  [MLHR-1338] - Front-End Build with Gulp - Angular Templates Injection  [MLHR-1341] - create userStorage service for user settings  [MLHR-1358] - Breadcrumbs to jump between collection elements  [MLHR-1361] - Logical/Physical DAG - Stream Locality Legend  [MLHR-1365] - AngularJS Migration - Widget Settings Modal - Height Management", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#task_12", 
            "text": "[MLHR-1208] - Convert contrib apps to app bundles  [MLHR-1244] - Update tuple recorder topic to use \"applications. .tupleRecorder. \"  [MLHR-1254] - Create a test case for UniqueValueCountAppender  [MLHR-1264] - Start Logical Operator Page  [MLHR-1265] - Start Physical Operator Page  [MLHR-1266] - Start Container Page View  [MLHR-1271] - Container Page: overview  [MLHR-1273] - Container Page: operator list  [MLHR-1275] - Container Page: log viewer  [MLHR-1276] - Container Page: page module  [MLHR-1284] - Stream View: overview  [MLHR-1285] - Stream View: page module  [MLHR-1289] - Port View: page module  [MLHR-1302] - Operations Landing Page: memory gauge  [MLHR-1311] - Vertical Resize (malhar-angular-dashboard)  [MLHR-1318] - Move webSocket service to malhar-angular-widgets  [MLHR-1326] - Create .jshintrc for IDE/editor, grunt tasks  [MLHR-1330] - Document gulp usage in README  [MLHR-1333] - add \"gulp coverage\" task  [MLHR-1339] - Need a dynamic partitioner for File processing operator where the count of the operator instances is controlled by the backlog present  [MLHR-1348] - Change app bundle to app package on all demos  [MLHR-1349] - Link tables to userStorage service  [MLHR-1372] - Adding the tool-tip capability to console  [MLHR-1376] - Add counters and counters aggregator to mobile demo", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#version-103", 
            "text": "", 
            "title": "Version 1.0.3"
        }, 
        {
            "location": "/release_notes/#bug_16", 
            "text": "[SPOI-2620] - S3 reader error  [SPOI-2673] - Move the Kafka Exactly Once Producer to Malhar Library  [SPOI-2775] - Log file being processed on seperate port to monitor progress by input operators.  [SPOI-2813] - Send top gateways in descending order  [SPOI-2814] - Change field name of input event in CDREvent  [SPOI-2815] - Show top 3 gateways using a widget  [SPOI-2816] - No query poll in alert information view  [SPOI-2817] - For non-finalized buckets threshold flags should not be set  [SPOI-2819] - Send data continuously to the application  [SPOI-2820] - Provide more alerts in the UI  [SPOI-2837] - Operators crashing sporadically when alert information requested from frontend  [SPOI-2838] - Clear out the cdr event files and aggregates are cleared  [SPOI-2844] - Machine data demo waits long before starting  [SPOI-2845] - Recovery is failing in cdr events output operator after upstream operator redeploys  [SPOI-2854] - Buckets are not being finalized correctly when there is a bucket gap in input data  [SPOI-2861] - GATEWAY_CONNECT_ADDRESS not set during install  [SPOI-2866] - Change lablel of top gateways to top 3 gateways  [SPOI-2867] - Change null to empty value in the UI for alert event information  [SPOI-2869] - Need a cleanup script to delete old files from hdfs  [SPOI-2882] - Backup the allatory log file for flume integration build as well  [SPOI-2883] - NPE in bufferserver  [SPOI-2892] - DTGateway WS fails to get YARN logs  [SPOI-2894] - Investigate bucket manager for cdr event storage  [SPOI-2900] - First few events for an application do not get published  [SPOI-2902] - Documentation for sandbox missing images  [SPOI-2910] - PartitioningTest.testDynamicDefaultPartitioning sometimes fails and sometimes succeeds   [SPOI-2913] - Events API returns no events when limit and offset not supplied  [SPOI-2919] - Change cleanup to cleanup folders older than an hour  [SPOI-2920] - make stats recorder asynchronous so that it won't block stram  [SPOI-2921] - Test Node Locality feature on HDP  [SPOI-2932] - Historical containers don't show up on gateway if there is a second attempt on application  [SPOI-2936] - Historical data store high level design  [SPOI-2937] - Gateway throwing exception while launching app", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_16", 
            "text": "[SPOI-2811] - Show input record in alert information dialog  [SPOI-2812] - Show top gateways for all defect aggregates and possibly all aggregates  [SPOI-2881] - Show actual memory usage in dashboard  [SPOI-2891] - DTGateway default log setting too verbose  [SPOI-2911] - Create a Bean2String codec", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_13", 
            "text": "[SPOI-2228] - Gateway to load and to allow manipulation of app bundles  [SPOI-2753] - Front End POC - Aggregates Table  [SPOI-2757] - Front End POC - Kafka Operations  [SPOI-2758] - Front End POC - Historical Data Navigation  [SPOI-2808] - Support YARN log aggregation for dead container log retrieval in the gateway  [SPOI-2826] - Maven archetype for assembling app bundle  [SPOI-2827] - CLI to load app bundles  [SPOI-2828] - Supports launching apps in app bundle in CLI  [SPOI-2829] - Gateway to allow uploading and changing configuration of existing App Bundles  [SPOI-2849] - Demonstrate Datatorrent RTS as front end ingestion for Spark.  [SPOI-2879] - Kafka Request/Response Debug Collapsible Panel  [SPOI-2896] - Unique identifier for stram events", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#story_3", 
            "text": "[SPOI-2835] - Streamlet Design - 1.0.3  [SPOI-2836] - Kafka integration for 1.0.3  [SPOI-2851] - Platform Excellence - 1.0.3", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#task_13", 
            "text": "[SPOI-2212] - Determine logistics of application bundles  [SPOI-2521] - Add counters for checkpointo operations  [SPOI-2546] - Pi demo does not seem to work in 512MB  [SPOI-2547] - Twitter demo needs 1024MB  [SPOI-2684] - Automated distro certification - collect minimum requirements  [SPOI-2786] - Include setters and getters for output fields in the event  [SPOI-2795] - Add top 3 gateways to alert aggregate csv in HDFS  [SPOI-2802] - Dynamic partitioning for S3 input operator.  [SPOI-2822] - Add operator to write cdr events to hdfs  [SPOI-2823] - Fetch events from hdfs for alert events query response  [SPOI-2846] - Partition Aggregations operator  [SPOI-2859] - Separate out query processing from aggregations operator  [SPOI-2870] - Application template classpath does not include malhar libraries  [SPOI-2874] - Add Port Queue Capacity usage as part of the operator stats  [SPOI-2878] - Move the gateway randomization to generator  [SPOI-2890] - Update support contacts to malhar-users group  [SPOI-2914] - Configure Store operator to join logger and tracker stream.", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_4", 
            "text": "[SPOI-2747] - Total emitted window of an operator keeps on increasing even when it has stopped emitting  [SPOI-2850] - Sort out the dependencies for dt-flume so we do not include the jars which are already part of dt dist and hadoop dist  [SPOI-2873] - Deprecate ShipContainingJars.  [SPOI-2895] - launch -license does not work  [SPOI-2907] - Counters appear in physical plan only when a Counters aggregator or a stats listener is set on the operator", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#bug_17", 
            "text": "[MLHR-1176] - Update demo section of README.md  [MLHR-1219] - Non-chrome browsers cannot parse date format from date picker  [MLHR-1227] - AbstractKafkaOutputOperator fails with java.lang.NoClassDefFoundError: com/yammer/metrics/Metrics  [MLHR-1235] - close button on gateway info modal does not close the modal", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_17", 
            "text": "[MLHR-1213] - Use trackBy for malhar-angular-table  [MLHR-1216] - DB key value store manager enhancement  [MLHR-1218] - Clicking on an open stram log event should close it in UI", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_14", 
            "text": "[MLHR-1212] - Make scrollbar draggable on malhar-angular-table  [MLHR-1225] - Logical Operator Table  [MLHR-1226] - Stram Event widget  [MLHR-1229] - Provide better widget options dialog  [MLHR-1231] - Custom template/controller for widget options", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#task_14", 
            "text": "[MLHR-1205] - Restructure directory structure in angular console  [MLHR-1206] - Change malhar pom to use provided scope to prevent DT and hadoop jars from being sucked into the runtime classpath  [MLHR-1210] - Create script to automatically add scripts to index.html  [MLHR-1224] - Kafka enabled AdsDimensions demo", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#version-102", 
            "text": "", 
            "title": "Version 1.0.2"
        }, 
        {
            "location": "/release_notes/#bug_18", 
            "text": "[SPOI-431] - Fix mergeSort operator to actually do mergeSort  [SPOI-2499] - Provide license delegation tokens in secure environment  [SPOI-2500] - Provide new delegation token before an old one expires in secure enviroment  [SPOI-2501] - dt.log not written under CDH5 GA  [SPOI-2506] - [MapR]FileSystem.mkdirs() doesn't work for existing folders in MapRFileSystem  [SPOI-2605] - DT install as root cannot use /user/chetan/dt0528 as DFS location and not printing out detailed error  [SPOI-2608] - LicensingAppMaster's name should not be obfuscated  [SPOI-2617] - Memory usage counted by license agent and not released after app is killed  [SPOI-2619] - Front End Server - fill missing slots for time series  [SPOI-2630] - Not able to launch apps in cloudera cluster  [SPOI-2634] - Uninstall is removing entire datatorrent folder as opposed to the release folder only  [SPOI-2639] - dtcli allows apps to be launched before configuration is finished  [SPOI-2668] - Parent jira for Kafka work for 1.0.2  [SPOI-2678] - Put Fraud demo into the demo UI  [SPOI-2679] - Front End Server - Query ID as Query JSON  [SPOI-2683] - Appmaster Logs are not shown in the Console  [SPOI-2689] - set-operator-property produces NPE on stram   [SPOI-2699] - Mobile demo app mis-behaving   [SPOI-2703] - Gateway fails to start in devel mode with hadoop 2.3.0  [SPOI-2705] - Demo UI instructions missing from install README  [SPOI-2706] - Logical plan change fails w/ obfuscated build  [SPOI-2709] - Partition events should not be registered if same no. partitions result  [SPOI-2710] - Containers running the unifier are not released  [SPOI-2723] - Design and develop the File Ingestion app  [SPOI-2737] - Gateway installation replaces dt-site.xml with invalid version  [SPOI-2744] - ZIP of version's docs not being made available for download  [SPOI-2748] - dtdemos service fails to stop in sandbox  [SPOI-2749] - docs distribution files at root path  [SPOI-2759] - Installation instructions for user setup  [SPOI-2760] - Get container log content fails with 500  [SPOI-2761] - Remove dtadmin reference from install wizzard  [SPOI-2762] - EMR configuration issues  [SPOI-2779] - dtcli failed to read the license file which was uploaded through UI  [SPOI-2798] - Request for log content outside of range results in 500, long request time  [SPOI-2839] - set-operator-property broken in master", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_18", 
            "text": "[SPOI-2602] - Containers published via websocket should only contain live containers and recently finished containers  [SPOI-2610] - Parent: Automatic unobfuscation tool  [SPOI-2613] - Create web tool to run unobfuscator  [SPOI-2614] - Automate transfer of allatori-log.xml to web server  [SPOI-2712] - Add window_width to app info REST call  [SPOI-2736] - Merge /systemAlerts/alerts and /systemAlerts/inAlerts API calls  [SPOI-2751] - Include sandbox README.html with docs.zip and on website", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_15", 
            "text": "[SPOI-950] - Specify memory requirements on per operator basis (duplicate)  [SPOI-2220] - Gateway App Bundle API spec  [SPOI-2339] - Enable container size for each operator(s)  [SPOI-2515] - Ability to dynamically change the logger level for any instantiated loggers within the application  [SPOI-2523] - System alerts for application state and metrics  [SPOI-2525] - Add operator serialization check in local mode  [SPOI-2528] - Expose \"counters\" in REST API   [SPOI-2575] - Kafka Pub/Sub Protocol  [SPOI-2588] - Record physical counters per window  [SPOI-2589] - Expose counters logical and physical through web services/ web socket  [SPOI-2606] - Initializing loggers with levels specified in the configuration  [SPOI-2627] - Front End Server - \"Countdown\" usage  [SPOI-2629] - Front End Server - Support of Multiple Partitions  [SPOI-2631] - Front End Server - Cache Expiration Strategy  [SPOI-2632] - Front End Server - Kafka Reconnect  [SPOI-2633] - Front End Server - Page Performance (Visibility API)  [SPOI-2638] - Front End Server - Kafka Errors Handling  [SPOI-2645] - Front End Server - Kafka SimpleConsumer  [SPOI-2652] - Develop and Stage Hadoop Summit Landing Page  [SPOI-2696] - Front End Server - Top N Metrics  [SPOI-2697] - Front End Server - Dynamic Publisher and Site  [SPOI-2698] - Front End Server - Kafka Troubleshooting  [SPOI-2752] - Front End Server - LRU Cache  [SPOI-2756] - Front End POC - Alert Modal  [SPOI-2804] - Create a hbase operator that uses a config to map incoming csv tuples to hbase table data and saves them in hbase", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#story_4", 
            "text": "[SPOI-2455] - HDFS storage layer for 1.0.2  [SPOI-2565] - Front End Server - Define cache policy for push data in Node.js  [SPOI-2571] - Parent jira for Hadoop Summit work  [SPOI-2590] - Parent: GA Testing  [SPOI-2611] - Parent jira for facilitating trouble shooting and debugging in 1.0.2  [SPOI-2615] - Parent jira for security work in 1.0.2  [SPOI-2662] - Parent jira for licensing security for 1.0.2  [SPOI-2664] - HDFS Storage, Ingestion, Access application data and file storage for 1.0.2  [SPOI-2667] - Automate distro certification and make it part of CI for 1.0.2  [SPOI-2781] - Determine which fields are present in output that are not present in input", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#task_15", 
            "text": "[SPOI-2341] - Update on authentication and authorization in Yarn-open source  [SPOI-2434] - Short document on deduper checkpointing scheme  [SPOI-2440] - Kafka support in POC for 1.0.2  [SPOI-2509] - Investigate how to use Kafka to replace the pub/sub mechanism in Gateway  [SPOI-2532] - Join Operator  [SPOI-2534] - Add support of getting container info of dead applications and dead containers  [SPOI-2535] - Add support for retrieving Aggregated Counters from Response processStats of StatsListener  [SPOI-2542] - Get user to look at instructions before .bin file download  [SPOI-2544] - Twitter demo \"feedMultiplier\" should be RW and test to ensure that it can be changed in runtime  [SPOI-2548] - Memory gauge always shows 100%  [SPOI-2549] - Total memory on right corner needs to be discussed  [SPOI-2552] - Need to list certified Distros on site  [SPOI-2558] - Help set up Kafka operations  [SPOI-2568] - Making Kafka Producer Exactly Once  [SPOI-2569] - Add the Rewind Feature   [SPOI-2573] - Integrate the Scoring of the quality logs  into the DAG  [SPOI-2574] - Add counter calculation in hdfs operator  [SPOI-2584] - DT counters as Key,Val (String,Number)  [SPOI-2585] - Troubleshooting work for reporting min, max, ave, ... on resources  [SPOI-2591] - GA Testing: CDH5 end-to-end  [SPOI-2592] - GA Testing: HDP2 end-to-end  [SPOI-2593] - GA Testing: MapR end-to-end  [SPOI-2594] - GA Testing: UI end-to-end  [SPOI-2595] - GA Testing: dtcli / gateway end-to-end  [SPOI-2596] - GA Testing: Apache Hadoop end-to-end  [SPOI-2597] - GA Testing: High Availability / Recovery  [SPOI-2598] - GA Testing: Application configuration and launch  [SPOI-2599] - GA Testing: Sandbox functionality  [SPOI-2600] - GA Testing: Sandbox UX  [SPOI-2621] - Top 10 support  [SPOI-2622] - Partitioning of DimensionStore operator  [SPOI-2624] - Front End Server - Architecture  [SPOI-2626] - Setting up of number of partitions for Kafka Producer  [SPOI-2636] - Twitter demo app with hashtag top 10  [SPOI-2640] - Setup UI server for HDP grid for Hadoop Summit  [SPOI-2641] - Setup Ambari for Hadoop Summit Demo  [SPOI-2642] - Run Twitter demo on HDP cluster for Hadoop Summit  [SPOI-2643] - Setup Mobile demo for HDP demo at Hadoop Summit  [SPOI-2644] - Set up machine data demo on HDP cluster for Hadoop Summit  [SPOI-2646] - Setup Ads demo on HDP cluster for Hadoop Summit  [SPOI-2647] - Add the new Twitter HashTag top 10 demo to Frontend server for Summit  [SPOI-2650] - Change default license memory settings  [SPOI-2651] - Milestone 1 update  [SPOI-2670] - Launch twitter hashtags on CDH and HDP clusters  [SPOI-2671] - Update CDH cluster DT UI  [SPOI-2672] - Move the HDFSOutputOperator to Malhar Library  [SPOI-2676] - Get events, tuple records from kafka  [SPOI-2680] - Make more managable kafka cluster  [SPOI-2682] - Customer Demos Support  [SPOI-2685] - Automated distro certification - virtualization review  [SPOI-2686] - Automated distro certification - provisioning review  [SPOI-2687] - Certify DT on Pivotal HD  [SPOI-2688] - Make twitter HashTags links in demo  [SPOI-2690] - Upgrade security sections in guides  [SPOI-2700] - Test DimensionStore operator checkpointing and recovery  [SPOI-2714] - Document stram event types  [SPOI-2732] - Create a multi level key map with multiple keys  [SPOI-2733] - Create a CSV lookup class to load CSV and lookup values based on row keys  [SPOI-2735] - Ensure all dimemsions are implemented and covered by test  [SPOI-2763] - Create a csv parser framework that can handle the different csv file formats  [SPOI-2764] - Create a template application  [SPOI-2765] - Create the template configuration file  [SPOI-2766] - Create kafka input for the application   [SPOI-2767] - Add a directory scan operator that progressively scans for new folders and files for input data to the application  [SPOI-2768] - Add a kafka upload script to upload data to the application  [SPOI-2769] - Create an upload script that uploads application files to hdfs  [SPOI-2773] - Add a property to clear the aggregates in the AggregationsOperator  [SPOI-2785] - Include the input event string in the CDR event  [SPOI-2789] - Add query support for getting events for alerts  [SPOI-2790] - Scale the input events into millions  [SPOI-2792] - Send top 3 gateway list in alert event details  [SPOI-2793] - Add the capability to finalize a bucket   [SPOI-2794] - Add ability to clear out old results when replaying data  [SPOI-2797] - Add query support to send the latest five minute buckets and aggregates  [SPOI-2800] - Install a kafka web-console in our cluster  [SPOI-2805] - Investigate yarn log aggregation to see how this fits into our container logs API", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_5", 
            "text": "[SPOI-2725] - Create skeleton App  [SPOI-2728] - Convert the Skeleton Operators into Real Operators - Part I  [SPOI-2739] - File input operator  [SPOI-2740] - DAG level unit test  [SPOI-2776] - Test and Certify RPM packaging for Cloudera  [SPOI-2777] - Gateway throws a bunch of exceptions when trying to install using rpm", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#bug_19", 
            "text": "[MLHR-724] - RabbitMQ test timing issue  [MLHR-1128] - Breadcrumbs should say physical operator, not operator on port page  [MLHR-1129] - DAG should not display underneath controls  [MLHR-1137] - Memory gauge always shows 100%  [MLHR-1142] - Front End Server - Kafka Reconnect  [MLHR-1143] - KafkaOutputOperator is not configurable for Kafka Producer properties  [MLHR-1144] - Container Actions - use wording that's not confusing  [MLHR-1157] - Parent JIRA for Front End Server  [MLHR-1159] - Stop Recording function does not give visual indication of stopped recording unless refreshed  [MLHR-1169] - Update readme for console repo  [MLHR-1170] - Appmaster Logs are not shown in the Console  [MLHR-1185] - Stram Event widget should check physical plan for existence of operator  [MLHR-1186] - some numbers in logical dag hidden behind control  [MLHR-1189] - AbstractHdfsTupleFileOutputOperator output port is not optional  [MLHR-1192] - Parametrize the demos  [MLHR-1195] - App master container is not showing proper metrics  [MLHR-1202] - failureCount missing for physical operators", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_19", 
            "text": "[MLHR-1132] - Improvements to errors in the stram events widget  [MLHR-1146] - Only get active containers for initial container list  [MLHR-1167] - UI support for searching log levels of classes  [MLHR-1168] - Validate input for log level widget  [MLHR-1194] - Align app counts with labels in cluster overview widget  [MLHR-1209] - Support JMS providers other than ActiveMQ", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_16", 
            "text": "[MLHR-921] - Installer - List of Errors/Codes  [MLHR-1111] - Container Log Widget, using new REST call  [MLHR-1121] - Front End Server - Kafka Query JSON  [MLHR-1122] - Front End Server - Kafka Keep Alive  [MLHR-1123] - Front End Server - Kafka Parameterized Queries  [MLHR-1124] - Front End Server - Kafka Response Debug/Format  [MLHR-1125] - Front End Server - Kafka Pub/Sub Protocol for Widgets  [MLHR-1130] - Widget to change the log levels of classes dynamically  [MLHR-1133] - Put \"float\" control in widget config popup  [MLHR-1134] - Front End Server - Editable JSON request  [MLHR-1135] - Front End Server - Setup Instructions  [MLHR-1136] - Front End Server - Packaging/Distribution  [MLHR-1147] - Front End Server - Selecting Metrics  [MLHR-1149] - Front End Server - Use Latest Kafka Offset  [MLHR-1151] - Front End Server - Data Validation  [MLHR-1152] - Front End Server - Page Performance (Visibility API)  [MLHR-1175] - Widget Library - Select Widget  [MLHR-1179] - AngularJS Migration - Application Instance Page  [MLHR-1181] - AngularJS Migration - Application Overview Widget  [MLHR-1182] - Angular JS Migration - App Structure  [MLHR-1187] - UI for adding system alerts  [MLHR-1193] - Create Basic Counters and its aggregator in library", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#story_5", 
            "text": "[MLHR-1178] - AngularJS Migration for 1.0.2", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#task_16", 
            "text": "[MLHR-960] - Create a Tableau output adapter  [MLHR-1090] - Create first resource objects (models) for angular console  [MLHR-1106] - Improve ended app page  [MLHR-1139] - Add HTTP Get Operator  [MLHR-1140] - Add HDFS output operator that writes to file names specified in tuple  [MLHR-1160] - Create HDFS Output Operator to write with exactly once semantics  [MLHR-1162] - Implement app list with angular  [MLHR-1164] - Wireframe for UI DAG builder  [MLHR-1171] - Make twitter hashTab a link in twitter demo  [MLHR-1172] - Create a Cassandra read operator  [MLHR-1173] - Create a Cassandra write operator", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_6", 
            "text": "[MLHR-1163] - Wireframe for DAG builder", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#version-101", 
            "text": "", 
            "title": "Version 1.0.1"
        }, 
        {
            "location": "/release_notes/#bug_20", 
            "text": "[SPOI-1404] - Create seperate environment for node0 for demos  [SPOI-1952] - Operation and Install guide review  [SPOI-2203] - Configure adsDimesions and machine data to use minimum memory  [SPOI-2340] - License Agent is persisting state in a wrong location  [SPOI-2397] - Monitoring and fixing the bugs in Customer demos  [SPOI-2408] - Verify DT Platform on HDP 2.1  [SPOI-2410] - Master script to restart hadoop on the cluster  [SPOI-2411] - Not able to starte license with 0.9.5 release  [SPOI-2420] - Core doesn't compile with hadoop 2.4  [SPOI-2448] - Test xml ingestion demo on a cluster  [SPOI-2449] - Test golden gate demo on a cluster  [SPOI-2450] - Test twitter sentiment demo on a cluster  [SPOI-2474] - Containers not provisioned diagnostics error  [SPOI-2477] - LicenceAgent client fails with NPE  [SPOI-2489] - Operator downstream of EXACTLY_ONCE operator not checkpointing  [SPOI-2490] - HDFS   [SPOI-2491] - Testing the HDFS drain for Flume Sink  [SPOI-2493] - Add HDFS operator updates into Malhar  [SPOI-2496] - viewdag does not work any more  [SPOI-2503] - Provide a rewind feature to replay tuples from a previous time  [SPOI-2505] - Sandbox demos need customization  [SPOI-2512] - Apps fail to start due to serialization errors on CDH5  [SPOI-2533] - Fraud app needs debugging  [SPOI-2543] - Applications not reporting the license information to License Agent if LA is restarted  [SPOI-2550] - License Agent Dying  [SPOI-2554] - Third party libraries which are required for demos are missing from /opt/datatorrent/releases/version/lib/  [SPOI-2556] - Add demo guides to documentation index", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_20", 
            "text": "[SPOI-2210] - Default license generation should be part of build process  [SPOI-2468] - README for certifications and benchmarks  [SPOI-2482] - DTCli should handle ^C more gracefully  [SPOI-2494] - Add HTTP Get operator to Malhar  [SPOI-2555] - Include README, LICENSE, and CHANGELOG in docs", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_17", 
            "text": "[SPOI-406] - Document and test download to work with Hortonworks HDP 2.0  [SPOI-408] - Certify with MapR (Hadoop 2.3) release in 1.0.1  [SPOI-1146] - API for retrieving raw Container Logs  [SPOI-1834] - Pub-Sub/REST mechanisms for exceptions thrown by an application  [SPOI-2080] - Application to certify installation  [SPOI-2253] - Username/password feature in the installer  [SPOI-2381] - dt-site.xml logistics  [SPOI-2387] - Aggregate Logical Operators on Gateway  [SPOI-2402] - Update dt-site.xml with container memory less than 512MB  after getting the app-memory-automation results for all the apps.  [SPOI-2460] - dt-site.xml new logistics implementation  [SPOI-2470] - Create a certification property file for Distro/Install Certification  [SPOI-2486] - Workaround for \"FileSyste.getScheme is not overrided by MapRFileSystem\"  [SPOI-2519] - add grep feature when getting container log  [SPOI-2526] - Add REST call to return info of historical containers", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#task_17", 
            "text": "[SPOI-62] - Buffer Server logging  [SPOI-1206] - Review Licensing of Third Party Libraries  [SPOI-1745] - Document Demo Application Setup  [SPOI-1784] - License agent app to have webservice  [SPOI-2199] - Readme file for AWS installation  [SPOI-2223] - Test if all the applications with reduced container memory sizes run in sandbox 0.9.4.  [SPOI-2348] - Enable checkpoint for aggregator operator  [SPOI-2354] - Certify demos on 0.9.4 sandbox  [SPOI-2370] - Fully automate Hadoop cluster restart  [SPOI-2379] - Certify DT with Hadoop 2.3/CDH  [SPOI-2392] - Add more basic demos to sandbox launcher  [SPOI-2393] - Certify Amazon EMR Hadoop 2 version  [SPOI-2414] - Stop the nightly benchmarking jenkins jobs   [SPOI-2415] - remove the \"page has not loaded since 60 second\" message at top  [SPOI-2422] - Add HDFS cache limit on FlumeAgent operator  [SPOI-2424] - Log metrics across all poc operators to a file  [SPOI-2425] - Compare HDFS with customer site  [SPOI-2426] - Benchmark HDFS drain rate by agent operator  [SPOI-2427] - Enable dedupper to be run with checkpointing turned OFF  [SPOI-2428] - Evaluate bigger bucket size for checkpointing for deduper  [SPOI-2430] - Number validation for poc  [SPOI-2432] - Compute the checkpoint I/O for dedupper  [SPOI-2439] - Compute deduper HDFS I/O for current checkpointing scheme  [SPOI-2441] - Estimate the feed time for HDFS agent replay  [SPOI-2445] - Certify Cloudera CDH 5.0 GA  [SPOI-2451] - Gateway Load Test  [SPOI-2457] - Widget Library  [SPOI-2458] - Generate certification report for HDP 2.1  [SPOI-2466] - Monitoring and notifications for cluster  [SPOI-2475] - Document process to certify demo re-start  [SPOI-2476] - Document process for grid restart  [SPOI-2483] - Node.js Kafka Client POC  [SPOI-2513] - Create demos ui service wrapper compatible with CentOS  [SPOI-2517] - Configure customer demos ui for as a service  [SPOI-2531] - HDFSOutputOperator write with exactly once semantics  [SPOI-2535] - Add support for retrieving Aggregated Counters from Response processStats of StatsListener  [SPOI-2537] - Need to list previous versions on datatorrent site  [SPOI-2540] - Fix URL for AWS EMR document  [SPOI-2551] - Define a process on backend to ensure archive of previous downloads works for customers  [SPOI-2553] - Holding ticket for Malhar maintenance work", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_7", 
            "text": "[MLHR-774] - Terms of service license  [MLHR-976] - Implement table module for angular", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#bug_21", 
            "text": "[MLHR-998] - Compilation error while using UniqueValueCount operator.  [MLHR-1054] - Update topic and rest calls for Logical/Physical Split  [MLHR-1056] - Rename demo apps to include Demo suffix  [MLHR-1089] - Error handling for license page  [MLHR-1093] - AbstractKafkaInputOperator is not committing the txn to Kafka in commit call back  [MLHR-1096] - Get last N events when tailing stram events  [MLHR-1108] - Logical DAG resizing  [MLHR-1113] - AdsDimension demo fails to launch  [MLHR-1114] - FraudDetect demo fails to launch  [MLHR-1115] - MachineData demo fails to launch  [MLHR-1127] - HdfsTextFileInputOperator Fails during checkpointing  [MLHR-1138] - Remove unused CPU columns in logical operator table", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_21", 
            "text": "[MLHR-1092] - Add environment variable override to rename demo apps  [MLHR-1098] - Improved code coverage for malhar-angular-dashboard  [MLHR-1099] - Extend dataModelOptions with non-serializable defaults  [MLHR-1100] - Improved widget directive in malhar-angular-dashboard  [MLHR-1116] - Use negative offset to get last N events for stram events widget  [MLHR-1119] - Add a confirm box when killing app master container  [MLHR-1120] - Save range selection between page loads for stram events", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_18", 
            "text": "[MLHR-783] - DAG should adjust in height as the widget height changes  [MLHR-784] - WidgetOutputOperator - Schema Update  [MLHR-984] - STRAM decisions widget  [MLHR-1070] - Web Demos - Upgrade to AngularJS 1.2.16  [MLHR-1076] - Widget Library - WebSocket  [MLHR-1077] - Widget Library - Data Models  [MLHR-1087] - Dashboard Component - AngularJS Scope Documentation  [MLHR-1088] - Dashboard Web App - Dependencies Documentation  [MLHR-1095] - Node.js Kafka Integration  [MLHR-1097] - Implement storage on malhar-angular-dashboard  [MLHR-1118] - Add storage hash for clearing out invalid dashboards", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#story_6", 
            "text": "[MLHR-719] - Time Series Forecasting  [MLHR-1060] - Widget Library  [MLHR-1061] - Dashboard Component  [MLHR-1082] - Parent jira for Real Time ETL Application", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#task_18", 
            "text": "[MLHR-700] - Develop operator for calculating Coefficient of Determination (RSquare)  [MLHR-841] - Demo guide for machine data application  [MLHR-850] - Demo guide for Ads Dimension demo  [MLHR-852] - Demo guide for Twitter (rolling topwards) demo application  [MLHR-874] - Demo guide for charting demo - Yahoo finance  [MLHR-896] - widget output operator - widget type should not be decided in backend  [MLHR-950] - Create a demo for distributed distinct  [MLHR-953] - Set up \"upgrade path\" for CustomerApplications  [MLHR-983] - UI to expose the \"events\" API of the gateway  [MLHR-1042] - ETL: Consolidate output operators properties  [MLHR-1063] - Create contributing section on the main page of malhar repo  [MLHR-1064] - Create contributing secion on main page of UI repos  [MLHR-1065] - create line chart operator  [MLHR-1075] - add .travis.yml file to ui-console  [MLHR-1078] - Update license header script, keep year updated  [MLHR-1080] - create real time chart output operator  [MLHR-1083] - Consolidate realtime output operator properties  [MLHR-1084] - Consolidate aggregate operator properties  [MLHR-1085] - Consolidate linechart operator properties  [MLHR-1104] - Design App Bundle and Upload/Launch Feature", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#version-100", 
            "text": "", 
            "title": "Version 1.0.0"
        }, 
        {
            "location": "/release_notes/#bug_22", 
            "text": "[SPOI-2086] - creation of logs directory under .dt folder of user  [SPOI-2497] - Remove hard-coded \"hdfs\" scheme   [SPOI-2501] - dt.log not written under CDH5 GA  [SPOI-2577] - After installation, I cannot launch license agent  [SPOI-2604] - The example DFS directory should be changed from /home/... to /user/...  [SPOI-2607] - Install page should only list error (not warnings) issues  [SPOI-2609] - When yarn.scheduler.minimum-allocation-mb is set to 256, PiDemo fails with NPE", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_22", 
            "text": "[SPOI-2522] - Replace InputPort.getStreamCodec method with an attribute on InputPort called StreamCodec  [SPOI-2576] - Change local install from .datatorrent to datatorrent directory  [SPOI-2603] - installer complains about gateway has trouble starting but it actually runs OK  [SPOI-2637] - Improve the Getting Started Guide", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#task_19", 
            "text": "[SPOI-2520] - Change the name of the CustomStats to Counters  [SPOI-2578] - Check dir before mkdir call (MapR requirement)  [SPOI-2579] - Take out launch from gateway webservice spec document  [SPOI-2649] - Update software license", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#bug_23", 
            "text": "[MLHR-1101] - Install errors formatting  [MLHR-1103] - Change installer instructions for DFS directory path", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#version-0951", 
            "text": "", 
            "title": "Version 0.9.5.1"
        }, 
        {
            "location": "/release_notes/#bug_24", 
            "text": "[SPOI-2554] - Third party libraries which are required for demos are missing from /opt/datatorrent/releases/version/lib/  [MLHR-1113] - AdsDimension demo fails to launch  [MLHR-1114] - FraudDetect demo fails to launch  [MLHR-1115] - MachineData demo fails to launch", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#version-095", 
            "text": "", 
            "title": "Version 0.9.5"
        }, 
        {
            "location": "/release_notes/#sub-task_8", 
            "text": "[SPOI-1692] - Certify fault tolerance for multiple containers failure  [SPOI-1693] - Develop automated test scenario for StrAM failure to certify one of high availability aspects.   [SPOI-2008] - Setup virtual cluster for high availability certification  [SPOI-2009] - WordCount app to be used in certification of high availability  [SPOI-2127] - Error/Warning message not clear - nightly build installer (datatorrent-dist-0.9.4-SNAPSHOT.bin) if executed as non-root user.  [SPOI-2128] - Nightly build installer needs to check if port 9090 is available before attempting to run dtgateway service.  [SPOI-2129] - Error/Warning message not clear while updating the Hadoop configuration through Installer-Web-based UI  [SPOI-2247] - Gateway user authentication REST API spec  [SPOI-2248] - Implement user authentication and basic authorization in gateway  [SPOI-2255] - Password protect the web socket  [SPOI-2257] - Support HTTPS in gateway  [SPOI-2277] - Support HTTPS and auth in pubsub connection from stram to Gateway  [SPOI-2291] - Copy GW configurations to HDFS on every write", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#technical-task", 
            "text": "[SPOI-2005] - Verify and add Machine demo to app memory usage automation", 
            "title": "Technical task"
        }, 
        {
            "location": "/release_notes/#bug_25", 
            "text": "[SPOI-1801] - Mobile app has two logical input gen - Need to handle containers on servers with two diff clocks  [SPOI-1890] - Application behavior when resources are not available  [SPOI-1954] - Incorrect Processed Stats  [SPOI-1995] - Sandbox instructions are outdated  [SPOI-1998] - Killing of node manager on node running License AM  [SPOI-2067] - Move apps from contrib to demos  [SPOI-2152] - Stabillity of mobile demo  [SPOI-2194] - Gateway start script prints out repetitive message  [SPOI-2205] - Make generate license tool work with web tool to generate 1TB license in the new flow  [SPOI-2207] - Deduper showed high latency and crashed in staging environment.  [SPOI-2221] - Analyze FileSystem.get calls and if necessary replace them with FileSystem.newInstance  [SPOI-2278] - Relative hdfs path should be created under application directory  [SPOI-2283] - Do not make assumption about FSStorageAgent being the StorageAgent or the default configuration for it being the configuration the user set  [SPOI-2284] - User 'dtadmin' should be added to the group having access to hadoop services and hdfs  [SPOI-2285] - uninstall.sh does not uninstall the platform  [SPOI-2288] - NullPointerException in buffer server  [SPOI-2289] - FSStorageAgent store throws java.nio.channels.ClosedChannelException   [SPOI-2295] - Monitoring and fixing the bugs in Customer demos  [SPOI-2297] - Mobile Demo going to unstable state due to out of order tuple sequence error  [SPOI-2298] - Events returned from REST API has funky keys  [SPOI-2302] - Normalize WebSocket   REST format for Stram Events  [SPOI-2314] - Fix application names  [SPOI-2315] - node1 application launch-xxx macros not working after upgrade  [SPOI-2342] - DTCLI ignoring annotated Application Names  [SPOI-2344] - dtcli fails to load dt-env.sh  [SPOI-2349] - Update default license to 4 months  [SPOI-2357] - Unable to launch application using launch alias, if the application names shown in the dtcli list are picked from dt-site.xml.  [SPOI-2358] - Installer bin extrac does not preserve permissions  [SPOI-2359] - Gateway fails to restart due to missing JAVA_HOME  [SPOI-2361] - After global install local .dt directory may be missing when executing dtcli  [SPOI-2364] - Catastrophic Error- tuples out of sequence in Generic Node  [SPOI-2365] - Hadoop Shell throws exception for dt.attr.APPLICATION_PATH  [SPOI-2371] - DTCLI updating the dt-site.xml  [SPOI-2373] - Sandbox dt-site.xml is missing configurations related to Map reduce applications.  [SPOI-2374] - The apps in sandbox use 1GB as the container size and not 512MB as specified under misc/sandbox/conf/dt/dt-site.xml  [SPOI-2375] - configuration from dt-site.xml in user's directory is not picked by dtcli  [SPOI-2376] - Gateway classpath references invalid directory  [SPOI-2380] - get-physical-operator-properties wrong help text  [SPOI-2389] - DataTorrent discovers override dt-site.xml in CLASSPATH  [SPOI-2390] - DTGateway is killed after terminal is closed  [SPOI-2391] - dtdemos service does not stop on sandbox  [SPOI-2395] - Missing FraudDetection/AdsDeminsions/MachineData demos from sandbox.  [SPOI-2396] - gateway not started after stopping and starting hadoop/datatorrent services in sandbox.  [SPOI-2398] - Extra directory created by gateway in user home  [SPOI-2400] - An operator exits normally when there is OutOfMemory error   [SPOI-2401] - The AppMaster finishes after retrying 6 times to deploy input operator  [SPOI-2412] - App restart HDFS file permission issue on CDH cluster  [SPOI-2417] - DTGateway dt-site.xml backup location  [SPOI-2419] - Ensure principal name propagates to containers in non-secure mode  [SPOI-2435] - Latest changes to latency calculation break tests.", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_23", 
            "text": "[SPOI-1376] - Have app specific configuration to stram-site.xml in sandbox  [SPOI-1855] - Ability to continue/shutdown application  [SPOI-1986] - In Logical View do not include the unifier stats  [SPOI-2001] - Update Release Process document (GDrive)  [SPOI-2222] - Constraint format in license file to be in JSON  [SPOI-2287] - Allow specifying a complete list of dependencies while deploying the application  [SPOI-2335] - Make BaseOperator Java Serializable  [SPOI-2367] - Create service wrapper for demos server  [SPOI-2385] - Improve the performance of HDFSStorage for POC  [SPOI-2404] - Reset the Application Failure Count upon successful recovery  [SPOI-2405] - Add more params to /ws/v1/applications/{appid}/events call  [SPOI-2407] - Need ability to supply the class name representing the property", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_19", 
            "text": "[SPOI-91] - Design and implement a socket adapter operator  [SPOI-2007] - Automation to certify High Availability  [SPOI-2012] - Run the app-memory-usage with lower container sizes starting from 128M to 512M when the grid is restarted.  [SPOI-2249] - Simple user authentication / authorization support in the UI  [SPOI-2355] - Add REST call to make existing license file the current license  [SPOI-2386] - REST call to retrieve the datatorrent conf directory  [SPOI-2399] - Starter Web Application", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#story_7", 
            "text": "[SPOI-2215] - Password protect the dashboard", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#task_20", 
            "text": "[SPOI-746] - DT Phome home customer side work  [SPOI-1642] - Certify support for node failure  [SPOI-2045] - Discuss license cleanup/improvements  [SPOI-2099] - Show mandatory fields with '*' in the installer  [SPOI-2101] - Say \"Terms of Service\" on the text  [SPOI-2103] - Show error message on the side  [SPOI-2117] - An error during uninstall  [SPOI-2118] - 1 yr license mail not getting mailed  [SPOI-2145] - Set up separate access to jars for big customers   [SPOI-2178] - Filters HashMap from HDFSLoader  [SPOI-2190] - Create web page (navigator and table view) for 5 dimensions and 2 computations  [SPOI-2201] - Create a upgrade license page  [SPOI-2202] - Upgrade Machine Data to latest code  [SPOI-2211] - Getting too many deprecated warnings in demos  [SPOI-2213] - Be able to diagnose Stram decisions  [SPOI-2214] - Ability to access Stram decisions for failed/finished/killed app  [SPOI-2216] - Upgrade Twitter Demo to latest Code  [SPOI-2217] - Upgrade Fraud Detection Demo to latest Code  [SPOI-2218] - Upgrade Ads Dimension Demo to latest Code  [SPOI-2219] - Upgrade Mobile Locator Demo to latest Code  [SPOI-2224] - Fraud demo issues   [SPOI-2225] - Issues with Machine Data Demo  [SPOI-2226] - Issues with Ads demo  [SPOI-2233] - Remove max num container parameter that is included by default  [SPOI-2234] - Give warning when operators are automatically inlined  [SPOI-2237] - CPU column on logical view  [SPOI-2243] - Use hammerdb to generate load against Oracle  [SPOI-2256] - Document certification suite used to certify datatorrent platform  [SPOI-2286] - Implement Goldengate java handler to capture db change and send to kafka  [SPOI-2290] - Hide configuration and launch app for GA  [SPOI-2292] - Add to README: not recommended to launch more than one gateway  [SPOI-2293] - Manage license file on HDFS  [SPOI-2303] - Make DFS error keys more fine grained   [SPOI-2309] - Use connection to RM to guess the cluster accessible IP address for GATEWAY_CONNECT_ADDRESS  [SPOI-2310] - Allow different filesystem from fs.defaultFS for dt.dfsRootDirectory  [SPOI-2338] - The \"#\" in the boxes should not change  [SPOI-2351] - Bundle the default passwd file (with admin/admin) in the installer  [SPOI-2362] - Create service wrappers for Apache Hadoop on Sandbox  [SPOI-2363] - Update dtdemo sandbox script to work with Hadoop/DTGateway service wrappers  [SPOI-2394] - Install wizard shows up again after gateway restart  [SPOI-2416] - Separate malhar and malhar-ui-console  [SPOI-2418] - Create an ability for the DTFlumeSink to backoff if it suspects that the DAG is not processing the data in healthy fashion.  [SPOI-2421] - Update end-user documentation", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_9", 
            "text": "[MLHR-726] - Time Series Forecasting Operator using Holt's Linear Trend Model  [MLHR-933] - Time Series Forecasting using Holt-Winters' Seasonal Method  [MLHR-965] - create non transactional output operator  [MLHR-966] - create transactional output operator  [MLHR-967] - create a data store writer implementation to use in real time etl app  [MLHR-1036] - Create incremental model for Holt's Linear Trend Forecasting Algorithm  [MLHR-1040] - Develop incremental model creation for Holt-Winters' Multiplicative Method Time-Series Forecasting", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#bug_26", 
            "text": "[MLHR-778] - Logical Operators list has no palette  [MLHR-857] - Machinedata demo stop working with new Redis operator  [MLHR-963] - create output operator to add historical data to supplied datastore  [MLHR-964] - create interface for data stores that can be used by output operator to persist historical data  [MLHR-968] - Remove test related code from AbstractTransactionableStoreOutputOperator  [MLHR-969] - Create a manageable version of our demo docs to be included with our installation  [MLHR-978] - create resource leak for the hdfs input/output operators  [MLHR-979] - AdsDimension: Redis operator shows high latency  [MLHR-986] - Page Not Found errors from the web-site  [MLHR-1000] - Invalid IP address selected by default during upgrade  [MLHR-1005] - CPU column on logical view  [MLHR-1008] - twitter application failed with exception  [MLHR-1015] - State is missing from physical operator overview widget  [MLHR-1019] - Make gateway ip and port inputs on same line in installer  [MLHR-1020] - License screen does not reflect new license upload  [MLHR-1021] - Unifiers should not have link to nonexistent logical operator  [MLHR-1022] - Update installer license text to reflect correct trial period  [MLHR-1028] - Cannot kill application in state ACCEPTED  [MLHR-1030] - Sorting on logical operators list fails until you sort on name  [MLHR-1031] - UI Showing wrong total Memory usage  [MLHR-1049] - Streams not showing up in dag overview  [MLHR-1050] - Remove the \"Development\" mode  [MLHR-1055] - Unnecessary gateway restart requested during installation navigation  [MLHR-1057] - Overview in dag widget not accurate to visible area  [MLHR-1058] - Change install wizard from widget in a page to just a page  [MLHR-1066] - Install Wizard tab order and auto-focus  [MLHR-1067] - Install Wizard invalid DFS directory check  [MLHR-1072] - Socket input operator test fail  [MLHR-1074] - Remove stram events page/widget from 0.9.5", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_24", 
            "text": "[MLHR-721] - Migrate database cache lookup to the new db framework  [MLHR-985] - AdsDimension: Plug-in Dimension Unifier   [MLHR-1010] - Remove info icon from installer text  [MLHR-1011] - Change \"property\" to \"item\" in install step tables  [MLHR-1012] - Review and edit installer text  [MLHR-1013] - Installer: Rework Hadoop and System screens  [MLHR-1035] - Remove darker grey background from console  [MLHR-1046] - Operators/Containers not being subscribed to on App Page  [MLHR-1062] - Install Wizard examples of Hadoop and DFS path  [MLHR-1069] - License summary section in Install Wizard", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_20", 
            "text": "[MLHR-684] - Invalidate dashboard state from previous versions  [MLHR-881] - ETL Web App - Packaging  [MLHR-882] - ETL Web App - Dashboard  [MLHR-885] - ETL Web App - Sample Data Generation  [MLHR-997] - ETL Web App - Historical Data from MongoDB  [MLHR-1014] - Remove config properties edit page, replace with config home  [MLHR-1052] - Bar Chart Widget  [MLHR-1053] - Widget Library - Demo Application", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#story_8", 
            "text": "[MLHR-703] - Logstream UI  [MLHR-709] - Widgets Library as Independent Project", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#task_21", 
            "text": "[MLHR-661] - Migrate JDBC adapters to use the new database adapter interface  [MLHR-866] - Annotate all stateless operators in Malhar as such  [MLHR-868] - Demo Guide for Map-Reduce Operator - LogsCountApplication  [MLHR-869] - Demo guide for Map-Reduce Operator - NewWordCountApplication.  [MLHR-870] - Demo guide for Map-Reduce Operator - InvertedIndexApplication.  [MLHR-922] - Machine data demo is using deprecated attributes. Gives warnings on launch  [MLHR-951] - Remove deprecated warnings from demos  [MLHR-954] - Need to debug older demos  [MLHR-961] - Develop a Goldengate input adapter  [MLHR-970] - Re-create the application that we build for iAd Poc in Malhar  [MLHR-1006] - In Logical View do not include the unifier stats  [MLHR-1007] - Simple user authentication/authorization support in UI  [MLHR-1018] - Need a Kafka-HBase app  [MLHR-1023] - Update installer text on welcome screen  [MLHR-1024] - Create issues summary page under configuration  [MLHR-1025] - Add progress status indicator for installer wizard  [MLHR-1026] - Reorder install wizard screens  [MLHR-1027] - Remove external upgrade link from licensing screen  [MLHR-1041] - ETL : Consolidate input operator properties in json format  [MLHR-1044] - ETL: Add input operator to ETL Application in ETL branch  [MLHR-1071] - Create stateless deduper - aka deduper which forgets its state upon failure or repartitioning", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#version-094", 
            "text": "", 
            "title": "Version 0.9.4"
        }, 
        {
            "location": "/release_notes/#bug_27", 
            "text": "[SPOI-455] - Cleanup maven repository workaround in install.sh  [SPOI-1636] - Update all node1 demo to 0.9.3 release  [SPOI-1774] - Thread local performance drop from 65M to 40 M tuplesProcessed/sec  [SPOI-1775] - MROperator demo applications fail when launched from Sandbox  [SPOI-1847] - Intermittent - WebSocket Publishing ignores endWindow  [SPOI-1864] - POC partitioned operators not getting correct initial state  [SPOI-1866] - Datatorrent applications not starting on CDH5 Vagrant cluster  [SPOI-1867] - Investigate InstallAnywhere use for DataTorrent installation  [SPOI-1882] - When not able to contact license agent, the application should not die  [SPOI-1883] - Stram crashes with Unknown-Container issue  [SPOI-1885] - Determine minimum amount of memory needed to run twitter app  [SPOI-1886] - Determine minimum amount of memory needed to run mobile app  [SPOI-1888] - Install license in the new console configuration  [SPOI-1889] - Licensing needs to support eval and free  [SPOI-1902] - Dynamic MxN partitioning does not handle scale down to single M instance   [SPOI-1903] - MiniClusterTests fails because ~/.dt/dt-site.xml dependency  [SPOI-1906] - history does not get flushed to the history file until the next command prompt  [SPOI-1917] - Licensing error  [SPOI-1922] - Loading license does not work  [SPOI-1923] - Close button does not work in several pop-up windows  [SPOI-1936] - No uninstall script available with the installer.   [SPOI-1953] - Installation video on our website should be refreshed to reflect the latest version(s)  [SPOI-1960] - application incorrectly marked as succeeded  [SPOI-1968] - 404 for logical plan url  [SPOI-1970] - Negative requested container count logged by Stram  [SPOI-1976] - Fix AdsDimensions in certification   [SPOI-1992] - the latency in the application overview freezes after a container gets killed.  [SPOI-1999] - Evaluate Yarn cluster issue  [SPOI-2002] - Gateway fails to load properties  [SPOI-2037] - Redirect to welcome page on first run / install  [SPOI-2039] - DTGateway logs to .dt/logs in service mode  [SPOI-2040] - Apache logs application under contrib folder fails to run  [SPOI-2047] - dtgateway service not starting when machine boots  [SPOI-2049] - Better error message on invalid hadoop directory  [SPOI-2051] - RPM install reports install script failure  [SPOI-2052] - DTGateway restart fails when running with -service  [SPOI-2055] - dtgateway service reports OK on startup failure  [SPOI-2056] - LicensingAgentProtocolHelper.getLicensingAgentProtocol gets stuck when YARN is not running  [SPOI-2057] - DTCLI is not working after intallation  [SPOI-2058] - launch-demos macro not available after installing the platform from self extracting intaller  [SPOI-2059] - Show understandable error message if the root hdfs directory could't be created  [SPOI-2060] - UI Shows Nan B in Allocated Memory  [SPOI-2062] - Gateway needs to check hadoop version  [SPOI-2064] - dtgateway-ctl stop doesn't work  [SPOI-2065] - Readme File is not udpated  [SPOI-2066] - installer not recognizing -l option  [SPOI-2070] - Installer: echo_success command doesn't work with Ubuntu  [SPOI-2073] - Invalid Ip Address in Installer UI  [SPOI-2074] - install script needs to check hadoop version  [SPOI-2075] - lauch-local-demos  [SPOI-2076] - Licensing agent RPC gives NPE  [SPOI-2077] - Installer: we need a separate page for hadoop installation directory   [SPOI-2078] - Change the certification and benchmarking script and code to use the new location of benchmarking apps  [SPOI-2083] - install script, when run by rpm, complains about invalid group dtadmin  [SPOI-2088] - Map Reduce demo applications still show the classnames when listed using launch-demos  [SPOI-2089] - demo applications displayed after running the launch-demos command should be in alphabetical order  [SPOI-2090] - Error while requesting evaluation license from Datatorrent.com  [SPOI-2094] - Installer throws failed message while stopping gateway   [SPOI-2097] - launch-demos macro not available after installing the platform from self extracting intaller  [SPOI-2098] - App names still have full classpath  [SPOI-2120] - Installer - Restart Modal is not closed after Restart Failed (Happened Once)  [SPOI-2132] - Ensure HDFS does not blow up with millions of files per sec  [SPOI-2133] - Delete old files to ensure NN does not crash  [SPOI-2134] - Send POC1 to customer  [SPOI-2143] - Spelling error and reference to $HOME/.datatorrent  [SPOI-2146] - Move Kafka benchmark apps to contrib folder  [SPOI-2148] - Installer - Disable Closing Modals on Click  [SPOI-2149] - Address the confusion around gateway address.  [SPOI-2151] - User is not able to change the defaultFS during installation  [SPOI-2153] - Cryptic error message when launching app on node0  [SPOI-2157] - Getting logical plan returns error when one of the getters is bad  [SPOI-2159] - gateway is polling resourcemanager for appinfo w/o subscriber  [SPOI-2163] - Change directory before DTGateway launch  [SPOI-2165] - Installer - add reload ability to System screen  [SPOI-2170] - DTGateway classpath is duplicated after restart  [SPOI-2171] - Remove reload button from System configuration screen  [SPOI-2172] - Installer may display invalid port after starting DTGateway  [SPOI-2173] - Installer base location change not working  [SPOI-2192] - CLI command for getting list of operator classes from a jar  [SPOI-2193] - CLI command for getting properties of a operator class from a jar  [SPOI-2206] - dag view does not get rendered property in Firefox  [SPOI-2227] - (Re)start license agent when license file is uploaded  [SPOI-2229] - container local operators not redeployed  [SPOI-2238] - Installer complains about sudo running as root  [SPOI-2241] - DAG Firefox 28 Support", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_25", 
            "text": "[SPOI-1311] - Review platform documentation  [SPOI-1567] - Certify against commercial Hadoop distributions  [SPOI-1769] - Trying to kill a non dt app can return a better message  [SPOI-1844] - Ship project-template with a log4j.properties file with debugging level set to INFO  [SPOI-1852] - WebSocket client recovery logging  [SPOI-1853] - Create WebSocket clients in containers on demand  [SPOI-1854] - Option to retrieve only running and recently-ended apps  [SPOI-1856] - Need something that is like LicensingAgentClient but not specific for stram.  [SPOI-1857] - Gateway to warn about available licensed memory being low  [SPOI-1858] - CLI to directly connect to license agent to get live license info  [SPOI-1859] - Gateway to directly connect to license agent to get live license info  [SPOI-1861] - Gateway command to restart itself  [SPOI-1869] - Add UI build script to dist build file  [SPOI-1874] - the first operator that stalls for more than specific period, take it out so as to unclog the processing  [SPOI-1899] - Add appmaster container to container list  [SPOI-1904] - Updates needed to the README file  [SPOI-1911] - Run certification as part of nighty build  [SPOI-1915] - Using $DT_HOME in README   [SPOI-1961] - Show the activation date of the license with list-license-agent command   [SPOI-1965] - The file demos.jar should be installed by default by the Installer  [SPOI-1978] - Manual eval request (by e-mail) - template    [SPOI-1980] - DT Version in license request and generated license  [SPOI-1981] - Approve / update license verification e-mail  [SPOI-1997] - Certify against commercial Hadoop distributions  [SPOI-2026] - Add support to LogicalPlan for testing dag serialization  [SPOI-2150] - Update Readme file for the local install  [SPOI-2155] - Installer - Validate Fields on blur event  [SPOI-2156] - Installer - Navigation Code Cleanup  [SPOI-2158] - Installer - CSS Classes  [SPOI-2160] - Installer font size  [SPOI-2166] - Configuration screen navigation panel  [SPOI-2174] - Notify user with installation location and version  [SPOI-2175] - Notify user about local DTGateway management during installation  [SPOI-2188] - Installer - Register Instructions", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_21", 
            "text": "[SPOI-328] - Add annotation to declare the operator stateless  [SPOI-393] - High Availability for STRAM  [SPOI-868] - Setting operator properties as different types  [SPOI-1182] - Add Key-based filter functionality to malhar library for Min, Max, SimpleMovingAverage, StandardDeviation like operators  [SPOI-1654] - Logstream - aggregate top hits and bytes for URL, geo DMA, IP, URL/status code, url  [SPOI-1747] - create a filter operator to output multiple records based on filter  [SPOI-1756] - configuration for input adaptor  [SPOI-1757] - configuration for filter operator  [SPOI-1758] - configuration for dimension operator  [SPOI-1759] - configuration for aggreagation operator  [SPOI-1760] - configuration for web socket output  [SPOI-1849] - add dt HDFS directory in configuration   [SPOI-1907] - Installer: HDFS directory creation attempt via Gateway (as part of config updates)   [SPOI-1909] - Port re-selection by Gateway if 9090, 9091, 9092, etc are taken  [SPOI-1913] - Automate verifing the app memory for the demos  [SPOI-1928] - gateway needs to be able to start with standalone hadoop jar (without hadoop installation)  [SPOI-1929] - /ws/v1/about to include java version and hadoop location  [SPOI-1930] - New installation script  [SPOI-1966] - dtcli should be enabled to list app names (if available) as opposed to app class path  [SPOI-1974] - Add throughput,  totalTuplesProcessed and elapsed time to performance benchmarking  [SPOI-1975] - Display throughput, tuplesProcessed per sec and latency in a tabular format.  [SPOI-1987] - Copy License to Front-End Distribution  [SPOI-2003] - Verify and add all the demos except Machine data to app memory usage automation  [SPOI-2011] - Make a separate jar file for performance benchmarking demos  [SPOI-2013] - Support for doc link as an attribute for the application  [SPOI-2018] - Have a launch-performance command in dtcli  [SPOI-2021] - Rename all the apps under contrib to have meaningful names  [SPOI-2023] - Make a launch-contrib command available in stram cli  [SPOI-2027] - Packaging benchmarking demos  [SPOI-2042] - redirect user to welcome screen if dt.configStatus is not \"complete\"  [SPOI-2044] - set property dt.configStatus to \"complete\" when the user has completed the config wizard  [SPOI-2122] - Installer - Offline Email Template  [SPOI-2147] - Provide separate dt benchmarking package scripts to throughput and hdfs operators benchmarking  [SPOI-2195] - Gateway REST API to return operator classes in a jar given superclass (or not)  [SPOI-2196] - Gateway REST API to return properties of an operator in a jar   [SPOI-2200] - Installer - License Flow", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#story_9", 
            "text": "[SPOI-1608] - Platform Benchmarking (Platform1 and Platform2)", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#task_22", 
            "text": "[SPOI-722] - Document ads demo (add comments to codebase)  [SPOI-1403] - HDFS Operator Benchmark  [SPOI-1411] - Deprecate old Malhar webdemos once logstream is available  [SPOI-1513] - datatorrent.com webapp development - app testing  [SPOI-1610] - Develop benchmarking app for AdsDimension App (exactly once semantics) - Platform1  [SPOI-1612] - Benchmarking Ads Dimension demo app on Morado cluster (exactly once semantics) - Platform1  [SPOI-1618] - Benchmarking Machine Data app with Platform1  [SPOI-1694] - Document DT SandBox preparation  [SPOI-1730] - Default License in git needs to be replaced by the license cut by the real key   [SPOI-1732] - Create the real public/private key for licenses and store the private key in a safe place.  [SPOI-1745] - Document Demo Application Setup  [SPOI-1788] - CLI commands for licensing  [SPOI-1794] - Create license info as a string   [SPOI-1796] - Soft enforcement for normal paid app. 200% bump?  [SPOI-1848] - Gateway to support changing and getting config parameters  [SPOI-1851] - Document virtual cluster setup  [SPOI-1862] - Working on creating Wire Frames for the Installation of DT Platform  [SPOI-1863] - Make installer work w/o Maven  [SPOI-1865] - Allow user to configure application classpath  [SPOI-1868] - Support Book Keeping in the HDFSStorage  [SPOI-1870] - Validate dtcli generate-license-request  [SPOI-1872] - Modify generatelicense process  [SPOI-1873] - License process via console  [SPOI-1876] - Application Developer Guide Improvements  [SPOI-1877] - Download and build JDK Standrd Doclet Source as part of DT   [SPOI-1878] - DT Console Web UI Testing on Chrome  [SPOI-1879] - DT Console Web UI Testing for Demo Apps on Chrome  [SPOI-1884] - Operater developer guide review   [SPOI-1891] - Add allatori documentation  [SPOI-1892] - Add automatic build of front components to distribution  [SPOI-1893] - Quick Start Guide  [SPOI-1914] - cli get-app-info to include info from hadoop  [SPOI-1926] - Call license web service and return license file  [SPOI-1927] - Returns license request blob for UI to assemble mailto link  [SPOI-1939] - Twitter Top Counter Demo Applications Guide  [SPOI-1940] - RPM packaging for installer  [SPOI-1941] - Include demo UI into installer  [SPOI-1942] - Remove Allatori code expiration for GA  [SPOI-1943] - include more info in the license request   [SPOI-1945] - Add REST call to gateway to post license file  [SPOI-1946] - virtual cluster configuration changes  [SPOI-1947] - Create license@datatorrent.com  [SPOI-1949] - Java application (with main method) that returns information given a license request blob  [SPOI-1956] - License generation key pair expiration / private key protection  [SPOI-1963] - Evaluate Doclava Doclet from Google  [SPOI-1982] - E-mail verification success web page   [SPOI-1988] - Review Quick Start Guide  [SPOI-1989] - AdsDimension - Demo Applications Guide  [SPOI-1990] - Twitter Rolling Top Words Counter - Demo Applications Guide  [SPOI-2004] - Installer testing for GA  [SPOI-2006] - Grant Google Analytics Access To Following People  [SPOI-2010] - Configure the apps to use minimum memory as verified by app-memory-usage-automation.  [SPOI-2015] - Get Machine data into contrib.jar  [SPOI-2016] - Fraud Detection in contrib.jar  [SPOI-2017] - Quick Start Guide version 2  [SPOI-2019] - List NxN performance apps (different event size vs different stream locality  [SPOI-2025] - Getting Start Guide - Launch this copy.  [SPOI-2032] - Certify Cloudera CDH 5.0  [SPOI-2048] - Uninstall script  [SPOI-2050] - Start gateway as service flag  [SPOI-2069] - Test Installer  [SPOI-2091] - Update installation license agreement  [SPOI-2093] - Verify demo UI is bundled with installer  [SPOI-2100] - For terms of service box, change \"continue\" to \"accept and continue\"  [SPOI-2106] - Change the message on 1 yr registration  [SPOI-2107] - Change the message on 1 yr registration  [SPOI-2108] - Put timeglass and \"loading\" or spinning.... while Hadoop system properties are being loaded  [SPOI-2109] - Gateway down creates bad error message  [SPOI-2110] - Remove errors popping on the right hand of console  [SPOI-2111] - If gateway outage is discovered add a message to get them back  [SPOI-2112] - Change the message on Hadoop screen  [SPOI-2114] - Error if HDFS does not exist  [SPOI-2115] - Create a list of issues summary screen  [SPOI-2116] - Show more instructions on the completed screen  [SPOI-2131] - Ingestion POC  [SPOI-2136] - Do cartesian products for key, val pair  [SPOI-2142] - Allow customization of cartesian product   [SPOI-2161] - prereq message on welcome screen  [SPOI-2162] - DFS error message  [SPOI-2164] - DFS location validation  [SPOI-2167] - Evaluate errors in Cloudera certification  [SPOI-2223] - Test if all the applications with reduced container memory sizes run in sandbox 0.9.4.  [SPOI-2230] - Uninstaller for RPM  [SPOI-2231] - Provide Environment with Running Demos  [SPOI-2240] - Set Up DataTorrent Demos on Dev Environment", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_10", 
            "text": "[SPOI-1682] - Too many mbassy threads!!!  [SPOI-1718] - Update HA Documentation  [SPOI-1729] - Restore operator recovery checkpoints in AM recovery   [SPOI-1786] - Users should be able to generate license at datatorrent.com  [SPOI-1931] - Installer - determine OS type and version  [SPOI-1932] - installer with sudo/root user creation, service installs  [SPOI-1933] - service wrappers for DTGateway  [SPOI-1934] - HDFS directory creation during install  [SPOI-1935] - Search for hadoop binaries standard paths  [SPOI-1958] - Create HDFS Word input operator  [SPOI-1962] - Add test to jekins nightly build  [SPOI-1971] - Verify launch script for other apps from demo   [SPOI-1972] - When some app fail, the main monitor should still keep looking at the other apps  [SPOI-2028] - Provide shell script 'benchmark-throughput' to produce a single summary table   [SPOI-2030] - Provide a list of individual demos used for benchmarking through dtcli benchmarking, so that the user can launch the demos individually.  [SPOI-2031] - Package benchmarking suite into the installer and sandbox  [SPOI-2046] - Update licenses location in installation script  [SPOI-2053] - Certify CDH5.0 as part of Cloudera certification for inclusion in their process  [SPOI-2054] - Certify installer on HW  [SPOI-2079] - Run certification on bin install  [SPOI-2085] - Change the certification resource xml files to contain the certification type.", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#bug_28", 
            "text": "[MLHR-729] - Columns in table wrap in Firefox  [MLHR-730] - Dev server does not escape double-quotes in error message  [MLHR-732] - Links Outline in Firefox  [MLHR-754] - JarList and DepJarList headers point to nonexistent text items  [MLHR-767] - \"FINISHING\" has no icon in status  [MLHR-785] - NaN in file size column of jar list  [MLHR-809] - Inconsistent landing page across browsers  [MLHR-814] - Selecting operators in the logical operators widget does not activate any actions  [MLHR-816] - Names of partitions in the partitions widget should not be shown as links  [MLHR-817] - Clickin on 'outputPort' leads to PageNotFound error  [MLHR-824] - Don't show license agent detail when license agent is not running  [MLHR-826] - Dep Jars fail to load in specify dep jars modal  [MLHR-830] - Optimize RandomWordInput operator used in perfromance benhcmarking to use local final variables to improve performance.   [MLHR-835] - Fix hard-coded file path in com.datatorrent.demos.wordcount.WordCountInputOperator  [MLHR-836] - Need AbstractHDFSOutputAdapter  [MLHR-863] - Add license headers to AbstractHdfsOutputOperator  [MLHR-879] - Installer - Issues Management  [MLHR-887] - Input operator that tails a growing log file in a directory  [MLHR-899] - Give default name for all demo applications  [MLHR-908] - Mark installation complete with dt.configStatus property  [MLHR-911] - UI Shows Nan B in Allocated Memory  [MLHR-912] - Installer - System Section - Show Field Specific Errors  [MLHR-913] - Installer - System Section - Server-Side Error Messages  [MLHR-918] - Yahoo finance with Alerts : Modify to accept multiple ticker symbols and remove hard-coded values.  [MLHR-920] - Give desciptive names to benchmarking apps  [MLHR-924] - Tail Operator should take care of the truncation of file  [MLHR-930] - UI container list should not show time for last heartbeat for if the value is \"-1\"  [MLHR-938] - gateway address property change  [MLHR-939] - Put GatewayRestart into \"actions\" hash in settings.js  [MLHR-940] - Delete the space in the name  [MLHR-943] - Change the method name from isConnected to Connected in the db api  [MLHR-991] - DAG Stream Locality", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_26", 
            "text": "[MLHR-731] - Compile LESS on the fly in dev environment  [MLHR-735] - Remove unused bundling make commands, update README  [MLHR-745] - Add icon to left widget manager drawer  [MLHR-746] - Use \"cores\" not % cpu for cluster metric total cpu usage  [MLHR-747] - Remove \"#\" for \"number of container\", et al labels  [MLHR-748] - Only load all applications from RM on demand  [MLHR-749] - Improve notification history pane  [MLHR-750] - Normalize labels for everything in console  [MLHR-751] - Add lock where close dashboard icon would be for default dashboard  [MLHR-753] - Remove avg app age in cluster metrics  [MLHR-755] - Change \"max alloc mem\" to \"peak alloc mem\" in cluster metrics  [MLHR-756] - Add memory levels to tooltip of license mem gauge in top right  [MLHR-768] - Remove beefy from npm shrinkwrap  [MLHR-770] - Clean up BaseUtil, BaseModel, BaseCollection, add tests  [MLHR-781] - shorten link to log file in container page  [MLHR-798] - Add \"config.adsdimensions.redis.dbindex\" configuration in webdemo  [MLHR-873] - Add equals and hashcode to JdbcOperatorBase  [MLHR-931] - ETL: Create a converter api and provide an implementation for Json Object to flat map conversion  [MLHR-935] - Have \"silentErrors\" option for models and collections  [MLHR-937] - ETL: Create a unifier for DimensionComputation operator  [MLHR-947] - Improve overall look and feel of install wizard  [MLHR-948] - Remove mock issues from SummaryView in installer wizard  [MLHR-993] - Demo UI - Default Applications Names for Application Discovery", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_22", 
            "text": "[MLHR-688] - Discard Real-Time Updates When Page is not Active  [MLHR-757] - Dashboard - Save Widget Width to Local Storage  [MLHR-759] - Dashboard - Widget Definitions Collection  [MLHR-763] - Line Chart Widget - hAxis options  [MLHR-764] - Dashboard App - Meteor Data Source  [MLHR-765] - Shutdown Container Interface  [MLHR-773] - Configuration wizard page  [MLHR-788] - Web Demos - Redis and MongoDB Config  [MLHR-789] - Web Demos - Fraud Demo MongoDB Database Name  [MLHR-790] - Web Demos - Single Configuration File  [MLHR-791] - Web Demos - Start Script  [MLHR-792] - Web Demos - Single Page App  [MLHR-793] - Web Demos - JS/CSS Bundle  [MLHR-794] - Web Demos - WebSocket Pub/Sub  [MLHR-795] - Web Demos - Resources Clean Up on Scope Destroy  [MLHR-796] - Web Demos - Distribution Files  [MLHR-797] - Web Demos - Running Instructions  [MLHR-804] - Config Page (Manage Properties)  [MLHR-805] - Web Demos - Distribution Package Instructions  [MLHR-806] - Installer - License Requests (REST API Calls)  [MLHR-807] - Installer - License Text  [MLHR-818] - Installer - License Section  [MLHR-819] - Installer - System Properties Section  [MLHR-820] - Console - Node.js Dev Mode  [MLHR-821] - Web Demos - Distribution Package Launch Script  [MLHR-831] - Installer - License Flow  [MLHR-834] - Installer - License - Registration  [MLHR-858] - Installer - Gateway Restart  [MLHR-861] - Installer - System Properties - IP List  [MLHR-864] - Installer - Handling Hadoop Not Found  [MLHR-903] - Installer - WebSocket DataSource Disconnect  [MLHR-909] - Installer - Restart Confirmation  [MLHR-914] - Installer - Error Messages  [MLHR-919] - License Bar  [MLHR-923] - Installer - Properties Update \"Loading\" Indicator  [MLHR-925] - Installer Update  [MLHR-934] - Allow overrides to $.().modal(options) for Modals", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#story_10", 
            "text": "[MLHR-705] - Node.js Pub/Sub Service  [MLHR-708] - Evaluate Node.js Pub/Sub Services  [MLHR-710] - Pie Chart Data Model  [MLHR-712] - Dashboard App - Historical Data Support  [MLHR-714] - Dashboard App - MongoDB Integration  [MLHR-715] - Create Dashboard from Running App - Widgets Auto Discovery  [MLHR-736] - Console Firefox Support  [MLHR-737] - Console Safari Support  [MLHR-801] - Installer/Config UI  [MLHR-802] - Web Demos - Distribution Package  [MLHR-803] - Installer (Wizard) Page", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#task_23", 
            "text": "[MLHR-349] - Add build script to Malhar/front  [MLHR-402] - Logstream - aggregate top hits and bytes for URL, geo DMA, IP, URL/status code, url  [MLHR-651] - Use compatible version of jersey/jackson/jetty in Malhar  [MLHR-659] - Migrate MongoDB adapters to use the new database adapter interface  [MLHR-741] - Web Apps (Demos) Firefox Support  [MLHR-744] - Web Apps (Demos) Safari Support  [MLHR-760] - Dashboard App - Meteor Integration  [MLHR-761] - Dashboard App - Derby.js Integration  [MLHR-762] - create install script for ui  [MLHR-839] - Review guide on MachineData app  [MLHR-842] - Demo guide for Pi Application  [MLHR-843] - Demo guide for Twitter Top URL Counter demo application  [MLHR-851] - Demo guide for Fraud detection demo application  [MLHR-853] - Demo guide for Mobile demo application  [MLHR-854] - Demo Guide for Word-Count Application  [MLHR-855] - Demo guide for Pi Calculator application  [MLHR-856] - Demo guide for Twitter Rolling Top Words Application  [MLHR-859] - upgrade kryo to 2.23  [MLHR-871] - Demo guide for Twitter Top URL Counter - Launch This Copy.  [MLHR-872] - Demo Guide for Word-Count Application - Launch this copy.  [MLHR-875] - Demo guide for Yahoo finance application  [MLHR-876] - Demo guide for Yahoo finance alerting application  [MLHR-877] - Demo guide for Yahoo finance application with Derby SQL  [MLHR-892] - ETL logstream application - study the log stream application  [MLHR-893] - ETL- Use the generic dimension operator that was created for a POC in Log stream   [MLHR-900] - ETL- Operators used by logstream application need to be generic and moved to library  [MLHR-904] - Fix the nightly and trigger builds broken due to removal of api.codec and api.util  [MLHR-905] - Dedup: Make deduper and bucket manager part of malhar library  [MLHR-910] - Demo guide for Twitter Rolling Top Words Application - Launch This Copy  [MLHR-915] - CLONE - Demo guide for Pi Application - Launch this copy  [MLHR-916] - CLONE - Demo guide for Pi Calculator application - Launch this copy  [MLHR-917] - CLONE - Demo guide for Mobile demo application - Launch this copy  [MLHR-936] - Create new Redis Store using Lettuce redis client  [MLHR-949] - Add confirmation to DTGateway restart button in System Properties  [MLHR-962] - ETL : Create a sifter operator   [MLHR-980] - CLONE - Demo guide for Yahoo finance alerting application - launch this copy  [MLHR-981] - CLONE - Demo guide for Yahoo finance application - launch this copy  [MLHR-982] - CLONE - Demo guide for Yahoo finance application with Derby SQL - launch this copy", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_11", 
            "text": "[MLHR-678] - Time Series Forecasting with Simple Linear Regression  [MLHR-718] - Time Series Forecasting using Simple/Single Exponential Smoothing  [MLHR-726] - Time Series Forecasting Operator using Holt's Linear Trend Model  [MLHR-727] - Develop application for a telecom related use case for time series forecasting with Simple Linear Regression and CMA smoothing  [MLHR-932] - Create Centered Moving Average Smoothing Operator", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#version-093", 
            "text": "", 
            "title": "Version 0.9.3"
        }, 
        {
            "location": "/release_notes/#new-feature_23", 
            "text": "[SPOI-261] - Design a general purpose read from stream and write to cassandra  [SPOI-400] - Each streaming application should license check  [SPOI-1622] - Input operator - XML parser  [SPOI-1647] - LogStream UI   [SPOI-1770] - Gateway should expose list of available topics  [SPOI-1778] - Open readme on sandbox startup  [SPOI-1804] - Start license app on launch app if not running  [SPOI-1805] - command to show license file info in cli  [SPOI-1812] - Create REST call for specific license agent, given a license id  [SPOI-1823] - Gateway REST API - Get Running Applications List  [SPOI-1829] - Semantic URLs for Web Apps", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvement_27", 
            "text": "[SPOI-1202] - Provide a way to check whether an operator is partitioned  [SPOI-1783] - Add allocatedMB to main application list  [SPOI-1789] - Change frequency of heartbeats to license app  [SPOI-1795] - License file to clearly state hard enforcement or soft enforcement  [SPOI-1816] - support simple variable substitution in the cli  [SPOI-1835] - support gateway status command", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#story_11", 
            "text": "[SPOI-1320] - Support MQTT protocol  [SPOI-1542] - Input operator - Directory Scan", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#bug_29", 
            "text": "[SPOI-1696] - Make de-duper dynamically partitionable  [SPOI-1704] - Stram enforcement to lock physical plan changes when license memory limit is reached  [SPOI-1706] - Design a enforcement format for the license policy  [SPOI-1711] - Encryption/obfuscation of communication between stram and license agent  [SPOI-1779] - Update sandbox documentation terminology  [SPOI-1802] - Provide total license memory via stram web services  [SPOI-1807] - Unknown outage on Machine data demo  [SPOI-1809] - Suppress expected error message in dtgateway-ctl    [SPOI-1810] - sample-stram-site.xml generates warnings  [SPOI-1813] - Add SNAPSHOT repository to install pom  [SPOI-1815] - Make stram memory reporting to license manager asynchronous   [SPOI-1818] - Chance \"className\" in license file to id  [SPOI-1820] - dtcli script doesn't exit when maven command fails  [SPOI-1826] - Update documentation title  [SPOI-1827] - Install script errors  [SPOI-1831] - CLI warning about trouble with license manager  [SPOI-1833] - Use encrypted byte arrays for RPC wire protocol for licensing  [SPOI-1840] - Change default license memory limit to 25GB  [SPOI-1841] - Make stram memory enforcement tolerances property settings for the enforcer  [SPOI-1842] - Investigate the possibility of engine obfuscation jar not containing any references to license package path  [SPOI-1880] - GET nonexistent container returns 500 error", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#task_24", 
            "text": "[SPOI-1375] - All sandbox apps must work in 8G VM. Need to test each  [SPOI-1467] - DB lookup for Cassandra  [SPOI-1507] - datatorrent.com webapp development - pilot test of Angular and WP integration  [SPOI-1509] - datatorrent.com webapp development - db design  [SPOI-1511] - datatorrent.com webapp development - app design  [SPOI-1512] - datatorrent.com webapp development - app dev  [SPOI-1515] - datatorrent.com webapp deveopment - integrate standalone app with cms  [SPOI-1516] - datatorrent.com webapp development - add GA info during registration  [SPOI-1517] - datatorrent.com webapp development - background jobs  [SPOI-1617] - Benchmarking Performance app with Platform2  [SPOI-1641] - Benchmarking Ads Dimension App - Platform2  [SPOI-1715] - Show remainingLicensedMB and allocatedMB in UI for each application  [SPOI-1763] - Provide support for Accumulo NoSQL db  [SPOI-1780] - Sandbox - activate license automatically  [SPOI-1781] - Sandbox - increase memory to 8GB  [SPOI-1782] - License App should use much less memory (256MB or less?)  [SPOI-1787] - Add license instructions to README  [SPOI-1790] - Ensure update to license app on any resource change by StrAM  [SPOI-1791] - Hard enforcement for free license (6GB), and eval license  [SPOI-1793] - Hide sub-license and make license object behave as \u201cwhat is license data right now?\u201d  [SPOI-1798] - Date format change in license file  [SPOI-1799] - Change name \"Sublicense\" to \"Section\" or \"LicenseSection\"  [SPOI-1800] - Webservice specs for Gateway for license info  [SPOI-1825] - Update end user documentation  [SPOI-1832] - Support CDH default log4j setup", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_12", 
            "text": "[SPOI-1451] - Show critical path  [SPOI-1721] - Augment the Partitionable interface to inform of all the partitions which actually were deployed  [SPOI-1733] - Container heartbeat RPC failover", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#new-feature_24", 
            "text": "[MLHR-5] - UI component for license information  [MLHR-6] - Google Line Chart Widget  [MLHR-7] - Gauge Widget  [MLHR-8] - Top N Widget  [MLHR-9] - Compile Widget from HTML Template  [MLHR-10] - Make widgets resizable and renameable in ui-builder  [MLHR-11] - Dashboard Component Grunt Tasks  [MLHR-653] - Dynamically Connect Widgets to WebSocket Topics  [MLHR-655] - Create serializing mechanism for instantiated widgets and dashboard(s)  [MLHR-656] - Add/Compile Widgets from templateUrl  [MLHR-664] - Support MQTT protocol  [MLHR-668] - Set up widget configure dialog  [MLHR-669] - Visual Data Demo App  [MLHR-671] - Add allocatedMB column in main application list  [MLHR-673] - Dashboard App - Notification Service  [MLHR-674] - Explicit Saving/Loading of Dash configurations in ui builder  [MLHR-687] - Dashboard App - Filter WebSocket Topics  [MLHR-689] - Dashboard App - Widget Options Modal  [MLHR-690] - Dashboard App - Widgets Schema  [MLHR-691] - WebSocket Topics Debugger Widget  [MLHR-692] - JSON Widget  [MLHR-693] - Progressbar Widget  [MLHR-695] - Pie Chart Widget  [MLHR-696] - Dashboard App - Development/Production Scripts  [MLHR-697] - Dashboard App - Node.js Configuration  [MLHR-698] - Dashboard App - WebSocket/REST API Configuration", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#improvement_28", 
            "text": "[MLHR-686] - Deglobalize the visibly component", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#bug_30", 
            "text": "[MLHR-4] - Use new livechart module for OpChart Widget  [MLHR-13] - Console status column display  [MLHR-648] - Update Issues section in README files   [MLHR-650] - Changing metrics on logical dag fails  [MLHR-654] - Some widgets' height changes when changing width  [MLHR-667] - Add UI version to console  [MLHR-670] - Memory leak in console  [MLHR-677] - Widgets Data Models  [MLHR-680] - Update license information dialog for new REST call info  [MLHR-725] - WindowId Formatter  [MLHR-739] - Stream Locality Toggle fails for DAG view", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#story_12", 
            "text": "[MLHR-1] - Reusable Dashboard Component with AngularJS  [MLHR-2] - Dashboard Widgets  [MLHR-3] - Dashboard App", 
            "title": "Story"
        }, 
        {
            "location": "/release_notes/#task_25", 
            "text": "[MLHR-321] - Directory Scan operator  [MLHR-452] - Create a De-duplication operator  [MLHR-603] - Supports upload of dependency jars  [MLHR-638] - Test streaming application for dynamic partition  [MLHR-645] - More fields in drop down in logicalDAG widget  [MLHR-646] - Document issue tracking location in README  [MLHR-652] - Parallel Simple Linear Regression  [MLHR-657] - Migrate memcache adapters to use the new database adapter interface  [MLHR-662] - Migrate Redis adapters to use the new database adapter interface  [MLHR-663] - Design new DB adapters interface  [MLHR-666] - DB lookup for Cassandra", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#version-092", 
            "text": "", 
            "title": "Version 0.9.2"
        }, 
        {
            "location": "/release_notes/#bug_31", 
            "text": "[SPOI-1327] - AtLeastOnceTest.testInlineOperatorsRecovery intermittent failure  [SPOI-1342] - DTCli should check license and relay the information with each application launched  [SPOI-1383] - Last window id and recovery window id do not update on 0.9  [SPOI-1439] - Gateway should be secured  [SPOI-1445] - Add version detection for Gateway  [SPOI-1456] - Free Memory in container widget changes too rapidly  [SPOI-1540] - Specification of license handlers and enforcers in the license file.  [SPOI-1632] - jar upload fails  [SPOI-1634] - Uptime at 1Billion events/s (Machine data)  [SPOI-1635] - Update node1 with latest machine data demo  [SPOI-1676] - Incremental Obfuscation of dt-flume directory  [SPOI-1677] - Supports uploading of dependency jars  [SPOI-1678] - When loading jars, make sure they are in their separate space so they don't conflict with gateway, cli and other jars  [SPOI-1679] - When uploading jar and when dependencies are not met, allow the upload with a message about dependencies  [SPOI-1680] - gateway throws errors when retrieving web service info from stram  [SPOI-1687] - Support launching jar and showing logical plan from HDFS   [SPOI-1688] - Map Reduce Monitor Does Not Publish WebSocket Data  [SPOI-1697] - Update demo configuration on node2  [SPOI-1703] - Update auto provisioning with DataTorrent 0.9.1 and GCE GA  [SPOI-1707] - License agent should handle license expiry  [SPOI-1708] - Stram should store license expiry  [SPOI-1709] - Show license object information in gateway  [SPOI-1710] - License cutting utility  [SPOI-1712] - Gateway to gracefully handle stram being a newer version than itself  [SPOI-1714] - Dynamic partition stop working if you start from only 1  partition  [SPOI-1727] - ApplicationInfoAutoPublisher unit test error  [SPOI-1728] - StramEvent exception prevents package name obfuscation  [SPOI-1739] - recordingStartTime of operator stats is showing -1 from time to time  [SPOI-1743] - Tuple recording on port is not showing up in web services  [SPOI-1744] - Recording says ended even if the recording is still going on", 
            "title": "Bug"
        }, 
        {
            "location": "/release_notes/#improvement_29", 
            "text": "[SPOI-1098] - event recorder logging improvements  [SPOI-1370] - Make the partition logic available to the end-users  [SPOI-1448] - DAG Visualization - Stream Types  [SPOI-1603] - BufferServerStatsCollection - dont check against bufferserverpublisher and subscriber  [SPOI-1613] - Update the User Interface guide to reflect latest version (0.9.1)", 
            "title": "Improvement"
        }, 
        {
            "location": "/release_notes/#new-feature_25", 
            "text": "[SPOI-165] - Parent jira for authentication  [SPOI-170] - Stream should authenticate before allowing an operator to connect  [SPOI-258] - Develop Flume Sink and corresponding DT input adapter  [SPOI-327] - Parent jira for Security  [SPOI-401] - Licensing alert mechanisms  [SPOI-411] - Ability to modify (add, upgrade, downgrade) license while the app is running  [SPOI-436] - Provide Web Service for obtaining license information (Usage limits, etc)  [SPOI-729] - Include license data in DT phone home  [SPOI-872] - Logical View of Running Application  [SPOI-975] - Support DataLocal Functionality  [SPOI-1406] - Add log file path and/or URL to each container info map  [SPOI-1621] - Input operator - CDR parser using CSV   [SPOI-1699] - Add locality (and maybe id?) to physical streams in REST calls", 
            "title": "New Feature"
        }, 
        {
            "location": "/release_notes/#task_26", 
            "text": "[SPOI-1689] - Map Reduce Monitor Web App  [SPOI-739] - Hadoop 2.2 certification  [SPOI-763] - Competition study  [SPOI-1140] - Annotate dag visualization with stream throughout and other data  [SPOI-1246] - Support versioning for Gateway to STRAM communications   [SPOI-1253] - Create DataTorrent Application which provides licensing server functionality  [SPOI-1389] - ContainerList view should show the log file name (stderr, stdout) in the info widget  [SPOI-1405] - Design macros for node0 and node1  [SPOI-1609] - Competitive analysis - DT (Platform1 and Platform2)  [SPOI-1611] - Benchmarking Ads Dimension on Morado cluster (at-least-once semantics) - Platform1  [SPOI-1616] - Benchmarking Performance app with Platform1  [SPOI-1670] - Ensure that Dedup operator is fault tolerant  [SPOI-1673] - use public/private key encryption for dt phone home  [SPOI-1675] - Map Reduce Jobs  [SPOI-1686] - Launch separate process when loading classes from application jars  [SPOI-1722] - Create a utility to create default license  [SPOI-1724] - Create a command line utility to generate customer license  [SPOI-1736] - CLI warning on License Violation  [SPOI-1742] - Update end-user documentation", 
            "title": "Task"
        }, 
        {
            "location": "/release_notes/#sub-task_13", 
            "text": "[SPOI-919] - Certify secure mode with Hadoop 2.2.0  [SPOI-966] - Create a licensing agent application  [SPOI-1413] - Flume sink part   [SPOI-1414] - DT Input Adapter for Flume  [SPOI-1475] - Augment Kafka operator to dynamically adapt to load and broker/partition changes  [SPOI-1538] - Develop Ads Dimension on Morado cluster (at-least-once semantics) Platform1  [SPOI-1713] - Secure communication between gateway and stram  [SPOI-1720] - Ensure that the Partionable interface and StatsListener interface callbacks are made from the same thread  [SPOI-1723] - The default license generation should be integrated with build  [SPOI-1731] - Sync execution layer deployment state after recovery", 
            "title": "Sub-task"
        }, 
        {
            "location": "/release_notes/#github-datatorrentmalhar", 
            "text": "[616] - fix #615 Update Web Apps Instructions  [615] - Update Web Apps Instructions  [614] - corrected typo  [613] - Fixes #599. Upload and specify dependency jars  [612] - fixes #597  [611] - fixes #610  [610] - Telecom tests failing  [609] - Github 597  [608] - #fix 607 Machine Data Demo Day Format  [607] - Machine Data Demo Day Format  [606] - fixes #457 added xml parse operator to parse and pick values from xml nodes and att...  [605] - added the history for hadoop 1.x  [603] - Map Reduce Monitor - Elapsed Time  [602] - Map Reduce Monitor - Elapsed Time  [601] - Map Reduce Monitor - Bootstrap JS, Server Errors Notification, Header Alignment  [599] - Provide UI for uploading and specifying dependency jars  [598] - Map Reduce Monitor - Server Errors Notification  [597] - Map Reduce Monitor App - CPU/Memory History  [595] - Map Reduce Monitor (History Charts, Animations, Readme, AngularJS Upgrade)  [594] - Map Reduce Monitor - Production Files (dist)  [593] - Map Reduce Monitor - AngularUI Bootstrap Progressbar issue with ngAnimate  [592] - Map Reduce Monitor Update (Readme, App List, History Charts)  [591] - Map Reduce Monitor - Map/Reduce History Charts  [590] - Fixes #401, adds zoom to physical DAG  [589] - Map Reduce Monitor - App List Columns  [586] - Map Reduce Monitor - App Id AngularJS Filter  [585] - Fixes #569, Cosmetic changes  [584] - Map Reduce Monitor - Show Active Job First  [581] - Map Reduce Monitor Update (Loading Indicator, Animations, Delayed Promise)  [580] - Map Reduce Monitor - AngularJS Animations  [579] - Map Reduce Monitor - Upgrade to AngularJS 1.2.6  [578] - Map Reduce Monitor - App List Loading Indicator  [577] - fixes #553, fixes #575  [576] - Map Reduce Monitor - AngularJS Delayed $q Promise  [575] - Map Reduce Monitor App - Send Job Stats immediately on Subscribe Request  [573] - Map Reduce Monitor Update (App List Grid, Progress Bars, Job Removal)  [572] - fixes #570  [571] - Map Reduce Monitor - Remove Job on WebSocket Message  [570] - clipPath issue when multiple charts on the same page  [569] - Various cosmetic updates for console  [568] - fixes #542, tooltip no longer obstructed by graph lines  [567] - Bird's Eye View for Physical DAG view  [566] - fixes #544, windowIds can now handle initial value of -1 or 0  [565] - Map Reduce Monitor - Job Selection  [564] - fixes #357, added logical operator page  [563] - Map Reduce Monitor - App List ng-grid  [561] - Map Reduce Monitor - App List Table Filter  [560] - Fixed exception with KryoSerializableStreamCodec #559  [558] - CDR simulator #524  [557] - Github 525  [556] - set the name of the io threads created by ning asynchttpclient  [554] - Squashed commit of the following:  [553] - Map Reduce Monitor App - Store Map/Reduce Progress History  [551] - fixes #550  [550] - Map Reduce Monitor App - App Should Broadcast Special Message on Unsubscription  [549] - Map Reduce Monitor - Stop Updates after Job Unsubscribe  [548] - Map Reduce Monitor - Found Job Notification  [547] - Fix Github #545  [544] - Console does not handle initial windowId  [542] - Tooltip for line graphs show up behind graphs after turning series on and off  [541] - fixes #535  [539] - Map Reduce Monitor - Merge Progress Bars with Progress Table  [538] - Map Reduce Monitor - Combine Map/Reduce Counters  [536] - Map Reduce Monitor - App List Running Jobs Progress Bar  [535] - Console breaks when switching to other page  [534] - Fixes #510, Unsubscribe Logical Operators when not used by any widget  [533] - Fixes #521, Refactored WindowId usage  [532] - Map Reduce Monitor - Counters  [530] - Map Reduce Monitor - App List Sort  [526] - Map Reduce Monitor - Counters  [525] - CDR processing DAG prototype  [523] - Github 512  [521] - Normalize all WindowId objects by overriding \"set\" method of appropriate models  [519] - Map Reduce Monitor - Header Alignment on Resize  [518] - Map Reduce Web App  [517] - Map Reduce Monitor - License Headers  [516] - Map Reduce Monitor - Map Reduce Jobs List  [515] - Map Reduce Monitor - AngularJS Modules Definition  [514] - Map Reduce Monitor - Readme (Deployment and Running Instructions)  [513] - Map Reduce Monitor - Job Query Loading Indicator  [512] - Support Normalization Operator  [511] - Map Reduce Monitor - AngularJS Settings Provider  [510] - Unsubscribe logicalOperators on InstancePage when not in use by widget  [509] - Fixes #505. Also removes one more instance of free memory metric for containers  [507] - Map Reduce Monitor - Single Config (Server and Client)  [505] - Add processed and emitted metrics to container overview widget  [504] - fixes #356, container log url now available in container info widget  [503] - Map Reduce Monitor - Active Job Highlight  [502] - Map Reduce Monitor - AngularJS Parent Scope Event Propagation  [501] - Fixes #364, removed free memory from container metrics  [500] - Map Reduce Monitor - Mock Server  [499] - Adding support for R. Basic operations - min, max and std deviation support added. Also adding support to run R scripts.  [498] - Rsupport pull  [496] - Map Reduce Monitor - Progress Line Chart  [495] - Map Reduce Monitor - Running MAPREDUCE Applications Discovery  [494] - fixes #420, can now set explicit height for widgets  [493] - fix #488 added delay before reconnection  [492] - CPU/RAM Metrics for Map Reduce Jobs  [491] - fix #488 added delay before reconnection  [489] - Map Reduce Monitor - Job Controller  [488] - WebSocketOutputOperator should wait a specified number of seconds before reconnection  [487] - Using uniform naming convention for applications. Fixed incorrect application names. Fixes #486.  [486] - Application names are not uniform  [485] - CPU/RAM Metrics for Map Reduce Jobs (Map Reduce Monitor App)  [484] - Map Reduce Monitor - AngularJS UI-Router Nested Views  [483] - Enhance the AbstractSlidingWindow #480, Add a SortedSlidingWindow operator #423  [482] - fixes #411. bundling on server.js, monkeypatching fs to avoid EMFILE  [479] - fix #443 reconnection when the connection is dropped  [478] - fix #443 Handles reconnection when the connection is dropped  [477] - Improvements to LogicalDagWidget. Fixes #399, #473, #475, #476  [476] - Logical DAG Widget: Limit scroll scale extent  [475] - Logical DAG Widget: add ability to reset initial dag view  [474] - Map Reduce Monitor - AngularJS UI-Router  [473] - Logical DAG Widget: only zoom when alt/option is held down  [472] - Map Reduce Monitor App - App Does not Publish Completed Maps  [471] - Map Redice Monitor - Reduce Progress Grid  [470] - Map Redice Monitor - Map Progress Grid  [469] - Map Redice Monitor - AngularJS Percentage Filter  [468] - Map Reduce Monitor - Monitored Jobs Grid  [467] - Add a general CSV parser operator to parse string, byte[] input to Map #451  [466] - Map Redice Monitor - AngularJS Util Service  [465] - Map Reduce Monitor - Unsubscribe Action  [464] - Map Reduce Monitor - Client-Side Settings  [463] - Map Reduce Monitor - WebSocket Unsubscribe  [461] - Map Reduce Monitor -  Multiple Jobs Monitoring  [460] - Map Reduce Monitor - Progress Bar Animation  [459] - Map Reduce Monitor - Upgrade to AngularJS 1.2.4  [458] - added xml parser operator and its test, fixes #457  [456] - Github 444  [454] - Map Reduce Monitor - AngularUI Bootstrap  [453] - Map Reduce Monitor - Production Build with Grunt  [452] - Map Reduce Monitor - jshint  [451] - CSV input operator (CDR processing)  [450] - Map Reduce Monitor - Progress Bars   [449] - Map Reduce Monitor App - WebSocket Query  [448] - Map Reduce Monitor - Error Notifications with pnotify   [444] - Map Redice Monitor App - Publish Map/Reduce Updates as Array  [443] - Map Reduce Monitor App WebSocket Issue  [442] - Map Reduce Monitor - Node.js Proxy for Hadoop ResourceManager  [441] - Map Reduce Monitor - REST Service  [439] - Map Reduce Monitor - Server Configuration  [438] - Map Reduce Monitor - Settings  [436] - Map Reduce Monitor - Job Progress Grid  [435] - Map Reduce Monitor - WebSocket Service with AngularJS provider  [434] - Map Reduce Monitor - Unit Tests  [433] - Map Reduce Monitor - AngularJS Directives (widgets)  [432] - Map Reduce Monitor - Page Layout with Bootstrap  [431] - Map Reduce Monitor - Node.js Server  [430] - Map Reduce Monitor - Yeoman Generated App  [428] - Normalization operator (CDR processing)  [427] - Filter operator (CDR Processing)  [426] - Enrichment operator (CDR processing)  [425] - Aggregator operator (CDR processing)  [422] - Github 421  [421] - Create RedisOperator taking String,String for performance  [420] - Allow widgets to have adjustable height  [419] - DAG Styling, DAG Firefox Issue  [418] - Logical DAG - Firefox Bottom Margin Issue  [417] - Logical DAG Styling  [416] - Fix jquery build error  [415] - Organized scripts and server  [414] - fix #408, fix #413 Logical DAG - Show Stream Locality on Demand  [413] - Logical DAG - Right Aligned Legned and Show Locality Link  [412] - Improve Front Dev Environment  [411] - Improved dev environment for front  [410] - Map Reduce Monitor Web App  [409] - fix #393 Front Node.js Proxy  [408] - Logical DAG - Show Stream Locality on Demand  [407] - fixes #373  [401] - Physical DAG - Smart Zoom  [399] - Physical DAG - Bird's-Eye View  [393] - Front Node.js Proxy  [375] - fixes #367, improves reload time during dev on front  [374] - fixes #367, improves reload time during dev on front  [372] - Logical DAG Legend Styling  [371] - Logical DAG Legend  [370] - Fix issue316 issue317 pull  [369] - Logical DAG - Legend  [368] - Squashed commit of the following:  [367] - Precompile templates for better dev process  [366] - Documenting demos pull  [365] - Normalized all \"processed\" and \"emitted\" labels  [364] - Remove free memory from container metrics in UI  [362] - Dependency to dagre-d3 fork  [361] - Logical DAG - Stream Locality  [360] - Update physical operators collection to fetch from physical plan  [359] - Add source and sinks to physical operator list   [358] - Normalize processed/s emitted/s labels across data tables and dag view  [357] - Create Logical Operator Page  [356] - ContainerList view should show the log file name (stderr, stdout) in the info widget  [355] - fixes #349, recently-launched app does not request operator list  [354] - Make partitionable kafka input operator adjust partitions ifself for kafka partition change(del/add)  [353] - Upgrade kafka to 0.8 release  [352] - fixes #322  [351] - Non-partitioned operators  [350] - Key/Value lookup Storage Manager changes  [349] - Application launch error in console  [348] - Fixes #339, switches cluster metrics to websocket topic  [347] - fix #346 Physical DAG - Remove Container IDs  [346] - Physical DAG - Remove Container IDs  [345] - Added sensible default display for avg app age field in cluster metrics widget #341  [344] - Added build cmd to travis script, fixes #343  [343] - Build step for front not in travis script  [342] - Fix for #328  [341] - Cluster overview display items (initial launch)  [339] - Cluster stats should come from WebSocket topic  [337] - Add Write failed: Broken pipe", 
            "title": "GitHub - DataTorrent/Malhar"
        }, 
        {
            "location": "/glossary/", 
            "text": "Glossary of Terms\n\n\nApache Apex\n\n\n\n\nApache Hadoop\n\u00a0- \u00a0\nApache Hadoop\n is a programming framework that supports the processing of large data sets in a distributed computing environment.\n\n\nApplication\n\u00a0- unified batch and real-time stream processing application running on Apache Apex platform.\n\n\nContainer\n\u00a0- A physical resource with CPU and memory constraints allocated by YARN\u2019s Resource Manager.\n\n\nCurrent Window Id\n\u00a0- Sequentially increasing identifier for a specific computation time period within Apache Apex platform.\n\n\nDAG\n\u00a0- Directed Acyclic Graph, formed by a collection of vertices and directed edges without cycles.  Sometimes interchangeably used to describe Apache Apex applications, specifically referring to their \nLogical\n / \nPhysical\n plans, composed of operators connected by streams.\n\n\nData Tuples Emitted\n\u00a0- Number of data objects emitted by the operators with an output port.\n\n\nData Tuples Processed\n\u00a0- Number of data objects processed by the operators in an Apache Apex application.\n\n\nLogical Plan\n\u00a0- Logical representation of an Apache Apex application, where the computational nodes are called \nOperators\n and the data-flow edges are called\u00a0\nStreams\n.\n\n\nOperator\n\u00a0- An entity that holds a computational logic to process the data tuples. It is part of a real-time stream processing application. The Operator computational logic gets executed inside a YARN Container.\n\n\nPhysical Operator\n - Physical representation of the operator, which contains information such as the name of container and the Hadoop node where operator instance is running.\n\n\nPhysical Plan\n\u00a0- Physical representation of the Logical Plan of the application and is a blueprint of how the application will run inside YARN containers deployed across nodes in a Hadoop cluster.\n\n\nPort\n\u00a0- Each operator can have ports on which it can receive input data tuples and also output processed data tuples.\n\n\nRecovery Window Id\n\u00a0- Identifier for the last computational window at which the operator state was check-pointed into HFDS.\n\n\nResource Manager\n\u00a0- YARN component that allocates and arbitrates the resources such as CPU, Memory and Network.\n\n\nSTRAM\n - Streaming Application Manager is the first process that is activated upon application launch and orchestrates the deployment, management, and monitoring of the Apache Apex applications throughout their lifecycle.\n\n\nStream\n\u00a0- A stream consists of data tuples that flow from one port of an operator to another.\n\n\nYARN\n\u00a0- \nApache Hadoop YARN\n (Yet Another Resource Negotiator) is a cluster resource management technology, introduced with Hadoop 2.0.\n\n\n\n\nDataTorrent RTS\n\n\n\n\ndtAssemble\n\u00a0- graphical application assembly tool used to develop applications.\n\n\ndtDashboard\n\u00a0- graphical visualization tool to view and query system and application data.\n\n\ndtGateway\n\u00a0- HTTP server used by dtManage to interact with STRAM, YARN Resource Manager, and HDFS\n\n\ndtManage\n\u00a0- the web based interface to install, configure, manage \n monitor Apache Apex applications running in a Hadoop Cluster", 
            "title": "Glossary"
        }, 
        {
            "location": "/glossary/#glossary-of-terms", 
            "text": "", 
            "title": "Glossary of Terms"
        }, 
        {
            "location": "/glossary/#apache-apex", 
            "text": "Apache Hadoop \u00a0- \u00a0 Apache Hadoop  is a programming framework that supports the processing of large data sets in a distributed computing environment.  Application \u00a0- unified batch and real-time stream processing application running on Apache Apex platform.  Container \u00a0- A physical resource with CPU and memory constraints allocated by YARN\u2019s Resource Manager.  Current Window Id \u00a0- Sequentially increasing identifier for a specific computation time period within Apache Apex platform.  DAG \u00a0- Directed Acyclic Graph, formed by a collection of vertices and directed edges without cycles.  Sometimes interchangeably used to describe Apache Apex applications, specifically referring to their  Logical  /  Physical  plans, composed of operators connected by streams.  Data Tuples Emitted \u00a0- Number of data objects emitted by the operators with an output port.  Data Tuples Processed \u00a0- Number of data objects processed by the operators in an Apache Apex application.  Logical Plan \u00a0- Logical representation of an Apache Apex application, where the computational nodes are called  Operators  and the data-flow edges are called\u00a0 Streams .  Operator \u00a0- An entity that holds a computational logic to process the data tuples. It is part of a real-time stream processing application. The Operator computational logic gets executed inside a YARN Container.  Physical Operator  - Physical representation of the operator, which contains information such as the name of container and the Hadoop node where operator instance is running.  Physical Plan \u00a0- Physical representation of the Logical Plan of the application and is a blueprint of how the application will run inside YARN containers deployed across nodes in a Hadoop cluster.  Port \u00a0- Each operator can have ports on which it can receive input data tuples and also output processed data tuples.  Recovery Window Id \u00a0- Identifier for the last computational window at which the operator state was check-pointed into HFDS.  Resource Manager \u00a0- YARN component that allocates and arbitrates the resources such as CPU, Memory and Network.  STRAM  - Streaming Application Manager is the first process that is activated upon application launch and orchestrates the deployment, management, and monitoring of the Apache Apex applications throughout their lifecycle.  Stream \u00a0- A stream consists of data tuples that flow from one port of an operator to another.  YARN \u00a0-  Apache Hadoop YARN  (Yet Another Resource Negotiator) is a cluster resource management technology, introduced with Hadoop 2.0.", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/glossary/#datatorrent-rts", 
            "text": "dtAssemble \u00a0- graphical application assembly tool used to develop applications.  dtDashboard \u00a0- graphical visualization tool to view and query system and application data.  dtGateway \u00a0- HTTP server used by dtManage to interact with STRAM, YARN Resource Manager, and HDFS  dtManage \u00a0- the web based interface to install, configure, manage   monitor Apache Apex applications running in a Hadoop Cluster", 
            "title": "DataTorrent RTS"
        }, 
        {
            "location": "/additional_docs/", 
            "text": "Apache Apex\n\n\nFor more information, please go to \nApache Apex\n.  You can find information on how to subscribe to mailing list, contribute, or attend meetup in your area.\n\n\nDataTorrent RTS\n\n\n\n\nApex Comparison\n\n\nArchitecture\n\n\nFeatures Overview\n\n\nBlogs\n\n\nFeatured Resources\n\n\nWebinars\n\n\nRelease Notes\n\n\n\n\nDocumentation Archive\n\n\n\n\nDataTorrent RTS 3.1.0\n\n\nDataTorrent RTS 3.0.0\n\n\nDataTorrent RTS 2.0.1\n\n\nDataTorrent RTS 2.0.0\n\n\nDataTorrent RTS 1.0.4\n\n\nWebsite Documentation Links", 
            "title": "Resources"
        }, 
        {
            "location": "/additional_docs/#apache-apex", 
            "text": "For more information, please go to  Apache Apex .  You can find information on how to subscribe to mailing list, contribute, or attend meetup in your area.", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/additional_docs/#datatorrent-rts", 
            "text": "Apex Comparison  Architecture  Features Overview  Blogs  Featured Resources  Webinars  Release Notes", 
            "title": "DataTorrent RTS"
        }, 
        {
            "location": "/additional_docs/#documentation-archive", 
            "text": "DataTorrent RTS 3.1.0  DataTorrent RTS 3.0.0  DataTorrent RTS 2.0.1  DataTorrent RTS 2.0.0  DataTorrent RTS 1.0.4  Website Documentation Links", 
            "title": "Documentation Archive"
        }
    ]
}